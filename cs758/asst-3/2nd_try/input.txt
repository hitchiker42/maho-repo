
	Applying Patches To The Linux Kernel
	------------------------------------

	Original by: Jesper Juhl, August 2005
	Last update: 2006-01-05


A frequently asked question on the Linux Kernel Mailing List is how to apply
a patch to the kernel or, more specifically, what base kernel a patch for
one of the many trees/branches should be applied to. Hopefully this document
will explain this to you.

In addition to explaining how to apply and revert patches, a brief
description of the different kernel trees (and examples of how to apply
their specific patches) is also provided.


What is a patch?
---
 A patch is a small text document containing a delta of changes between two
different versions of a source tree. Patches are created with the `diff'
program.
To correctly apply a patch you need to know what base it was generated from
and what new version the patch will change the source tree into. These
should both be present in the patch file metadata or be possible to deduce
from the filename.


How do I apply or revert a patch?
---
 You apply a patch with the `patch' program. The patch program reads a diff
(or patch) file and makes the changes to the source tree described in it.

Patches for the Linux kernel are generated relative to the parent directory
holding the kernel source dir.

This means that paths to files inside the patch file contain the name of the
kernel source directories it was generated against (or some other directory
names like "a/" and "b/").
Since this is unlikely to match the name of the kernel source dir on your
local machine (but is often useful info to see what version an otherwise
unlabeled patch was generated against) you should change into your kernel
source directory and then strip the first element of the path from filenames
in the patch file when applying it (the -p1 argument to `patch' does this).

To revert a previously applied patch, use the -R argument to patch.
So, if you applied a patch like this:
	patch -p1 < ../patch-x.y.z

You can revert (undo) it like this:
	patch -R -p1 < ../patch-x.y.z


How do I feed a patch/diff file to `patch'?
---
 This (as usual with Linux and other UNIX like operating systems) can be
done in several different ways.
In all the examples below I feed the file (in uncompressed form) to patch
via stdin using the following syntax:
	patch -p1 < path/to/patch-x.y.z

If you just want to be able to follow the examples below and don't want to
know of more than one way to use patch, then you can stop reading this
section here.

Patch can also get the name of the file to use via the -i argument, like
this:
	patch -p1 -i path/to/patch-x.y.z

If your patch file is compressed with gzip or bzip2 and you don't want to
uncompress it before applying it, then you can feed it to patch like this
instead:
	zcat path/to/patch-x.y.z.gz | patch -p1
	bzcat path/to/patch-x.y.z.bz2 | patch -p1

If you wish to uncompress the patch file by hand first before applying it
(what I assume you've done in the examples below), then you simply run
gunzip or bunzip2 on the file -- like this:
	gunzip patch-x.y.z.gz
	bunzip2 patch-x.y.z.bz2

Which will leave you with a plain text patch-x.y.z file that you can feed to
patch via stdin or the -i argument, as you prefer.

A few other nice arguments for patch are -s which causes patch to be silent
except for errors which is nice to prevent errors from scrolling out of the
screen too fast, and --dry-run which causes patch to just print a listing of
what would happen, but doesn't actually make any changes. Finally --verbose
tells patch to print more information about the work being done.


Common errors when patching
---
 When patch applies a patch file it attempts to verify the sanity of the
file in different ways.
Checking that the file looks like a valid patch file & checking the code
around the bits being modified matches the context provided in the patch are
just two of the basic sanity checks patch does.

If patch encounters something that doesn't look quite right it has two
options. It can either refuse to apply the changes and abort or it can try
to find a way to make the patch apply with a few minor changes.

One example of something that's not 'quite right' that patch will attempt to
fix up is if all the context matches, the lines being changed match, but the
line numbers are different. This can happen, for example, if the patch makes
a change in the middle of the file but for some reasons a few lines have
been added or removed near the beginning of the file. In that case
everything looks good it has just moved up or down a bit, and patch will
usually adjust the line numbers and apply the patch.

Whenever patch applies a patch that it had to modify a bit to make it fit
it'll tell you about it by saying the patch applied with 'fuzz'.
You should be wary of such changes since even though patch probably got it
right it doesn't /always/ get it right, and the result will sometimes be
wrong.

When patch encounters a change that it can't fix up with fuzz it rejects it
outright and leaves a file with a .rej extension (a reject file). You can
read this file to see exactly what change couldn't be applied, so you can
go fix it up by hand if you wish.

If you don't have any third-party patches applied to your kernel source, but
only patches from kernel.org and you apply the patches in the correct order,
and have made no modifications yourself to the source files, then you should
never see a fuzz or reject message from patch. If you do see such messages
anyway, then there's a high risk that either your local source tree or the
patch file is corrupted in some way. In that case you should probably try
re-downloading the patch and if things are still not OK then you'd be advised
to start with a fresh tree downloaded in full from kernel.org.

Let's look a bit more at some of the messages patch can produce.

If patch stops and presents a "File to patch:" prompt, then patch could not
find a file to be patched. Most likely you forgot to specify -p1 or you are
in the wrong directory. Less often, you'll find patches that need to be
applied with -p0 instead of -p1 (reading the patch file should reveal if
this is the case -- if so, then this is an error by the person who created
the patch but is not fatal).

If you get "Hunk #2 succeeded at 1887 with fuzz 2 (offset 7 lines)." or a
message similar to that, then it means that patch had to adjust the location
of the change (in this example it needed to move 7 lines from where it
expected to make the change to make it fit).
The resulting file may or may not be OK, depending on the reason the file
was different than expected.
This often happens if you try to apply a patch that was generated against a
different kernel version than the one you are trying to patch.

If you get a message like "Hunk #3 FAILED at 2387.", then it means that the
patch could not be applied correctly and the patch program was unable to
fuzz its way through. This will generate a .rej file with the change that
caused the patch to fail and also a .orig file showing you the original
content that couldn't be changed.

If you get "Reversed (or previously applied) patch detected!  Assume -R? [n]"
then patch detected that the change contained in the patch seems to have
already been made.
If you actually did apply this patch previously and you just re-applied it
in error, then just say [n]o and abort this patch. If you applied this patch
previously and actually intended to revert it, but forgot to specify -R,
then you can say [y]es here to make patch revert it for you.
This can also happen if the creator of the patch reversed the source and
destination directories when creating the patch, and in that case reverting
the patch will in fact apply it.

A message similar to "patch: **** unexpected end of file in patch" or "patch
unexpectedly ends in middle of line" means that patch could make no sense of
the file you fed to it. Either your download is broken, you tried to feed
patch a compressed patch file without uncompressing it first, or the patch
file that you are using has been mangled by a mail client or mail transfer
agent along the way somewhere, e.g., by splitting a long line into two lines.
Often these warnings can easily be fixed by joining (concatenating) the
two lines that had been split.

As I already mentioned above, these errors should never happen if you apply
a patch from kernel.org to the correct version of an unmodified source tree.
So if you get these errors with kernel.org patches then you should probably
assume that either your patch file or your tree is broken and I'd advise you
to start over with a fresh download of a full kernel tree and the patch you
wish to apply.


Are there any alternatives to `patch'?
---
 Yes there are alternatives.

 You can use the `interdiff' program (http://cyberelk.net/tim/patchutils/) to
generate a patch representing the differences between two patches and then
apply the result.
This will let you move from something like 2.6.12.2 to 2.6.12.3 in a single
step. The -z flag to interdiff will even let you feed it patches in gzip or
bzip2 compressed form directly without the use of zcat or bzcat or manual
decompression.

Here's how you'd go from 2.6.12.2 to 2.6.12.3 in a single step:
	interdiff -z ../patch-2.6.12.2.bz2 ../patch-2.6.12.3.gz | patch -p1

Although interdiff may save you a step or two you are generally advised to
do the additional steps since interdiff can get things wrong in some cases.

 Another alternative is `ketchup', which is a python script for automatic
downloading and applying of patches (http://www.selenic.com/ketchup/).

 Other nice tools are diffstat, which shows a summary of changes made by a
patch; lsdiff, which displays a short listing of affected files in a patch
file, along with (optionally) the line numbers of the start of each patch;
and grepdiff, which displays a list of the files modified by a patch where
the patch contains a given regular expression.


Where can I download the patches?
---
 The patches are available at http://kernel.org/
Most recent patches are linked from the front page, but they also have
specific homes.

The 2.6.x.y (-stable) and 2.6.x patches live at
 ftp://ftp.kernel.org/pub/linux/kernel/v2.6/

The -rc patches live at
 ftp://ftp.kernel.org/pub/linux/kernel/v2.6/testing/

The -git patches live at
 ftp://ftp.kernel.org/pub/linux/kernel/v2.6/snapshots/

The -mm kernels live at
 ftp://ftp.kernel.org/pub/linux/kernel/people/akpm/patches/2.6/

In place of ftp.kernel.org you can use ftp.cc.kernel.org, where cc is a
country code. This way you'll be downloading from a mirror site that's most
likely geographically closer to you, resulting in faster downloads for you,
less bandwidth used globally and less load on the main kernel.org servers --
these are good things, so do use mirrors when possible.


The 2.6.x kernels
---
 These are the base stable releases released by Linus. The highest numbered
release is the most recent.

If regressions or other serious flaws are found, then a -stable fix patch
will be released (see below) on top of this base. Once a new 2.6.x base
kernel is released, a patch is made available that is a delta between the
previous 2.6.x kernel and the new one.

To apply a patch moving from 2.6.11 to 2.6.12, you'd do the following (note
that such patches do *NOT* apply on top of 2.6.x.y kernels but on top of the
base 2.6.x kernel -- if you need to move from 2.6.x.y to 2.6.x+1 you need to
first revert the 2.6.x.y patch).

Here are some examples:

# moving from 2.6.11 to 2.6.12
$ cd ~/linux-2.6.11			# change to kernel source dir
$ patch -p1 < ../patch-2.6.12		# apply the 2.6.12 patch
$ cd ..
$ mv linux-2.6.11 linux-2.6.12		# rename source dir

# moving from 2.6.11.1 to 2.6.12
$ cd ~/linux-2.6.11.1			# change to kernel source dir
$ patch -p1 -R < ../patch-2.6.11.1	# revert the 2.6.11.1 patch
					# source dir is now 2.6.11
$ patch -p1 < ../patch-2.6.12		# apply new 2.6.12 patch
$ cd ..
$ mv linux-2.6.11.1 linux-2.6.12		# rename source dir


The 2.6.x.y kernels
---
 Kernels with 4-digit versions are -stable kernels. They contain small(ish)
critical fixes for security problems or significant regressions discovered
in a given 2.6.x kernel.

This is the recommended branch for users who want the most recent stable
kernel and are not interested in helping test development/experimental
versions.

If no 2.6.x.y kernel is available, then the highest numbered 2.6.x kernel is
the current stable kernel.

 note: the -stable team usually do make incremental patches available as well
 as patches against the latest mainline release, but I only cover the
 non-incremental ones below. The incremental ones can be found at
 ftp://ftp.kernel.org/pub/linux/kernel/v2.6/incr/

These patches are not incremental, meaning that for example the 2.6.12.3
patch does not apply on top of the 2.6.12.2 kernel source, but rather on top
of the base 2.6.12 kernel source .
So, in order to apply the 2.6.12.3 patch to your existing 2.6.12.2 kernel
source you have to first back out the 2.6.12.2 patch (so you are left with a
base 2.6.12 kernel source) and then apply the new 2.6.12.3 patch.

Here's a small example:

$ cd ~/linux-2.6.12.2			# change into the kernel source dir
$ patch -p1 -R < ../patch-2.6.12.2	# revert the 2.6.12.2 patch
$ patch -p1 < ../patch-2.6.12.3		# apply the new 2.6.12.3 patch
$ cd ..
$ mv linux-2.6.12.2 linux-2.6.12.3	# rename the kernel source dir


The -rc kernels
---
 These are release-candidate kernels. These are development kernels released
by Linus whenever he deems the current git (the kernel's source management
tool) tree to be in a reasonably sane state adequate for testing.

These kernels are not stable and you should expect occasional breakage if
you intend to run them. This is however the most stable of the main
development branches and is also what will eventually turn into the next
stable kernel, so it is important that it be tested by as many people as
possible.

This is a good branch to run for people who want to help out testing
development kernels but do not want to run some of the really experimental
stuff (such people should see the sections about -git and -mm kernels below).

The -rc patches are not incremental, they apply to a base 2.6.x kernel, just
like the 2.6.x.y patches described above. The kernel version before the -rcN
suffix denotes the version of the kernel that this -rc kernel will eventually
turn into.
So, 2.6.13-rc5 means that this is the fifth release candidate for the 2.6.13
kernel and the patch should be applied on top of the 2.6.12 kernel source.

Here are 3 examples of how to apply these patches:

# first an example of moving from 2.6.12 to 2.6.13-rc3
$ cd ~/linux-2.6.12			# change into the 2.6.12 source dir
$ patch -p1 < ../patch-2.6.13-rc3	# apply the 2.6.13-rc3 patch
$ cd ..
$ mv linux-2.6.12 linux-2.6.13-rc3	# rename the source dir

# now let's move from 2.6.13-rc3 to 2.6.13-rc5
$ cd ~/linux-2.6.13-rc3			# change into the 2.6.13-rc3 dir
$ patch -p1 -R < ../patch-2.6.13-rc3	# revert the 2.6.13-rc3 patch
$ patch -p1 < ../patch-2.6.13-rc5	# apply the new 2.6.13-rc5 patch
$ cd ..
$ mv linux-2.6.13-rc3 linux-2.6.13-rc5	# rename the source dir

# finally let's try and move from 2.6.12.3 to 2.6.13-rc5
$ cd ~/linux-2.6.12.3			# change to the kernel source dir
$ patch -p1 -R < ../patch-2.6.12.3	# revert the 2.6.12.3 patch
$ patch -p1 < ../patch-2.6.13-rc5	# apply new 2.6.13-rc5 patch
$ cd ..
$ mv linux-2.6.12.3 linux-2.6.13-rc5	# rename the kernel source dir


The -git kernels
---
 These are daily snapshots of Linus' kernel tree (managed in a git
repository, hence the name).

These patches are usually released daily and represent the current state of
Linus's tree. They are more experimental than -rc kernels since they are
generated automatically without even a cursory glance to see if they are
sane.

-git patches are not incremental and apply either to a base 2.6.x kernel or
a base 2.6.x-rc kernel -- you can see which from their name.
A patch named 2.6.12-git1 applies to the 2.6.12 kernel source and a patch
named 2.6.13-rc3-git2 applies to the source of the 2.6.13-rc3 kernel.

Here are some examples of how to apply these patches:

# moving from 2.6.12 to 2.6.12-git1
$ cd ~/linux-2.6.12			# change to the kernel source dir
$ patch -p1 < ../patch-2.6.12-git1	# apply the 2.6.12-git1 patch
$ cd ..
$ mv linux-2.6.12 linux-2.6.12-git1	# rename the kernel source dir

# moving from 2.6.12-git1 to 2.6.13-rc2-git3
$ cd ~/linux-2.6.12-git1		# change to the kernel source dir
$ patch -p1 -R < ../patch-2.6.12-git1	# revert the 2.6.12-git1 patch
					# we now have a 2.6.12 kernel
$ patch -p1 < ../patch-2.6.13-rc2	# apply the 2.6.13-rc2 patch
					# the kernel is now 2.6.13-rc2
$ patch -p1 < ../patch-2.6.13-rc2-git3	# apply the 2.6.13-rc2-git3 patch
					# the kernel is now 2.6.13-rc2-git3
$ cd ..
$ mv linux-2.6.12-git1 linux-2.6.13-rc2-git3	# rename source dir


The -mm kernels
---
 These are experimental kernels released by Andrew Morton.

The -mm tree serves as a sort of proving ground for new features and other
experimental patches.
Once a patch has proved its worth in -mm for a while Andrew pushes it on to
Linus for inclusion in mainline.

Although it's encouraged that patches flow to Linus via the -mm tree, this
is not always enforced.
Subsystem maintainers (or individuals) sometimes push their patches directly
to Linus, even though (or after) they have been merged and tested in -mm (or
sometimes even without prior testing in -mm).

You should generally strive to get your patches into mainline via -mm to
ensure maximum testing.

This branch is in constant flux and contains many experimental features, a
lot of debugging patches not appropriate for mainline etc., and is the most
experimental of the branches described in this document.

These kernels are not appropriate for use on systems that are supposed to be
stable and they are more risky to run than any of the other branches (make
sure you have up-to-date backups -- that goes for any experimental kernel but
even more so for -mm kernels).

These kernels in addition to all the other experimental patches they contain
usually also contain any changes in the mainline -git kernels available at
the time of release.

Testing of -mm kernels is greatly appreciated since the whole point of the
tree is to weed out regressions, crashes, data corruption bugs, build
breakage (and any other bug in general) before changes are merged into the
more stable mainline Linus tree.
But testers of -mm should be aware that breakage in this tree is more common
than in any other tree.

The -mm kernels are not released on a fixed schedule, but usually a few -mm
kernels are released in between each -rc kernel (1 to 3 is common).
The -mm kernels apply to either a base 2.6.x kernel (when no -rc kernels
have been released yet) or to a Linus -rc kernel.

Here are some examples of applying the -mm patches:

# moving from 2.6.12 to 2.6.12-mm1
$ cd ~/linux-2.6.12			# change to the 2.6.12 source dir
$ patch -p1 < ../2.6.12-mm1		# apply the 2.6.12-mm1 patch
$ cd ..
$ mv linux-2.6.12 linux-2.6.12-mm1	# rename the source appropriately

# moving from 2.6.12-mm1 to 2.6.13-rc3-mm3
$ cd ~/linux-2.6.12-mm1
$ patch -p1 -R < ../2.6.12-mm1		# revert the 2.6.12-mm1 patch
					# we now have a 2.6.12 source
$ patch -p1 < ../patch-2.6.13-rc3	# apply the 2.6.13-rc3 patch
					# we now have a 2.6.13-rc3 source
$ patch -p1 < ../2.6.13-rc3-mm3		# apply the 2.6.13-rc3-mm3 patch
$ cd ..
$ mv linux-2.6.12-mm1 linux-2.6.13-rc3-mm3	# rename the source dir


This concludes this list of explanations of the various kernel trees.
I hope you are now clear on how to apply the various patches and help testing
the kernel.

Thank you's to Randy Dunlap, Rolf Eike Beer, Linus Torvalds, Bodo Eggert,
Johannes Stezenbach, Grant Coady, Pavel Machek and others that I may have
forgotten for their reviews and contributions to this document.

		   ========================================
		   GENERIC ASSOCIATIVE ARRAY IMPLEMENTATION
		   ========================================

Contents:

 - Overview.

 - The public API.
   - Edit script.
   - Operations table.
   - Manipulation functions.
   - Access functions.
   - Index key form.

 - Internal workings.
   - Basic internal tree layout.
   - Shortcuts.
   - Splitting and collapsing nodes.
   - Non-recursive iteration.
   - Simultaneous alteration and iteration.


========
OVERVIEW
========

This associative array implementation is an object container with the following
properties:

 (1) Objects are opaque pointers.  The implementation does not care where they
     point (if anywhere) or what they point to (if anything).

     [!] NOTE: Pointers to objects _must_ be zero in the least significant bit.

 (2) Objects do not need to contain linkage blocks for use by the array.  This
     permits an object to be located in multiple arrays simultaneously.
     Rather, the array is made up of metadata blocks that point to objects.

 (3) Objects require index keys to locate them within the array.

 (4) Index keys must be unique.  Inserting an object with the same key as one
     already in the array will replace the old object.

 (5) Index keys can be of any length and can be of different lengths.

 (6) Index keys should encode the length early on, before any variation due to
     length is seen.

 (7) Index keys can include a hash to scatter objects throughout the array.

 (8) The array can iterated over.  The objects will not necessarily come out in
     key order.

 (9) The array can be iterated over whilst it is being modified, provided the
     RCU readlock is being held by the iterator.  Note, however, under these
     circumstances, some objects may be seen more than once.  If this is a
     problem, the iterator should lock against modification.  Objects will not
     be missed, however, unless deleted.

(10) Objects in the array can be looked up by means of their index key.

(11) Objects can be looked up whilst the array is being modified, provided the
     RCU readlock is being held by the thread doing the look up.

The implementation uses a tree of 16-pointer nodes internally that are indexed
on each level by nibbles from the index key in the same manner as in a radix
tree.  To improve memory efficiency, shortcuts can be emplaced to skip over
what would otherwise be a series of single-occupancy nodes.  Further, nodes
pack leaf object pointers into spare space in the node rather than making an
extra branch until as such time an object needs to be added to a full node.


==============
THE PUBLIC API
==============

The public API can be found in <linux/assoc_array.h>.  The associative array is
rooted on the following structure:

	struct assoc_array {
		...
	};

The code is selected by enabling CONFIG_ASSOCIATIVE_ARRAY.


EDIT SCRIPT
-----------

The insertion and deletion functions produce an 'edit script' that can later be
applied to effect the changes without risking ENOMEM.  This retains the
preallocated metadata blocks that will be installed in the internal tree and
keeps track of the metadata blocks that will be removed from the tree when the
script is applied.

This is also used to keep track of dead blocks and dead objects after the
script has been applied so that they can be freed later.  The freeing is done
after an RCU grace period has passed - thus allowing access functions to
proceed under the RCU read lock.

The script appears as outside of the API as a pointer of the type:

	struct assoc_array_edit;

There are two functions for dealing with the script:

 (1) Apply an edit script.

	void assoc_array_apply_edit(struct assoc_array_edit *edit);

     This will perform the edit functions, interpolating various write barriers
     to permit accesses under the RCU read lock to continue.  The edit script
     will then be passed to call_rcu() to free it and any dead stuff it points
     to.

 (2) Cancel an edit script.

	void assoc_array_cancel_edit(struct assoc_array_edit *edit);

     This frees the edit script and all preallocated memory immediately.  If
     this was for insertion, the new object is _not_ released by this function,
     but must rather be released by the caller.

These functions are guaranteed not to fail.


OPERATIONS TABLE
----------------

Various functions take a table of operations:

	struct assoc_array_ops {
		...
	};

This points to a number of methods, all of which need to be provided:

 (1) Get a chunk of index key from caller data:

	unsigned long (*get_key_chunk)(const void *index_key, int level);

     This should return a chunk of caller-supplied index key starting at the
     *bit* position given by the level argument.  The level argument will be a
     multiple of ASSOC_ARRAY_KEY_CHUNK_SIZE and the function should return
     ASSOC_ARRAY_KEY_CHUNK_SIZE bits.  No error is possible.


 (2) Get a chunk of an object's index key.

	unsigned long (*get_object_key_chunk)(const void *object, int level);

     As the previous function, but gets its data from an object in the array
     rather than from a caller-supplied index key.


 (3) See if this is the object we're looking for.

	bool (*compare_object)(const void *object, const void *index_key);

     Compare the object against an index key and return true if it matches and
     false if it doesn't.


 (4) Diff the index keys of two objects.

	int (*diff_objects)(const void *object, const void *index_key);

     Return the bit position at which the index key of the specified object
     differs from the given index key or -1 if they are the same.


 (5) Free an object.

	void (*free_object)(void *object);

     Free the specified object.  Note that this may be called an RCU grace
     period after assoc_array_apply_edit() was called, so synchronize_rcu() may
     be necessary on module unloading.


MANIPULATION FUNCTIONS
----------------------

There are a number of functions for manipulating an associative array:

 (1) Initialise an associative array.

	void assoc_array_init(struct assoc_array *array);

     This initialises the base structure for an associative array.  It can't
     fail.


 (2) Insert/replace an object in an associative array.

	struct assoc_array_edit *
	assoc_array_insert(struct assoc_array *array,
			   const struct assoc_array_ops *ops,
			   const void *index_key,
			   void *object);

     This inserts the given object into the array.  Note that the least
     significant bit of the pointer must be zero as it's used to type-mark
     pointers internally.

     If an object already exists for that key then it will be replaced with the
     new object and the old one will be freed automatically.

     The index_key argument should hold index key information and is
     passed to the methods in the ops table when they are called.

     This function makes no alteration to the array itself, but rather returns
     an edit script that must be applied.  -ENOMEM is returned in the case of
     an out-of-memory error.

     The caller should lock exclusively against other modifiers of the array.


 (3) Delete an object from an associative array.

	struct assoc_array_edit *
	assoc_array_delete(struct assoc_array *array,
			   const struct assoc_array_ops *ops,
			   const void *index_key);

     This deletes an object that matches the specified data from the array.

     The index_key argument should hold index key information and is
     passed to the methods in the ops table when they are called.

     This function makes no alteration to the array itself, but rather returns
     an edit script that must be applied.  -ENOMEM is returned in the case of
     an out-of-memory error.  NULL will be returned if the specified object is
     not found within the array.

     The caller should lock exclusively against other modifiers of the array.


 (4) Delete all objects from an associative array.

	struct assoc_array_edit *
	assoc_array_clear(struct assoc_array *array,
			  const struct assoc_array_ops *ops);

     This deletes all the objects from an associative array and leaves it
     completely empty.

     This function makes no alteration to the array itself, but rather returns
     an edit script that must be applied.  -ENOMEM is returned in the case of
     an out-of-memory error.

     The caller should lock exclusively against other modifiers of the array.


 (5) Destroy an associative array, deleting all objects.

	void assoc_array_destroy(struct assoc_array *array,
				 const struct assoc_array_ops *ops);

     This destroys the contents of the associative array and leaves it
     completely empty.  It is not permitted for another thread to be traversing
     the array under the RCU read lock at the same time as this function is
     destroying it as no RCU deferral is performed on memory release -
     something that would require memory to be allocated.

     The caller should lock exclusively against other modifiers and accessors
     of the array.


 (6) Garbage collect an associative array.

	int assoc_array_gc(struct assoc_array *array,
			   const struct assoc_array_ops *ops,
			   bool (*iterator)(void *object, void *iterator_data),
			   void *iterator_data);

     This iterates over the objects in an associative array and passes each one
     to iterator().  If iterator() returns true, the object is kept.  If it
     returns false, the object will be freed.  If the iterator() function
     returns true, it must perform any appropriate refcount incrementing on the
     object before returning.

     The internal tree will be packed down if possible as part of the iteration
     to reduce the number of nodes in it.

     The iterator_data is passed directly to iterator() and is otherwise
     ignored by the function.

     The function will return 0 if successful and -ENOMEM if there wasn't
     enough memory.

     It is possible for other threads to iterate over or search the array under
     the RCU read lock whilst this function is in progress.  The caller should
     lock exclusively against other modifiers of the array.


ACCESS FUNCTIONS
----------------

There are two functions for accessing an associative array:

 (1) Iterate over all the objects in an associative array.

	int assoc_array_iterate(const struct assoc_array *array,
				int (*iterator)(const void *object,
						void *iterator_data),
				void *iterator_data);

     This passes each object in the array to the iterator callback function.
     iterator_data is private data for that function.

     This may be used on an array at the same time as the array is being
     modified, provided the RCU read lock is held.  Under such circumstances,
     it is possible for the iteration function to see some objects twice.  If
     this is a problem, then modification should be locked against.  The
     iteration algorithm should not, however, miss any objects.

     The function will return 0 if no objects were in the array or else it will
     return the result of the last iterator function called.  Iteration stops
     immediately if any call to the iteration function results in a non-zero
     return.


 (2) Find an object in an associative array.

	void *assoc_array_find(const struct assoc_array *array,
			       const struct assoc_array_ops *ops,
			       const void *index_key);

     This walks through the array's internal tree directly to the object
     specified by the index key..

     This may be used on an array at the same time as the array is being
     modified, provided the RCU read lock is held.

     The function will return the object if found (and set *_type to the object
     type) or will return NULL if the object was not found.


INDEX KEY FORM
--------------

The index key can be of any form, but since the algorithms aren't told how long
the key is, it is strongly recommended that the index key includes its length
very early on before any variation due to the length would have an effect on
comparisons.

This will cause leaves with different length keys to scatter away from each
other - and those with the same length keys to cluster together.

It is also recommended that the index key begin with a hash of the rest of the
key to maximise scattering throughout keyspace.

The better the scattering, the wider and lower the internal tree will be.

Poor scattering isn't too much of a problem as there are shortcuts and nodes
can contain mixtures of leaves and metadata pointers.

The index key is read in chunks of machine word.  Each chunk is subdivided into
one nibble (4 bits) per level, so on a 32-bit CPU this is good for 8 levels and
on a 64-bit CPU, 16 levels.  Unless the scattering is really poor, it is
unlikely that more than one word of any particular index key will have to be
used.


=================
INTERNAL WORKINGS
=================

The associative array data structure has an internal tree.  This tree is
constructed of two types of metadata blocks: nodes and shortcuts.

A node is an array of slots.  Each slot can contain one of four things:

 (*) A NULL pointer, indicating that the slot is empty.

 (*) A pointer to an object (a leaf).

 (*) A pointer to a node at the next level.

 (*) A pointer to a shortcut.


BASIC INTERNAL TREE LAYOUT
--------------------------

Ignoring shortcuts for the moment, the nodes form a multilevel tree.  The index
key space is strictly subdivided by the nodes in the tree and nodes occur on
fixed levels.  For example:

 Level:	0		1		2		3
	===============	===============	===============	===============
							NODE D
			NODE B		NODE C	+------>+---+
		+------>+---+	+------>+---+	|	| 0 |
	NODE A	|	| 0 |	|	| 0 |	|	+---+
	+---+	|	+---+	|	+---+	|	:   :
	| 0 |	|	:   :	|	:   :	|	+---+
	+---+	|	+---+	|	+---+	|	| f |
	| 1 |---+	| 3 |---+	| 7 |---+	+---+
	+---+		+---+		+---+
	:   :		:   :		| 8 |---+
	+---+		+---+		+---+	|	NODE E
	| e |---+	| f |		:   :   +------>+---+
	+---+	|	+---+		+---+		| 0 |
	| f |	|			| f |		+---+
	+---+	|			+---+		:   :
		|	NODE F				+---+
		+------>+---+				| f |
			| 0 |		NODE G		+---+
			+---+	+------>+---+
			:   :	|	| 0 |
			+---+	|	+---+
			| 6 |---+	:   :
			+---+		+---+
			:   :		| f |
			+---+		+---+
			| f |
			+---+

In the above example, there are 7 nodes (A-G), each with 16 slots (0-f).
Assuming no other meta data nodes in the tree, the key space is divided thusly:

	KEY PREFIX	NODE
	==========	====
	137*		D
	138*		E
	13[0-69-f]*	C
	1[0-24-f]*	B
	e6*		G
	e[0-57-f]*	F
	[02-df]*	A

So, for instance, keys with the following example index keys will be found in
the appropriate nodes:

	INDEX KEY	PREFIX	NODE
	===============	=======	====
	13694892892489	13	C
	13795289025897	137	D
	13889dde88793	138	E
	138bbb89003093	138	E
	1394879524789	12	C
	1458952489	1	B
	9431809de993ba	-	A
	b4542910809cd	-	A
	e5284310def98	e	F
	e68428974237	e6	G
	e7fffcbd443	e	F
	f3842239082	-	A

To save memory, if a node can hold all the leaves in its portion of keyspace,
then the node will have all those leaves in it and will not have any metadata
pointers - even if some of those leaves would like to be in the same slot.

A node can contain a heterogeneous mix of leaves and metadata pointers.
Metadata pointers must be in the slots that match their subdivisions of key
space.  The leaves can be in any slot not occupied by a metadata pointer.  It
is guaranteed that none of the leaves in a node will match a slot occupied by a
metadata pointer.  If the metadata pointer is there, any leaf whose key matches
the metadata key prefix must be in the subtree that the metadata pointer points
to.

In the above example list of index keys, node A will contain:

	SLOT	CONTENT		INDEX KEY (PREFIX)
	====	===============	==================
	1	PTR TO NODE B	1*
	any	LEAF		9431809de993ba
	any	LEAF		b4542910809cd
	e	PTR TO NODE F	e*
	any	LEAF		f3842239082

and node B:

	3	PTR TO NODE C	13*
	any	LEAF		1458952489


SHORTCUTS
---------

Shortcuts are metadata records that jump over a piece of keyspace.  A shortcut
is a replacement for a series of single-occupancy nodes ascending through the
levels.  Shortcuts exist to save memory and to speed up traversal.

It is possible for the root of the tree to be a shortcut - say, for example,
the tree contains at least 17 nodes all with key prefix '1111'.  The insertion
algorithm will insert a shortcut to skip over the '1111' keyspace in a single
bound and get to the fourth level where these actually become different.


SPLITTING AND COLLAPSING NODES
------------------------------

Each node has a maximum capacity of 16 leaves and metadata pointers.  If the
insertion algorithm finds that it is trying to insert a 17th object into a
node, that node will be split such that at least two leaves that have a common
key segment at that level end up in a separate node rooted on that slot for
that common key segment.

If the leaves in a full node and the leaf that is being inserted are
sufficiently similar, then a shortcut will be inserted into the tree.

When the number of objects in the subtree rooted at a node falls to 16 or
fewer, then the subtree will be collapsed down to a single node - and this will
ripple towards the root if possible.


NON-RECURSIVE ITERATION
-----------------------

Each node and shortcut contains a back pointer to its parent and the number of
slot in that parent that points to it.  None-recursive iteration uses these to
proceed rootwards through the tree, going to the parent node, slot N + 1 to
make sure progress is made without the need for a stack.

The backpointers, however, make simultaneous alteration and iteration tricky.


SIMULTANEOUS ALTERATION AND ITERATION
-------------------------------------

There are a number of cases to consider:

 (1) Simple insert/replace.  This involves simply replacing a NULL or old
     matching leaf pointer with the pointer to the new leaf after a barrier.
     The metadata blocks don't change otherwise.  An old leaf won't be freed
     until after the RCU grace period.

 (2) Simple delete.  This involves just clearing an old matching leaf.  The
     metadata blocks don't change otherwise.  The old leaf won't be freed until
     after the RCU grace period.

 (3) Insertion replacing part of a subtree that we haven't yet entered.  This
     may involve replacement of part of that subtree - but that won't affect
     the iteration as we won't have reached the pointer to it yet and the
     ancestry blocks are not replaced (the layout of those does not change).

 (4) Insertion replacing nodes that we're actively processing.  This isn't a
     problem as we've passed the anchoring pointer and won't switch onto the
     new layout until we follow the back pointers - at which point we've
     already examined the leaves in the replaced node (we iterate over all the
     leaves in a node before following any of its metadata pointers).

     We might, however, re-see some leaves that have been split out into a new
     branch that's in a slot further along than we were at.

 (5) Insertion replacing nodes that we're processing a dependent branch of.
     This won't affect us until we follow the back pointers.  Similar to (4).

 (6) Deletion collapsing a branch under us.  This doesn't affect us because the
     back pointers will get us back to the parent of the new node before we
     could see the new node.  The entire collapsed subtree is thrown away
     unchanged - and will still be rooted on the same slot, so we shouldn't
     process it a second time as we'll go back to slot + 1.

Note:

 (*) Under some circumstances, we need to simultaneously change the parent
     pointer and the parent slot pointer on a node (say, for example, we
     inserted another node before it and moved it up a level).  We cannot do
     this without locking against a read - so we have to replace that node too.

     However, when we're changing a shortcut into a node this isn't a problem
     as shortcuts only have one slot and so the parent slot number isn't used
     when traversing backwards over one.  This means that it's okay to change
     the slot number first - provided suitable barriers are used to make sure
     the parent slot number is read after the back pointer.

Obsolete blocks and leaves are freed up after an RCU grace period has passed,
so as long as anyone doing walking or iteration holds the RCU read lock, the
old superstructure should not go away on them.
		Semantics and Behavior of Atomic and
		         Bitmask Operations

			  David S. Miller	 

	This document is intended to serve as a guide to Linux port
maintainers on how to implement atomic counter, bitops, and spinlock
interfaces properly.

	The atomic_t type should be defined as a signed integer.
Also, it should be made opaque such that any kind of cast to a normal
C integer type will fail.  Something like the following should
suffice:

	typedef struct { int counter; } atomic_t;

Historically, counter has been declared volatile.  This is now discouraged.
See Documentation/volatile-considered-harmful.txt for the complete rationale.

local_t is very similar to atomic_t. If the counter is per CPU and only
updated by one CPU, local_t is probably more appropriate. Please see
Documentation/local_ops.txt for the semantics of local_t.

The first operations to implement for atomic_t's are the initializers and
plain reads.

	#define ATOMIC_INIT(i)		{ (i) }
	#define atomic_set(v, i)	((v)->counter = (i))

The first macro is used in definitions, such as:

static atomic_t my_counter = ATOMIC_INIT(1);

The initializer is atomic in that the return values of the atomic operations
are guaranteed to be correct reflecting the initialized value if the
initializer is used before runtime.  If the initializer is used at runtime, a
proper implicit or explicit read memory barrier is needed before reading the
value with atomic_read from another thread.

The second interface can be used at runtime, as in:

	struct foo { atomic_t counter; };
	...

	struct foo *k;

	k = kmalloc(sizeof(*k), GFP_KERNEL);
	if (!k)
		return -ENOMEM;
	atomic_set(&k->counter, 0);

The setting is atomic in that the return values of the atomic operations by
all threads are guaranteed to be correct reflecting either the value that has
been set with this operation or set with another operation.  A proper implicit
or explicit memory barrier is needed before the value set with the operation
is guaranteed to be readable with atomic_read from another thread.

Next, we have:

	#define atomic_read(v)	((v)->counter)

which simply reads the counter value currently visible to the calling thread.
The read is atomic in that the return value is guaranteed to be one of the
values initialized or modified with the interface operations if a proper
implicit or explicit memory barrier is used after possible runtime
initialization by any other thread and the value is modified only with the
interface operations.  atomic_read does not guarantee that the runtime
initialization by any other thread is visible yet, so the user of the
interface must take care of that with a proper implicit or explicit memory
barrier.

*** WARNING: atomic_read() and atomic_set() DO NOT IMPLY BARRIERS! ***

Some architectures may choose to use the volatile keyword, barriers, or inline
assembly to guarantee some degree of immediacy for atomic_read() and
atomic_set().  This is not uniformly guaranteed, and may change in the future,
so all users of atomic_t should treat atomic_read() and atomic_set() as simple
C statements that may be reordered or optimized away entirely by the compiler
or processor, and explicitly invoke the appropriate compiler and/or memory
barrier for each use case.  Failure to do so will result in code that may
suddenly break when used with different architectures or compiler
optimizations, or even changes in unrelated code which changes how the
compiler optimizes the section accessing atomic_t variables.

*** YOU HAVE BEEN WARNED! ***

Properly aligned pointers, longs, ints, and chars (and unsigned
equivalents) may be atomically loaded from and stored to in the same
sense as described for atomic_read() and atomic_set().  The ACCESS_ONCE()
macro should be used to prevent the compiler from using optimizations
that might otherwise optimize accesses out of existence on the one hand,
or that might create unsolicited accesses on the other.

For example consider the following code:

	while (a > 0)
		do_something();

If the compiler can prove that do_something() does not store to the
variable a, then the compiler is within its rights transforming this to
the following:

	tmp = a;
	if (a > 0)
		for (;;)
			do_something();

If you don't want the compiler to do this (and you probably don't), then
you should use something like the following:

	while (ACCESS_ONCE(a) < 0)
		do_something();

Alternatively, you could place a barrier() call in the loop.

For another example, consider the following code:

	tmp_a = a;
	do_something_with(tmp_a);
	do_something_else_with(tmp_a);

If the compiler can prove that do_something_with() does not store to the
variable a, then the compiler is within its rights to manufacture an
additional load as follows:

	tmp_a = a;
	do_something_with(tmp_a);
	tmp_a = a;
	do_something_else_with(tmp_a);

This could fatally confuse your code if it expected the same value
to be passed to do_something_with() and do_something_else_with().

The compiler would be likely to manufacture this additional load if
do_something_with() was an inline function that made very heavy use
of registers: reloading from variable a could save a flush to the
stack and later reload.  To prevent the compiler from attacking your
code in this manner, write the following:

	tmp_a = ACCESS_ONCE(a);
	do_something_with(tmp_a);
	do_something_else_with(tmp_a);

For a final example, consider the following code, assuming that the
variable a is set at boot time before the second CPU is brought online
and never changed later, so that memory barriers are not needed:

	if (a)
		b = 9;
	else
		b = 42;

The compiler is within its rights to manufacture an additional store
by transforming the above code into the following:

	b = 42;
	if (a)
		b = 9;

This could come as a fatal surprise to other code running concurrently
that expected b to never have the value 42 if a was zero.  To prevent
the compiler from doing this, write something like:

	if (a)
		ACCESS_ONCE(b) = 9;
	else
		ACCESS_ONCE(b) = 42;

Don't even -think- about doing this without proper use of memory barriers,
locks, or atomic operations if variable a can change at runtime!

*** WARNING: ACCESS_ONCE() DOES NOT IMPLY A BARRIER! ***

Now, we move onto the atomic operation interfaces typically implemented with
the help of assembly code.

	void atomic_add(int i, atomic_t *v);
	void atomic_sub(int i, atomic_t *v);
	void atomic_inc(atomic_t *v);
	void atomic_dec(atomic_t *v);

These four routines add and subtract integral values to/from the given
atomic_t value.  The first two routines pass explicit integers by
which to make the adjustment, whereas the latter two use an implicit
adjustment value of "1".

One very important aspect of these two routines is that they DO NOT
require any explicit memory barriers.  They need only perform the
atomic_t counter update in an SMP safe manner.

Next, we have:

	int atomic_inc_return(atomic_t *v);
	int atomic_dec_return(atomic_t *v);

These routines add 1 and subtract 1, respectively, from the given
atomic_t and return the new counter value after the operation is
performed.

Unlike the above routines, it is required that explicit memory
barriers are performed before and after the operation.  It must be
done such that all memory operations before and after the atomic
operation calls are strongly ordered with respect to the atomic
operation itself.

For example, it should behave as if a smp_mb() call existed both
before and after the atomic operation.

If the atomic instructions used in an implementation provide explicit
memory barrier semantics which satisfy the above requirements, that is
fine as well.

Let's move on:

	int atomic_add_return(int i, atomic_t *v);
	int atomic_sub_return(int i, atomic_t *v);

These behave just like atomic_{inc,dec}_return() except that an
explicit counter adjustment is given instead of the implicit "1".
This means that like atomic_{inc,dec}_return(), the memory barrier
semantics are required.

Next:

	int atomic_inc_and_test(atomic_t *v);
	int atomic_dec_and_test(atomic_t *v);

These two routines increment and decrement by 1, respectively, the
given atomic counter.  They return a boolean indicating whether the
resulting counter value was zero or not.

It requires explicit memory barrier semantics around the operation as
above.

	int atomic_sub_and_test(int i, atomic_t *v);

This is identical to atomic_dec_and_test() except that an explicit
decrement is given instead of the implicit "1".  It requires explicit
memory barrier semantics around the operation.

	int atomic_add_negative(int i, atomic_t *v);

The given increment is added to the given atomic counter value.  A
boolean is return which indicates whether the resulting counter value
is negative.  It requires explicit memory barrier semantics around the
operation.

Then:

	int atomic_xchg(atomic_t *v, int new);

This performs an atomic exchange operation on the atomic variable v, setting
the given new value.  It returns the old value that the atomic variable v had
just before the operation.

atomic_xchg requires explicit memory barriers around the operation.

	int atomic_cmpxchg(atomic_t *v, int old, int new);

This performs an atomic compare exchange operation on the atomic value v,
with the given old and new values. Like all atomic_xxx operations,
atomic_cmpxchg will only satisfy its atomicity semantics as long as all
other accesses of *v are performed through atomic_xxx operations.

atomic_cmpxchg requires explicit memory barriers around the operation.

The semantics for atomic_cmpxchg are the same as those defined for 'cas'
below.

Finally:

	int atomic_add_unless(atomic_t *v, int a, int u);

If the atomic value v is not equal to u, this function adds a to v, and
returns non zero. If v is equal to u then it returns zero. This is done as
an atomic operation.

atomic_add_unless requires explicit memory barriers around the operation
unless it fails (returns 0).

atomic_inc_not_zero, equivalent to atomic_add_unless(v, 1, 0)


If a caller requires memory barrier semantics around an atomic_t
operation which does not return a value, a set of interfaces are
defined which accomplish this:

	void smp_mb__before_atomic(void);
	void smp_mb__after_atomic(void);

For example, smp_mb__before_atomic() can be used like so:

	obj->dead = 1;
	smp_mb__before_atomic();
	atomic_dec(&obj->ref_count);

It makes sure that all memory operations preceding the atomic_dec()
call are strongly ordered with respect to the atomic counter
operation.  In the above example, it guarantees that the assignment of
"1" to obj->dead will be globally visible to other cpus before the
atomic counter decrement.

Without the explicit smp_mb__before_atomic() call, the
implementation could legally allow the atomic counter update visible
to other cpus before the "obj->dead = 1;" assignment.

A missing memory barrier in the cases where they are required by the
atomic_t implementation above can have disastrous results.  Here is
an example, which follows a pattern occurring frequently in the Linux
kernel.  It is the use of atomic counters to implement reference
counting, and it works such that once the counter falls to zero it can
be guaranteed that no other entity can be accessing the object:

static void obj_list_add(struct obj *obj, struct list_head *head)
{
	obj->active = 1;
	list_add(&obj->list, head);
}

static void obj_list_del(struct obj *obj)
{
	list_del(&obj->list);
	obj->active = 0;
}

static void obj_destroy(struct obj *obj)
{
	BUG_ON(obj->active);
	kfree(obj);
}

struct obj *obj_list_peek(struct list_head *head)
{
	if (!list_empty(head)) {
		struct obj *obj;

		obj = list_entry(head->next, struct obj, list);
		atomic_inc(&obj->refcnt);
		return obj;
	}
	return NULL;
}

void obj_poke(void)
{
	struct obj *obj;

	spin_lock(&global_list_lock);
	obj = obj_list_peek(&global_list);
	spin_unlock(&global_list_lock);

	if (obj) {
		obj->ops->poke(obj);
		if (atomic_dec_and_test(&obj->refcnt))
			obj_destroy(obj);
	}
}

void obj_timeout(struct obj *obj)
{
	spin_lock(&global_list_lock);
	obj_list_del(obj);
	spin_unlock(&global_list_lock);

	if (atomic_dec_and_test(&obj->refcnt))
		obj_destroy(obj);
}

(This is a simplification of the ARP queue management in the
 generic neighbour discover code of the networking.  Olaf Kirch
 found a bug wrt. memory barriers in kfree_skb() that exposed
 the atomic_t memory barrier requirements quite clearly.)

Given the above scheme, it must be the case that the obj->active
update done by the obj list deletion be visible to other processors
before the atomic counter decrement is performed.

Otherwise, the counter could fall to zero, yet obj->active would still
be set, thus triggering the assertion in obj_destroy().  The error
sequence looks like this:

	cpu 0				cpu 1
	obj_poke()			obj_timeout()
	obj = obj_list_peek();
	... gains ref to obj, refcnt=2
					obj_list_del(obj);
					obj->active = 0 ...
					... visibility delayed ...
					atomic_dec_and_test()
					... refcnt drops to 1 ...
	atomic_dec_and_test()
	... refcount drops to 0 ...
	obj_destroy()
	BUG() triggers since obj->active
	still seen as one
					obj->active update visibility occurs

With the memory barrier semantics required of the atomic_t operations
which return values, the above sequence of memory visibility can never
happen.  Specifically, in the above case the atomic_dec_and_test()
counter decrement would not become globally visible until the
obj->active update does.

As a historical note, 32-bit Sparc used to only allow usage of
24-bits of its atomic_t type.  This was because it used 8 bits
as a spinlock for SMP safety.  Sparc32 lacked a "compare and swap"
type instruction.  However, 32-bit Sparc has since been moved over
to a "hash table of spinlocks" scheme, that allows the full 32-bit
counter to be realized.  Essentially, an array of spinlocks are
indexed into based upon the address of the atomic_t being operated
on, and that lock protects the atomic operation.  Parisc uses the
same scheme.

Another note is that the atomic_t operations returning values are
extremely slow on an old 386.

We will now cover the atomic bitmask operations.  You will find that
their SMP and memory barrier semantics are similar in shape and scope
to the atomic_t ops above.

Native atomic bit operations are defined to operate on objects aligned
to the size of an "unsigned long" C data type, and are least of that
size.  The endianness of the bits within each "unsigned long" are the
native endianness of the cpu.

	void set_bit(unsigned long nr, volatile unsigned long *addr);
	void clear_bit(unsigned long nr, volatile unsigned long *addr);
	void change_bit(unsigned long nr, volatile unsigned long *addr);

These routines set, clear, and change, respectively, the bit number
indicated by "nr" on the bit mask pointed to by "ADDR".

They must execute atomically, yet there are no implicit memory barrier
semantics required of these interfaces.

	int test_and_set_bit(unsigned long nr, volatile unsigned long *addr);
	int test_and_clear_bit(unsigned long nr, volatile unsigned long *addr);
	int test_and_change_bit(unsigned long nr, volatile unsigned long *addr);

Like the above, except that these routines return a boolean which
indicates whether the changed bit was set _BEFORE_ the atomic bit
operation.

WARNING! It is incredibly important that the value be a boolean,
ie. "0" or "1".  Do not try to be fancy and save a few instructions by
declaring the above to return "long" and just returning something like
"old_val & mask" because that will not work.

For one thing, this return value gets truncated to int in many code
paths using these interfaces, so on 64-bit if the bit is set in the
upper 32-bits then testers will never see that.

One great example of where this problem crops up are the thread_info
flag operations.  Routines such as test_and_set_ti_thread_flag() chop
the return value into an int.  There are other places where things
like this occur as well.

These routines, like the atomic_t counter operations returning values,
require explicit memory barrier semantics around their execution.  All
memory operations before the atomic bit operation call must be made
visible globally before the atomic bit operation is made visible.
Likewise, the atomic bit operation must be visible globally before any
subsequent memory operation is made visible.  For example:

	obj->dead = 1;
	if (test_and_set_bit(0, &obj->flags))
		/* ... */;
	obj->killed = 1;

The implementation of test_and_set_bit() must guarantee that
"obj->dead = 1;" is visible to cpus before the atomic memory operation
done by test_and_set_bit() becomes visible.  Likewise, the atomic
memory operation done by test_and_set_bit() must become visible before
"obj->killed = 1;" is visible.

Finally there is the basic operation:

	int test_bit(unsigned long nr, __const__ volatile unsigned long *addr);

Which returns a boolean indicating if bit "nr" is set in the bitmask
pointed to by "addr".

If explicit memory barriers are required around {set,clear}_bit() (which do
not return a value, and thus does not need to provide memory barrier
semantics), two interfaces are provided:

	void smp_mb__before_atomic(void);
	void smp_mb__after_atomic(void);

They are used as follows, and are akin to their atomic_t operation
brothers:

	/* All memory operations before this call will
	 * be globally visible before the clear_bit().
	 */
	smp_mb__before_atomic();
	clear_bit( ... );

	/* The clear_bit() will be visible before all
	 * subsequent memory operations.
	 */
	 smp_mb__after_atomic();

There are two special bitops with lock barrier semantics (acquire/release,
same as spinlocks). These operate in the same way as their non-_lock/unlock
postfixed variants, except that they are to provide acquire/release semantics,
respectively. This means they can be used for bit_spin_trylock and
bit_spin_unlock type operations without specifying any more barriers.

	int test_and_set_bit_lock(unsigned long nr, unsigned long *addr);
	void clear_bit_unlock(unsigned long nr, unsigned long *addr);
	void __clear_bit_unlock(unsigned long nr, unsigned long *addr);

The __clear_bit_unlock version is non-atomic, however it still implements
unlock barrier semantics. This can be useful if the lock itself is protecting
the other bits in the word.

Finally, there are non-atomic versions of the bitmask operations
provided.  They are used in contexts where some other higher-level SMP
locking scheme is being used to protect the bitmask, and thus less
expensive non-atomic operations may be used in the implementation.
They have names similar to the above bitmask operation interfaces,
except that two underscores are prefixed to the interface name.

	void __set_bit(unsigned long nr, volatile unsigned long *addr);
	void __clear_bit(unsigned long nr, volatile unsigned long *addr);
	void __change_bit(unsigned long nr, volatile unsigned long *addr);
	int __test_and_set_bit(unsigned long nr, volatile unsigned long *addr);
	int __test_and_clear_bit(unsigned long nr, volatile unsigned long *addr);
	int __test_and_change_bit(unsigned long nr, volatile unsigned long *addr);

These non-atomic variants also do not require any special memory
barrier semantics.

The routines xchg() and cmpxchg() need the same exact memory barriers
as the atomic and bit operations returning values.

Spinlocks and rwlocks have memory barrier expectations as well.
The rule to follow is simple:

1) When acquiring a lock, the implementation must make it globally
   visible before any subsequent memory operation.

2) When releasing a lock, the implementation must make it such that
   all previous memory operations are globally visible before the
   lock release.

Which finally brings us to _atomic_dec_and_lock().  There is an
architecture-neutral version implemented in lib/dec_and_lock.c,
but most platforms will wish to optimize this in assembler.

	int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);

Atomically decrement the given counter, and if will drop to zero
atomically acquire the given spinlock and perform the decrement
of the counter to zero.  If it does not drop to zero, do nothing
with the spinlock.

It is actually pretty simple to get the memory barrier correct.
Simply satisfy the spinlock grab requirements, which is make
sure the spinlock operation is globally visible before any
subsequent memory operation.

We can demonstrate this operation more clearly if we define
an abstract atomic operation:

	long cas(long *mem, long old, long new);

"cas" stands for "compare and swap".  It atomically:

1) Compares "old" with the value currently at "mem".
2) If they are equal, "new" is written to "mem".
3) Regardless, the current value at "mem" is returned.

As an example usage, here is what an atomic counter update
might look like:

void example_atomic_inc(long *counter)
{
	long old, new, ret;

	while (1) {
		old = *counter;
		new = old + 1;

		ret = cas(counter, old, new);
		if (ret == old)
			break;
	}
}

Let's use cas() in order to build a pseudo-C atomic_dec_and_lock():

int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
{
	long old, new, ret;
	int went_to_zero;

	went_to_zero = 0;
	while (1) {
		old = atomic_read(atomic);
		new = old - 1;
		if (new == 0) {
			went_to_zero = 1;
			spin_lock(lock);
		}
		ret = cas(atomic, old, new);
		if (ret == old)
			break;
		if (went_to_zero) {
			spin_unlock(lock);
			went_to_zero = 0;
		}
	}

	return went_to_zero;
}

Now, as far as memory barriers go, as long as spin_lock()
strictly orders all subsequent memory operations (including
the cas()) with respect to itself, things will be fine.

Said another way, _atomic_dec_and_lock() must guarantee that
a counter dropping to zero is never made visible before the
spinlock being acquired.

Note that this also means that for the case where the counter
is not dropping to zero, there are no memory ordering
requirements.
March 2008
Jan-Simon Moeller, dl9pf@gmx.de


How to deal with bad memory e.g. reported by memtest86+ ?
#########################################################

There are three possibilities I know of:

1) Reinsert/swap the memory modules

2) Buy new modules (best!) or try to exchange the memory
   if you have spare-parts

3) Use BadRAM or memmap

This Howto is about number 3) .


BadRAM
######
BadRAM is the actively developed and available as kernel-patch
here:  http://rick.vanrein.org/linux/badram/

For more details see the BadRAM documentation.

memmap
######

memmap is already in the kernel and usable as kernel-parameter at
boot-time.  Its syntax is slightly strange and you may need to
calculate the values by yourself!

Syntax to exclude a memory area (see kernel-parameters.txt for details):
memmap=<size>$<address>

Example: memtest86+ reported here errors at address 0x18691458, 0x18698424 and
         some others. All had 0x1869xxxx in common, so I chose a pattern of
         0x18690000,0xffff0000.

With the numbers of the example above:
memmap=64K$0x18690000
 or
memmap=0x10000$0x18690000

These instructions are deliberately very basic. If you want something clever,
go read the real docs ;-) Please don't add more stuff, but feel free to 
correct my mistakes ;-)    (mbligh@aracnet.com)
Thanks to John Levon, Dave Hansen, et al. for help writing this.

<test> is the thing you're trying to measure.
Make sure you have the correct System.map / vmlinux referenced!

It is probably easiest to use "make install" for linux and hack
/sbin/installkernel to copy vmlinux to /boot, in addition to vmlinuz,
config, System.map, which are usually installed by default.

Readprofile
-----------
A recent readprofile command is needed for 2.6, such as found in util-linux
2.12a, which can be downloaded from:

http://www.kernel.org/pub/linux/utils/util-linux/

Most distributions will ship it already.

Add "profile=2" to the kernel command line.

clear		readprofile -r
		<test>
dump output	readprofile -m /boot/System.map > captured_profile

Oprofile
--------

Get the source (see Changes for required version) from
http://oprofile.sourceforge.net/ and add "idle=poll" to the kernel command
line.

Configure with CONFIG_PROFILING=y and CONFIG_OPROFILE=y & reboot on new kernel

./configure --with-kernel-support
make install

For superior results, be sure to enable the local APIC. If opreport sees
a 0Hz CPU, APIC was not on. Be aware that idle=poll may mean a performance
penalty.

One time setup:
		opcontrol --setup --vmlinux=/boot/vmlinux

clear		opcontrol --reset
start		opcontrol --start
		<test>
stop		opcontrol --stop
dump output	opreport >  output_file

To only report on the kernel, run opreport -l /boot/vmlinux > output_file

A reset is needed to clear old statistics, which survive a reboot.

Say you've got a big slow raid 6, and an X-25E or three. Wouldn't it be
nice if you could use them as cache... Hence bcache.

Wiki and git repositories are at:
  http://bcache.evilpiepirate.org
  http://evilpiepirate.org/git/linux-bcache.git
  http://evilpiepirate.org/git/bcache-tools.git

It's designed around the performance characteristics of SSDs - it only allocates
in erase block sized buckets, and it uses a hybrid btree/log to track cached
extants (which can be anywhere from a single sector to the bucket size). It's
designed to avoid random writes at all costs; it fills up an erase block
sequentially, then issues a discard before reusing it.

Both writethrough and writeback caching are supported. Writeback defaults to
off, but can be switched on and off arbitrarily at runtime. Bcache goes to
great lengths to protect your data - it reliably handles unclean shutdown. (It
doesn't even have a notion of a clean shutdown; bcache simply doesn't return
writes as completed until they're on stable storage).

Writeback caching can use most of the cache for buffering writes - writing
dirty data to the backing device is always done sequentially, scanning from the
start to the end of the index.

Since random IO is what SSDs excel at, there generally won't be much benefit
to caching large sequential IO. Bcache detects sequential IO and skips it;
it also keeps a rolling average of the IO sizes per task, and as long as the
average is above the cutoff it will skip all IO from that task - instead of
caching the first 512k after every seek. Backups and large file copies should
thus entirely bypass the cache.

In the event of a data IO error on the flash it will try to recover by reading
from disk or invalidating cache entries.  For unrecoverable errors (meta data
or dirty data), caching is automatically disabled; if dirty data was present
in the cache it first disables writeback caching and waits for all dirty data
to be flushed.

Getting started:
You'll need make-bcache from the bcache-tools repository. Both the cache device
and backing device must be formatted before use.
  make-bcache -B /dev/sdb
  make-bcache -C /dev/sdc

make-bcache has the ability to format multiple devices at the same time - if
you format your backing devices and cache device at the same time, you won't
have to manually attach:
  make-bcache -B /dev/sda /dev/sdb -C /dev/sdc

bcache-tools now ships udev rules, and bcache devices are known to the kernel
immediately.  Without udev, you can manually register devices like this:

  echo /dev/sdb > /sys/fs/bcache/register
  echo /dev/sdc > /sys/fs/bcache/register

Registering the backing device makes the bcache device show up in /dev; you can
now format it and use it as normal. But the first time using a new bcache
device, it'll be running in passthrough mode until you attach it to a cache.
See the section on attaching.

The devices show up as:

  /dev/bcache<N>

As well as (with udev):

  /dev/bcache/by-uuid/<uuid>
  /dev/bcache/by-label/<label>

To get started:

  mkfs.ext4 /dev/bcache0
  mount /dev/bcache0 /mnt

You can control bcache devices through sysfs at /sys/block/bcache<N>/bcache .

Cache devices are managed as sets; multiple caches per set isn't supported yet
but will allow for mirroring of metadata and dirty data in the future. Your new
cache set shows up as /sys/fs/bcache/<UUID>

ATTACHING:

After your cache device and backing device are registered, the backing device
must be attached to your cache set to enable caching. Attaching a backing
device to a cache set is done thusly, with the UUID of the cache set in
/sys/fs/bcache:

  echo <CSET-UUID> > /sys/block/bcache0/bcache/attach

This only has to be done once. The next time you reboot, just reregister all
your bcache devices. If a backing device has data in a cache somewhere, the
/dev/bcache<N> device won't be created until the cache shows up - particularly
important if you have writeback caching turned on.

If you're booting up and your cache device is gone and never coming back, you
can force run the backing device:

  echo 1 > /sys/block/sdb/bcache/running

(You need to use /sys/block/sdb (or whatever your backing device is called), not
/sys/block/bcache0, because bcache0 doesn't exist yet. If you're using a
partition, the bcache directory would be at /sys/block/sdb/sdb2/bcache)

The backing device will still use that cache set if it shows up in the future,
but all the cached data will be invalidated. If there was dirty data in the
cache, don't expect the filesystem to be recoverable - you will have massive
filesystem corruption, though ext4's fsck does work miracles.

ERROR HANDLING:

Bcache tries to transparently handle IO errors to/from the cache device without
affecting normal operation; if it sees too many errors (the threshold is
configurable, and defaults to 0) it shuts down the cache device and switches all
the backing devices to passthrough mode.

 - For reads from the cache, if they error we just retry the read from the
   backing device.

 - For writethrough writes, if the write to the cache errors we just switch to
   invalidating the data at that lba in the cache (i.e. the same thing we do for
   a write that bypasses the cache)

 - For writeback writes, we currently pass that error back up to the
   filesystem/userspace. This could be improved - we could retry it as a write
   that skips the cache so we don't have to error the write.

 - When we detach, we first try to flush any dirty data (if we were running in
   writeback mode). It currently doesn't do anything intelligent if it fails to
   read some of the dirty data, though.

TROUBLESHOOTING PERFORMANCE:

Bcache has a bunch of config options and tunables. The defaults are intended to
be reasonable for typical desktop and server workloads, but they're not what you
want for getting the best possible numbers when benchmarking.

 - Bad write performance

   If write performance is not what you expected, you probably wanted to be
   running in writeback mode, which isn't the default (not due to a lack of
   maturity, but simply because in writeback mode you'll lose data if something
   happens to your SSD)

   # echo writeback > /sys/block/bcache0/cache_mode

 - Bad performance, or traffic not going to the SSD that you'd expect

   By default, bcache doesn't cache everything. It tries to skip sequential IO -
   because you really want to be caching the random IO, and if you copy a 10
   gigabyte file you probably don't want that pushing 10 gigabytes of randomly
   accessed data out of your cache.

   But if you want to benchmark reads from cache, and you start out with fio
   writing an 8 gigabyte test file - so you want to disable that.

   # echo 0 > /sys/block/bcache0/bcache/sequential_cutoff

   To set it back to the default (4 mb), do

   # echo 4M > /sys/block/bcache0/bcache/sequential_cutoff

 - Traffic's still going to the spindle/still getting cache misses

   In the real world, SSDs don't always keep up with disks - particularly with
   slower SSDs, many disks being cached by one SSD, or mostly sequential IO. So
   you want to avoid being bottlenecked by the SSD and having it slow everything
   down.

   To avoid that bcache tracks latency to the cache device, and gradually
   throttles traffic if the latency exceeds a threshold (it does this by
   cranking down the sequential bypass).

   You can disable this if you need to by setting the thresholds to 0:

   # echo 0 > /sys/fs/bcache/<cache set>/congested_read_threshold_us
   # echo 0 > /sys/fs/bcache/<cache set>/congested_write_threshold_us

   The default is 2000 us (2 milliseconds) for reads, and 20000 for writes.

 - Still getting cache misses, of the same data

   One last issue that sometimes trips people up is actually an old bug, due to
   the way cache coherency is handled for cache misses. If a btree node is full,
   a cache miss won't be able to insert a key for the new data and the data
   won't be written to the cache.

   In practice this isn't an issue because as soon as a write comes along it'll
   cause the btree node to be split, and you need almost no write traffic for
   this to not show up enough to be noticeable (especially since bcache's btree
   nodes are huge and index large regions of the device). But when you're
   benchmarking, if you're trying to warm the cache by reading a bunch of data
   and there's no other traffic - that can be a problem.

   Solution: warm the cache by doing writes, or use the testing branch (there's
   a fix for the issue there).

SYSFS - BACKING DEVICE:

Available at /sys/block/<bdev>/bcache, /sys/block/bcache*/bcache and
(if attached) /sys/fs/bcache/<cset-uuid>/bdev*

attach
  Echo the UUID of a cache set to this file to enable caching.

cache_mode
  Can be one of either writethrough, writeback, writearound or none.

clear_stats
  Writing to this file resets the running total stats (not the day/hour/5 minute
  decaying versions).

detach
  Write to this file to detach from a cache set. If there is dirty data in the
  cache, it will be flushed first.

dirty_data
  Amount of dirty data for this backing device in the cache. Continuously
  updated unlike the cache set's version, but may be slightly off.

label
  Name of underlying device.

readahead
  Size of readahead that should be performed.  Defaults to 0.  If set to e.g.
  1M, it will round cache miss reads up to that size, but without overlapping
  existing cache entries.

running
  1 if bcache is running (i.e. whether the /dev/bcache device exists, whether
  it's in passthrough mode or caching).

sequential_cutoff
  A sequential IO will bypass the cache once it passes this threshold; the
  most recent 128 IOs are tracked so sequential IO can be detected even when
  it isn't all done at once.

sequential_merge
  If non zero, bcache keeps a list of the last 128 requests submitted to compare
  against all new requests to determine which new requests are sequential
  continuations of previous requests for the purpose of determining sequential
  cutoff. This is necessary if the sequential cutoff value is greater than the
  maximum acceptable sequential size for any single request. 

state
  The backing device can be in one of four different states:

  no cache: Has never been attached to a cache set.

  clean: Part of a cache set, and there is no cached dirty data.

  dirty: Part of a cache set, and there is cached dirty data.

  inconsistent: The backing device was forcibly run by the user when there was
  dirty data cached but the cache set was unavailable; whatever data was on the
  backing device has likely been corrupted.

stop
  Write to this file to shut down the bcache device and close the backing
  device.

writeback_delay
  When dirty data is written to the cache and it previously did not contain
  any, waits some number of seconds before initiating writeback. Defaults to
  30.

writeback_percent
  If nonzero, bcache tries to keep around this percentage of the cache dirty by
  throttling background writeback and using a PD controller to smoothly adjust
  the rate.

writeback_rate
  Rate in sectors per second - if writeback_percent is nonzero, background
  writeback is throttled to this rate. Continuously adjusted by bcache but may
  also be set by the user.

writeback_running
  If off, writeback of dirty data will not take place at all. Dirty data will
  still be added to the cache until it is mostly full; only meant for
  benchmarking. Defaults to on.

SYSFS - BACKING DEVICE STATS:

There are directories with these numbers for a running total, as well as
versions that decay over the past day, hour and 5 minutes; they're also
aggregated in the cache set directory as well.

bypassed
  Amount of IO (both reads and writes) that has bypassed the cache

cache_hits
cache_misses
cache_hit_ratio
  Hits and misses are counted per individual IO as bcache sees them; a
  partial hit is counted as a miss.

cache_bypass_hits
cache_bypass_misses
  Hits and misses for IO that is intended to skip the cache are still counted,
  but broken out here.

cache_miss_collisions
  Counts instances where data was going to be inserted into the cache from a
  cache miss, but raced with a write and data was already present (usually 0
  since the synchronization for cache misses was rewritten)

cache_readaheads
  Count of times readahead occurred.

SYSFS - CACHE SET:

Available at /sys/fs/bcache/<cset-uuid>

average_key_size
  Average data per key in the btree.

bdev<0..n>
  Symlink to each of the attached backing devices.

block_size
  Block size of the cache devices.

btree_cache_size
  Amount of memory currently used by the btree cache

bucket_size
  Size of buckets

cache<0..n>
  Symlink to each of the cache devices comprising this cache set. 

cache_available_percent
  Percentage of cache device which doesn't contain dirty data, and could
  potentially be used for writeback.  This doesn't mean this space isn't used
  for clean cached data; the unused statistic (in priority_stats) is typically
  much lower.

clear_stats
  Clears the statistics associated with this cache

dirty_data
  Amount of dirty data is in the cache (updated when garbage collection runs).

flash_vol_create
  Echoing a size to this file (in human readable units, k/M/G) creates a thinly
  provisioned volume backed by the cache set.

io_error_halflife
io_error_limit
  These determines how many errors we accept before disabling the cache.
  Each error is decayed by the half life (in # ios).  If the decaying count
  reaches io_error_limit dirty data is written out and the cache is disabled.

journal_delay_ms
  Journal writes will delay for up to this many milliseconds, unless a cache
  flush happens sooner. Defaults to 100.

root_usage_percent
  Percentage of the root btree node in use.  If this gets too high the node
  will split, increasing the tree depth.

stop
  Write to this file to shut down the cache set - waits until all attached
  backing devices have been shut down.

tree_depth
  Depth of the btree (A single node btree has depth 0).

unregister
  Detaches all backing devices and closes the cache devices; if dirty data is
  present it will disable writeback caching and wait for it to be flushed.

SYSFS - CACHE SET INTERNAL:

This directory also exposes timings for a number of internal operations, with
separate files for average duration, average frequency, last occurrence and max
duration: garbage collection, btree read, btree node sorts and btree splits.

active_journal_entries
  Number of journal entries that are newer than the index.

btree_nodes
  Total nodes in the btree.

btree_used_percent
  Average fraction of btree in use.

bset_tree_stats
  Statistics about the auxiliary search trees

btree_cache_max_chain
  Longest chain in the btree node cache's hash table

cache_read_races
  Counts instances where while data was being read from the cache, the bucket
  was reused and invalidated - i.e. where the pointer was stale after the read
  completed. When this occurs the data is reread from the backing device.

trigger_gc
  Writing to this file forces garbage collection to run.

SYSFS - CACHE DEVICE:

Available at /sys/block/<cdev>/bcache

block_size
  Minimum granularity of writes - should match hardware sector size.

btree_written
  Sum of all btree writes, in (kilo/mega/giga) bytes

bucket_size
  Size of buckets

cache_replacement_policy
  One of either lru, fifo or random.

discard
  Boolean; if on a discard/TRIM will be issued to each bucket before it is
  reused. Defaults to off, since SATA TRIM is an unqueued command (and thus
  slow).

freelist_percent
  Size of the freelist as a percentage of nbuckets. Can be written to to
  increase the number of buckets kept on the freelist, which lets you
  artificially reduce the size of the cache at runtime. Mostly for testing
  purposes (i.e. testing how different size caches affect your hit rate), but
  since buckets are discarded when they move on to the freelist will also make
  the SSD's garbage collection easier by effectively giving it more reserved
  space.

io_errors
  Number of errors that have occurred, decayed by io_error_halflife.

metadata_written
  Sum of all non data writes (btree writes and all other metadata).

nbuckets
  Total buckets in this cache

priority_stats
  Statistics about how recently data in the cache has been accessed.
  This can reveal your working set size.  Unused is the percentage of
  the cache that doesn't contain any data.  Metadata is bcache's
  metadata overhead.  Average is the average priority of cache buckets.
  Next is a list of quantiles with the priority threshold of each.

written
  Sum of all data that has been written to the cache; comparison with
  btree_written gives the amount of write inflation in bcache.
     Kernel Support for miscellaneous (your favourite) Binary Formats v1.1
     =====================================================================

This Kernel feature allows you to invoke almost (for restrictions see below)
every program by simply typing its name in the shell.
This includes for example compiled Java(TM), Python or Emacs programs.

To achieve this you must tell binfmt_misc which interpreter has to be invoked
with which binary. Binfmt_misc recognises the binary-type by matching some bytes
at the beginning of the file with a magic byte sequence (masking out specified
bits) you have supplied. Binfmt_misc can also recognise a filename extension
aka '.com' or '.exe'.

First you must mount binfmt_misc:
	mount binfmt_misc -t binfmt_misc /proc/sys/fs/binfmt_misc 

To actually register a new binary type, you have to set up a string looking like
:name:type:offset:magic:mask:interpreter:flags (where you can choose the ':' upon
your needs) and echo it to /proc/sys/fs/binfmt_misc/register.
Here is what the fields mean:
 - 'name' is an identifier string. A new /proc file will be created with this
   name below /proc/sys/fs/binfmt_misc
 - 'type' is the type of recognition. Give 'M' for magic and 'E' for extension.
 - 'offset' is the offset of the magic/mask in the file, counted in bytes. This
   defaults to 0 if you omit it (i.e. you write ':name:type::magic...')
 - 'magic' is the byte sequence binfmt_misc is matching for. The magic string
   may contain hex-encoded characters like \x0a or \xA4. In a shell environment
   you will have to write \\x0a to prevent the shell from eating your \.
   If you chose filename extension matching, this is the extension to be
   recognised (without the '.', the \x0a specials are not allowed). Extension
   matching is case sensitive!
 - 'mask' is an (optional, defaults to all 0xff) mask. You can mask out some
   bits from matching by supplying a string like magic and as long as magic.
   The mask is anded with the byte sequence of the file.
 - 'interpreter' is the program that should be invoked with the binary as first
   argument (specify the full path)
 - 'flags' is an optional field that controls several aspects of the invocation
   of the interpreter. It is a string of capital letters, each controls a certain
   aspect. The following flags are supported -
      'P' - preserve-argv[0].  Legacy behavior of binfmt_misc is to overwrite the
            original argv[0] with the full path to the binary.  When this flag is
            included, binfmt_misc will add an argument to the argument vector for
            this purpose, thus preserving the original argv[0].
      'O' - open-binary. Legacy behavior of binfmt_misc is to pass the full path
            of the binary to the interpreter as an argument. When this flag is
            included, binfmt_misc will open the file for reading and pass its
            descriptor as an argument, instead of the full path, thus allowing
            the interpreter to execute non-readable binaries. This feature should
            be used with care - the interpreter has to be trusted not to emit
            the contents of the non-readable binary.
      'C' - credentials. Currently, the behavior of binfmt_misc is to calculate
            the credentials and security token of the new process according to
            the interpreter. When this flag is included, these attributes are
            calculated according to the binary. It also implies the 'O' flag.
            This feature should be used with care as the interpreter
            will run with root permissions when a setuid binary owned by root
            is run with binfmt_misc.


There are some restrictions:
 - the whole register string may not exceed 255 characters
 - the magic must reside in the first 128 bytes of the file, i.e.
   offset+size(magic) has to be less than 128
 - the interpreter string may not exceed 127 characters

To use binfmt_misc you have to mount it first. You can mount it with
"mount -t binfmt_misc none /proc/sys/fs/binfmt_misc" command, or you can add
a line "none  /proc/sys/fs/binfmt_misc binfmt_misc defaults 0 0" to your
/etc/fstab so it auto mounts on boot.

You may want to add the binary formats in one of your /etc/rc scripts during
boot-up. Read the manual of your init program to figure out how to do this
right.

Think about the order of adding entries! Later added entries are matched first!


A few examples (assumed you are in /proc/sys/fs/binfmt_misc):

- enable support for em86 (like binfmt_em86, for Alpha AXP only):
  echo ':i386:M::\x7fELF\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x03:\xff\xff\xff\xff\xff\xfe\xfe\xff\xff\xff\xff\xff\xff\xff\xff\xff\xfb\xff\xff:/bin/em86:' > register
  echo ':i486:M::\x7fELF\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x06:\xff\xff\xff\xff\xff\xfe\xfe\xff\xff\xff\xff\xff\xff\xff\xff\xff\xfb\xff\xff:/bin/em86:' > register

- enable support for packed DOS applications (pre-configured dosemu hdimages):
  echo ':DEXE:M::\x0eDEX::/usr/bin/dosexec:' > register

- enable support for Windows executables using wine:
  echo ':DOSWin:M::MZ::/usr/local/bin/wine:' > register

For java support see Documentation/java.txt


You can enable/disable binfmt_misc or one binary type by echoing 0 (to disable)
or 1 (to enable) to /proc/sys/fs/binfmt_misc/status or /proc/.../the_name.
Catting the file tells you the current status of binfmt_misc/the entry.

You can remove one entry or all entries by echoing -1 to /proc/.../the_name
or /proc/sys/fs/binfmt_misc/status.


HINTS:
======

If you want to pass special arguments to your interpreter, you can
write a wrapper script for it. See Documentation/java.txt for an
example.

Your interpreter should NOT look in the PATH for the filename; the kernel
passes it the full filename (or the file descriptor) to use.  Using $PATH can
cause unexpected behaviour and can be a security hazard.


There is a web page about binfmt_misc at
http://www.tat.physik.uni-tuebingen.de

Richard Günther <rguenth@tat.physik.uni-tuebingen.de>
                       Linux Braille Console

To get early boot messages on a braille device (before userspace screen
readers can start), you first need to compile the support for the usual serial
console (see serial-console.txt), and for braille device (in Device Drivers -
Accessibility).

Then you need to specify a console=brl, option on the kernel command line, the
format is:

	console=brl,serial_options...

where serial_options... are the same as described in serial-console.txt

So for instance you can use console=brl,ttyS0 if the braille device is connected
to the first serial port, and console=brl,ttyS0,115200 to override the baud rate
to 115200, etc.

By default, the braille device will just show the last kernel message (console
mode).  To review previous messages, press the Insert key to switch to the VT
review mode.  In review mode, the arrow keys permit to browse in the VT content,
page up/down keys go at the top/bottom of the screen, and the home key goes back
to the cursor, hence providing very basic screen reviewing facility.

Sound feedback can be obtained by adding the braille_console.sound=1 kernel
parameter.

For simplicity, only one braille console can be enabled, other uses of
console=brl,... will be discarded.  Also note that it does not interfere with
the console selection mechanism described in serial-console.txt

For now, only the VisioBraille device is supported.

Samuel Thibault <samuel.thibault@ens-lyon.org>
===============================================================
==  BT8XXGPIO driver                                         ==
==                                                           ==
==  A driver for a selfmade cheap BT8xx based PCI GPIO-card  ==
==                                                           ==
==  For advanced documentation, see                          ==
==  http://www.bu3sch.de/btgpio.php                          ==
===============================================================


A generic digital 24-port PCI GPIO card can be built out of an ordinary
Brooktree bt848, bt849, bt878 or bt879 based analog TV tuner card. The
Brooktree chip is used in old analog Hauppauge WinTV PCI cards. You can easily
find them used for low prices on the net.

The bt8xx chip does have 24 digital GPIO ports.
These ports are accessible via 24 pins on the SMD chip package.


==============================================
==  How to physically access the GPIO pins  ==
==============================================

The are several ways to access these pins. One might unsolder the whole chip
and put it on a custom PCI board, or one might only unsolder each individual
GPIO pin and solder that to some tiny wire. As the chip package really is tiny
there are some advanced soldering skills needed in any case.

The physical pinouts are drawn in the following ASCII art.
The GPIO pins are marked with G00-G23

                                           G G G G G G G G G G G G     G G G G G G
                                           0 0 0 0 0 0 0 0 0 0 1 1     1 1 1 1 1 1
                                           0 1 2 3 4 5 6 7 8 9 0 1     2 3 4 5 6 7
           | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |
           ---------------------------------------------------------------------------
         --|                               ^                                     ^   |--
         --|                               pin 86                           pin 67   |--
         --|                                                                         |--
         --|                                                               pin 61 >  |-- G18
         --|                                                                         |-- G19
         --|                                                                         |-- G20
         --|                                                                         |-- G21
         --|                                                                         |-- G22
         --|                                                               pin 56 >  |-- G23
         --|                                                                         |--
         --|                           Brooktree 878/879                             |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|                                                                         |--
         --|   O                                                                     |--
         --|                                                                         |--
           ---------------------------------------------------------------------------
           | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |
           ^
           This is pin 1

=======================================================================
		README for btmrvl driver
=======================================================================


All commands are used via debugfs interface.

=====================
Set/get driver configurations:

Path:	/debug/btmrvl/config/

gpiogap=[n]
hscfgcmd
	These commands are used to configure the host sleep parameters.
	bit 8:0  -- Gap
	bit 16:8 -- GPIO

	where GPIO is the pin number of GPIO used to wake up the host.
	It could be any valid GPIO pin# (e.g. 0-7) or 0xff (SDIO interface
	wakeup will be used instead).

	where Gap is the gap in milli seconds between wakeup signal and
	wakeup event, or 0xff for special host sleep setting.

	Usage:
		# Use SDIO interface to wake up the host and set GAP to 0x80:
		echo 0xff80 > /debug/btmrvl/config/gpiogap
		echo 1 > /debug/btmrvl/config/hscfgcmd

		# Use GPIO pin #3 to wake up the host and set GAP to 0xff:
		echo 0x03ff >  /debug/btmrvl/config/gpiogap
		echo 1 > /debug/btmrvl/config/hscfgcmd

psmode=[n]
pscmd
	These commands are used to enable/disable auto sleep mode

	where the option is:
			1 	-- Enable auto sleep mode
			0 	-- Disable auto sleep mode

	Usage:
		# Enable auto sleep mode
		echo 1 > /debug/btmrvl/config/psmode
		echo 1 > /debug/btmrvl/config/pscmd

		# Disable auto sleep mode
		echo 0 > /debug/btmrvl/config/psmode
		echo 1 > /debug/btmrvl/config/pscmd


hsmode=[n]
hscmd
	These commands are used to enable host sleep or wake up firmware

	where the option is:
			1	-- Enable host sleep
			0	-- Wake up firmware

	Usage:
		# Enable host sleep
		echo 1 > /debug/btmrvl/config/hsmode
		echo 1 > /debug/btmrvl/config/hscmd

		# Wake up firmware
		echo 0 > /debug/btmrvl/config/hsmode
		echo 1 > /debug/btmrvl/config/hscmd


======================
Get driver status:

Path:	/debug/btmrvl/status/

Usage:
	cat /debug/btmrvl/status/<args>

where the args are:

curpsmode
	This command displays current auto sleep status.

psstate
	This command display the power save state.

hsstate
	This command display the host sleep state.

txdnldrdy
	This command displays the value of Tx download ready flag.


=====================

Use hcitool to issue raw hci command, refer to hcitool manual

	Usage: Hcitool cmd <ogf> <ocf> [Parameters]

	Interface Control Command
	hcitool cmd 0x3f 0x5b 0xf5 0x01 0x00    --Enable All interface
	hcitool cmd 0x3f 0x5b 0xf5 0x01 0x01    --Enable Wlan interface
	hcitool cmd 0x3f 0x5b 0xf5 0x01 0x02    --Enable BT interface
	hcitool cmd 0x3f 0x5b 0xf5 0x00 0x00    --Disable All interface
	hcitool cmd 0x3f 0x5b 0xf5 0x00 0x01    --Disable Wlan interface
	hcitool cmd 0x3f 0x5b 0xf5 0x00 0x02    --Disable BT interface

=======================================================================


SD8688 firmware:

/lib/firmware/sd8688_helper.bin
/lib/firmware/sd8688.bin


The images can be downloaded from:

git.infradead.org/users/dwmw2/linux-firmware.git/libertas/
[ NOTE: The virt_to_bus() and bus_to_virt() functions have been
	superseded by the functionality provided by the PCI DMA interface
	(see Documentation/DMA-API-HOWTO.txt).  They continue
	to be documented below for historical purposes, but new code
	must not use them. --davidm 00/12/12 ]

[ This is a mail message in response to a query on IO mapping, thus the
  strange format for a "document" ]

The AHA-1542 is a bus-master device, and your patch makes the driver give the
controller the physical address of the buffers, which is correct on x86
(because all bus master devices see the physical memory mappings directly). 

However, on many setups, there are actually _three_ different ways of looking
at memory addresses, and in this case we actually want the third, the
so-called "bus address". 

Essentially, the three ways of addressing memory are (this is "real memory",
that is, normal RAM--see later about other details): 

 - CPU untranslated.  This is the "physical" address.  Physical address 
   0 is what the CPU sees when it drives zeroes on the memory bus.

 - CPU translated address. This is the "virtual" address, and is 
   completely internal to the CPU itself with the CPU doing the appropriate
   translations into "CPU untranslated". 

 - bus address. This is the address of memory as seen by OTHER devices, 
   not the CPU. Now, in theory there could be many different bus 
   addresses, with each device seeing memory in some device-specific way, but
   happily most hardware designers aren't actually actively trying to make
   things any more complex than necessary, so you can assume that all 
   external hardware sees the memory the same way. 

Now, on normal PCs the bus address is exactly the same as the physical
address, and things are very simple indeed. However, they are that simple
because the memory and the devices share the same address space, and that is
not generally necessarily true on other PCI/ISA setups. 

Now, just as an example, on the PReP (PowerPC Reference Platform), the 
CPU sees a memory map something like this (this is from memory):

	0-2 GB		"real memory"
	2 GB-3 GB	"system IO" (inb/out and similar accesses on x86)
	3 GB-4 GB 	"IO memory" (shared memory over the IO bus)

Now, that looks simple enough. However, when you look at the same thing from
the viewpoint of the devices, you have the reverse, and the physical memory
address 0 actually shows up as address 2 GB for any IO master.

So when the CPU wants any bus master to write to physical memory 0, it 
has to give the master address 0x80000000 as the memory address.

So, for example, depending on how the kernel is actually mapped on the 
PPC, you can end up with a setup like this:

 physical address:	0
 virtual address:	0xC0000000
 bus address:		0x80000000

where all the addresses actually point to the same thing.  It's just seen 
through different translations..

Similarly, on the Alpha, the normal translation is

 physical address:	0
 virtual address:	0xfffffc0000000000
 bus address:		0x40000000

(but there are also Alphas where the physical address and the bus address
are the same). 

Anyway, the way to look up all these translations, you do

	#include <asm/io.h>

	phys_addr = virt_to_phys(virt_addr);
	virt_addr = phys_to_virt(phys_addr);
	 bus_addr = virt_to_bus(virt_addr);
	virt_addr = bus_to_virt(bus_addr);

Now, when do you need these?

You want the _virtual_ address when you are actually going to access that 
pointer from the kernel. So you can have something like this:

	/*
	 * this is the hardware "mailbox" we use to communicate with
	 * the controller. The controller sees this directly.
	 */
	struct mailbox {
		__u32 status;
		__u32 bufstart;
		__u32 buflen;
		..
	} mbox;

		unsigned char * retbuffer;

		/* get the address from the controller */
		retbuffer = bus_to_virt(mbox.bufstart);
		switch (retbuffer[0]) {
			case STATUS_OK:
				...

on the other hand, you want the bus address when you have a buffer that 
you want to give to the controller:

	/* ask the controller to read the sense status into "sense_buffer" */
	mbox.bufstart = virt_to_bus(&sense_buffer);
	mbox.buflen = sizeof(sense_buffer);
	mbox.status = 0;
	notify_controller(&mbox);

And you generally _never_ want to use the physical address, because you can't
use that from the CPU (the CPU only uses translated virtual addresses), and
you can't use it from the bus master. 

So why do we care about the physical address at all? We do need the physical
address in some cases, it's just not very often in normal code.  The physical
address is needed if you use memory mappings, for example, because the
"remap_pfn_range()" mm function wants the physical address of the memory to
be remapped as measured in units of pages, a.k.a. the pfn (the memory
management layer doesn't know about devices outside the CPU, so it
shouldn't need to know about "bus addresses" etc).

NOTE NOTE NOTE! The above is only one part of the whole equation. The above
only talks about "real memory", that is, CPU memory (RAM). 

There is a completely different type of memory too, and that's the "shared
memory" on the PCI or ISA bus. That's generally not RAM (although in the case
of a video graphics card it can be normal DRAM that is just used for a frame
buffer), but can be things like a packet buffer in a network card etc. 

This memory is called "PCI memory" or "shared memory" or "IO memory" or
whatever, and there is only one way to access it: the readb/writeb and
related functions. You should never take the address of such memory, because
there is really nothing you can do with such an address: it's not
conceptually in the same memory space as "real memory" at all, so you cannot
just dereference a pointer. (Sadly, on x86 it _is_ in the same memory space,
so on x86 it actually works to just deference a pointer, but it's not
portable). 

For such memory, you can do things like

 - reading:
	/*
	 * read first 32 bits from ISA memory at 0xC0000, aka
	 * C000:0000 in DOS terms
	 */
	unsigned int signature = isa_readl(0xC0000);

 - remapping and writing:
	/*
	 * remap framebuffer PCI memory area at 0xFC000000,
	 * size 1MB, so that we can access it: We can directly
	 * access only the 640k-1MB area, so anything else
	 * has to be remapped.
	 */
	void __iomem *baseptr = ioremap(0xFC000000, 1024*1024);

	/* write a 'A' to the offset 10 of the area */
	writeb('A',baseptr+10);

	/* unmap when we unload the driver */
	iounmap(baseptr);

 - copying and clearing:
	/* get the 6-byte Ethernet address at ISA address E000:0040 */
	memcpy_fromio(kernel_buffer, 0xE0040, 6);
	/* write a packet to the driver */
	memcpy_toio(0xE1000, skb->data, skb->len);
	/* clear the frame buffer */
	memset_io(0xA0000, 0, 0x10000);

OK, that just about covers the basics of accessing IO portably.  Questions?
Comments? You may think that all the above is overly complex, but one day you
might find yourself with a 500 MHz Alpha in front of you, and then you'll be
happy that your driver works ;)

Note that kernel versions 2.0.x (and earlier) mistakenly called the
ioremap() function "vremap()".  ioremap() is the proper name, but I
didn't think straight when I wrote it originally.  People who have to
support both can do something like:
 
	/* support old naming silliness */
	#if LINUX_VERSION_CODE < 0x020100                                     
	#define ioremap vremap
	#define iounmap vfree                                                     
	#endif
 
at the top of their source files, and then they can use the right names
even on 2.0.x systems. 

And the above sounds worse than it really is.  Most real drivers really
don't do all that complex things (or rather: the complexity is not so
much in the actual IO accesses as in error handling and timeouts etc). 
It's generally not hard to fix drivers, and in many cases the code
actually looks better afterwards:

	unsigned long signature = *(unsigned int *) 0xC0000;
		vs
	unsigned long signature = readl(0xC0000);

I think the second version actually is more readable, no?

		Linus

		Cache and TLB Flushing
		     Under Linux

	    David S. Miller <davem@redhat.com>

This document describes the cache/tlb flushing interfaces called
by the Linux VM subsystem.  It enumerates over each interface,
describes its intended purpose, and what side effect is expected
after the interface is invoked.

The side effects described below are stated for a uniprocessor
implementation, and what is to happen on that single processor.  The
SMP cases are a simple extension, in that you just extend the
definition such that the side effect for a particular interface occurs
on all processors in the system.  Don't let this scare you into
thinking SMP cache/tlb flushing must be so inefficient, this is in
fact an area where many optimizations are possible.  For example,
if it can be proven that a user address space has never executed
on a cpu (see mm_cpumask()), one need not perform a flush
for this address space on that cpu.

First, the TLB flushing interfaces, since they are the simplest.  The
"TLB" is abstracted under Linux as something the cpu uses to cache
virtual-->physical address translations obtained from the software
page tables.  Meaning that if the software page tables change, it is
possible for stale translations to exist in this "TLB" cache.
Therefore when software page table changes occur, the kernel will
invoke one of the following flush methods _after_ the page table
changes occur:

1) void flush_tlb_all(void)

	The most severe flush of all.  After this interface runs,
	any previous page table modification whatsoever will be
	visible to the cpu.

	This is usually invoked when the kernel page tables are
	changed, since such translations are "global" in nature.

2) void flush_tlb_mm(struct mm_struct *mm)

	This interface flushes an entire user address space from
	the TLB.  After running, this interface must make sure that
	any previous page table modifications for the address space
	'mm' will be visible to the cpu.  That is, after running,
	there will be no entries in the TLB for 'mm'.

	This interface is used to handle whole address space
	page table operations such as what happens during
	fork, and exec.

3) void flush_tlb_range(struct vm_area_struct *vma,
			unsigned long start, unsigned long end)

	Here we are flushing a specific range of (user) virtual
	address translations from the TLB.  After running, this
	interface must make sure that any previous page table
	modifications for the address space 'vma->vm_mm' in the range
	'start' to 'end-1' will be visible to the cpu.  That is, after
	running, there will be no entries in the TLB for 'mm' for
	virtual addresses in the range 'start' to 'end-1'.

	The "vma" is the backing store being used for the region.
	Primarily, this is used for munmap() type operations.

	The interface is provided in hopes that the port can find
	a suitably efficient method for removing multiple page
	sized translations from the TLB, instead of having the kernel
	call flush_tlb_page (see below) for each entry which may be
	modified.

4) void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr)

	This time we need to remove the PAGE_SIZE sized translation
	from the TLB.  The 'vma' is the backing structure used by
	Linux to keep track of mmap'd regions for a process, the
	address space is available via vma->vm_mm.  Also, one may
	test (vma->vm_flags & VM_EXEC) to see if this region is
	executable (and thus could be in the 'instruction TLB' in
	split-tlb type setups).

	After running, this interface must make sure that any previous
	page table modification for address space 'vma->vm_mm' for
	user virtual address 'addr' will be visible to the cpu.  That
	is, after running, there will be no entries in the TLB for
	'vma->vm_mm' for virtual address 'addr'.

	This is used primarily during fault processing.

5) void update_mmu_cache(struct vm_area_struct *vma,
			 unsigned long address, pte_t *ptep)

	At the end of every page fault, this routine is invoked to
	tell the architecture specific code that a translation
	now exists at virtual address "address" for address space
	"vma->vm_mm", in the software page tables.

	A port may use this information in any way it so chooses.
	For example, it could use this event to pre-load TLB
	translations for software managed TLB configurations.
	The sparc64 port currently does this.

6) void tlb_migrate_finish(struct mm_struct *mm)

	This interface is called at the end of an explicit
	process migration. This interface provides a hook
	to allow a platform to update TLB or context-specific
	information for the address space.

	The ia64 sn2 platform is one example of a platform
	that uses this interface.

Next, we have the cache flushing interfaces.  In general, when Linux
is changing an existing virtual-->physical mapping to a new value,
the sequence will be in one of the following forms:

	1) flush_cache_mm(mm);
	   change_all_page_tables_of(mm);
	   flush_tlb_mm(mm);

	2) flush_cache_range(vma, start, end);
	   change_range_of_page_tables(mm, start, end);
	   flush_tlb_range(vma, start, end);

	3) flush_cache_page(vma, addr, pfn);
	   set_pte(pte_pointer, new_pte_val);
	   flush_tlb_page(vma, addr);

The cache level flush will always be first, because this allows
us to properly handle systems whose caches are strict and require
a virtual-->physical translation to exist for a virtual address
when that virtual address is flushed from the cache.  The HyperSparc
cpu is one such cpu with this attribute.

The cache flushing routines below need only deal with cache flushing
to the extent that it is necessary for a particular cpu.  Mostly,
these routines must be implemented for cpus which have virtually
indexed caches which must be flushed when virtual-->physical
translations are changed or removed.  So, for example, the physically
indexed physically tagged caches of IA32 processors have no need to
implement these interfaces since the caches are fully synchronized
and have no dependency on translation information.

Here are the routines, one by one:

1) void flush_cache_mm(struct mm_struct *mm)

	This interface flushes an entire user address space from
	the caches.  That is, after running, there will be no cache
	lines associated with 'mm'.

	This interface is used to handle whole address space
	page table operations such as what happens during exit and exec.

2) void flush_cache_dup_mm(struct mm_struct *mm)

	This interface flushes an entire user address space from
	the caches.  That is, after running, there will be no cache
	lines associated with 'mm'.

	This interface is used to handle whole address space
	page table operations such as what happens during fork.

	This option is separate from flush_cache_mm to allow some
	optimizations for VIPT caches.

3) void flush_cache_range(struct vm_area_struct *vma,
			  unsigned long start, unsigned long end)

	Here we are flushing a specific range of (user) virtual
	addresses from the cache.  After running, there will be no
	entries in the cache for 'vma->vm_mm' for virtual addresses in
	the range 'start' to 'end-1'.

	The "vma" is the backing store being used for the region.
	Primarily, this is used for munmap() type operations.

	The interface is provided in hopes that the port can find
	a suitably efficient method for removing multiple page
	sized regions from the cache, instead of having the kernel
	call flush_cache_page (see below) for each entry which may be
	modified.

4) void flush_cache_page(struct vm_area_struct *vma, unsigned long addr, unsigned long pfn)

	This time we need to remove a PAGE_SIZE sized range
	from the cache.  The 'vma' is the backing structure used by
	Linux to keep track of mmap'd regions for a process, the
	address space is available via vma->vm_mm.  Also, one may
	test (vma->vm_flags & VM_EXEC) to see if this region is
	executable (and thus could be in the 'instruction cache' in
	"Harvard" type cache layouts).

	The 'pfn' indicates the physical page frame (shift this value
	left by PAGE_SHIFT to get the physical address) that 'addr'
	translates to.  It is this mapping which should be removed from
	the cache.

	After running, there will be no entries in the cache for
	'vma->vm_mm' for virtual address 'addr' which translates
	to 'pfn'.

	This is used primarily during fault processing.

5) void flush_cache_kmaps(void)

	This routine need only be implemented if the platform utilizes
	highmem.  It will be called right before all of the kmaps
	are invalidated.

	After running, there will be no entries in the cache for
	the kernel virtual address range PKMAP_ADDR(0) to
	PKMAP_ADDR(LAST_PKMAP).

	This routing should be implemented in asm/highmem.h

6) void flush_cache_vmap(unsigned long start, unsigned long end)
   void flush_cache_vunmap(unsigned long start, unsigned long end)

	Here in these two interfaces we are flushing a specific range
	of (kernel) virtual addresses from the cache.  After running,
	there will be no entries in the cache for the kernel address
	space for virtual addresses in the range 'start' to 'end-1'.

	The first of these two routines is invoked after map_vm_area()
	has installed the page table entries.  The second is invoked
	before unmap_kernel_range() deletes the page table entries.

There exists another whole class of cpu cache issues which currently
require a whole different set of interfaces to handle properly.
The biggest problem is that of virtual aliasing in the data cache
of a processor.

Is your port susceptible to virtual aliasing in its D-cache?
Well, if your D-cache is virtually indexed, is larger in size than
PAGE_SIZE, and does not prevent multiple cache lines for the same
physical address from existing at once, you have this problem.

If your D-cache has this problem, first define asm/shmparam.h SHMLBA
properly, it should essentially be the size of your virtually
addressed D-cache (or if the size is variable, the largest possible
size).  This setting will force the SYSv IPC layer to only allow user
processes to mmap shared memory at address which are a multiple of
this value.

NOTE: This does not fix shared mmaps, check out the sparc64 port for
one way to solve this (in particular SPARC_FLAG_MMAPSHARED).

Next, you have to solve the D-cache aliasing issue for all
other cases.  Please keep in mind that fact that, for a given page
mapped into some user address space, there is always at least one more
mapping, that of the kernel in its linear mapping starting at
PAGE_OFFSET.  So immediately, once the first user maps a given
physical page into its address space, by implication the D-cache
aliasing problem has the potential to exist since the kernel already
maps this page at its virtual address.

  void copy_user_page(void *to, void *from, unsigned long addr, struct page *page)
  void clear_user_page(void *to, unsigned long addr, struct page *page)

	These two routines store data in user anonymous or COW
	pages.  It allows a port to efficiently avoid D-cache alias
	issues between userspace and the kernel.

	For example, a port may temporarily map 'from' and 'to' to
	kernel virtual addresses during the copy.  The virtual address
	for these two pages is chosen in such a way that the kernel
	load/store instructions happen to virtual addresses which are
	of the same "color" as the user mapping of the page.  Sparc64
	for example, uses this technique.

	The 'addr' parameter tells the virtual address where the
	user will ultimately have this page mapped, and the 'page'
	parameter gives a pointer to the struct page of the target.

	If D-cache aliasing is not an issue, these two routines may
	simply call memcpy/memset directly and do nothing more.

  void flush_dcache_page(struct page *page)

	Any time the kernel writes to a page cache page, _OR_
	the kernel is about to read from a page cache page and
	user space shared/writable mappings of this page potentially
	exist, this routine is called.

	NOTE: This routine need only be called for page cache pages
	      which can potentially ever be mapped into the address
	      space of a user process.  So for example, VFS layer code
	      handling vfs symlinks in the page cache need not call
	      this interface at all.

	The phrase "kernel writes to a page cache page" means,
	specifically, that the kernel executes store instructions
	that dirty data in that page at the page->virtual mapping
	of that page.  It is important to flush here to handle
	D-cache aliasing, to make sure these kernel stores are
	visible to user space mappings of that page.

	The corollary case is just as important, if there are users
	which have shared+writable mappings of this file, we must make
	sure that kernel reads of these pages will see the most recent
	stores done by the user.

	If D-cache aliasing is not an issue, this routine may
	simply be defined as a nop on that architecture.

        There is a bit set aside in page->flags (PG_arch_1) as
	"architecture private".  The kernel guarantees that,
	for pagecache pages, it will clear this bit when such
	a page first enters the pagecache.

	This allows these interfaces to be implemented much more
	efficiently.  It allows one to "defer" (perhaps indefinitely)
	the actual flush if there are currently no user processes
	mapping this page.  See sparc64's flush_dcache_page and
	update_mmu_cache implementations for an example of how to go
	about doing this.

	The idea is, first at flush_dcache_page() time, if
	page->mapping->i_mmap is an empty tree and ->i_mmap_nonlinear
	an empty list, just mark the architecture private page flag bit.
	Later, in update_mmu_cache(), a check is made of this flag bit,
	and if set the flush is done and the flag bit is cleared.

	IMPORTANT NOTE: It is often important, if you defer the flush,
			that the actual flush occurs on the same CPU
			as did the cpu stores into the page to make it
			dirty.  Again, see sparc64 for examples of how
			to deal with this.

  void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
                         unsigned long user_vaddr,
                         void *dst, void *src, int len)
  void copy_from_user_page(struct vm_area_struct *vma, struct page *page,
                           unsigned long user_vaddr,
                           void *dst, void *src, int len)
	When the kernel needs to copy arbitrary data in and out
	of arbitrary user pages (f.e. for ptrace()) it will use
	these two routines.

	Any necessary cache flushing or other coherency operations
	that need to occur should happen here.  If the processor's
	instruction cache does not snoop cpu stores, it is very
	likely that you will need to flush the instruction cache
	for copy_to_user_page().

  void flush_anon_page(struct vm_area_struct *vma, struct page *page,
                       unsigned long vmaddr)
  	When the kernel needs to access the contents of an anonymous
	page, it calls this function (currently only
	get_user_pages()).  Note: flush_dcache_page() deliberately
	doesn't work for an anonymous page.  The default
	implementation is a nop (and should remain so for all coherent
	architectures).  For incoherent architectures, it should flush
	the cache of the page at vmaddr.

  void flush_kernel_dcache_page(struct page *page)
	When the kernel needs to modify a user page is has obtained
	with kmap, it calls this function after all modifications are
	complete (but before kunmapping it) to bring the underlying
	page up to date.  It is assumed here that the user has no
	incoherent cached copies (i.e. the original page was obtained
	from a mechanism like get_user_pages()).  The default
	implementation is a nop and should remain so on all coherent
	architectures.  On incoherent architectures, this should flush
	the kernel cache for page (using page_address(page)).


  void flush_icache_range(unsigned long start, unsigned long end)
  	When the kernel stores into addresses that it will execute
	out of (eg when loading modules), this function is called.

	If the icache does not snoop stores then this routine will need
	to flush it.

  void flush_icache_page(struct vm_area_struct *vma, struct page *page)
	All the functionality of flush_icache_page can be implemented in
	flush_dcache_page and update_mmu_cache. In the future, the hope
	is to remove this interface completely.

The final category of APIs is for I/O to deliberately aliased address
ranges inside the kernel.  Such aliases are set up by use of the
vmap/vmalloc API.  Since kernel I/O goes via physical pages, the I/O
subsystem assumes that the user mapping and kernel offset mapping are
the only aliases.  This isn't true for vmap aliases, so anything in
the kernel trying to do I/O to vmap areas must manually manage
coherency.  It must do this by flushing the vmap range before doing
I/O and invalidating it after the I/O returns.

  void flush_kernel_vmap_range(void *vaddr, int size)
       flushes the kernel cache for a given virtual address range in
       the vmap area.  This is to make sure that any data the kernel
       modified in the vmap range is made visible to the physical
       page.  The design is to make this area safe to perform I/O on.
       Note that this API does *not* also flush the offset map alias
       of the area.

  void invalidate_kernel_vmap_range(void *vaddr, int size) invalidates
       the cache for a given virtual address range in the vmap area
       which prevents the processor from making the cache stale by
       speculatively reading data while the I/O was occurring to the
       physical pages.  This is only necessary for data reads into the
       vmap area.
			       ================
			       CIRCULAR BUFFERS
			       ================

By: David Howells <dhowells@redhat.com>
    Paul E. McKenney <paulmck@linux.vnet.ibm.com>


Linux provides a number of features that can be used to implement circular
buffering.  There are two sets of such features:

 (1) Convenience functions for determining information about power-of-2 sized
     buffers.

 (2) Memory barriers for when the producer and the consumer of objects in the
     buffer don't want to share a lock.

To use these facilities, as discussed below, there needs to be just one
producer and just one consumer.  It is possible to handle multiple producers by
serialising them, and to handle multiple consumers by serialising them.


Contents:

 (*) What is a circular buffer?

 (*) Measuring power-of-2 buffers.

 (*) Using memory barriers with circular buffers.
     - The producer.
     - The consumer.


==========================
WHAT IS A CIRCULAR BUFFER?
==========================

First of all, what is a circular buffer?  A circular buffer is a buffer of
fixed, finite size into which there are two indices:

 (1) A 'head' index - the point at which the producer inserts items into the
     buffer.

 (2) A 'tail' index - the point at which the consumer finds the next item in
     the buffer.

Typically when the tail pointer is equal to the head pointer, the buffer is
empty; and the buffer is full when the head pointer is one less than the tail
pointer.

The head index is incremented when items are added, and the tail index when
items are removed.  The tail index should never jump the head index, and both
indices should be wrapped to 0 when they reach the end of the buffer, thus
allowing an infinite amount of data to flow through the buffer.

Typically, items will all be of the same unit size, but this isn't strictly
required to use the techniques below.  The indices can be increased by more
than 1 if multiple items or variable-sized items are to be included in the
buffer, provided that neither index overtakes the other.  The implementer must
be careful, however, as a region more than one unit in size may wrap the end of
the buffer and be broken into two segments.


============================
MEASURING POWER-OF-2 BUFFERS
============================

Calculation of the occupancy or the remaining capacity of an arbitrarily sized
circular buffer would normally be a slow operation, requiring the use of a
modulus (divide) instruction.  However, if the buffer is of a power-of-2 size,
then a much quicker bitwise-AND instruction can be used instead.

Linux provides a set of macros for handling power-of-2 circular buffers.  These
can be made use of by:

	#include <linux/circ_buf.h>

The macros are:

 (*) Measure the remaining capacity of a buffer:

	CIRC_SPACE(head_index, tail_index, buffer_size);

     This returns the amount of space left in the buffer[1] into which items
     can be inserted.


 (*) Measure the maximum consecutive immediate space in a buffer:

	CIRC_SPACE_TO_END(head_index, tail_index, buffer_size);

     This returns the amount of consecutive space left in the buffer[1] into
     which items can be immediately inserted without having to wrap back to the
     beginning of the buffer.


 (*) Measure the occupancy of a buffer:

	CIRC_CNT(head_index, tail_index, buffer_size);

     This returns the number of items currently occupying a buffer[2].


 (*) Measure the non-wrapping occupancy of a buffer:

	CIRC_CNT_TO_END(head_index, tail_index, buffer_size);

     This returns the number of consecutive items[2] that can be extracted from
     the buffer without having to wrap back to the beginning of the buffer.


Each of these macros will nominally return a value between 0 and buffer_size-1,
however:

 [1] CIRC_SPACE*() are intended to be used in the producer.  To the producer
     they will return a lower bound as the producer controls the head index,
     but the consumer may still be depleting the buffer on another CPU and
     moving the tail index.

     To the consumer it will show an upper bound as the producer may be busy
     depleting the space.

 [2] CIRC_CNT*() are intended to be used in the consumer.  To the consumer they
     will return a lower bound as the consumer controls the tail index, but the
     producer may still be filling the buffer on another CPU and moving the
     head index.

     To the producer it will show an upper bound as the consumer may be busy
     emptying the buffer.

 [3] To a third party, the order in which the writes to the indices by the
     producer and consumer become visible cannot be guaranteed as they are
     independent and may be made on different CPUs - so the result in such a
     situation will merely be a guess, and may even be negative.


===========================================
USING MEMORY BARRIERS WITH CIRCULAR BUFFERS
===========================================

By using memory barriers in conjunction with circular buffers, you can avoid
the need to:

 (1) use a single lock to govern access to both ends of the buffer, thus
     allowing the buffer to be filled and emptied at the same time; and

 (2) use atomic counter operations.

There are two sides to this: the producer that fills the buffer, and the
consumer that empties it.  Only one thing should be filling a buffer at any one
time, and only one thing should be emptying a buffer at any one time, but the
two sides can operate simultaneously.


THE PRODUCER
------------

The producer will look something like this:

	spin_lock(&producer_lock);

	unsigned long head = buffer->head;
	/* The spin_unlock() and next spin_lock() provide needed ordering. */
	unsigned long tail = ACCESS_ONCE(buffer->tail);

	if (CIRC_SPACE(head, tail, buffer->size) >= 1) {
		/* insert one item into the buffer */
		struct item *item = buffer[head];

		produce_item(item);

		smp_store_release(buffer->head,
				  (head + 1) & (buffer->size - 1));

		/* wake_up() will make sure that the head is committed before
		 * waking anyone up */
		wake_up(consumer);
	}

	spin_unlock(&producer_lock);

This will instruct the CPU that the contents of the new item must be written
before the head index makes it available to the consumer and then instructs the
CPU that the revised head index must be written before the consumer is woken.

Note that wake_up() does not guarantee any sort of barrier unless something
is actually awakened.  We therefore cannot rely on it for ordering.  However,
there is always one element of the array left empty.  Therefore, the
producer must produce two elements before it could possibly corrupt the
element currently being read by the consumer.  Therefore, the unlock-lock
pair between consecutive invocations of the consumer provides the necessary
ordering between the read of the index indicating that the consumer has
vacated a given element and the write by the producer to that same element.


THE CONSUMER
------------

The consumer will look something like this:

	spin_lock(&consumer_lock);

	/* Read index before reading contents at that index. */
	unsigned long head = smp_load_acquire(buffer->head);
	unsigned long tail = buffer->tail;

	if (CIRC_CNT(head, tail, buffer->size) >= 1) {

		/* extract one item from the buffer */
		struct item *item = buffer[tail];

		consume_item(item);

		/* Finish reading descriptor before incrementing tail. */
		smp_store_release(buffer->tail,
				  (tail + 1) & (buffer->size - 1));
	}

	spin_unlock(&consumer_lock);

This will instruct the CPU to make sure the index is up to date before reading
the new item, and then it shall make sure the CPU has finished reading the item
before it writes the new tail pointer, which will erase the item.

Note the use of ACCESS_ONCE() and smp_load_acquire() to read the
opposition index.  This prevents the compiler from discarding and
reloading its cached value - which some compilers will do across
smp_read_barrier_depends().  This isn't strictly needed if you can
be sure that the opposition index will _only_ be used the once.
The smp_load_acquire() additionally forces the CPU to order against
subsequent memory references.  Similarly, smp_store_release() is used
in both algorithms to write the thread's index.  This documents the
fact that we are writing to something that can be read concurrently,
prevents the compiler from tearing the store, and enforces ordering
against previous accesses.


===============
FURTHER READING
===============

See also Documentation/memory-barriers.txt for a description of Linux's memory
barrier facilities.
		The Common Clk Framework
		Mike Turquette <mturquette@ti.com>

This document endeavours to explain the common clk framework details,
and how to port a platform over to this framework.  It is not yet a
detailed explanation of the clock api in include/linux/clk.h, but
perhaps someday it will include that information.

	Part 1 - introduction and interface split

The common clk framework is an interface to control the clock nodes
available on various devices today.  This may come in the form of clock
gating, rate adjustment, muxing or other operations.  This framework is
enabled with the CONFIG_COMMON_CLK option.

The interface itself is divided into two halves, each shielded from the
details of its counterpart.  First is the common definition of struct
clk which unifies the framework-level accounting and infrastructure that
has traditionally been duplicated across a variety of platforms.  Second
is a common implementation of the clk.h api, defined in
drivers/clk/clk.c.  Finally there is struct clk_ops, whose operations
are invoked by the clk api implementation.

The second half of the interface is comprised of the hardware-specific
callbacks registered with struct clk_ops and the corresponding
hardware-specific structures needed to model a particular clock.  For
the remainder of this document any reference to a callback in struct
clk_ops, such as .enable or .set_rate, implies the hardware-specific
implementation of that code.  Likewise, references to struct clk_foo
serve as a convenient shorthand for the implementation of the
hardware-specific bits for the hypothetical "foo" hardware.

Tying the two halves of this interface together is struct clk_hw, which
is defined in struct clk_foo and pointed to within struct clk.  This
allows for easy navigation between the two discrete halves of the common
clock interface.

	Part 2 - common data structures and api

Below is the common struct clk definition from
include/linux/clk-private.h, modified for brevity:

	struct clk {
		const char		*name;
		const struct clk_ops	*ops;
		struct clk_hw		*hw;
		char			**parent_names;
		struct clk		**parents;
		struct clk		*parent;
		struct hlist_head	children;
		struct hlist_node	child_node;
		...
	};

The members above make up the core of the clk tree topology.  The clk
api itself defines several driver-facing functions which operate on
struct clk.  That api is documented in include/linux/clk.h.

Platforms and devices utilizing the common struct clk use the struct
clk_ops pointer in struct clk to perform the hardware-specific parts of
the operations defined in clk.h:

	struct clk_ops {
		int		(*prepare)(struct clk_hw *hw);
		void		(*unprepare)(struct clk_hw *hw);
		int		(*enable)(struct clk_hw *hw);
		void		(*disable)(struct clk_hw *hw);
		int		(*is_enabled)(struct clk_hw *hw);
		unsigned long	(*recalc_rate)(struct clk_hw *hw,
						unsigned long parent_rate);
		long		(*round_rate)(struct clk_hw *hw,
						unsigned long rate,
						unsigned long *parent_rate);
		long		(*determine_rate)(struct clk_hw *hw,
						unsigned long rate,
						unsigned long *best_parent_rate,
						struct clk **best_parent_clk);
		int		(*set_parent)(struct clk_hw *hw, u8 index);
		u8		(*get_parent)(struct clk_hw *hw);
		int		(*set_rate)(struct clk_hw *hw,
					    unsigned long rate,
					    unsigned long parent_rate);
		int		(*set_rate_and_parent)(struct clk_hw *hw,
					    unsigned long rate,
					    unsigned long parent_rate,
					    u8 index);
		unsigned long	(*recalc_accuracy)(struct clk_hw *hw,
						unsigned long parent_accuracy);
		void		(*init)(struct clk_hw *hw);
		int		(*debug_init)(struct clk_hw *hw,
					      struct dentry *dentry);
	};

	Part 3 - hardware clk implementations

The strength of the common struct clk comes from its .ops and .hw pointers
which abstract the details of struct clk from the hardware-specific bits, and
vice versa.  To illustrate consider the simple gateable clk implementation in
drivers/clk/clk-gate.c:

struct clk_gate {
	struct clk_hw	hw;
	void __iomem    *reg;
	u8              bit_idx;
	...
};

struct clk_gate contains struct clk_hw hw as well as hardware-specific
knowledge about which register and bit controls this clk's gating.
Nothing about clock topology or accounting, such as enable_count or
notifier_count, is needed here.  That is all handled by the common
framework code and struct clk.

Let's walk through enabling this clk from driver code:

	struct clk *clk;
	clk = clk_get(NULL, "my_gateable_clk");

	clk_prepare(clk);
	clk_enable(clk);

The call graph for clk_enable is very simple:

clk_enable(clk);
	clk->ops->enable(clk->hw);
	[resolves to...]
		clk_gate_enable(hw);
		[resolves struct clk gate with to_clk_gate(hw)]
			clk_gate_set_bit(gate);

And the definition of clk_gate_set_bit:

static void clk_gate_set_bit(struct clk_gate *gate)
{
	u32 reg;

	reg = __raw_readl(gate->reg);
	reg |= BIT(gate->bit_idx);
	writel(reg, gate->reg);
}

Note that to_clk_gate is defined as:

#define to_clk_gate(_hw) container_of(_hw, struct clk_gate, clk)

This pattern of abstraction is used for every clock hardware
representation.

	Part 4 - supporting your own clk hardware

When implementing support for a new type of clock it only necessary to
include the following header:

#include <linux/clk-provider.h>

include/linux/clk.h is included within that header and clk-private.h
must never be included from the code which implements the operations for
a clock.  More on that below in Part 5.

To construct a clk hardware structure for your platform you must define
the following:

struct clk_foo {
	struct clk_hw hw;
	... hardware specific data goes here ...
};

To take advantage of your data you'll need to support valid operations
for your clk:

struct clk_ops clk_foo_ops {
	.enable		= &clk_foo_enable;
	.disable	= &clk_foo_disable;
};

Implement the above functions using container_of:

#define to_clk_foo(_hw) container_of(_hw, struct clk_foo, hw)

int clk_foo_enable(struct clk_hw *hw)
{
	struct clk_foo *foo;

	foo = to_clk_foo(hw);

	... perform magic on foo ...

	return 0;
};

Below is a matrix detailing which clk_ops are mandatory based upon the
hardware capabilities of that clock.  A cell marked as "y" means
mandatory, a cell marked as "n" implies that either including that
callback is invalid or otherwise unnecessary.  Empty cells are either
optional or must be evaluated on a case-by-case basis.

                              clock hardware characteristics
                -----------------------------------------------------------
                | gate | change rate | single parent | multiplexer | root |
                |------|-------------|---------------|-------------|------|
.prepare        |      |             |               |             |      |
.unprepare      |      |             |               |             |      |
                |      |             |               |             |      |
.enable         | y    |             |               |             |      |
.disable        | y    |             |               |             |      |
.is_enabled     | y    |             |               |             |      |
                |      |             |               |             |      |
.recalc_rate    |      | y           |               |             |      |
.round_rate     |      | y [1]       |               |             |      |
.determine_rate |      | y [1]       |               |             |      |
.set_rate       |      | y           |               |             |      |
                |      |             |               |             |      |
.set_parent     |      |             | n             | y           | n    |
.get_parent     |      |             | n             | y           | n    |
                |      |             |               |             |      |
.recalc_accuracy|      |             |               |             |      |
                |      |             |               |             |      |
.init           |      |             |               |             |      |
                -----------------------------------------------------------
[1] either one of round_rate or determine_rate is required.

Finally, register your clock at run-time with a hardware-specific
registration function.  This function simply populates struct clk_foo's
data and then passes the common struct clk parameters to the framework
with a call to:

clk_register(...)

See the basic clock types in drivers/clk/clk-*.c for examples.

	Part 5 - static initialization of clock data

For platforms with many clocks (often numbering into the hundreds) it
may be desirable to statically initialize some clock data.  This
presents a problem since the definition of struct clk should be hidden
from everyone except for the clock core in drivers/clk/clk.c.

To get around this problem struct clk's definition is exposed in
include/linux/clk-private.h along with some macros for more easily
initializing instances of the basic clock types.  These clocks must
still be initialized with the common clock framework via a call to
__clk_init.

clk-private.h must NEVER be included by code which implements struct
clk_ops callbacks, nor must it be included by any logic which pokes
around inside of struct clk at run-time.  To do so is a layering
violation.

To better enforce this policy, always follow this simple rule: any
statically initialized clock data MUST be defined in a separate file
from the logic that implements its ops.  Basically separate the logic
from the data and all is well.

	Part 6 - Disabling clock gating of unused clocks

Sometimes during development it can be useful to be able to bypass the
default disabling of unused clocks. For example, if drivers aren't enabling
clocks properly but rely on them being on from the bootloader, bypassing
the disabling means that the driver will remain functional while the issues
are sorted out.

To bypass this disabling, include "clk_ignore_unused" in the bootargs to the
kernel.

	Part 7 - Locking

The common clock framework uses two global locks, the prepare lock and the
enable lock.

The enable lock is a spinlock and is held across calls to the .enable,
.disable and .is_enabled operations. Those operations are thus not allowed to
sleep, and calls to the clk_enable(), clk_disable() and clk_is_enabled() API
functions are allowed in atomic context.

The prepare lock is a mutex and is held across calls to all other operations.
All those operations are allowed to sleep, and calls to the corresponding API
functions are not allowed in atomic context.

This effectively divides operations in two groups from a locking perspective.

Drivers don't need to manually protect resources shared between the operations
of one group, regardless of whether those resources are shared by multiple
clocks or not. However, access to resources that are shared between operations
of the two groups needs to be protected by the drivers. An example of such a
resource would be a register that controls both the clock rate and the clock
enable/disable state.

The clock framework is reentrant, in that a driver is allowed to call clock
framework functions from within its implementation of clock operations. This
can for instance cause a .set_rate operation of one clock being called from
within the .set_rate operation of another clock. This case must be considered
in the driver implementations, but the code flow is usually controlled by the
driver in that case.

Note that locking must also be considered when code outside of the common
clock framework needs to access resources used by the clock operations. This
is considered out of scope of this document.
Copyright 2010 Nicolas Palix <npalix@diku.dk>
Copyright 2010 Julia Lawall <julia@diku.dk>
Copyright 2010 Gilles Muller <Gilles.Muller@lip6.fr>


 Getting Coccinelle
~~~~~~~~~~~~~~~~~~~~

The semantic patches included in the kernel use features and options
which are provided by Coccinelle version 1.0.0-rc11 and above.
Using earlier versions will fail as the option names used by
the Coccinelle files and coccicheck have been updated.

Coccinelle is available through the package manager
of many distributions, e.g. :

 - Debian
 - Fedora
 - Ubuntu
 - OpenSUSE
 - Arch Linux
 - NetBSD
 - FreeBSD


You can get the latest version released from the Coccinelle homepage at
http://coccinelle.lip6.fr/

Information and tips about Coccinelle are also provided on the wiki
pages at http://cocci.ekstranet.diku.dk/wiki/doku.php

Once you have it, run the following command:

     	./configure
        make

as a regular user, and install it with

        sudo make install

 Using Coccinelle on the Linux kernel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Coccinelle-specific target is defined in the top level
Makefile. This target is named 'coccicheck' and calls the 'coccicheck'
front-end in the 'scripts' directory.

Four basic modes are defined: patch, report, context, and org. The mode to
use is specified by setting the MODE variable with 'MODE=<mode>'.

'patch' proposes a fix, when possible.

'report' generates a list in the following format:
  file:line:column-column: message

'context' highlights lines of interest and their context in a
diff-like style.Lines of interest are indicated with '-'.

'org' generates a report in the Org mode format of Emacs.

Note that not all semantic patches implement all modes. For easy use
of Coccinelle, the default mode is "report".

Two other modes provide some common combinations of these modes.

'chain' tries the previous modes in the order above until one succeeds.

'rep+ctxt' runs successively the report mode and the context mode.
	   It should be used with the C option (described later)
	   which checks the code on a file basis.

Examples:
	To make a report for every semantic patch, run the following command:

		make coccicheck MODE=report

	To produce patches, run:

		make coccicheck MODE=patch


The coccicheck target applies every semantic patch available in the
sub-directories of 'scripts/coccinelle' to the entire Linux kernel.

For each semantic patch, a commit message is proposed.  It gives a
description of the problem being checked by the semantic patch, and
includes a reference to Coccinelle.

As any static code analyzer, Coccinelle produces false
positives. Thus, reports must be carefully checked, and patches
reviewed.

To enable verbose messages set the V= variable, for example:

   make coccicheck MODE=report V=1

By default, coccicheck tries to run as parallel as possible. To change
the parallelism, set the J= variable. For example, to run across 4 CPUs:

   make coccicheck MODE=report J=4


 Using Coccinelle with a single semantic patch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The optional make variable COCCI can be used to check a single
semantic patch. In that case, the variable must be initialized with
the name of the semantic patch to apply.

For instance:

	make coccicheck COCCI=<my_SP.cocci> MODE=patch
or
	make coccicheck COCCI=<my_SP.cocci> MODE=report


 Controlling Which Files are Processed by Coccinelle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
By default the entire kernel source tree is checked.

To apply Coccinelle to a specific directory, M= can be used.
For example, to check drivers/net/wireless/ one may write:

    make coccicheck M=drivers/net/wireless/

To apply Coccinelle on a file basis, instead of a directory basis, the
following command may be used:

    make C=1 CHECK="scripts/coccicheck"

To check only newly edited code, use the value 2 for the C flag, i.e.

    make C=2 CHECK="scripts/coccicheck"

In these modes, which works on a file basis, there is no information
about semantic patches displayed, and no commit message proposed.

This runs every semantic patch in scripts/coccinelle by default. The
COCCI variable may additionally be used to only apply a single
semantic patch as shown in the previous section.

The "report" mode is the default. You can select another one with the
MODE variable explained above.

 Additional flags
~~~~~~~~~~~~~~~~~~

Additional flags can be passed to spatch through the SPFLAGS
variable.

    make SPFLAGS=--use-glimpse coccicheck
    make SPFLAGS=--use-idutils coccicheck

See spatch --help to learn more about spatch options.

Note that the '--use-glimpse' and '--use-idutils' options
require external tools for indexing the code. None of them is
thus active by default. However, by indexing the code with
one of these tools, and according to the cocci file used,
spatch could proceed the entire code base more quickly.

 Proposing new semantic patches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

New semantic patches can be proposed and submitted by kernel
developers. For sake of clarity, they should be organized in the
sub-directories of 'scripts/coccinelle/'.


 Detailed description of the 'report' mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

'report' generates a list in the following format:
  file:line:column-column: message

Example:

Running

	make coccicheck MODE=report COCCI=scripts/coccinelle/api/err_cast.cocci

will execute the following part of the SmPL script.

<smpl>
@r depends on !context && !patch && (org || report)@
expression x;
position p;
@@

 ERR_PTR@p(PTR_ERR(x))

@script:python depends on report@
p << r.p;
x << r.x;
@@

msg="ERR_CAST can be used with %s" % (x)
coccilib.report.print_report(p[0], msg)
</smpl>

This SmPL excerpt generates entries on the standard output, as
illustrated below:

/home/user/linux/crypto/ctr.c:188:9-16: ERR_CAST can be used with alg
/home/user/linux/crypto/authenc.c:619:9-16: ERR_CAST can be used with auth
/home/user/linux/crypto/xts.c:227:9-16: ERR_CAST can be used with alg


 Detailed description of the 'patch' mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When the 'patch' mode is available, it proposes a fix for each problem
identified.

Example:

Running
	make coccicheck MODE=patch COCCI=scripts/coccinelle/api/err_cast.cocci

will execute the following part of the SmPL script.

<smpl>
@ depends on !context && patch && !org && !report @
expression x;
@@

- ERR_PTR(PTR_ERR(x))
+ ERR_CAST(x)
</smpl>

This SmPL excerpt generates patch hunks on the standard output, as
illustrated below:

diff -u -p a/crypto/ctr.c b/crypto/ctr.c
--- a/crypto/ctr.c 2010-05-26 10:49:38.000000000 +0200
+++ b/crypto/ctr.c 2010-06-03 23:44:49.000000000 +0200
@@ -185,7 +185,7 @@ static struct crypto_instance *crypto_ct
 	alg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_CIPHER,
 				  CRYPTO_ALG_TYPE_MASK);
 	if (IS_ERR(alg))
-		return ERR_PTR(PTR_ERR(alg));
+		return ERR_CAST(alg);
 
 	/* Block size must be >= 4 bytes. */
 	err = -EINVAL;

 Detailed description of the 'context' mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

'context' highlights lines of interest and their context
in a diff-like style.

NOTE: The diff-like output generated is NOT an applicable patch. The
      intent of the 'context' mode is to highlight the important lines
      (annotated with minus, '-') and gives some surrounding context
      lines around. This output can be used with the diff mode of
      Emacs to review the code.

Example:

Running
	make coccicheck MODE=context COCCI=scripts/coccinelle/api/err_cast.cocci

will execute the following part of the SmPL script.

<smpl>
@ depends on context && !patch && !org && !report@
expression x;
@@

* ERR_PTR(PTR_ERR(x))
</smpl>

This SmPL excerpt generates diff hunks on the standard output, as
illustrated below:

diff -u -p /home/user/linux/crypto/ctr.c /tmp/nothing
--- /home/user/linux/crypto/ctr.c	2010-05-26 10:49:38.000000000 +0200
+++ /tmp/nothing
@@ -185,7 +185,6 @@ static struct crypto_instance *crypto_ct
 	alg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_CIPHER,
 				  CRYPTO_ALG_TYPE_MASK);
 	if (IS_ERR(alg))
-		return ERR_PTR(PTR_ERR(alg));
 
 	/* Block size must be >= 4 bytes. */
 	err = -EINVAL;

 Detailed description of the 'org' mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

'org' generates a report in the Org mode format of Emacs.

Example:

Running
	make coccicheck MODE=org COCCI=scripts/coccinelle/api/err_cast.cocci

will execute the following part of the SmPL script.

<smpl>
@r depends on !context && !patch && (org || report)@
expression x;
position p;
@@

 ERR_PTR@p(PTR_ERR(x))

@script:python depends on org@
p << r.p;
x << r.x;
@@

msg="ERR_CAST can be used with %s" % (x)
msg_safe=msg.replace("[","@(").replace("]",")")
coccilib.org.print_todo(p[0], msg_safe)
</smpl>

This SmPL excerpt generates Org entries on the standard output, as
illustrated below:

* TODO [[view:/home/user/linux/crypto/ctr.c::face=ovl-face1::linb=188::colb=9::cole=16][ERR_CAST can be used with alg]]
* TODO [[view:/home/user/linux/crypto/authenc.c::face=ovl-face1::linb=619::colb=9::cole=16][ERR_CAST can be used with auth]]
* TODO [[view:/home/user/linux/crypto/xts.c::face=ovl-face1::linb=227::colb=9::cole=16][ERR_CAST can be used with alg]]
		CPU hotplug Support in Linux(tm) Kernel

		Maintainers:
		CPU Hotplug Core:
			Rusty Russell <rusty@rustcorp.com.au>
			Srivatsa Vaddagiri <vatsa@in.ibm.com>
		i386:
			Zwane Mwaikambo <zwanem@gmail.com>
		ppc64:
			Nathan Lynch <nathanl@austin.ibm.com>
			Joel Schopp <jschopp@austin.ibm.com>
		ia64/x86_64:
			Ashok Raj <ashok.raj@intel.com>
		s390:
			Heiko Carstens <heiko.carstens@de.ibm.com>

Authors: Ashok Raj <ashok.raj@intel.com>
Lots of feedback: Nathan Lynch <nathanl@austin.ibm.com>,
	     Joel Schopp <jschopp@austin.ibm.com>

Introduction

Modern advances in system architectures have introduced advanced error
reporting and correction capabilities in processors. CPU architectures permit
partitioning support, where compute resources of a single CPU could be made
available to virtual machine environments. There are couple OEMS that
support NUMA hardware which are hot pluggable as well, where physical
node insertion and removal require support for CPU hotplug.

Such advances require CPUs available to a kernel to be removed either for
provisioning reasons, or for RAS purposes to keep an offending CPU off
system execution path. Hence the need for CPU hotplug support in the
Linux kernel.

A more novel use of CPU-hotplug support is its use today in suspend
resume support for SMP. Dual-core and HT support makes even
a laptop run SMP kernels which didn't support these methods. SMP support
for suspend/resume is a work in progress.

General Stuff about CPU Hotplug
--------------------------------

Command Line Switches
---------------------
maxcpus=n    Restrict boot time cpus to n. Say if you have 4 cpus, using
             maxcpus=2 will only boot 2. You can choose to bring the
             other cpus later online, read FAQ's for more info.

additional_cpus=n (*)	Use this to limit hotpluggable cpus. This option sets
  			cpu_possible_mask = cpu_present_mask + additional_cpus

cede_offline={"off","on"}  Use this option to disable/enable putting offlined
		            processors to an extended H_CEDE state on
			    supported pseries platforms.
			    If nothing is specified,
			    cede_offline is set to "on".

(*) Option valid only for following architectures
- ia64

ia64 uses the number of disabled local apics in ACPI tables MADT to
determine the number of potentially hot-pluggable cpus. The implementation
should only rely on this to count the # of cpus, but *MUST* not rely
on the apicid values in those tables for disabled apics. In the event
BIOS doesn't mark such hot-pluggable cpus as disabled entries, one could
use this parameter "additional_cpus=x" to represent those cpus in the
cpu_possible_mask.

possible_cpus=n		[s390,x86_64] use this to set hotpluggable cpus.
			This option sets possible_cpus bits in
			cpu_possible_mask. Thus keeping the numbers of bits set
			constant even if the machine gets rebooted.

CPU maps and such
-----------------
[More on cpumaps and primitive to manipulate, please check
include/linux/cpumask.h that has more descriptive text.]

cpu_possible_mask: Bitmap of possible CPUs that can ever be available in the
system. This is used to allocate some boot time memory for per_cpu variables
that aren't designed to grow/shrink as CPUs are made available or removed.
Once set during boot time discovery phase, the map is static, i.e no bits
are added or removed anytime.  Trimming it accurately for your system needs
upfront can save some boot time memory. See below for how we use heuristics
in x86_64 case to keep this under check.

cpu_online_mask: Bitmap of all CPUs currently online. Its set in __cpu_up()
after a cpu is available for kernel scheduling and ready to receive
interrupts from devices. Its cleared when a cpu is brought down using
__cpu_disable(), before which all OS services including interrupts are
migrated to another target CPU.

cpu_present_mask: Bitmap of CPUs currently present in the system. Not all
of them may be online. When physical hotplug is processed by the relevant
subsystem (e.g ACPI) can change and new bit either be added or removed
from the map depending on the event is hot-add/hot-remove. There are currently
no locking rules as of now. Typical usage is to init topology during boot,
at which time hotplug is disabled.

You really dont need to manipulate any of the system cpu maps. They should
be read-only for most use. When setting up per-cpu resources almost always use
cpu_possible_mask/for_each_possible_cpu() to iterate.

Never use anything other than cpumask_t to represent bitmap of CPUs.

	#include <linux/cpumask.h>

	for_each_possible_cpu     - Iterate over cpu_possible_mask
	for_each_online_cpu       - Iterate over cpu_online_mask
	for_each_present_cpu      - Iterate over cpu_present_mask
	for_each_cpu_mask(x,mask) - Iterate over some random collection of cpu mask.

	#include <linux/cpu.h>
	get_online_cpus() and put_online_cpus():

The above calls are used to inhibit cpu hotplug operations. While the
cpu_hotplug.refcount is non zero, the cpu_online_mask will not change.
If you merely need to avoid cpus going away, you could also use
preempt_disable() and preempt_enable() for those sections.
Just remember the critical section cannot call any
function that can sleep or schedule this process away. The preempt_disable()
will work as long as stop_machine_run() is used to take a cpu down.

CPU Hotplug - Frequently Asked Questions.

Q: How to enable my kernel to support CPU hotplug?
A: When doing make defconfig, Enable CPU hotplug support

   "Processor type and Features" -> Support for Hotpluggable CPUs

Make sure that you have CONFIG_SMP turned on as well.

You would need to enable CONFIG_HOTPLUG_CPU for SMP suspend/resume support
as well.

Q: What architectures support CPU hotplug?
A: As of 2.6.14, the following architectures support CPU hotplug.

i386 (Intel), ppc, ppc64, parisc, s390, ia64 and x86_64

Q: How to test if hotplug is supported on the newly built kernel?
A: You should now notice an entry in sysfs.

Check if sysfs is mounted, using the "mount" command. You should notice
an entry as shown below in the output.

	....
	none on /sys type sysfs (rw)
	....

If this is not mounted, do the following.

	 #mkdir /sysfs
	#mount -t sysfs sys /sys

Now you should see entries for all present cpu, the following is an example
in a 8-way system.

	#pwd
	#/sys/devices/system/cpu
	#ls -l
	total 0
	drwxr-xr-x  10 root root 0 Sep 19 07:44 .
	drwxr-xr-x  13 root root 0 Sep 19 07:45 ..
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu0
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu1
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu2
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu3
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu4
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu5
	drwxr-xr-x   3 root root 0 Sep 19 07:44 cpu6
	drwxr-xr-x   3 root root 0 Sep 19 07:48 cpu7

Under each directory you would find an "online" file which is the control
file to logically online/offline a processor.

Q: Does hot-add/hot-remove refer to physical add/remove of cpus?
A: The usage of hot-add/remove may not be very consistently used in the code.
CONFIG_HOTPLUG_CPU enables logical online/offline capability in the kernel.
To support physical addition/removal, one would need some BIOS hooks and
the platform should have something like an attention button in PCI hotplug.
CONFIG_ACPI_HOTPLUG_CPU enables ACPI support for physical add/remove of CPUs.

Q: How do i logically offline a CPU?
A: Do the following.

	#echo 0 > /sys/devices/system/cpu/cpuX/online

Once the logical offline is successful, check

	#cat /proc/interrupts

You should now not see the CPU that you removed. Also online file will report
the state as 0 when a cpu if offline and 1 when its online.

	#To display the current cpu state.
	#cat /sys/devices/system/cpu/cpuX/online

Q: Why can't i remove CPU0 on some systems?
A: Some architectures may have some special dependency on a certain CPU.

For e.g in IA64 platforms we have ability to sent platform interrupts to the
OS. a.k.a Corrected Platform Error Interrupts (CPEI). In current ACPI
specifications, we didn't have a way to change the target CPU. Hence if the
current ACPI version doesn't support such re-direction, we disable that CPU
by making it not-removable.

In such cases you will also notice that the online file is missing under cpu0.

Q: Is CPU0 removable on X86?
A: Yes. If kernel is compiled with CONFIG_BOOTPARAM_HOTPLUG_CPU0=y, CPU0 is
removable by default. Otherwise, CPU0 is also removable by kernel option
cpu0_hotplug.

But some features depend on CPU0. Two known dependencies are:

1. Resume from hibernate/suspend depends on CPU0. Hibernate/suspend will fail if
CPU0 is offline and you need to online CPU0 before hibernate/suspend can
continue.
2. PIC interrupts also depend on CPU0. CPU0 can't be removed if a PIC interrupt
is detected.

It's said poweroff/reboot may depend on CPU0 on some machines although I haven't
seen any poweroff/reboot failure so far after CPU0 is offline on a few tested
machines.

Please let me know if you know or see any other dependencies of CPU0.

If the dependencies are under your control, you can turn on CPU0 hotplug feature
either by CONFIG_BOOTPARAM_HOTPLUG_CPU0 or by kernel parameter cpu0_hotplug.

--Fenghua Yu <fenghua.yu@intel.com>

Q: How do i find out if a particular CPU is not removable?
A: Depending on the implementation, some architectures may show this by the
absence of the "online" file. This is done if it can be determined ahead of
time that this CPU cannot be removed.

In some situations, this can be a run time check, i.e if you try to remove the
last CPU, this will not be permitted. You can find such failures by
investigating the return value of the "echo" command.

Q: What happens when a CPU is being logically offlined?
A: The following happen, listed in no particular order :-)

- A notification is sent to in-kernel registered modules by sending an event
  CPU_DOWN_PREPARE or CPU_DOWN_PREPARE_FROZEN, depending on whether or not the
  CPU is being offlined while tasks are frozen due to a suspend operation in
  progress
- All processes are migrated away from this outgoing CPU to new CPUs.
  The new CPU is chosen from each process' current cpuset, which may be
  a subset of all online CPUs.
- All interrupts targeted to this CPU is migrated to a new CPU
- timers/bottom half/task lets are also migrated to a new CPU
- Once all services are migrated, kernel calls an arch specific routine
  __cpu_disable() to perform arch specific cleanup.
- Once this is successful, an event for successful cleanup is sent by an event
  CPU_DEAD (or CPU_DEAD_FROZEN if tasks are frozen due to a suspend while the
  CPU is being offlined).

  "It is expected that each service cleans up when the CPU_DOWN_PREPARE
  notifier is called, when CPU_DEAD is called its expected there is nothing
  running on behalf of this CPU that was offlined"

Q: If i have some kernel code that needs to be aware of CPU arrival and
   departure, how to i arrange for proper notification?
A: This is what you would need in your kernel code to receive notifications.

	#include <linux/cpu.h>
	static int foobar_cpu_callback(struct notifier_block *nfb,
				       unsigned long action, void *hcpu)
	{
		unsigned int cpu = (unsigned long)hcpu;

		switch (action) {
		case CPU_ONLINE:
		case CPU_ONLINE_FROZEN:
			foobar_online_action(cpu);
			break;
		case CPU_DEAD:
		case CPU_DEAD_FROZEN:
			foobar_dead_action(cpu);
			break;
		}
		return NOTIFY_OK;
	}

	static struct notifier_block foobar_cpu_notifier =
	{
	   .notifier_call = foobar_cpu_callback,
	};

You need to call register_cpu_notifier() from your init function.
Init functions could be of two types:
1. early init (init function called when only the boot processor is online).
2. late init (init function called _after_ all the CPUs are online).

For the first case, you should add the following to your init function

	register_cpu_notifier(&foobar_cpu_notifier);

For the second case, you should add the following to your init function

	register_hotcpu_notifier(&foobar_cpu_notifier);

You can fail PREPARE notifiers if something doesn't work to prepare resources.
This will stop the activity and send a following CANCELED event back.

CPU_DEAD should not be failed, its just a goodness indication, but bad
things will happen if a notifier in path sent a BAD notify code.

Q: I don't see my action being called for all CPUs already up and running?
A: Yes, CPU notifiers are called only when new CPUs are on-lined or offlined.
   If you need to perform some action for each cpu already in the system, then
   do this:

	for_each_online_cpu(i) {
		foobar_cpu_callback(&foobar_cpu_notifier, CPU_UP_PREPARE, i);
		foobar_cpu_callback(&foobar_cpu_notifier, CPU_ONLINE, i);
	}

   However, if you want to register a hotplug callback, as well as perform
   some initialization for CPUs that are already online, then do this:

   Version 1: (Correct)
   ---------

   	cpu_notifier_register_begin();

		for_each_online_cpu(i) {
			foobar_cpu_callback(&foobar_cpu_notifier,
					    CPU_UP_PREPARE, i);
			foobar_cpu_callback(&foobar_cpu_notifier,
					    CPU_ONLINE, i);
		}

	/* Note the use of the double underscored version of the API */
	__register_cpu_notifier(&foobar_cpu_notifier);

	cpu_notifier_register_done();

   Note that the following code is *NOT* the right way to achieve this,
   because it is prone to an ABBA deadlock between the cpu_add_remove_lock
   and the cpu_hotplug.lock.

   Version 2: (Wrong!)
   ---------

	get_online_cpus();

		for_each_online_cpu(i) {
			foobar_cpu_callback(&foobar_cpu_notifier,
					    CPU_UP_PREPARE, i);
			foobar_cpu_callback(&foobar_cpu_notifier,
					    CPU_ONLINE, i);
		}

	register_cpu_notifier(&foobar_cpu_notifier);

	put_online_cpus();

    So always use the first version shown above when you want to register
    callbacks as well as initialize the already online CPUs.


Q: If i would like to develop cpu hotplug support for a new architecture,
   what do i need at a minimum?
A: The following are what is required for CPU hotplug infrastructure to work
   correctly.

    - Make sure you have an entry in Kconfig to enable CONFIG_HOTPLUG_CPU
    - __cpu_up()        - Arch interface to bring up a CPU
    - __cpu_disable()   - Arch interface to shutdown a CPU, no more interrupts
                          can be handled by the kernel after the routine
                          returns. Including local APIC timers etc are
                          shutdown.
     - __cpu_die()      - This actually supposed to ensure death of the CPU.
                          Actually look at some example code in other arch
                          that implement CPU hotplug. The processor is taken
                          down from the idle() loop for that specific
                          architecture. __cpu_die() typically waits for some
                          per_cpu state to be set, to ensure the processor
                          dead routine is called to be sure positively.

Q: I need to ensure that a particular cpu is not removed when there is some
   work specific to this cpu is in progress.
A: There are two ways.  If your code can be run in interrupt context, use
   smp_call_function_single(), otherwise use work_on_cpu().  Note that
   work_on_cpu() is slow, and can fail due to out of memory:

	int my_func_on_cpu(int cpu)
	{
		int err;
		get_online_cpus();
		if (!cpu_online(cpu))
			err = -EINVAL;
		else
#if NEEDS_BLOCKING
			err = work_on_cpu(cpu, __my_func_on_cpu, NULL);
#else
			smp_call_function_single(cpu, __my_func_on_cpu, &err,
						 true);
#endif
		put_online_cpus();
		return err;
	}

Q: How do we determine how many CPUs are available for hotplug.
A: There is no clear spec defined way from ACPI that can give us that
   information today. Based on some input from Natalie of Unisys,
   that the ACPI MADT (Multiple APIC Description Tables) marks those possible
   CPUs in a system with disabled status.

   Andi implemented some simple heuristics that count the number of disabled
   CPUs in MADT as hotpluggable CPUS.  In the case there are no disabled CPUS
   we assume 1/2 the number of CPUs currently present can be hotplugged.

   Caveat: ACPI MADT can only provide 256 entries in systems with only ACPI 2.0c
   or earlier ACPI version supported, because the apicid field in MADT is only
   8 bits. From ACPI 3.0, this limitation was removed since the apicid field
   was extended to 32 bits with x2APIC introduced.

User Space Notification

Hotplug support for devices is common in Linux today. Its being used today to
support automatic configuration of network, usb and pci devices. A hotplug
event can be used to invoke an agent script to perform the configuration task.

You can add /etc/hotplug/cpu.agent to handle hotplug notification user space
scripts.

	#!/bin/bash
	# $Id: cpu.agent
	# Kernel hotplug params include:
	#ACTION=%s [online or offline]
	#DEVPATH=%s
	#
	cd /etc/hotplug
	. ./hotplug.functions

	case $ACTION in
		online)
			echo `date` ":cpu.agent" add cpu >> /tmp/hotplug.txt
			;;
		offline)
			echo `date` ":cpu.agent" remove cpu >>/tmp/hotplug.txt
			;;
		*)
			debug_mesg CPU $ACTION event not supported
        exit 1
        ;;
	esac
CPU load
--------

Linux exports various bits of information via `/proc/stat' and
`/proc/uptime' that userland tools, such as top(1), use to calculate
the average time system spent in a particular state, for example:

    $ iostat
    Linux 2.6.18.3-exp (linmac)     02/20/2007

    avg-cpu:  %user   %nice %system %iowait  %steal   %idle
              10.01    0.00    2.92    5.44    0.00   81.63

    ...

Here the system thinks that over the default sampling period the
system spent 10.01% of the time doing work in user space, 2.92% in the
kernel, and was overall 81.63% of the time idle.

In most cases the `/proc/stat' information reflects the reality quite
closely, however due to the nature of how/when the kernel collects
this data sometimes it can not be trusted at all.

So how is this information collected?  Whenever timer interrupt is
signalled the kernel looks what kind of task was running at this
moment and increments the counter that corresponds to this tasks
kind/state.  The problem with this is that the system could have
switched between various states multiple times between two timer
interrupts yet the counter is incremented only for the last state.


Example
-------

If we imagine the system with one task that periodically burns cycles
in the following manner:

 time line between two timer interrupts
|--------------------------------------|
 ^                                    ^
 |_ something begins working          |
                                      |_ something goes to sleep
                                     (only to be awaken quite soon)

In the above situation the system will be 0% loaded according to the
`/proc/stat' (since the timer interrupt will always happen when the
system is executing the idle handler), but in reality the load is
closer to 99%.

One can imagine many more situations where this behavior of the kernel
will lead to quite erratic information inside `/proc/stat'.


/* gcc -o hog smallhog.c */
#include <time.h>
#include <limits.h>
#include <signal.h>
#include <sys/time.h>
#define HIST 10

static volatile sig_atomic_t stop;

static void sighandler (int signr)
{
     (void) signr;
     stop = 1;
}
static unsigned long hog (unsigned long niters)
{
     stop = 0;
     while (!stop && --niters);
     return niters;
}
int main (void)
{
     int i;
     struct itimerval it = { .it_interval = { .tv_sec = 0, .tv_usec = 1 },
                             .it_value = { .tv_sec = 0, .tv_usec = 1 } };
     sigset_t set;
     unsigned long v[HIST];
     double tmp = 0.0;
     unsigned long n;
     signal (SIGALRM, &sighandler);
     setitimer (ITIMER_REAL, &it, NULL);

     hog (ULONG_MAX);
     for (i = 0; i < HIST; ++i) v[i] = ULONG_MAX - hog (ULONG_MAX);
     for (i = 0; i < HIST; ++i) tmp += v[i];
     tmp /= HIST;
     n = tmp - (tmp / 3.0);

     sigemptyset (&set);
     sigaddset (&set, SIGALRM);

     for (;;) {
         hog (n);
         sigwait (&set, &i);
     }
     return 0;
}


References
----------

http://lkml.org/lkml/2007/2/12/6
Documentation/filesystems/proc.txt (1.8)


Thanks
------

Con Kolivas, Pavel Machek

Export CPU topology info via sysfs. Items (attributes) are similar
to /proc/cpuinfo.

1) /sys/devices/system/cpu/cpuX/topology/physical_package_id:

	physical package id of cpuX. Typically corresponds to a physical
	socket number, but the actual value is architecture and platform
	dependent.

2) /sys/devices/system/cpu/cpuX/topology/core_id:

	the CPU core ID of cpuX. Typically it is the hardware platform's
	identifier (rather than the kernel's).  The actual value is
	architecture and platform dependent.

3) /sys/devices/system/cpu/cpuX/topology/book_id:

	the book ID of cpuX. Typically it is the hardware platform's
	identifier (rather than the kernel's).	The actual value is
	architecture and platform dependent.

4) /sys/devices/system/cpu/cpuX/topology/thread_siblings:

	internal kernel map of cpuX's hardware threads within the same
	core as cpuX

5) /sys/devices/system/cpu/cpuX/topology/core_siblings:

	internal kernel map of cpuX's hardware threads within the same
	physical_package_id.

6) /sys/devices/system/cpu/cpuX/topology/book_siblings:

	internal kernel map of cpuX's hardware threads within the same
	book_id.

To implement it in an architecture-neutral way, a new source file,
drivers/base/topology.c, is to export the 4 or 6 attributes. The two book
related sysfs files will only be created if CONFIG_SCHED_BOOK is selected.

For an architecture to support this feature, it must define some of
these macros in include/asm-XXX/topology.h:
#define topology_physical_package_id(cpu)
#define topology_core_id(cpu)
#define topology_book_id(cpu)
#define topology_thread_cpumask(cpu)
#define topology_core_cpumask(cpu)
#define topology_book_cpumask(cpu)

The type of **_id is int.
The type of siblings is (const) struct cpumask *.

To be consistent on all architectures, include/linux/topology.h
provides default definitions for any of the above macros that are
not defined by include/asm-XXX/topology.h:
1) physical_package_id: -1
2) core_id: 0
3) thread_siblings: just the given CPU
4) core_siblings: just the given CPU

For architectures that don't support books (CONFIG_SCHED_BOOK) there are no
default definitions for topology_book_id() and topology_book_cpumask().

Additionally, CPU topology information is provided under
/sys/devices/system/cpu and includes these files.  The internal
source for the output is in brackets ("[]").

    kernel_max: the maximum CPU index allowed by the kernel configuration.
		[NR_CPUS-1]

    offline:	CPUs that are not online because they have been
		HOTPLUGGED off (see cpu-hotplug.txt) or exceed the limit
		of CPUs allowed by the kernel configuration (kernel_max
		above). [~cpu_online_mask + cpus >= NR_CPUS]

    online:	CPUs that are online and being scheduled [cpu_online_mask]

    possible:	CPUs that have been allocated resources and can be
		brought online if they are present. [cpu_possible_mask]

    present:	CPUs that have been identified as being present in the
		system. [cpu_present_mask]

The format for the above output is compatible with cpulist_parse()
[see <linux/cpumask.h>].  Some examples follow.

In this example, there are 64 CPUs in the system but cpus 32-63 exceed
the kernel max which is limited to 0..31 by the NR_CPUS config option
being 32.  Note also that CPUs 2 and 4-31 are not online but could be
brought online as they are both present and possible.

     kernel_max: 31
        offline: 2,4-31,32-63
         online: 0-1,3
       possible: 0-31
        present: 0-31

In this example, the NR_CPUS config option is 128, but the kernel was
started with possible_cpus=144.  There are 4 CPUs in the system and cpu2
was manually taken offline (and is the only CPU that can be brought
online.)

     kernel_max: 127
        offline: 2,4-127,128-143
         online: 0-1,3
       possible: 0-127
        present: 0-3

See cpu-hotplug.txt for the possible_cpus=NUM kernel start parameter
as well as more information on the various cpumasks.
A brief CRC tutorial.

A CRC is a long-division remainder.  You add the CRC to the message,
and the whole thing (message+CRC) is a multiple of the given
CRC polynomial.  To check the CRC, you can either check that the
CRC matches the recomputed value, *or* you can check that the
remainder computed on the message+CRC is 0.  This latter approach
is used by a lot of hardware implementations, and is why so many
protocols put the end-of-frame flag after the CRC.

It's actually the same long division you learned in school, except that
- We're working in binary, so the digits are only 0 and 1, and
- When dividing polynomials, there are no carries.  Rather than add and
  subtract, we just xor.  Thus, we tend to get a bit sloppy about
  the difference between adding and subtracting.

Like all division, the remainder is always smaller than the divisor.
To produce a 32-bit CRC, the divisor is actually a 33-bit CRC polynomial.
Since it's 33 bits long, bit 32 is always going to be set, so usually the
CRC is written in hex with the most significant bit omitted.  (If you're
familiar with the IEEE 754 floating-point format, it's the same idea.)

Note that a CRC is computed over a string of *bits*, so you have
to decide on the endianness of the bits within each byte.  To get
the best error-detecting properties, this should correspond to the
order they're actually sent.  For example, standard RS-232 serial is
little-endian; the most significant bit (sometimes used for parity)
is sent last.  And when appending a CRC word to a message, you should
do it in the right order, matching the endianness.

Just like with ordinary division, you proceed one digit (bit) at a time.
Each step of the division you take one more digit (bit) of the dividend
and append it to the current remainder.  Then you figure out the
appropriate multiple of the divisor to subtract to being the remainder
back into range.  In binary, this is easy - it has to be either 0 or 1,
and to make the XOR cancel, it's just a copy of bit 32 of the remainder.

When computing a CRC, we don't care about the quotient, so we can
throw the quotient bit away, but subtract the appropriate multiple of
the polynomial from the remainder and we're back to where we started,
ready to process the next bit.

A big-endian CRC written this way would be coded like:
for (i = 0; i < input_bits; i++) {
	multiple = remainder & 0x80000000 ? CRCPOLY : 0;
	remainder = (remainder << 1 | next_input_bit()) ^ multiple;
}

Notice how, to get at bit 32 of the shifted remainder, we look
at bit 31 of the remainder *before* shifting it.

But also notice how the next_input_bit() bits we're shifting into
the remainder don't actually affect any decision-making until
32 bits later.  Thus, the first 32 cycles of this are pretty boring.
Also, to add the CRC to a message, we need a 32-bit-long hole for it at
the end, so we have to add 32 extra cycles shifting in zeros at the
end of every message,

These details lead to a standard trick: rearrange merging in the
next_input_bit() until the moment it's needed.  Then the first 32 cycles
can be precomputed, and merging in the final 32 zero bits to make room
for the CRC can be skipped entirely.  This changes the code to:

for (i = 0; i < input_bits; i++) {
	remainder ^= next_input_bit() << 31;
	multiple = (remainder & 0x80000000) ? CRCPOLY : 0;
	remainder = (remainder << 1) ^ multiple;
}

With this optimization, the little-endian code is particularly simple:
for (i = 0; i < input_bits; i++) {
	remainder ^= next_input_bit();
	multiple = (remainder & 1) ? CRCPOLY : 0;
	remainder = (remainder >> 1) ^ multiple;
}

The most significant coefficient of the remainder polynomial is stored
in the least significant bit of the binary "remainder" variable.
The other details of endianness have been hidden in CRCPOLY (which must
be bit-reversed) and next_input_bit().

As long as next_input_bit is returning the bits in a sensible order, we don't
*have* to wait until the last possible moment to merge in additional bits.
We can do it 8 bits at a time rather than 1 bit at a time:
for (i = 0; i < input_bytes; i++) {
	remainder ^= next_input_byte() << 24;
	for (j = 0; j < 8; j++) {
		multiple = (remainder & 0x80000000) ? CRCPOLY : 0;
		remainder = (remainder << 1) ^ multiple;
	}
}

Or in little-endian:
for (i = 0; i < input_bytes; i++) {
	remainder ^= next_input_byte();
	for (j = 0; j < 8; j++) {
		multiple = (remainder & 1) ? CRCPOLY : 0;
		remainder = (remainder >> 1) ^ multiple;
	}
}

If the input is a multiple of 32 bits, you can even XOR in a 32-bit
word at a time and increase the inner loop count to 32.

You can also mix and match the two loop styles, for example doing the
bulk of a message byte-at-a-time and adding bit-at-a-time processing
for any fractional bytes at the end.

To reduce the number of conditional branches, software commonly uses
the byte-at-a-time table method, popularized by Dilip V. Sarwate,
"Computation of Cyclic Redundancy Checks via Table Look-Up", Comm. ACM
v.31 no.8 (August 1998) p. 1008-1013.

Here, rather than just shifting one bit of the remainder to decide
in the correct multiple to subtract, we can shift a byte at a time.
This produces a 40-bit (rather than a 33-bit) intermediate remainder,
and the correct multiple of the polynomial to subtract is found using
a 256-entry lookup table indexed by the high 8 bits.

(The table entries are simply the CRC-32 of the given one-byte messages.)

When space is more constrained, smaller tables can be used, e.g. two
4-bit shifts followed by a lookup in a 16-entry table.

It is not practical to process much more than 8 bits at a time using this
technique, because tables larger than 256 entries use too much memory and,
more importantly, too much of the L1 cache.

To get higher software performance, a "slicing" technique can be used.
See "High Octane CRC Generation with the Intel Slicing-by-8 Algorithm",
ftp://download.intel.com/technology/comms/perfnet/download/slicing-by-8.pdf

This does not change the number of table lookups, but does increase
the parallelism.  With the classic Sarwate algorithm, each table lookup
must be completed before the index of the next can be computed.

A "slicing by 2" technique would shift the remainder 16 bits at a time,
producing a 48-bit intermediate remainder.  Rather than doing a single
lookup in a 65536-entry table, the two high bytes are looked up in
two different 256-entry tables.  Each contains the remainder required
to cancel out the corresponding byte.  The tables are different because the
polynomials to cancel are different.  One has non-zero coefficients from
x^32 to x^39, while the other goes from x^40 to x^47.

Since modern processors can handle many parallel memory operations, this
takes barely longer than a single table look-up and thus performs almost
twice as fast as the basic Sarwate algorithm.

This can be extended to "slicing by 4" using 4 256-entry tables.
Each step, 32 bits of data is fetched, XORed with the CRC, and the result
broken into bytes and looked up in the tables.  Because the 32-bit shift
leaves the low-order bits of the intermediate remainder zero, the
final CRC is simply the XOR of the 4 table look-ups.

But this still enforces sequential execution: a second group of table
look-ups cannot begin until the previous groups 4 table look-ups have all
been completed.  Thus, the processor's load/store unit is sometimes idle.

To make maximum use of the processor, "slicing by 8" performs 8 look-ups
in parallel.  Each step, the 32-bit CRC is shifted 64 bits and XORed
with 64 bits of input data.  What is important to note is that 4 of
those 8 bytes are simply copies of the input data; they do not depend
on the previous CRC at all.  Thus, those 4 table look-ups may commence
immediately, without waiting for the previous loop iteration.

By always having 4 loads in flight, a modern superscalar processor can
be kept busy and make full use of its L1 cache.

Two more details about CRC implementation in the real world:

Normally, appending zero bits to a message which is already a multiple
of a polynomial produces a larger multiple of that polynomial.  Thus,
a basic CRC will not detect appended zero bits (or bytes).  To enable
a CRC to detect this condition, it's common to invert the CRC before
appending it.  This makes the remainder of the message+crc come out not
as zero, but some fixed non-zero value.  (The CRC of the inversion
pattern, 0xffffffff.)

The same problem applies to zero bits prepended to the message, and a
similar solution is used.  Instead of starting the CRC computation with
a remainder of 0, an initial remainder of all ones is used.  As long as
you start the same way on decoding, it doesn't make a difference.
Overview

The Dell Systems Management Base Driver provides a sysfs interface for
systems management software such as Dell OpenManage to perform system
management interrupts and host control actions (system power cycle or
power off after OS shutdown) on certain Dell systems.

Dell OpenManage requires this driver on the following Dell PowerEdge systems:
300, 1300, 1400, 400SC, 500SC, 1500SC, 1550, 600SC, 1600SC, 650, 1655MC,
700, and 750.  Other Dell software such as the open source libsmbios project
is expected to make use of this driver, and it may include the use of this
driver on other Dell systems.

The Dell libsmbios project aims towards providing access to as much BIOS
information as possible.  See http://linux.dell.com/libsmbios/main/ for
more information about the libsmbios project.


System Management Interrupt

On some Dell systems, systems management software must access certain
management information via a system management interrupt (SMI).  The SMI data
buffer must reside in 32-bit address space, and the physical address of the
buffer is required for the SMI.  The driver maintains the memory required for
the SMI and provides a way for the application to generate the SMI.
The driver creates the following sysfs entries for systems management
software to perform these system management interrupts:

/sys/devices/platform/dcdbas/smi_data
/sys/devices/platform/dcdbas/smi_data_buf_phys_addr
/sys/devices/platform/dcdbas/smi_data_buf_size
/sys/devices/platform/dcdbas/smi_request

Systems management software must perform the following steps to execute
a SMI using this driver:

1) Lock smi_data.
2) Write system management command to smi_data.
3) Write "1" to smi_request to generate a calling interface SMI or
   "2" to generate a raw SMI.
4) Read system management command response from smi_data.
5) Unlock smi_data.


Host Control Action

Dell OpenManage supports a host control feature that allows the administrator
to perform a power cycle or power off of the system after the OS has finished
shutting down.  On some Dell systems, this host control feature requires that
a driver perform a SMI after the OS has finished shutting down.

The driver creates the following sysfs entries for systems management software
to schedule the driver to perform a power cycle or power off host control
action after the system has finished shutting down:

/sys/devices/platform/dcdbas/host_control_action
/sys/devices/platform/dcdbas/host_control_smi_type
/sys/devices/platform/dcdbas/host_control_on_shutdown

Dell OpenManage performs the following steps to execute a power cycle or
power off host control action using this driver:

1) Write host control action to be performed to host_control_action.
2) Write type of SMI that driver needs to perform to host_control_smi_type.
3) Write "1" to host_control_on_shutdown to enable host control action.
4) Initiate OS shutdown.
   (Driver will perform host control SMI when it is notified that the OS
   has finished shutting down.)


Host Control SMI Type

The following table shows the value to write to host_control_smi_type to
perform a power cycle or power off host control action:

PowerEdge System    Host Control SMI Type
----------------    ---------------------
      300             HC_SMITYPE_TYPE1
     1300             HC_SMITYPE_TYPE1
     1400             HC_SMITYPE_TYPE2
      500SC           HC_SMITYPE_TYPE2
     1500SC           HC_SMITYPE_TYPE2
     1550             HC_SMITYPE_TYPE2
      600SC           HC_SMITYPE_TYPE2
     1600SC           HC_SMITYPE_TYPE2
      650             HC_SMITYPE_TYPE2
     1655MC           HC_SMITYPE_TYPE2
      700             HC_SMITYPE_TYPE3
      750             HC_SMITYPE_TYPE3


Debugging Modules after 2.6.3
-----------------------------

In almost all distributions, the kernel asks for modules which don't
exist, such as "net-pf-10" or whatever.  Changing "modprobe -q" to
"succeed" in this case is hacky and breaks some setups, and also we
want to know if it failed for the fallback code for old aliases in
fs/char_dev.c, for example.

In the past a debugging message which would fill people's logs was
emitted.  This debugging message has been removed.  The correct way
of debugging module problems is something like this:

echo '#! /bin/sh' > /tmp/modprobe
echo 'echo "$@" >> /tmp/modprobe.log' >> /tmp/modprobe
echo 'exec /sbin/modprobe "$@"' >> /tmp/modprobe
chmod a+x /tmp/modprobe
echo /tmp/modprobe > /proc/sys/kernel/modprobe

Note that the above applies only when the *kernel* is requesting
that the module be loaded -- it won't have any effect if that module
is being loaded explicitly using "modprobe" from userspace.

  Using physical DMA provided by OHCI-1394 FireWire controllers for debugging
  ---------------------------------------------------------------------------

Introduction
------------

Basically all FireWire controllers which are in use today are compliant
to the OHCI-1394 specification which defines the controller to be a PCI
bus master which uses DMA to offload data transfers from the CPU and has
a "Physical Response Unit" which executes specific requests by employing
PCI-Bus master DMA after applying filters defined by the OHCI-1394 driver.

Once properly configured, remote machines can send these requests to
ask the OHCI-1394 controller to perform read and write requests on
physical system memory and, for read requests, send the result of
the physical memory read back to the requester.

With that, it is possible to debug issues by reading interesting memory
locations such as buffers like the printk buffer or the process table.

Retrieving a full system memory dump is also possible over the FireWire,
using data transfer rates in the order of 10MB/s or more.

With most FireWire controllers, memory access is limited to the low 4 GB
of physical address space.  This can be a problem on IA64 machines where
memory is located mostly above that limit, but it is rarely a problem on
more common hardware such as x86, x86-64 and PowerPC.

At least LSI FW643e and FW643e2 controllers are known to support access to
physical addresses above 4 GB, but this feature is currently not enabled by
Linux.

Together with a early initialization of the OHCI-1394 controller for debugging,
this facility proved most useful for examining long debugs logs in the printk
buffer on to debug early boot problems in areas like ACPI where the system
fails to boot and other means for debugging (serial port) are either not
available (notebooks) or too slow for extensive debug information (like ACPI).

Drivers
-------

The firewire-ohci driver in drivers/firewire uses filtered physical
DMA by default, which is more secure but not suitable for remote debugging.
Pass the remote_dma=1 parameter to the driver to get unfiltered physical DMA.

Because the firewire-ohci driver depends on the PCI enumeration to be
completed, an initialization routine which runs pretty early has been
implemented for x86.  This routine runs long before console_init() can be
called, i.e. before the printk buffer appears on the console.

To activate it, enable CONFIG_PROVIDE_OHCI1394_DMA_INIT (Kernel hacking menu:
Remote debugging over FireWire early on boot) and pass the parameter
"ohci1394_dma=early" to the recompiled kernel on boot.

Tools
-----

firescope - Originally developed by Benjamin Herrenschmidt, Andi Kleen ported
it from PowerPC to x86 and x86_64 and added functionality, firescope can now
be used to view the printk buffer of a remote machine, even with live update.

Bernhard Kaindl enhanced firescope to support accessing 64-bit machines
from 32-bit firescope and vice versa:
- http://v3.sk/~lkundrak/firescope/

and he implemented fast system dump (alpha version - read README.txt):
- http://halobates.de/firewire/firedump-0.1.tar.bz2

There is also a gdb proxy for firewire which allows to use gdb to access
data which can be referenced from symbols found by gdb in vmlinux:
- http://halobates.de/firewire/fireproxy-0.33.tar.bz2

The latest version of this gdb proxy (fireproxy-0.34) can communicate (not
yet stable) with kgdb over an memory-based communication module (kgdbom).

Getting Started
---------------

The OHCI-1394 specification regulates that the OHCI-1394 controller must
disable all physical DMA on each bus reset.

This means that if you want to debug an issue in a system state where
interrupts are disabled and where no polling of the OHCI-1394 controller
for bus resets takes place, you have to establish any FireWire cable
connections and fully initialize all FireWire hardware __before__ the
system enters such state.

Step-by-step instructions for using firescope with early OHCI initialization:

1) Verify that your hardware is supported:

   Load the firewire-ohci module and check your kernel logs.
   You should see a line similar to

   firewire_ohci 0000:15:00.1: added OHCI v1.0 device as card 2, 4 IR + 4 IT
   ... contexts, quirks 0x11

   when loading the driver. If you have no supported controller, many PCI,
   CardBus and even some Express cards which are fully compliant to OHCI-1394
   specification are available. If it requires no driver for Windows operating
   systems, it most likely is. Only specialized shops have cards which are not
   compliant, they are based on TI PCILynx chips and require drivers for Win-
   dows operating systems.

   The mentioned kernel log message contains the string "physUB" if the
   controller implements a writable Physical Upper Bound register.  This is
   required for physical DMA above 4 GB (but not utilized by Linux yet).

2) Establish a working FireWire cable connection:

   Any FireWire cable, as long at it provides electrically and mechanically
   stable connection and has matching connectors (there are small 4-pin and
   large 6-pin FireWire ports) will do.

   If an driver is running on both machines you should see a line like

   firewire_core 0000:15:00.1: created device fw1: GUID 00061b0020105917, S400

   on both machines in the kernel log when the cable is plugged in
   and connects the two machines.

3) Test physical DMA using firescope:

   On the debug host, make sure that /dev/fw* is accessible,
   then start firescope:

	$ firescope
	Port 0 (/dev/fw1) opened, 2 nodes detected

	FireScope
	---------
	Target : <unspecified>
	Gen    : 1
	[Ctrl-T] choose target
	[Ctrl-H] this menu
	[Ctrl-Q] quit

    ------> Press Ctrl-T now, the output should be similar to:

	2 nodes available, local node is: 0
	 0: ffc0, uuid: 00000000 00000000 [LOCAL]
	 1: ffc1, uuid: 00279000 ba4bb801

   Besides the [LOCAL] node, it must show another node without error message.

4) Prepare for debugging with early OHCI-1394 initialization:

   4.1) Kernel compilation and installation on debug target

   Compile the kernel to be debugged with CONFIG_PROVIDE_OHCI1394_DMA_INIT
   (Kernel hacking: Provide code for enabling DMA over FireWire early on boot)
   enabled and install it on the machine to be debugged (debug target).

   4.2) Transfer the System.map of the debugged kernel to the debug host

   Copy the System.map of the kernel be debugged to the debug host (the host
   which is connected to the debugged machine over the FireWire cable).

5) Retrieving the printk buffer contents:

   With the FireWire cable connected, the OHCI-1394 driver on the debugging
   host loaded, reboot the debugged machine, booting the kernel which has
   CONFIG_PROVIDE_OHCI1394_DMA_INIT enabled, with the option ohci1394_dma=early.

   Then, on the debugging host, run firescope, for example by using -A:

	firescope -A System.map-of-debug-target-kernel

   Note: -A automatically attaches to the first non-local node. It only works
   reliably if only connected two machines are connected using FireWire.

   After having attached to the debug target, press Ctrl-D to view the
   complete printk buffer or Ctrl-U to enter auto update mode and get an
   updated live view of recent kernel messages logged on the debug target.

   Call "firescope -h" to get more information on firescope's options.

Notes
-----
Documentation and specifications: http://halobates.de/firewire/

FireWire is a trademark of Apple Inc. - for more information please refer to:
http://en.wikipedia.org/wiki/FireWire
Purpose:
Demonstrate the usage of the new open sourced rbu (Remote BIOS Update) driver
for updating BIOS images on Dell servers and desktops.

Scope:
This document discusses the functionality of the rbu driver only.
It does not cover the support needed from applications to enable the BIOS to
update itself with the image downloaded in to the memory.

Overview:
This driver works with Dell OpenManage or Dell Update Packages for updating
the BIOS on Dell servers (starting from servers sold since 1999), desktops
and notebooks (starting from those sold in 2005).
Please go to  http://support.dell.com register and you can find info on
OpenManage and Dell Update packages (DUP).
Libsmbios can also be used to update BIOS on Dell systems go to
http://linux.dell.com/libsmbios/ for details.

Dell_RBU driver supports BIOS update using the monolithic image and packetized
image methods. In case of monolithic the driver allocates a contiguous chunk
of physical pages having the BIOS image. In case of packetized the app
using the driver breaks the image in to packets of fixed sizes and the driver
would place each packet in contiguous physical memory. The driver also
maintains a link list of packets for reading them back.
If the dell_rbu driver is unloaded all the allocated memory is freed.

The rbu driver needs to have an application (as mentioned above)which will
inform the BIOS to enable the update in the next system reboot.

The user should not unload the rbu driver after downloading the BIOS image
or updating.

The driver load creates the following directories under the /sys file system.
/sys/class/firmware/dell_rbu/loading
/sys/class/firmware/dell_rbu/data
/sys/devices/platform/dell_rbu/image_type
/sys/devices/platform/dell_rbu/data
/sys/devices/platform/dell_rbu/packet_size

The driver supports two types of update mechanism; monolithic and packetized.
These update mechanism depends upon the BIOS currently running on the system.
Most of the Dell systems support a monolithic update where the BIOS image is
copied to a single contiguous block of physical memory.
In case of packet mechanism the single memory can be broken in smaller chunks
of contiguous memory and the BIOS image is scattered in these packets.

By default the driver uses monolithic memory for the update type. This can be
changed to packets during the driver load time by specifying the load
parameter image_type=packet.  This can also be changed later as below
echo packet > /sys/devices/platform/dell_rbu/image_type

In packet update mode the packet size has to be given before any packets can
be downloaded. It is done as below
echo XXXX > /sys/devices/platform/dell_rbu/packet_size
In the packet update mechanism, the user needs to create a new file having
packets of data arranged back to back. It can be done as follows
The user creates packets header, gets the chunk of the BIOS image and
places it next to the packetheader; now, the packetheader + BIOS image chunk
added together should match the specified packet_size. This makes one
packet, the user needs to create more such packets out of the entire BIOS
image file and then arrange all these packets back to back in to one single
file.
This file is then copied to /sys/class/firmware/dell_rbu/data.
Once this file gets to the driver, the driver extracts packet_size data from
the file and spreads it across the physical memory in contiguous packet_sized
space.
This method makes sure that all the packets get to the driver in a single operation.

In monolithic update the user simply get the BIOS image (.hdr file) and copies
to the data file as is without any change to the BIOS image itself.

Do the steps below to download the BIOS image.
1) echo 1 > /sys/class/firmware/dell_rbu/loading
2) cp bios_image.hdr /sys/class/firmware/dell_rbu/data
3) echo 0 > /sys/class/firmware/dell_rbu/loading

The /sys/class/firmware/dell_rbu/ entries will remain till the following is
done.
echo -1 > /sys/class/firmware/dell_rbu/loading
Until this step is completed the driver cannot be unloaded.
Also echoing either mono, packet or init in to image_type will free up the
memory allocated by the driver.

If a user by accident executes steps 1 and 3 above without executing step 2;
it will make the /sys/class/firmware/dell_rbu/ entries disappear.
The entries can be recreated by doing the following
echo init > /sys/devices/platform/dell_rbu/image_type
NOTE: echoing init in image_type does not change it original value.

Also the driver provides /sys/devices/platform/dell_rbu/data readonly file to
read back the image downloaded.

NOTE:
This driver requires a patch for firmware_class.c which has the modified
request_firmware_nowait function.
Also after updating the BIOS image a user mode application needs to execute
code which sends the BIOS update request to the BIOS. So on the next reboot
the BIOS knows about the new image downloaded and it updates itself.
Also don't unload the rbu driver if the image has to be updated.


		    LINUX ALLOCATED DEVICES (2.6+ version)

	     Maintained by Alan Cox <device@lanana.org>

		      Last revised: 6th April 2009

This list is the Linux Device List, the official registry of allocated
device numbers and /dev directory nodes for the Linux operating
system.

The latest version of this list is available from
http://www.lanana.org/docs/device-list/ or
ftp://ftp.kernel.org/pub/linux/docs/device-list/.  This version may be
newer than the one distributed with the Linux kernel.

The LaTeX version of this document is no longer maintained.

This document is included by reference into the Filesystem Hierarchy
Standard (FHS).	 The FHS is available from http://www.pathname.com/fhs/.

Allocations marked (68k/Amiga) apply to Linux/68k on the Amiga
platform only.	Allocations marked (68k/Atari) apply to Linux/68k on
the Atari platform only.

The symbol {2.6} means the allocation is obsolete and scheduled for
removal once kernel version 2.6 (or equivalent) is released. Some of these
allocations have already been removed.

This document is in the public domain.	The author requests, however,
that semantically altered versions are not distributed without
permission of the author, assuming the author can be contacted without
an unreasonable effort.

In particular, please don't sent patches for this list to Linus, at
least not without contacting me first.

I do not have any information about these devices beyond what appears
on this list.  Any such information requests will be deleted without
reply.


	  **** DEVICE DRIVERS AUTHORS PLEASE READ THIS ****

To have a major number allocated, or a minor number in situations
where that applies (e.g. busmice), please contact me with the
appropriate device information.	 Also, if you have additional
information regarding any of the devices listed below, or if I have
made a mistake, I would greatly appreciate a note.

I do, however, make a few requests about the nature of your report.
This is necessary for me to be able to keep this list up to date and
correct in a timely manner.  First of all, *please* send it to the
correct address... <device@lanana.org>.  I receive hundreds of email
messages a day, so mail sent to other addresses may very well get lost
in the avalanche.  Please put in a descriptive subject, so I can find
your mail again should I need to.  Too many people send me email
saying just "device number request" in the subject.

Second, please include a description of the device *in the same format
as this list*.	The reason for this is that it is the only way I have
found to ensure I have all the requisite information to publish your
device and avoid conflicts.

Third, please don't assume that the distributed version of the list is
up to date.  Due to the number of registrations I have to maintain it
in "batch mode", so there is likely additional registrations that
haven't been listed yet.

Fourth, remember that Linux now has extensive support for dynamic allocation
of device numbering and can use sysfs and udev to handle the naming needs.
There are still some exceptions in the serial and boot device area. Before
asking for a device number make sure you actually need one.

Finally, sometimes I have to play "namespace police."  Please don't be
offended.  I often get submissions for /dev names that would be bound
to cause conflicts down the road.  I am trying to avoid getting in a
situation where we would have to suffer an incompatible forward
change.  Therefore, please consult with me *before* you make your
device names and numbers in any way public, at least to the point
where it would be at all difficult to get them changed.

Your cooperation is appreciated.


  0		Unnamed devices (e.g. non-device mounts)
		  0 = reserved as null device number
		See block major 144, 145, 146 for expansion areas.

  1 char	Memory devices
		  1 = /dev/mem		Physical memory access
		  2 = /dev/kmem		Kernel virtual memory access
		  3 = /dev/null		Null device
		  4 = /dev/port		I/O port access
		  5 = /dev/zero		Null byte source
		  6 = /dev/core		OBSOLETE - replaced by /proc/kcore
		  7 = /dev/full		Returns ENOSPC on write
		  8 = /dev/random	Nondeterministic random number gen.
		  9 = /dev/urandom	Faster, less secure random number gen.
		 10 = /dev/aio		Asynchronous I/O notification interface
		 11 = /dev/kmsg		Writes to this come out as printk's, reads
					export the buffered printk records.
		 12 = /dev/oldmem	OBSOLETE - replaced by /proc/vmcore

  1 block	RAM disk
		  0 = /dev/ram0		First RAM disk
		  1 = /dev/ram1		Second RAM disk
		    ...
		250 = /dev/initrd	Initial RAM disk

		Older kernels had /dev/ramdisk (1, 1) here.
		/dev/initrd refers to a RAM disk which was preloaded
		by the boot loader; newer kernels use /dev/ram0 for
		the initrd.

  2 char	Pseudo-TTY masters
		  0 = /dev/ptyp0	First PTY master
		  1 = /dev/ptyp1	Second PTY master
		    ...
		255 = /dev/ptyef	256th PTY master

		Pseudo-tty's are named as follows:
		* Masters are "pty", slaves are "tty";
		* the fourth letter is one of pqrstuvwxyzabcde indicating
		  the 1st through 16th series of 16 pseudo-ttys each, and
		* the fifth letter is one of 0123456789abcdef indicating
		  the position within the series.

		These are the old-style (BSD) PTY devices; Unix98
		devices are on major 128 and above and use the PTY
		master multiplex (/dev/ptmx) to acquire a PTY on
		demand.

  2 block	Floppy disks
		  0 = /dev/fd0		Controller 0, drive 0, autodetect
		  1 = /dev/fd1		Controller 0, drive 1, autodetect
		  2 = /dev/fd2		Controller 0, drive 2, autodetect
		  3 = /dev/fd3		Controller 0, drive 3, autodetect
		128 = /dev/fd4		Controller 1, drive 0, autodetect
		129 = /dev/fd5		Controller 1, drive 1, autodetect
		130 = /dev/fd6		Controller 1, drive 2, autodetect
		131 = /dev/fd7		Controller 1, drive 3, autodetect

		To specify format, add to the autodetect device number:
		  0 = /dev/fd?		Autodetect format
		  4 = /dev/fd?d360	5.25"  360K in a 360K  drive(1)
		 20 = /dev/fd?h360	5.25"  360K in a 1200K drive(1)
		 48 = /dev/fd?h410	5.25"  410K in a 1200K drive
		 64 = /dev/fd?h420	5.25"  420K in a 1200K drive
		 24 = /dev/fd?h720	5.25"  720K in a 1200K drive
		 80 = /dev/fd?h880	5.25"  880K in a 1200K drive(1)
		  8 = /dev/fd?h1200	5.25" 1200K in a 1200K drive(1)
		 40 = /dev/fd?h1440	5.25" 1440K in a 1200K drive(1)
		 56 = /dev/fd?h1476	5.25" 1476K in a 1200K drive
		 72 = /dev/fd?h1494	5.25" 1494K in a 1200K drive
		 92 = /dev/fd?h1600	5.25" 1600K in a 1200K drive(1)

		 12 = /dev/fd?u360	3.5"   360K Double Density(2)
		 16 = /dev/fd?u720	3.5"   720K Double Density(1)
		120 = /dev/fd?u800	3.5"   800K Double Density(2)
		 52 = /dev/fd?u820	3.5"   820K Double Density
		 68 = /dev/fd?u830	3.5"   830K Double Density
		 84 = /dev/fd?u1040	3.5"  1040K Double Density(1)
		 88 = /dev/fd?u1120	3.5"  1120K Double Density(1)
		 28 = /dev/fd?u1440	3.5"  1440K High Density(1)
		124 = /dev/fd?u1600	3.5"  1600K High Density(1)
		 44 = /dev/fd?u1680	3.5"  1680K High Density(3)
		 60 = /dev/fd?u1722	3.5"  1722K High Density
		 76 = /dev/fd?u1743	3.5"  1743K High Density
		 96 = /dev/fd?u1760	3.5"  1760K High Density
		116 = /dev/fd?u1840	3.5"  1840K High Density(3)
		100 = /dev/fd?u1920	3.5"  1920K High Density(1)
		 32 = /dev/fd?u2880	3.5"  2880K Extra Density(1)
		104 = /dev/fd?u3200	3.5"  3200K Extra Density
		108 = /dev/fd?u3520	3.5"  3520K Extra Density
		112 = /dev/fd?u3840	3.5"  3840K Extra Density(1)

		 36 = /dev/fd?CompaQ	Compaq 2880K drive; obsolete?

		(1) Autodetectable format
		(2) Autodetectable format in a Double Density (720K) drive only
		(3) Autodetectable format in a High Density (1440K) drive only

		NOTE: The letter in the device name (d, q, h or u)
		signifies the type of drive: 5.25" Double Density (d),
		5.25" Quad Density (q), 5.25" High Density (h) or 3.5"
		(any model, u).	 The use of the capital letters D, H
		and E for the 3.5" models have been deprecated, since
		the drive type is insignificant for these devices.

  3 char	Pseudo-TTY slaves
		  0 = /dev/ttyp0	First PTY slave
		  1 = /dev/ttyp1	Second PTY slave
		    ...
		255 = /dev/ttyef	256th PTY slave

		These are the old-style (BSD) PTY devices; Unix98
		devices are on major 136 and above.

  3 block	First MFM, RLL and IDE hard disk/CD-ROM interface
		  0 = /dev/hda		Master: whole disk (or CD-ROM)
		 64 = /dev/hdb		Slave: whole disk (or CD-ROM)

		For partitions, add to the whole disk device number:
		  0 = /dev/hd?		Whole disk
		  1 = /dev/hd?1		First partition
		  2 = /dev/hd?2		Second partition
		    ...
		 63 = /dev/hd?63	63rd partition

		For Linux/i386, partitions 1-4 are the primary
		partitions, and 5 and above are logical partitions.
		Other versions of Linux use partitioning schemes
		appropriate to their respective architectures.

  4 char	TTY devices
		  0 = /dev/tty0		Current virtual console

		  1 = /dev/tty1		First virtual console
		    ...
		 63 = /dev/tty63	63rd virtual console
		 64 = /dev/ttyS0	First UART serial port
		    ...
		255 = /dev/ttyS191	192nd UART serial port

		UART serial ports refer to 8250/16450/16550 series devices.

		Older versions of the Linux kernel used this major
		number for BSD PTY devices.  As of Linux 2.1.115, this
		is no longer supported.	 Use major numbers 2 and 3.

  4 block	Aliases for dynamically allocated major devices to be used
		when its not possible to create the real device nodes
		because the root filesystem is mounted read-only.

                  0 = /dev/root

  5 char	Alternate TTY devices
		  0 = /dev/tty		Current TTY device
		  1 = /dev/console	System console
		  2 = /dev/ptmx		PTY master multiplex
		  3 = /dev/ttyprintk	User messages via printk TTY device
		 64 = /dev/cua0		Callout device for ttyS0
		    ...
		255 = /dev/cua191	Callout device for ttyS191

		(5,1) is /dev/console starting with Linux 2.1.71.  See
		the section on terminal devices for more information
		on /dev/console.

  6 char	Parallel printer devices
		  0 = /dev/lp0		Parallel printer on parport0
		  1 = /dev/lp1		Parallel printer on parport1
		    ...

		Current Linux kernels no longer have a fixed mapping
		between parallel ports and I/O addresses.  Instead,
		they are redirected through the parport multiplex layer.

  7 char	Virtual console capture devices
		  0 = /dev/vcs		Current vc text contents
		  1 = /dev/vcs1		tty1 text contents
		    ...
		 63 = /dev/vcs63	tty63 text contents
		128 = /dev/vcsa		Current vc text/attribute contents
		129 = /dev/vcsa1	tty1 text/attribute contents
		    ...
		191 = /dev/vcsa63	tty63 text/attribute contents

		NOTE: These devices permit both read and write access.

  7 block	Loopback devices
		  0 = /dev/loop0	First loop device
		  1 = /dev/loop1	Second loop device
		    ...

		The loop devices are used to mount filesystems not
		associated with block devices.	The binding to the
		loop devices is handled by mount(8) or losetup(8).

  8 block	SCSI disk devices (0-15)
		  0 = /dev/sda		First SCSI disk whole disk
		 16 = /dev/sdb		Second SCSI disk whole disk
		 32 = /dev/sdc		Third SCSI disk whole disk
		    ...
		240 = /dev/sdp		Sixteenth SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

  9 char	SCSI tape devices
		  0 = /dev/st0		First SCSI tape, mode 0
		  1 = /dev/st1		Second SCSI tape, mode 0
		    ...
		 32 = /dev/st0l		First SCSI tape, mode 1
		 33 = /dev/st1l		Second SCSI tape, mode 1
		    ...
		 64 = /dev/st0m		First SCSI tape, mode 2
		 65 = /dev/st1m		Second SCSI tape, mode 2
		    ...
		 96 = /dev/st0a		First SCSI tape, mode 3
		 97 = /dev/st1a		Second SCSI tape, mode 3
		      ...
		128 = /dev/nst0		First SCSI tape, mode 0, no rewind
		129 = /dev/nst1		Second SCSI tape, mode 0, no rewind
		    ...
		160 = /dev/nst0l	First SCSI tape, mode 1, no rewind
		161 = /dev/nst1l	Second SCSI tape, mode 1, no rewind
		    ...
		192 = /dev/nst0m	First SCSI tape, mode 2, no rewind
		193 = /dev/nst1m	Second SCSI tape, mode 2, no rewind
		    ...
		224 = /dev/nst0a	First SCSI tape, mode 3, no rewind
		225 = /dev/nst1a	Second SCSI tape, mode 3, no rewind
		    ...

		"No rewind" refers to the omission of the default
		automatic rewind on device close.  The MTREW or MTOFFL
		ioctl()'s can be used to rewind the tape regardless of
		the device used to access it.

  9 block	Metadisk (RAID) devices
		  0 = /dev/md0		First metadisk group
		  1 = /dev/md1		Second metadisk group
		    ...

		The metadisk driver is used to span a
		filesystem across multiple physical disks.

 10 char	Non-serial mice, misc features
		  0 = /dev/logibm	Logitech bus mouse
		  1 = /dev/psaux	PS/2-style mouse port
		  2 = /dev/inportbm	Microsoft Inport bus mouse
		  3 = /dev/atibm	ATI XL bus mouse
		  4 = /dev/jbm		J-mouse
		  4 = /dev/amigamouse	Amiga mouse (68k/Amiga)
		  5 = /dev/atarimouse	Atari mouse
		  6 = /dev/sunmouse	Sun mouse
		  7 = /dev/amigamouse1	Second Amiga mouse
		  8 = /dev/smouse	Simple serial mouse driver
		  9 = /dev/pc110pad	IBM PC-110 digitizer pad
		 10 = /dev/adbmouse	Apple Desktop Bus mouse
		 11 = /dev/vrtpanel	Vr41xx embedded touch panel
		 13 = /dev/vpcmouse	Connectix Virtual PC Mouse
		 14 = /dev/touchscreen/ucb1x00  UCB 1x00 touchscreen
		 15 = /dev/touchscreen/mk712	MK712 touchscreen
		128 = /dev/beep		Fancy beep device
		129 =
		130 = /dev/watchdog	Watchdog timer port
		131 = /dev/temperature	Machine internal temperature
		132 = /dev/hwtrap	Hardware fault trap
		133 = /dev/exttrp	External device trap
		134 = /dev/apm_bios	Advanced Power Management BIOS
		135 = /dev/rtc		Real Time Clock
		137 = /dev/vhci		Bluetooth virtual HCI driver
		139 = /dev/openprom	SPARC OpenBoot PROM
		140 = /dev/relay8	Berkshire Products Octal relay card
		141 = /dev/relay16	Berkshire Products ISO-16 relay card
		142 =
		143 = /dev/pciconf	PCI configuration space
		144 = /dev/nvram	Non-volatile configuration RAM
		145 = /dev/hfmodem	Soundcard shortwave modem control
		146 = /dev/graphics	Linux/SGI graphics device
		147 = /dev/opengl	Linux/SGI OpenGL pipe
		148 = /dev/gfx		Linux/SGI graphics effects device
		149 = /dev/input/mouse	Linux/SGI Irix emulation mouse
		150 = /dev/input/keyboard Linux/SGI Irix emulation keyboard
		151 = /dev/led		Front panel LEDs
		152 = /dev/kpoll	Kernel Poll Driver
		153 = /dev/mergemem	Memory merge device
		154 = /dev/pmu		Macintosh PowerBook power manager
		155 = /dev/isictl	MultiTech ISICom serial control
		156 = /dev/lcd		Front panel LCD display
		157 = /dev/ac		Applicom Intl Profibus card
		158 = /dev/nwbutton	Netwinder external button
		159 = /dev/nwdebug	Netwinder debug interface
		160 = /dev/nwflash	Netwinder flash memory
		161 = /dev/userdma	User-space DMA access
		162 = /dev/smbus	System Management Bus
		163 = /dev/lik		Logitech Internet Keyboard
		164 = /dev/ipmo		Intel Intelligent Platform Management
		165 = /dev/vmmon	VMware virtual machine monitor
		166 = /dev/i2o/ctl	I2O configuration manager
		167 = /dev/specialix_sxctl Specialix serial control
		168 = /dev/tcldrv	Technology Concepts serial control
		169 = /dev/specialix_rioctl Specialix RIO serial control
		170 = /dev/thinkpad/thinkpad	IBM Thinkpad devices
		171 = /dev/srripc	QNX4 API IPC manager
		172 = /dev/usemaclone	Semaphore clone device
		173 = /dev/ipmikcs	Intelligent Platform Management
		174 = /dev/uctrl	SPARCbook 3 microcontroller
		175 = /dev/agpgart	AGP Graphics Address Remapping Table
		176 = /dev/gtrsc	Gorgy Timing radio clock
		177 = /dev/cbm		Serial CBM bus
		178 = /dev/jsflash	JavaStation OS flash SIMM
		179 = /dev/xsvc		High-speed shared-mem/semaphore service
		180 = /dev/vrbuttons	Vr41xx button input device
		181 = /dev/toshiba	Toshiba laptop SMM support
		182 = /dev/perfctr	Performance-monitoring counters
		183 = /dev/hwrng	Generic random number generator
		184 = /dev/cpu/microcode CPU microcode update interface
		186 = /dev/atomicps	Atomic shapshot of process state data
		187 = /dev/irnet	IrNET device
		188 = /dev/smbusbios	SMBus BIOS
		189 = /dev/ussp_ctl	User space serial port control
		190 = /dev/crash	Mission Critical Linux crash dump facility
		191 = /dev/pcl181	<information missing>
		192 = /dev/nas_xbus	NAS xbus LCD/buttons access
		193 = /dev/d7s		SPARC 7-segment display
		194 = /dev/zkshim	Zero-Knowledge network shim control
		195 = /dev/elographics/e2201	Elographics touchscreen E271-2201
		196 = /dev/vfio/vfio	VFIO userspace driver interface
		197 = /dev/pxa3xx-gcu	PXA3xx graphics controller unit driver
		198 = /dev/sexec	Signed executable interface
		199 = /dev/scanners/cuecat :CueCat barcode scanner
		200 = /dev/net/tun	TAP/TUN network device
		201 = /dev/button/gulpb	Transmeta GULP-B buttons
		202 = /dev/emd/ctl	Enhanced Metadisk RAID (EMD) control
		203 = /dev/cuse		Cuse (character device in user-space)
		204 = /dev/video/em8300		EM8300 DVD decoder control
		205 = /dev/video/em8300_mv	EM8300 DVD decoder video
		206 = /dev/video/em8300_ma	EM8300 DVD decoder audio
		207 = /dev/video/em8300_sp	EM8300 DVD decoder subpicture
		208 = /dev/compaq/cpqphpc	Compaq PCI Hot Plug Controller
		209 = /dev/compaq/cpqrid	Compaq Remote Insight Driver
		210 = /dev/impi/bt	IMPI coprocessor block transfer
		211 = /dev/impi/smic	IMPI coprocessor stream interface
		212 = /dev/watchdogs/0	First watchdog device
		213 = /dev/watchdogs/1	Second watchdog device
		214 = /dev/watchdogs/2	Third watchdog device
		215 = /dev/watchdogs/3	Fourth watchdog device
		216 = /dev/fujitsu/apanel	Fujitsu/Siemens application panel
		217 = /dev/ni/natmotn		National Instruments Motion
		218 = /dev/kchuid	Inter-process chuid control
		219 = /dev/modems/mwave	MWave modem firmware upload
		220 = /dev/mptctl	Message passing technology (MPT) control
		221 = /dev/mvista/hssdsi	Montavista PICMG hot swap system driver
		222 = /dev/mvista/hasi		Montavista PICMG high availability
		223 = /dev/input/uinput		User level driver support for input
		224 = /dev/tpm		TCPA TPM driver
		225 = /dev/pps		Pulse Per Second driver
		226 = /dev/systrace	Systrace device
		227 = /dev/mcelog	X86_64 Machine Check Exception driver
		228 = /dev/hpet		HPET driver
		229 = /dev/fuse		Fuse (virtual filesystem in user-space)
		230 = /dev/midishare	MidiShare driver
		231 = /dev/snapshot	System memory snapshot device
		232 = /dev/kvm		Kernel-based virtual machine (hardware virtualization extensions)
		233 = /dev/kmview	View-OS A process with a view
		234 = /dev/btrfs-control	Btrfs control device
		235 = /dev/autofs	Autofs control device
		236 = /dev/mapper/control	Device-Mapper control device
		237 = /dev/loop-control Loopback control device
		238 = /dev/vhost-net	Host kernel accelerator for virtio net
		239 = /dev/uhid		User-space I/O driver support for HID subsystem

		240-254			Reserved for local use
		255			Reserved for MISC_DYNAMIC_MINOR

 11 char	Raw keyboard device	(Linux/SPARC only)
		  0 = /dev/kbd		Raw keyboard device

 11 char	Serial Mux device	(Linux/PA-RISC only)
		  0 = /dev/ttyB0	First mux port
		  1 = /dev/ttyB1	Second mux port
		    ...

 11 block	SCSI CD-ROM devices
		  0 = /dev/scd0		First SCSI CD-ROM
		  1 = /dev/scd1		Second SCSI CD-ROM
		    ...

		The prefix /dev/sr (instead of /dev/scd) has been deprecated.

 12 char	QIC-02 tape
		  2 = /dev/ntpqic11	QIC-11, no rewind-on-close
		  3 = /dev/tpqic11	QIC-11, rewind-on-close
		  4 = /dev/ntpqic24	QIC-24, no rewind-on-close
		  5 = /dev/tpqic24	QIC-24, rewind-on-close
		  6 = /dev/ntpqic120	QIC-120, no rewind-on-close
		  7 = /dev/tpqic120	QIC-120, rewind-on-close
		  8 = /dev/ntpqic150	QIC-150, no rewind-on-close
		  9 = /dev/tpqic150	QIC-150, rewind-on-close

		The device names specified are proposed -- if there
		are "standard" names for these devices, please let me know.

 12 block

 13 char	Input core
		  0 = /dev/input/js0	First joystick
		  1 = /dev/input/js1	Second joystick
		    ...
		 32 = /dev/input/mouse0	First mouse
		 33 = /dev/input/mouse1	Second mouse
		    ...
		 63 = /dev/input/mice	Unified mouse
		 64 = /dev/input/event0	First event queue
		 65 = /dev/input/event1	Second event queue
		    ...

		Each device type has 5 bits (32 minors).

 13 block	Previously used for the XT disk (/dev/xdN)
		Deleted in kernel v3.9.

 14 char	Open Sound System (OSS)
		  0 = /dev/mixer	Mixer control
		  1 = /dev/sequencer	Audio sequencer
		  2 = /dev/midi00	First MIDI port
		  3 = /dev/dsp		Digital audio
		  4 = /dev/audio	Sun-compatible digital audio
		  6 =
		  7 = /dev/audioctl	SPARC audio control device
		  8 = /dev/sequencer2	Sequencer -- alternate device
		 16 = /dev/mixer1	Second soundcard mixer control
		 17 = /dev/patmgr0	Sequencer patch manager
		 18 = /dev/midi01	Second MIDI port
		 19 = /dev/dsp1		Second soundcard digital audio
		 20 = /dev/audio1	Second soundcard Sun digital audio
		 33 = /dev/patmgr1	Sequencer patch manager
		 34 = /dev/midi02	Third MIDI port
		 50 = /dev/midi03	Fourth MIDI port

 14 block

 15 char	Joystick
		  0 = /dev/js0		First analog joystick
		  1 = /dev/js1		Second analog joystick
		    ...
		128 = /dev/djs0		First digital joystick
		129 = /dev/djs1		Second digital joystick
		    ...
 15 block	Sony CDU-31A/CDU-33A CD-ROM
		  0 = /dev/sonycd	Sony CDU-31a CD-ROM

 16 char	Non-SCSI scanners
		  0 = /dev/gs4500	Genius 4500 handheld scanner

 16 block	GoldStar CD-ROM
		  0 = /dev/gscd		GoldStar CD-ROM

 17 char	OBSOLETE (was Chase serial card)
		  0 = /dev/ttyH0	First Chase port
		  1 = /dev/ttyH1	Second Chase port
		    ...
 17 block	Optics Storage CD-ROM
		  0 = /dev/optcd	Optics Storage CD-ROM

 18 char	OBSOLETE (was Chase serial card - alternate devices)
		  0 = /dev/cuh0		Callout device for ttyH0
		  1 = /dev/cuh1		Callout device for ttyH1
		    ...
 18 block	Sanyo CD-ROM
		  0 = /dev/sjcd		Sanyo CD-ROM

 19 char	Cyclades serial card
		  0 = /dev/ttyC0	First Cyclades port
		    ...
		 31 = /dev/ttyC31	32nd Cyclades port

 19 block	"Double" compressed disk
		  0 = /dev/double0	First compressed disk
		    ...
		  7 = /dev/double7	Eighth compressed disk
		128 = /dev/cdouble0	Mirror of first compressed disk
		    ...
		135 = /dev/cdouble7	Mirror of eighth compressed disk

		See the Double documentation for the meaning of the
		mirror devices.

 20 char	Cyclades serial card - alternate devices
		  0 = /dev/cub0		Callout device for ttyC0
		    ...
		 31 = /dev/cub31	Callout device for ttyC31

 20 block	Hitachi CD-ROM (under development)
		  0 = /dev/hitcd	Hitachi CD-ROM

 21 char	Generic SCSI access
		  0 = /dev/sg0		First generic SCSI device
		  1 = /dev/sg1		Second generic SCSI device
		    ...

		Most distributions name these /dev/sga, /dev/sgb...;
		this sets an unnecessary limit of 26 SCSI devices in
		the system and is counter to standard Linux
		device-naming practice.

 21 block	Acorn MFM hard drive interface
		  0 = /dev/mfma		First MFM drive whole disk
		 64 = /dev/mfmb		Second MFM drive whole disk

		This device is used on the ARM-based Acorn RiscPC.
		Partitions are handled the same way as for IDE disks
		(see major number 3).

 22 char	Digiboard serial card
		  0 = /dev/ttyD0	First Digiboard port
		  1 = /dev/ttyD1	Second Digiboard port
		    ...
 22 block	Second IDE hard disk/CD-ROM interface
		  0 = /dev/hdc		Master: whole disk (or CD-ROM)
		 64 = /dev/hdd		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 23 char	Digiboard serial card - alternate devices
		  0 = /dev/cud0		Callout device for ttyD0
		  1 = /dev/cud1		Callout device for ttyD1
		      ...
 23 block	Mitsumi proprietary CD-ROM
		  0 = /dev/mcd		Mitsumi CD-ROM

 24 char	Stallion serial card
		  0 = /dev/ttyE0	Stallion port 0 card 0
		  1 = /dev/ttyE1	Stallion port 1 card 0
		    ...
		 64 = /dev/ttyE64	Stallion port 0 card 1
		 65 = /dev/ttyE65	Stallion port 1 card 1
		      ...
		128 = /dev/ttyE128	Stallion port 0 card 2
		129 = /dev/ttyE129	Stallion port 1 card 2
		    ...
		192 = /dev/ttyE192	Stallion port 0 card 3
		193 = /dev/ttyE193	Stallion port 1 card 3
		    ...
 24 block	Sony CDU-535 CD-ROM
		  0 = /dev/cdu535	Sony CDU-535 CD-ROM

 25 char	Stallion serial card - alternate devices
		  0 = /dev/cue0		Callout device for ttyE0
		  1 = /dev/cue1		Callout device for ttyE1
		    ...
		 64 = /dev/cue64	Callout device for ttyE64
		 65 = /dev/cue65	Callout device for ttyE65
		    ...
		128 = /dev/cue128	Callout device for ttyE128
		129 = /dev/cue129	Callout device for ttyE129
		    ...
		192 = /dev/cue192	Callout device for ttyE192
		193 = /dev/cue193	Callout device for ttyE193
		      ...
 25 block	First Matsushita (Panasonic/SoundBlaster) CD-ROM
		  0 = /dev/sbpcd0	Panasonic CD-ROM controller 0 unit 0
		  1 = /dev/sbpcd1	Panasonic CD-ROM controller 0 unit 1
		  2 = /dev/sbpcd2	Panasonic CD-ROM controller 0 unit 2
		  3 = /dev/sbpcd3	Panasonic CD-ROM controller 0 unit 3

 26 char

 26 block	Second Matsushita (Panasonic/SoundBlaster) CD-ROM
		  0 = /dev/sbpcd4	Panasonic CD-ROM controller 1 unit 0
		  1 = /dev/sbpcd5	Panasonic CD-ROM controller 1 unit 1
		  2 = /dev/sbpcd6	Panasonic CD-ROM controller 1 unit 2
		  3 = /dev/sbpcd7	Panasonic CD-ROM controller 1 unit 3

 27 char	QIC-117 tape
		  0 = /dev/qft0		Unit 0, rewind-on-close
		  1 = /dev/qft1		Unit 1, rewind-on-close
		  2 = /dev/qft2		Unit 2, rewind-on-close
		  3 = /dev/qft3		Unit 3, rewind-on-close
		  4 = /dev/nqft0	Unit 0, no rewind-on-close
		  5 = /dev/nqft1	Unit 1, no rewind-on-close
		  6 = /dev/nqft2	Unit 2, no rewind-on-close
		  7 = /dev/nqft3	Unit 3, no rewind-on-close
		 16 = /dev/zqft0	Unit 0, rewind-on-close, compression
		 17 = /dev/zqft1	Unit 1, rewind-on-close, compression
		 18 = /dev/zqft2	Unit 2, rewind-on-close, compression
		 19 = /dev/zqft3	Unit 3, rewind-on-close, compression
		 20 = /dev/nzqft0	Unit 0, no rewind-on-close, compression
		 21 = /dev/nzqft1	Unit 1, no rewind-on-close, compression
		 22 = /dev/nzqft2	Unit 2, no rewind-on-close, compression
		 23 = /dev/nzqft3	Unit 3, no rewind-on-close, compression
		 32 = /dev/rawqft0	Unit 0, rewind-on-close, no file marks
		 33 = /dev/rawqft1	Unit 1, rewind-on-close, no file marks
		 34 = /dev/rawqft2	Unit 2, rewind-on-close, no file marks
		 35 = /dev/rawqft3	Unit 3, rewind-on-close, no file marks
		 36 = /dev/nrawqft0	Unit 0, no rewind-on-close, no file marks
		 37 = /dev/nrawqft1	Unit 1, no rewind-on-close, no file marks
		 38 = /dev/nrawqft2	Unit 2, no rewind-on-close, no file marks
		 39 = /dev/nrawqft3	Unit 3, no rewind-on-close, no file marks

 27 block	Third Matsushita (Panasonic/SoundBlaster) CD-ROM
		  0 = /dev/sbpcd8	Panasonic CD-ROM controller 2 unit 0
		  1 = /dev/sbpcd9	Panasonic CD-ROM controller 2 unit 1
		  2 = /dev/sbpcd10	Panasonic CD-ROM controller 2 unit 2
		  3 = /dev/sbpcd11	Panasonic CD-ROM controller 2 unit 3

 28 char	Stallion serial card - card programming
		  0 = /dev/staliomem0	First Stallion card I/O memory
		  1 = /dev/staliomem1	Second Stallion card I/O memory
		  2 = /dev/staliomem2	Third Stallion card I/O memory
		  3 = /dev/staliomem3	Fourth Stallion card I/O memory

 28 char	Atari SLM ACSI laser printer (68k/Atari)
		  0 = /dev/slm0		First SLM laser printer
		  1 = /dev/slm1		Second SLM laser printer
		    ...
 28 block	Fourth Matsushita (Panasonic/SoundBlaster) CD-ROM
		  0 = /dev/sbpcd12	Panasonic CD-ROM controller 3 unit 0
		  1 = /dev/sbpcd13	Panasonic CD-ROM controller 3 unit 1
		  2 = /dev/sbpcd14	Panasonic CD-ROM controller 3 unit 2
		  3 = /dev/sbpcd15	Panasonic CD-ROM controller 3 unit 3

 28 block	ACSI disk (68k/Atari)
		  0 = /dev/ada		First ACSI disk whole disk
		 16 = /dev/adb		Second ACSI disk whole disk
		 32 = /dev/adc		Third ACSI disk whole disk
		    ...
		240 = /dev/adp		16th ACSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15, like SCSI.

 29 char	Universal frame buffer
		  0 = /dev/fb0		First frame buffer
		  1 = /dev/fb1		Second frame buffer
		    ...
		 31 = /dev/fb31		32nd frame buffer

 29 block	Aztech/Orchid/Okano/Wearnes CD-ROM
		  0 = /dev/aztcd	Aztech CD-ROM

 30 char	iBCS-2 compatibility devices
		  0 = /dev/socksys	Socket access
		  1 = /dev/spx		SVR3 local X interface
		 32 = /dev/inet/ip	Network access
		 33 = /dev/inet/icmp
		 34 = /dev/inet/ggp
		 35 = /dev/inet/ipip
		 36 = /dev/inet/tcp
		 37 = /dev/inet/egp
		 38 = /dev/inet/pup
		 39 = /dev/inet/udp
		 40 = /dev/inet/idp
		 41 = /dev/inet/rawip

		Additionally, iBCS-2 requires the following links:

		/dev/ip -> /dev/inet/ip
		/dev/icmp -> /dev/inet/icmp
		/dev/ggp -> /dev/inet/ggp
		/dev/ipip -> /dev/inet/ipip
		/dev/tcp -> /dev/inet/tcp
		/dev/egp -> /dev/inet/egp
		/dev/pup -> /dev/inet/pup
		/dev/udp -> /dev/inet/udp
		/dev/idp -> /dev/inet/idp
		/dev/rawip -> /dev/inet/rawip
		/dev/inet/arp -> /dev/inet/udp
		/dev/inet/rip -> /dev/inet/udp
		/dev/nfsd -> /dev/socksys
		/dev/X0R -> /dev/null (? apparently not required ?)

 30 block	Philips LMS CM-205 CD-ROM
		  0 = /dev/cm205cd	Philips LMS CM-205 CD-ROM

		/dev/lmscd is an older name for this device.  This
		driver does not work with the CM-205MS CD-ROM.

 31 char	MPU-401 MIDI
		  0 = /dev/mpu401data	MPU-401 data port
		  1 = /dev/mpu401stat	MPU-401 status port

 31 block	ROM/flash memory card
		  0 = /dev/rom0		First ROM card (rw)
		      ...
		  7 = /dev/rom7		Eighth ROM card (rw)
		  8 = /dev/rrom0	First ROM card (ro)
		    ...
		 15 = /dev/rrom7	Eighth ROM card (ro)
		 16 = /dev/flash0	First flash memory card (rw)
		    ...
		 23 = /dev/flash7	Eighth flash memory card (rw)
		 24 = /dev/rflash0	First flash memory card (ro)
		    ...
		 31 = /dev/rflash7	Eighth flash memory card (ro)

		The read-write (rw) devices support back-caching
		written data in RAM, as well as writing to flash RAM
		devices.  The read-only devices (ro) support reading
		only.

 32 char	Specialix serial card
		  0 = /dev/ttyX0	First Specialix port
		  1 = /dev/ttyX1	Second Specialix port
		    ...
 32 block	Philips LMS CM-206 CD-ROM
		  0 = /dev/cm206cd	Philips LMS CM-206 CD-ROM

 33 char	Specialix serial card - alternate devices
		  0 = /dev/cux0		Callout device for ttyX0
		  1 = /dev/cux1		Callout device for ttyX1
		    ...
 33 block	Third IDE hard disk/CD-ROM interface
		  0 = /dev/hde		Master: whole disk (or CD-ROM)
		 64 = /dev/hdf		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 34 char	Z8530 HDLC driver
		  0 = /dev/scc0		First Z8530, first port
		  1 = /dev/scc1		First Z8530, second port
		  2 = /dev/scc2		Second Z8530, first port
		  3 = /dev/scc3		Second Z8530, second port
		    ...

		In a previous version these devices were named
		/dev/sc1 for /dev/scc0, /dev/sc2 for /dev/scc1, and so
		on.

 34 block	Fourth IDE hard disk/CD-ROM interface
		  0 = /dev/hdg		Master: whole disk (or CD-ROM)
		 64 = /dev/hdh		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 35 char	tclmidi MIDI driver
		  0 = /dev/midi0	First MIDI port, kernel timed
		  1 = /dev/midi1	Second MIDI port, kernel timed
		  2 = /dev/midi2	Third MIDI port, kernel timed
		  3 = /dev/midi3	Fourth MIDI port, kernel timed
		 64 = /dev/rmidi0	First MIDI port, untimed
		 65 = /dev/rmidi1	Second MIDI port, untimed
		 66 = /dev/rmidi2	Third MIDI port, untimed
		 67 = /dev/rmidi3	Fourth MIDI port, untimed
		128 = /dev/smpte0	First MIDI port, SMPTE timed
		129 = /dev/smpte1	Second MIDI port, SMPTE timed
		130 = /dev/smpte2	Third MIDI port, SMPTE timed
		131 = /dev/smpte3	Fourth MIDI port, SMPTE timed

 35 block	Slow memory ramdisk
		  0 = /dev/slram	Slow memory ramdisk

 36 char	Netlink support
		  0 = /dev/route	Routing, device updates, kernel to user
		  1 = /dev/skip		enSKIP security cache control
		  3 = /dev/fwmonitor	Firewall packet copies
		 16 = /dev/tap0		First Ethertap device
		    ...
		 31 = /dev/tap15	16th Ethertap device

 36 block	OBSOLETE (was MCA ESDI hard disk)

 37 char	IDE tape
		  0 = /dev/ht0		First IDE tape
		  1 = /dev/ht1		Second IDE tape
		    ...
		128 = /dev/nht0		First IDE tape, no rewind-on-close
		129 = /dev/nht1		Second IDE tape, no rewind-on-close
		    ...

		Currently, only one IDE tape drive is supported.

 37 block	Zorro II ramdisk
		  0 = /dev/z2ram	Zorro II ramdisk

 38 char	Myricom PCI Myrinet board
		  0 = /dev/mlanai0	First Myrinet board
		  1 = /dev/mlanai1	Second Myrinet board
		    ...

		This device is used for status query, board control
		and "user level packet I/O."  This board is also
		accessible as a standard networking "eth" device.

 38 block	OBSOLETE (was Linux/AP+)

 39 char	ML-16P experimental I/O board
		  0 = /dev/ml16pa-a0	First card, first analog channel
		  1 = /dev/ml16pa-a1	First card, second analog channel
		    ...
		 15 = /dev/ml16pa-a15	First card, 16th analog channel
		 16 = /dev/ml16pa-d	First card, digital lines
		 17 = /dev/ml16pa-c0	First card, first counter/timer
		 18 = /dev/ml16pa-c1	First card, second counter/timer
		 19 = /dev/ml16pa-c2	First card, third counter/timer
		 32 = /dev/ml16pb-a0	Second card, first analog channel
		 33 = /dev/ml16pb-a1	Second card, second analog channel
		    ...
		 47 = /dev/ml16pb-a15	Second card, 16th analog channel
		 48 = /dev/ml16pb-d	Second card, digital lines
		 49 = /dev/ml16pb-c0	Second card, first counter/timer
		 50 = /dev/ml16pb-c1	Second card, second counter/timer
		 51 = /dev/ml16pb-c2	Second card, third counter/timer
		      ...
 39 block

 40 char

 40 block

 41 char	Yet Another Micro Monitor
		  0 = /dev/yamm		Yet Another Micro Monitor

 41 block

 42 char	Demo/sample use

 42 block	Demo/sample use

		This number is intended for use in sample code, as
		well as a general "example" device number.  It
		should never be used for a device driver that is being
		distributed; either obtain an official number or use
		the local/experimental range.  The sudden addition or
		removal of a driver with this number should not cause
		ill effects to the system (bugs excepted.)

		IN PARTICULAR, ANY DISTRIBUTION WHICH CONTAINS A
		DEVICE DRIVER USING MAJOR NUMBER 42 IS NONCOMPLIANT.

 43 char	isdn4linux virtual modem
		  0 = /dev/ttyI0	First virtual modem
		    ...
		 63 = /dev/ttyI63	64th virtual modem

 43 block	Network block devices
		  0 = /dev/nb0		First network block device
		  1 = /dev/nb1		Second network block device
		    ...

		Network Block Device is somehow similar to loopback
		devices: If you read from it, it sends packet across
		network asking server for data. If you write to it, it
		sends packet telling server to write. It could be used
		to mounting filesystems over the net, swapping over
		the net, implementing block device in userland etc.

 44 char	isdn4linux virtual modem - alternate devices
		  0 = /dev/cui0		Callout device for ttyI0
		    ...
		 63 = /dev/cui63	Callout device for ttyI63

 44 block	Flash Translation Layer (FTL) filesystems
		  0 = /dev/ftla		FTL on first Memory Technology Device
		 16 = /dev/ftlb		FTL on second Memory Technology Device
		 32 = /dev/ftlc		FTL on third Memory Technology Device
		    ...
		240 = /dev/ftlp		FTL on 16th Memory Technology Device

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the partition
		limit is 15 rather than 63 per disk (same as SCSI.)

 45 char	isdn4linux ISDN BRI driver
		  0 = /dev/isdn0	First virtual B channel raw data
		    ...
		 63 = /dev/isdn63	64th virtual B channel raw data
		 64 = /dev/isdnctrl0	First channel control/debug
		    ...
		127 = /dev/isdnctrl63	64th channel control/debug

		128 = /dev/ippp0	First SyncPPP device
		    ...
		191 = /dev/ippp63	64th SyncPPP device

		255 = /dev/isdninfo	ISDN monitor interface

 45 block	Parallel port IDE disk devices
		  0 = /dev/pda		First parallel port IDE disk
		 16 = /dev/pdb		Second parallel port IDE disk
		 32 = /dev/pdc		Third parallel port IDE disk
		 48 = /dev/pdd		Fourth parallel port IDE disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the partition
		limit is 15 rather than 63 per disk.

 46 char	Comtrol Rocketport serial card
		  0 = /dev/ttyR0	First Rocketport port
		  1 = /dev/ttyR1	Second Rocketport port
		    ...
 46 block	Parallel port ATAPI CD-ROM devices
		  0 = /dev/pcd0		First parallel port ATAPI CD-ROM
		  1 = /dev/pcd1		Second parallel port ATAPI CD-ROM
		  2 = /dev/pcd2		Third parallel port ATAPI CD-ROM
		  3 = /dev/pcd3		Fourth parallel port ATAPI CD-ROM

 47 char	Comtrol Rocketport serial card - alternate devices
		  0 = /dev/cur0		Callout device for ttyR0
		  1 = /dev/cur1		Callout device for ttyR1
		    ...
 47 block	Parallel port ATAPI disk devices
		  0 = /dev/pf0		First parallel port ATAPI disk
		  1 = /dev/pf1		Second parallel port ATAPI disk
		  2 = /dev/pf2		Third parallel port ATAPI disk
		  3 = /dev/pf3		Fourth parallel port ATAPI disk

		This driver is intended for floppy disks and similar
		devices and hence does not support partitioning.

 48 char	SDL RISCom serial card
		  0 = /dev/ttyL0	First RISCom port
		  1 = /dev/ttyL1	Second RISCom port
		    ...
 48 block	Mylex DAC960 PCI RAID controller; first controller
		  0 = /dev/rd/c0d0	First disk, whole disk
		  8 = /dev/rd/c0d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c0d31	32nd disk, whole disk

		For partitions add:
		  0 = /dev/rd/c?d?	Whole disk
		  1 = /dev/rd/c?d?p1	First partition
		    ...
		  7 = /dev/rd/c?d?p7	Seventh partition

 49 char	SDL RISCom serial card - alternate devices
		  0 = /dev/cul0		Callout device for ttyL0
		  1 = /dev/cul1		Callout device for ttyL1
		    ...
 49 block	Mylex DAC960 PCI RAID controller; second controller
		  0 = /dev/rd/c1d0	First disk, whole disk
		  8 = /dev/rd/c1d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c1d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 50 char	Reserved for GLINT

 50 block	Mylex DAC960 PCI RAID controller; third controller
		  0 = /dev/rd/c2d0	First disk, whole disk
		  8 = /dev/rd/c2d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c2d31	32nd disk, whole disk

 51 char	Baycom radio modem OR Radio Tech BIM-XXX-RS232 radio modem
		  0 = /dev/bc0		First Baycom radio modem
		  1 = /dev/bc1		Second Baycom radio modem
		    ...
 51 block	Mylex DAC960 PCI RAID controller; fourth controller
		  0 = /dev/rd/c3d0	First disk, whole disk
		  8 = /dev/rd/c3d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c3d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 52 char	Spellcaster DataComm/BRI ISDN card
		  0 = /dev/dcbri0	First DataComm card
		  1 = /dev/dcbri1	Second DataComm card
		  2 = /dev/dcbri2	Third DataComm card
		  3 = /dev/dcbri3	Fourth DataComm card

 52 block	Mylex DAC960 PCI RAID controller; fifth controller
		  0 = /dev/rd/c4d0	First disk, whole disk
		  8 = /dev/rd/c4d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c4d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 53 char	BDM interface for remote debugging MC683xx microcontrollers
		  0 = /dev/pd_bdm0	PD BDM interface on lp0
		  1 = /dev/pd_bdm1	PD BDM interface on lp1
		  2 = /dev/pd_bdm2	PD BDM interface on lp2
		  4 = /dev/icd_bdm0	ICD BDM interface on lp0
		  5 = /dev/icd_bdm1	ICD BDM interface on lp1
		  6 = /dev/icd_bdm2	ICD BDM interface on lp2

		This device is used for the interfacing to the MC683xx
		microcontrollers via Background Debug Mode by use of a
		Parallel Port interface. PD is the Motorola Public
		Domain Interface and ICD is the commercial interface
		by P&E.

 53 block	Mylex DAC960 PCI RAID controller; sixth controller
		  0 = /dev/rd/c5d0	First disk, whole disk
		  8 = /dev/rd/c5d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c5d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 54 char	Electrocardiognosis Holter serial card
		  0 = /dev/holter0	First Holter port
		  1 = /dev/holter1	Second Holter port
		  2 = /dev/holter2	Third Holter port

		A custom serial card used by Electrocardiognosis SRL
		<mseritan@ottonel.pub.ro> to transfer data from Holter
		24-hour heart monitoring equipment.

 54 block	Mylex DAC960 PCI RAID controller; seventh controller
		  0 = /dev/rd/c6d0	First disk, whole disk
		  8 = /dev/rd/c6d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c6d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 55 char	DSP56001 digital signal processor
		  0 = /dev/dsp56k	First DSP56001

 55 block	Mylex DAC960 PCI RAID controller; eighth controller
		  0 = /dev/rd/c7d0	First disk, whole disk
		  8 = /dev/rd/c7d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c7d31	32nd disk, whole disk

		Partitions are handled as for major 48.

 56 char	Apple Desktop Bus
		  0 = /dev/adb		ADB bus control

		Additional devices will be added to this number, all
		starting with /dev/adb.

 56 block	Fifth IDE hard disk/CD-ROM interface
		  0 = /dev/hdi		Master: whole disk (or CD-ROM)
		 64 = /dev/hdj		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 57 char	Hayes ESP serial card
		  0 = /dev/ttyP0	First ESP port
		  1 = /dev/ttyP1	Second ESP port
		    ...

 57 block	Sixth IDE hard disk/CD-ROM interface
		  0 = /dev/hdk		Master: whole disk (or CD-ROM)
		 64 = /dev/hdl		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 58 char	Hayes ESP serial card - alternate devices
		  0 = /dev/cup0		Callout device for ttyP0
		  1 = /dev/cup1		Callout device for ttyP1
		    ...

 58 block	Reserved for logical volume manager

 59 char	sf firewall package
		  0 = /dev/firewall	Communication with sf kernel module

 59 block	Generic PDA filesystem device
		  0 = /dev/pda0		First PDA device
		  1 = /dev/pda1		Second PDA device
		    ...

		The pda devices are used to mount filesystems on
		remote pda's (basically slow handheld machines with
		proprietary OS's and limited memory and storage
		running small fs translation drivers) through serial /
		IRDA / parallel links.

		NAMING CONFLICT -- PROPOSED REVISED NAME /dev/rpda0 etc

 60-63 char	LOCAL/EXPERIMENTAL USE

 60-63 block	LOCAL/EXPERIMENTAL USE
		Allocated for local/experimental use.  For devices not
		assigned official numbers, these ranges should be
		used in order to avoid conflicting with future assignments.

 64 char	ENskip kernel encryption package
		  0 = /dev/enskip	Communication with ENskip kernel module

 64 block	Scramdisk/DriveCrypt encrypted devices
		  0 = /dev/scramdisk/master    Master node for ioctls
		  1 = /dev/scramdisk/1         First encrypted device
		  2 = /dev/scramdisk/2         Second encrypted device
		  ...
		255 = /dev/scramdisk/255       255th encrypted device

		The filename of the encrypted container and the passwords
		are sent via ioctls (using the sdmount tool) to the master
		node which then activates them via one of the
		/dev/scramdisk/x nodes for loop mounting (all handled
		through the sdmount tool).

		Requested by: andy@scramdisklinux.org

 65 char	Sundance "plink" Transputer boards (obsolete, unused)
		  0 = /dev/plink0	First plink device
		  1 = /dev/plink1	Second plink device
		  2 = /dev/plink2	Third plink device
		  3 = /dev/plink3	Fourth plink device
		 64 = /dev/rplink0	First plink device, raw
		 65 = /dev/rplink1	Second plink device, raw
		 66 = /dev/rplink2	Third plink device, raw
		 67 = /dev/rplink3	Fourth plink device, raw
		128 = /dev/plink0d	First plink device, debug
		129 = /dev/plink1d	Second plink device, debug
		130 = /dev/plink2d	Third plink device, debug
		131 = /dev/plink3d	Fourth plink device, debug
		192 = /dev/rplink0d	First plink device, raw, debug
		193 = /dev/rplink1d	Second plink device, raw, debug
		194 = /dev/rplink2d	Third plink device, raw, debug
		195 = /dev/rplink3d	Fourth plink device, raw, debug

		This is a commercial driver; contact James Howes
		<jth@prosig.demon.co.uk> for information.

 65 block	SCSI disk devices (16-31)
		  0 = /dev/sdq		17th SCSI disk whole disk
		 16 = /dev/sdr		18th SCSI disk whole disk
		 32 = /dev/sds		19th SCSI disk whole disk
		    ...
		240 = /dev/sdaf		32nd SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 66 char	YARC PowerPC PCI coprocessor card
		  0 = /dev/yppcpci0	First YARC card
		  1 = /dev/yppcpci1	Second YARC card
		    ...

 66 block	SCSI disk devices (32-47)
		  0 = /dev/sdag		33th SCSI disk whole disk
		 16 = /dev/sdah		34th SCSI disk whole disk
		 32 = /dev/sdai		35th SCSI disk whole disk
		    ...
		240 = /dev/sdav		48nd SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 67 char	Coda network file system
		  0 = /dev/cfs0		Coda cache manager

		See http://www.coda.cs.cmu.edu for information about Coda.

 67 block	SCSI disk devices (48-63)
		  0 = /dev/sdaw		49th SCSI disk whole disk
		 16 = /dev/sdax		50th SCSI disk whole disk
		 32 = /dev/sday		51st SCSI disk whole disk
		    ...
		240 = /dev/sdbl		64th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 68 char	CAPI 2.0 interface
		  0 = /dev/capi20	Control device
		  1 = /dev/capi20.00	First CAPI 2.0 application
		  2 = /dev/capi20.01	Second CAPI 2.0 application
		    ...
		 20 = /dev/capi20.19	19th CAPI 2.0 application

		ISDN CAPI 2.0 driver for use with CAPI 2.0
		applications; currently supports the AVM B1 card.

 68 block	SCSI disk devices (64-79)
		  0 = /dev/sdbm		65th SCSI disk whole disk
		 16 = /dev/sdbn		66th SCSI disk whole disk
		 32 = /dev/sdbo		67th SCSI disk whole disk
		    ...
		240 = /dev/sdcb		80th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 69 char	MA16 numeric accelerator card
		  0 = /dev/ma16		Board memory access

 69 block	SCSI disk devices (80-95)
		  0 = /dev/sdcc		81st SCSI disk whole disk
		 16 = /dev/sdcd		82nd SCSI disk whole disk
		 32 = /dev/sdce		83th SCSI disk whole disk
		    ...
		240 = /dev/sdcr		96th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 70 char	SpellCaster Protocol Services Interface
		  0 = /dev/apscfg	Configuration interface
		  1 = /dev/apsauth	Authentication interface
		  2 = /dev/apslog	Logging interface
		  3 = /dev/apsdbg	Debugging interface
		 64 = /dev/apsisdn	ISDN command interface
		 65 = /dev/apsasync	Async command interface
		128 = /dev/apsmon	Monitor interface

 70 block	SCSI disk devices (96-111)
		  0 = /dev/sdcs		97th SCSI disk whole disk
		 16 = /dev/sdct		98th SCSI disk whole disk
		 32 = /dev/sdcu		99th SCSI disk whole disk
		    ...
		240 = /dev/sddh		112nd SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 71 char	Computone IntelliPort II serial card
		  0 = /dev/ttyF0	IntelliPort II board 0, port 0
		  1 = /dev/ttyF1	IntelliPort II board 0, port 1
		    ...
		 63 = /dev/ttyF63	IntelliPort II board 0, port 63
		 64 = /dev/ttyF64	IntelliPort II board 1, port 0
		 65 = /dev/ttyF65	IntelliPort II board 1, port 1
		    ...
		127 = /dev/ttyF127	IntelliPort II board 1, port 63
		128 = /dev/ttyF128	IntelliPort II board 2, port 0
		129 = /dev/ttyF129	IntelliPort II board 2, port 1
		    ...
		191 = /dev/ttyF191	IntelliPort II board 2, port 63
		192 = /dev/ttyF192	IntelliPort II board 3, port 0
		193 = /dev/ttyF193	IntelliPort II board 3, port 1
		    ...
		255 = /dev/ttyF255	IntelliPort II board 3, port 63

 71 block	SCSI disk devices (112-127)
		  0 = /dev/sddi		113th SCSI disk whole disk
		 16 = /dev/sddj		114th SCSI disk whole disk
		 32 = /dev/sddk		115th SCSI disk whole disk
		    ...
		240 = /dev/sddx		128th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 72 char	Computone IntelliPort II serial card - alternate devices
		  0 = /dev/cuf0		Callout device for ttyF0
		  1 = /dev/cuf1		Callout device for ttyF1
		    ...
		 63 = /dev/cuf63	Callout device for ttyF63
		 64 = /dev/cuf64	Callout device for ttyF64
		 65 = /dev/cuf65	Callout device for ttyF65
		    ...
		127 = /dev/cuf127	Callout device for ttyF127
		128 = /dev/cuf128	Callout device for ttyF128
		129 = /dev/cuf129	Callout device for ttyF129
		    ...
		191 = /dev/cuf191	Callout device for ttyF191
		192 = /dev/cuf192	Callout device for ttyF192
		193 = /dev/cuf193	Callout device for ttyF193
		    ...
		255 = /dev/cuf255	Callout device for ttyF255

 72 block	Compaq Intelligent Drive Array, first controller
		  0 = /dev/ida/c0d0	First logical drive whole disk
		 16 = /dev/ida/c0d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c0d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 73 char	Computone IntelliPort II serial card - control devices
		  0 = /dev/ip2ipl0	Loadware device for board 0
		  1 = /dev/ip2stat0	Status device for board 0
		  4 = /dev/ip2ipl1	Loadware device for board 1
		  5 = /dev/ip2stat1	Status device for board 1
		  8 = /dev/ip2ipl2	Loadware device for board 2
		  9 = /dev/ip2stat2	Status device for board 2
		 12 = /dev/ip2ipl3	Loadware device for board 3
		 13 = /dev/ip2stat3	Status device for board 3

 73 block	Compaq Intelligent Drive Array, second controller
		  0 = /dev/ida/c1d0	First logical drive whole disk
		 16 = /dev/ida/c1d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c1d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 74 char	SCI bridge
		  0 = /dev/SCI/0	SCI device 0
		  1 = /dev/SCI/1	SCI device 1
		    ...

		Currently for Dolphin Interconnect Solutions' PCI-SCI
		bridge.

 74 block	Compaq Intelligent Drive Array, third controller
		  0 = /dev/ida/c2d0	First logical drive whole disk
		 16 = /dev/ida/c2d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c2d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 75 char	Specialix IO8+ serial card
		  0 = /dev/ttyW0	First IO8+ port, first card
		  1 = /dev/ttyW1	Second IO8+ port, first card
		    ...
		  8 = /dev/ttyW8	First IO8+ port, second card
		    ...

 75 block	Compaq Intelligent Drive Array, fourth controller
		  0 = /dev/ida/c3d0	First logical drive whole disk
		 16 = /dev/ida/c3d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c3d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 76 char	Specialix IO8+ serial card - alternate devices
		  0 = /dev/cuw0		Callout device for ttyW0
		  1 = /dev/cuw1		Callout device for ttyW1
		    ...
		  8 = /dev/cuw8		Callout device for ttyW8
		    ...

 76 block	Compaq Intelligent Drive Array, fifth controller
		  0 = /dev/ida/c4d0	First logical drive whole disk
		 16 = /dev/ida/c4d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c4d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.


 77 char	ComScire Quantum Noise Generator
		  0 = /dev/qng		ComScire Quantum Noise Generator

 77 block	Compaq Intelligent Drive Array, sixth controller
		  0 = /dev/ida/c5d0	First logical drive whole disk
		 16 = /dev/ida/c5d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c5d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 78 char	PAM Software's multimodem boards
		  0 = /dev/ttyM0	First PAM modem
		  1 = /dev/ttyM1	Second PAM modem
		    ...

 78 block	Compaq Intelligent Drive Array, seventh controller
		  0 = /dev/ida/c6d0	First logical drive whole disk
		 16 = /dev/ida/c6d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c6d15	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 79 char	PAM Software's multimodem boards - alternate devices
		  0 = /dev/cum0		Callout device for ttyM0
		  1 = /dev/cum1		Callout device for ttyM1
		    ...

 79 block	Compaq Intelligent Drive Array, eighth controller
		  0 = /dev/ida/c7d0	First logical drive whole disk
		 16 = /dev/ida/c7d1	Second logical drive whole disk
		    ...
		240 = /dev/ida/c715	16th logical drive whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

 80 char	Photometrics AT200 CCD camera
		  0 = /dev/at200	Photometrics AT200 CCD camera

 80 block	I2O hard disk
		  0 = /dev/i2o/hda	First I2O hard disk, whole disk
		 16 = /dev/i2o/hdb	Second I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdp	16th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 81 char	video4linux
		  0 = /dev/video0	Video capture/overlay device
		    ...
		 63 = /dev/video63	Video capture/overlay device
		 64 = /dev/radio0	Radio device
		    ...
		127 = /dev/radio63	Radio device
		128 = /dev/swradio0	Software Defined Radio device
		    ...
		191 = /dev/swradio63	Software Defined Radio device
		224 = /dev/vbi0		Vertical blank interrupt
		    ...
		255 = /dev/vbi31	Vertical blank interrupt

		Minor numbers are allocated dynamically unless
		CONFIG_VIDEO_FIXED_MINOR_RANGES (default n)
		configuration option is set.

 81 block	I2O hard disk
		  0 = /dev/i2o/hdq	17th I2O hard disk, whole disk
		 16 = /dev/i2o/hdr	18th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdaf	32nd I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 82 char	WiNRADiO communications receiver card
		  0 = /dev/winradio0	First WiNRADiO card
		  1 = /dev/winradio1	Second WiNRADiO card
		    ...

		The driver and documentation may be obtained from
		http://www.winradio.com/

 82 block	I2O hard disk
		  0 = /dev/i2o/hdag	33rd I2O hard disk, whole disk
		 16 = /dev/i2o/hdah	34th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdav	48th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 83 char	Matrox mga_vid video driver
 		 0 = /dev/mga_vid0	1st video card
		 1 = /dev/mga_vid1	2nd video card
		 2 = /dev/mga_vid2	3rd video card
		  ...
	        15 = /dev/mga_vid15	16th video card

 83 block	I2O hard disk
		  0 = /dev/i2o/hdaw	49th I2O hard disk, whole disk
		 16 = /dev/i2o/hdax	50th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdbl	64th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 84 char	Ikon 1011[57] Versatec Greensheet Interface
		  0 = /dev/ihcp0	First Greensheet port
		  1 = /dev/ihcp1	Second Greensheet port

 84 block	I2O hard disk
		  0 = /dev/i2o/hdbm	65th I2O hard disk, whole disk
		 16 = /dev/i2o/hdbn	66th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdcb	80th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 85 char	Linux/SGI shared memory input queue
		  0 = /dev/shmiq	Master shared input queue
		  1 = /dev/qcntl0	First device pushed
		  2 = /dev/qcntl1	Second device pushed
		    ...

 85 block	I2O hard disk
		  0 = /dev/i2o/hdcc	81st I2O hard disk, whole disk
		 16 = /dev/i2o/hdcd	82nd I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hdcr	96th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 86 char	SCSI media changer
		  0 = /dev/sch0		First SCSI media changer
		  1 = /dev/sch1		Second SCSI media changer
		    ...

 86 block	I2O hard disk
		  0 = /dev/i2o/hdcs	97th I2O hard disk, whole disk
		 16 = /dev/i2o/hdct	98th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hddh	112th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 87 char	Sony Control-A1 stereo control bus
		  0 = /dev/controla0	First device on chain
		  1 = /dev/controla1	Second device on chain
		    ...

 87 block	I2O hard disk
		  0 = /dev/i2o/hddi	113rd I2O hard disk, whole disk
		 16 = /dev/i2o/hddj	114th I2O hard disk, whole disk
		    ...
		240 = /dev/i2o/hddx	128th I2O hard disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 88 char	COMX synchronous serial card
		  0 = /dev/comx0	COMX channel 0
		  1 = /dev/comx1	COMX channel 1
		    ...

 88 block	Seventh IDE hard disk/CD-ROM interface
		  0 = /dev/hdm		Master: whole disk (or CD-ROM)
		 64 = /dev/hdn		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 89 char	I2C bus interface
		  0 = /dev/i2c-0	First I2C adapter
		  1 = /dev/i2c-1	Second I2C adapter
		    ...

 89 block	Eighth IDE hard disk/CD-ROM interface
		  0 = /dev/hdo		Master: whole disk (or CD-ROM)
		 64 = /dev/hdp		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 90 char	Memory Technology Device (RAM, ROM, Flash)
		  0 = /dev/mtd0		First MTD (rw)
		  1 = /dev/mtdr0	First MTD (ro)
		    ...
		 30 = /dev/mtd15	16th MTD (rw)
		 31 = /dev/mtdr15	16th MTD (ro)

 90 block	Ninth IDE hard disk/CD-ROM interface
		  0 = /dev/hdq		Master: whole disk (or CD-ROM)
		 64 = /dev/hdr		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 91 char	CAN-Bus devices
		  0 = /dev/can0		First CAN-Bus controller
		  1 = /dev/can1		Second CAN-Bus controller
		    ...

 91 block	Tenth IDE hard disk/CD-ROM interface
		  0 = /dev/hds		Master: whole disk (or CD-ROM)
		 64 = /dev/hdt		Slave: whole disk (or CD-ROM)

		Partitions are handled the same way as for the first
		interface (see major number 3).

 92 char	Reserved for ith Kommunikationstechnik MIC ISDN card

 92 block	PPDD encrypted disk driver
		  0 = /dev/ppdd0	First encrypted disk
		  1 = /dev/ppdd1	Second encrypted disk
		    ...

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

 93 char

 93 block	NAND Flash Translation Layer filesystem
		  0 = /dev/nftla	First NFTL layer
		 16 = /dev/nftlb	Second NFTL layer
		    ...
		240 = /dev/nftlp	16th NTFL layer

 94 char

 94 block	IBM S/390 DASD block storage
    		  0 = /dev/dasda First DASD device, major
    		  1 = /dev/dasda1 First DASD device, block 1
	    	  2 = /dev/dasda2 First DASD device, block 2
    		  3 = /dev/dasda3 First DASD device, block 3
    		  4 = /dev/dasdb Second DASD device, major
    		  5 = /dev/dasdb1 Second DASD device, block 1
    		  6 = /dev/dasdb2 Second DASD device, block 2
    		  7 = /dev/dasdb3 Second DASD device, block 3
		    ...

 95 char	IP filter
		  0 = /dev/ipl		Filter control device/log file
		  1 = /dev/ipnat	NAT control device/log file
		  2 = /dev/ipstate	State information log file
		  3 = /dev/ipauth	Authentication control device/log file
		    ...

 96 char	Parallel port ATAPI tape devices
		  0 = /dev/pt0		First parallel port ATAPI tape
		  1 = /dev/pt1		Second parallel port ATAPI tape
		    ...
		128 = /dev/npt0		First p.p. ATAPI tape, no rewind
		129 = /dev/npt1		Second p.p. ATAPI tape, no rewind
		    ...

 96 block	Inverse NAND Flash Translation Layer
		  0 = /dev/inftla First INFTL layer
		 16 = /dev/inftlb Second INFTL layer
		    ...
		240 = /dev/inftlp	16th INTFL layer

 97 char	Parallel port generic ATAPI interface
		  0 = /dev/pg0		First parallel port ATAPI device
		  1 = /dev/pg1		Second parallel port ATAPI device
		  2 = /dev/pg2		Third parallel port ATAPI device
		  3 = /dev/pg3		Fourth parallel port ATAPI device

		These devices support the same API as the generic SCSI
		devices.

 98 char	Control and Measurement Device (comedi)
		  0 = /dev/comedi0	First comedi device
		  1 = /dev/comedi1	Second comedi device
		    ...

		See http://stm.lbl.gov/comedi.

 98 block	User-mode virtual block device
		  0 = /dev/ubda		First user-mode block device
		 16 = /dev/udbb		Second user-mode block device
		    ...

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

		This device is used by the user-mode virtual kernel port.

 99 char	Raw parallel ports
		  0 = /dev/parport0	First parallel port
		  1 = /dev/parport1	Second parallel port
		    ...

 99 block	JavaStation flash disk
		  0 = /dev/jsfd		JavaStation flash disk

100 char	Telephony for Linux
		  0 = /dev/phone0	First telephony device
		  1 = /dev/phone1	Second telephony device
		    ...

101 char	Motorola DSP 56xxx board
		  0 = /dev/mdspstat	Status information
		  1 = /dev/mdsp1	First DSP board I/O controls
		    ...
		 16 = /dev/mdsp16	16th DSP board I/O controls

101 block	AMI HyperDisk RAID controller
		  0 = /dev/amiraid/ar0	First array whole disk
		 16 = /dev/amiraid/ar1	Second array whole disk
		    ...
		240 = /dev/amiraid/ar15	16th array whole disk

		For each device, partitions are added as:
		  0 = /dev/amiraid/ar?	  Whole disk
		  1 = /dev/amiraid/ar?p1  First partition
		  2 = /dev/amiraid/ar?p2  Second partition
		    ...
		 15 = /dev/amiraid/ar?p15 15th partition

102 char

102 block	Compressed block device
		  0 = /dev/cbd/a	First compressed block device, whole device
		 16 = /dev/cbd/b	Second compressed block device, whole device
		    ...
		240 = /dev/cbd/p	16th compressed block device, whole device

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

103 char	Arla network file system
		  0 = /dev/nnpfs0	First NNPFS device
		  1 = /dev/nnpfs1	Second NNPFS device

		Arla is a free clone of the Andrew File System, AFS.
		The NNPFS device gives user mode filesystem
		implementations a kernel presence for caching and easy
		mounting.  For more information about the project,
		write to <arla-drinkers@stacken.kth.se> or see
		http://www.stacken.kth.se/project/arla/

103 block	Audit device
		  0 = /dev/audit	Audit device

104 char	Flash BIOS support

104 block	Compaq Next Generation Drive Array, first controller
		  0 = /dev/cciss/c0d0	First logical drive, whole disk
		 16 = /dev/cciss/c0d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c0d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

105 char	Comtrol VS-1000 serial controller
		  0 = /dev/ttyV0	First VS-1000 port
		  1 = /dev/ttyV1	Second VS-1000 port
		    ...

105 block	Compaq Next Generation Drive Array, second controller
		  0 = /dev/cciss/c1d0	First logical drive, whole disk
		 16 = /dev/cciss/c1d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c1d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

106 char	Comtrol VS-1000 serial controller - alternate devices
		  0 = /dev/cuv0		First VS-1000 port
		  1 = /dev/cuv1		Second VS-1000 port
		    ...

106 block	Compaq Next Generation Drive Array, third controller
		  0 = /dev/cciss/c2d0	First logical drive, whole disk
		 16 = /dev/cciss/c2d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c2d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

107 char	3Dfx Voodoo Graphics device
		  0 = /dev/3dfx		Primary 3Dfx graphics device

107 block	Compaq Next Generation Drive Array, fourth controller
		  0 = /dev/cciss/c3d0	First logical drive, whole disk
		 16 = /dev/cciss/c3d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c3d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

108 char	Device independent PPP interface
		  0 = /dev/ppp		Device independent PPP interface

108 block	Compaq Next Generation Drive Array, fifth controller
		  0 = /dev/cciss/c4d0	First logical drive, whole disk
		 16 = /dev/cciss/c4d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c4d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

109 char	Reserved for logical volume manager

109 block	Compaq Next Generation Drive Array, sixth controller
		  0 = /dev/cciss/c5d0	First logical drive, whole disk
		 16 = /dev/cciss/c5d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c5d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

110 char	miroMEDIA Surround board
		  0 = /dev/srnd0	First miroMEDIA Surround board
		  1 = /dev/srnd1	Second miroMEDIA Surround board
		    ...

110 block	Compaq Next Generation Drive Array, seventh controller
		  0 = /dev/cciss/c6d0	First logical drive, whole disk
		 16 = /dev/cciss/c6d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c6d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

111 char

111 block	Compaq Next Generation Drive Array, eighth controller
		  0 = /dev/cciss/c7d0	First logical drive, whole disk
		 16 = /dev/cciss/c7d1	Second logical drive, whole disk
		    ...
		240 = /dev/cciss/c7d15	16th logical drive, whole disk

		Partitions are handled the same way as for Mylex
		DAC960 (see major number 48) except that the limit on
		partitions is 15.

112 char	ISI serial card
		  0 = /dev/ttyM0	First ISI port
		  1 = /dev/ttyM1	Second ISI port
		    ...

		There is currently a device-naming conflict between
		these and PAM multimodems (major 78).

112 block	IBM iSeries virtual disk
		  0 = /dev/iseries/vda	First virtual disk, whole disk
		  8 = /dev/iseries/vdb	Second virtual disk, whole disk
		    ...
		200 = /dev/iseries/vdz	26th virtual disk, whole disk
		208 = /dev/iseries/vdaa	27th virtual disk, whole disk
		    ...
		248 = /dev/iseries/vdaf	32nd virtual disk, whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 7.

113 char	ISI serial card - alternate devices
		  0 = /dev/cum0		Callout device for ttyM0
		  1 = /dev/cum1		Callout device for ttyM1
		    ...

113 block	IBM iSeries virtual CD-ROM
		  0 = /dev/iseries/vcda	First virtual CD-ROM
		  1 = /dev/iseries/vcdb	Second virtual CD-ROM
		    ...

114 char	Picture Elements ISE board
		  0 = /dev/ise0		First ISE board
		  1 = /dev/ise1		Second ISE board
		    ...
		128 = /dev/isex0	Control node for first ISE board
		129 = /dev/isex1	Control node for second ISE board
		    ...

		The ISE board is an embedded computer, optimized for
		image processing. The /dev/iseN nodes are the general
		I/O access to the board, the /dev/isex0 nodes command
		nodes used to control the board.

114 block       IDE BIOS powered software RAID interfaces such as the
                Promise Fastrak

                  0 = /dev/ataraid/d0
                  1 = /dev/ataraid/d0p1
                  2 = /dev/ataraid/d0p2
                  ...
                 16 = /dev/ataraid/d1
                 17 = /dev/ataraid/d1p1
                 18 = /dev/ataraid/d1p2
                  ...
                255 = /dev/ataraid/d15p15

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

115 char	TI link cable devices (115 was formerly the console driver speaker)
		  0 = /dev/tipar0    Parallel cable on first parallel port
		  ...
		  7 = /dev/tipar7    Parallel cable on seventh parallel port

		  8 = /dev/tiser0    Serial cable on first serial port
		  ...
		 15 = /dev/tiser7    Serial cable on seventh serial port

		 16 = /dev/tiusb0    First USB cable
		  ...
		 47 = /dev/tiusb31   32nd USB cable

115 block       NetWare (NWFS) Devices (0-255)

                The NWFS (NetWare) devices are used to present a
                collection of NetWare Mirror Groups or NetWare
                Partitions as a logical storage segment for
                use in mounting NetWare volumes.  A maximum of
                256 NetWare volumes can be supported in a single
                machine.

                http://cgfa.telepac.pt/ftp2/kernel.org/linux/kernel/people/jmerkey/nwfs/

                0 = /dev/nwfs/v0    First NetWare (NWFS) Logical Volume
                1 = /dev/nwfs/v1    Second NetWare (NWFS) Logical Volume
                2 = /dev/nwfs/v2    Third NetWare (NWFS) Logical Volume
                      ...
                255 = /dev/nwfs/v255    Last NetWare (NWFS) Logical Volume

116 char	Advanced Linux Sound Driver (ALSA)

116 block       MicroMemory battery backed RAM adapter (NVRAM)
                Supports 16 boards, 15 partitions each.
                Requested by neilb at cse.unsw.edu.au.

		 0 = /dev/umem/d0      Whole of first board
		 1 = /dev/umem/d0p1    First partition of first board
		 2 = /dev/umem/d0p2    Second partition of first board
		15 = /dev/umem/d0p15   15th partition of first board

		16 = /dev/umem/d1      Whole of second board
		17 = /dev/umem/d1p1    First partition of second board
		    ...
		255= /dev/umem/d15p15  15th partition of 16th board.

117 char	COSA/SRP synchronous serial card
		  0 = /dev/cosa0c0	1st board, 1st channel
		  1 = /dev/cosa0c1	1st board, 2nd channel
		    ...
		 16 = /dev/cosa1c0	2nd board, 1st channel
		 17 = /dev/cosa1c1	2nd board, 2nd channel
		    ...

117 block       Enterprise Volume Management System (EVMS)

                The EVMS driver uses a layered, plug-in model to provide
                unparalleled flexibility and extensibility in managing
                storage.  This allows for easy expansion or customization
                of various levels of volume management.  Requested by
                Mark Peloquin (peloquin at us.ibm.com).

                Note: EVMS populates and manages all the devnodes in
                /dev/evms.

                http://sf.net/projects/evms

                  0 = /dev/evms/block_device   EVMS block device
                  1 = /dev/evms/legacyname1    First EVMS legacy device
                  2 = /dev/evms/legacyname2    Second EVMS legacy device
                    ...
                    Both ranges can grow (down or up) until they meet.
                    ...
                254 = /dev/evms/EVMSname2      Second EVMS native device
                255 = /dev/evms/EVMSname1      First EVMS native device

                Note: legacyname(s) are derived from the normal legacy
                device names.  For example, /dev/hda5 would become
                /dev/evms/hda5.

118 char	IBM Cryptographic Accelerator
		  0 = /dev/ica	Virtual interface to all IBM Crypto Accelerators
		  1 = /dev/ica0	IBMCA Device 0
		  2 = /dev/ica1	IBMCA Device 1
		    ...

119 char	VMware virtual network control
		  0 = /dev/vnet0	1st virtual network
		  1 = /dev/vnet1	2nd virtual network
		    ...

120-127 char	LOCAL/EXPERIMENTAL USE

120-127 block	LOCAL/EXPERIMENTAL USE
		Allocated for local/experimental use.  For devices not
		assigned official numbers, these ranges should be
		used in order to avoid conflicting with future assignments.

128-135 char	Unix98 PTY masters

		These devices should not have corresponding device
		nodes; instead they should be accessed through the
		/dev/ptmx cloning interface.

128 block       SCSI disk devices (128-143)
                  0 = /dev/sddy         129th SCSI disk whole disk
                 16 = /dev/sddz         130th SCSI disk whole disk
                 32 = /dev/sdea         131th SCSI disk whole disk
                    ...
                240 = /dev/sden         144th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

129 block       SCSI disk devices (144-159)
                  0 = /dev/sdeo         145th SCSI disk whole disk
                 16 = /dev/sdep         146th SCSI disk whole disk
                 32 = /dev/sdeq         147th SCSI disk whole disk
                    ...
                240 = /dev/sdfd         160th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

130 char 	(Misc devices)

130 block       SCSI disk devices (160-175)
                  0 = /dev/sdfe         161st SCSI disk whole disk
                 16 = /dev/sdff         162nd SCSI disk whole disk
                 32 = /dev/sdfg         163rd SCSI disk whole disk
                    ...
                240 = /dev/sdft         176th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

131 block       SCSI disk devices (176-191)
                  0 = /dev/sdfu         177th SCSI disk whole disk
                 16 = /dev/sdfv         178th SCSI disk whole disk
                 32 = /dev/sdfw         179th SCSI disk whole disk
                    ...
                240 = /dev/sdgj         192nd SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

132 block       SCSI disk devices (192-207)
                  0 = /dev/sdgk         193rd SCSI disk whole disk
                 16 = /dev/sdgl         194th SCSI disk whole disk
                 32 = /dev/sdgm         195th SCSI disk whole disk
                    ...
                240 = /dev/sdgz         208th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

133 block       SCSI disk devices (208-223)
                  0 = /dev/sdha         209th SCSI disk whole disk
                 16 = /dev/sdhb         210th SCSI disk whole disk
                 32 = /dev/sdhc         211th SCSI disk whole disk
                    ...
                240 = /dev/sdhp         224th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

134 block       SCSI disk devices (224-239)
                  0 = /dev/sdhq         225th SCSI disk whole disk
                 16 = /dev/sdhr         226th SCSI disk whole disk
                 32 = /dev/sdhs         227th SCSI disk whole disk
                    ...
                240 = /dev/sdif         240th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

135 block       SCSI disk devices (240-255)
                  0 = /dev/sdig         241st SCSI disk whole disk
                 16 = /dev/sdih         242nd SCSI disk whole disk
                 32 = /dev/sdih         243rd SCSI disk whole disk
                    ...
                240 = /dev/sdiv         256th SCSI disk whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

136-143 char	Unix98 PTY slaves
		  0 = /dev/pts/0	First Unix98 pseudo-TTY
		  1 = /dev/pts/1	Second Unix98 pseudo-TTY
		    ...

		These device nodes are automatically generated with
		the proper permissions and modes by mounting the
		devpts filesystem onto /dev/pts with the appropriate
		mount options (distribution dependent, however, on
		*most* distributions the appropriate options are
		"mode=0620,gid=<gid of the "tty" group>".)

136 block	Mylex DAC960 PCI RAID controller; ninth controller
		  0 = /dev/rd/c8d0	First disk, whole disk
		  8 = /dev/rd/c8d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c8d31	32nd disk, whole disk

		Partitions are handled as for major 48.

137 block	Mylex DAC960 PCI RAID controller; tenth controller
		  0 = /dev/rd/c9d0	First disk, whole disk
		  8 = /dev/rd/c9d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c9d31	32nd disk, whole disk

		Partitions are handled as for major 48.

138 block	Mylex DAC960 PCI RAID controller; eleventh controller
		  0 = /dev/rd/c10d0	First disk, whole disk
		  8 = /dev/rd/c10d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c10d31	32nd disk, whole disk

		Partitions are handled as for major 48.

139 block	Mylex DAC960 PCI RAID controller; twelfth controller
		  0 = /dev/rd/c11d0	First disk, whole disk
		  8 = /dev/rd/c11d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c11d31	32nd disk, whole disk

		Partitions are handled as for major 48.

140 block	Mylex DAC960 PCI RAID controller; thirteenth controller
		  0 = /dev/rd/c12d0	First disk, whole disk
		  8 = /dev/rd/c12d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c12d31	32nd disk, whole disk

		Partitions are handled as for major 48.

141 block	Mylex DAC960 PCI RAID controller; fourteenth controller
		  0 = /dev/rd/c13d0	First disk, whole disk
		  8 = /dev/rd/c13d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c13d31	32nd disk, whole disk

		Partitions are handled as for major 48.

142 block	Mylex DAC960 PCI RAID controller; fifteenth controller
		  0 = /dev/rd/c14d0	First disk, whole disk
		  8 = /dev/rd/c14d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c14d31	32nd disk, whole disk

		Partitions are handled as for major 48.

143 block	Mylex DAC960 PCI RAID controller; sixteenth controller
		  0 = /dev/rd/c15d0	First disk, whole disk
		  8 = /dev/rd/c15d1	Second disk, whole disk
		    ...
		248 = /dev/rd/c15d31	32nd disk, whole disk

		Partitions are handled as for major 48.

144 char	Encapsulated PPP
		  0 = /dev/pppox0	First PPP over Ethernet
		    ...
		 63 = /dev/pppox63	64th PPP over Ethernet

		This is primarily used for ADSL.

		The SST 5136-DN DeviceNet interface driver has been
		relocated to major 183 due to an unfortunate conflict.

144 block	Expansion Area #1 for more non-device (e.g. NFS) mounts
		  0 = mounted device 256
		255 = mounted device 511

145 char	SAM9407-based soundcard
		  0 = /dev/sam0_mixer
		  1 = /dev/sam0_sequencer
		  2 = /dev/sam0_midi00
		  3 = /dev/sam0_dsp
		  4 = /dev/sam0_audio
		  6 = /dev/sam0_sndstat
		 18 = /dev/sam0_midi01
		 34 = /dev/sam0_midi02
		 50 = /dev/sam0_midi03
		 64 = /dev/sam1_mixer
		    ...
		128 = /dev/sam2_mixer
		    ...
		192 = /dev/sam3_mixer
		    ...

		Device functions match OSS, but offer a number of
		addons, which are sam9407 specific.  OSS can be
		operated simultaneously, taking care of the codec.

145 block	Expansion Area #2 for more non-device (e.g. NFS) mounts
		  0 = mounted device 512
		255 = mounted device 767

146 char	SYSTRAM SCRAMNet mirrored-memory network
		  0 = /dev/scramnet0	First SCRAMNet device
		  1 = /dev/scramnet1	Second SCRAMNet device
		    ...

146 block	Expansion Area #3 for more non-device (e.g. NFS) mounts
		  0 = mounted device 768
		255 = mounted device 1023

147 char	Aureal Semiconductor Vortex Audio device
		  0 = /dev/aureal0	First Aureal Vortex
		  1 = /dev/aureal1	Second Aureal Vortex
		    ...

147 block	Distributed Replicated Block Device (DRBD)
		  0 = /dev/drbd0	First DRBD device
		  1 = /dev/drbd1	Second DRBD device
		    ...

148 char	Technology Concepts serial card
		  0 = /dev/ttyT0	First TCL port
		  1 = /dev/ttyT1	Second TCL port
		    ...

149 char	Technology Concepts serial card - alternate devices
		  0 = /dev/cut0		Callout device for ttyT0
		  1 = /dev/cut0		Callout device for ttyT1
		    ...

150 char	Real-Time Linux FIFOs
		  0 = /dev/rtf0		First RTLinux FIFO
		  1 = /dev/rtf1		Second RTLinux FIFO
		    ...

151 char	DPT I2O SmartRaid V controller
		  0 = /dev/dpti0	First DPT I2O adapter
		  1 = /dev/dpti1	Second DPT I2O adapter
		    ...

152 char	EtherDrive Control Device
		  0 = /dev/etherd/ctl	Connect/Disconnect an EtherDrive
		  1 = /dev/etherd/err	Monitor errors
		  2 = /dev/etherd/raw	Raw AoE packet monitor

152 block	EtherDrive Block Devices
		  0 = /dev/etherd/0	EtherDrive 0
		    ...
		255 = /dev/etherd/255	EtherDrive 255

153 char	SPI Bus Interface (sometimes referred to as MicroWire)
		  0 = /dev/spi0		First SPI device on the bus
		  1 = /dev/spi1		Second SPI device on the bus
		    ...
		 15 = /dev/spi15	Sixteenth SPI device on the bus

153 block	Enhanced Metadisk RAID (EMD) storage units
		  0 = /dev/emd/0	First unit
		  1 = /dev/emd/0p1	Partition 1 on First unit
		  2 = /dev/emd/0p2	Partition 2 on First unit
		    ...
		 15 = /dev/emd/0p15	Partition 15 on First unit

		 16 = /dev/emd/1	Second unit
		 32 = /dev/emd/2	Third unit
		    ...
		240 = /dev/emd/15	Sixteenth unit

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 15.

154 char	Specialix RIO serial card
		  0 = /dev/ttySR0	First RIO port
		    ...
		255 = /dev/ttySR255	256th RIO port

155 char	Specialix RIO serial card - alternate devices
		  0 = /dev/cusr0	Callout device for ttySR0
		    ...
		255 = /dev/cusr255	Callout device for ttySR255

156 char	Specialix RIO serial card
		  0 = /dev/ttySR256	257th RIO port
		    ...
		255 = /dev/ttySR511	512th RIO port

157 char	Specialix RIO serial card - alternate devices
		  0 = /dev/cusr256	Callout device for ttySR256
		    ...
		255 = /dev/cusr511	Callout device for ttySR511

158 char	Dialogic GammaLink fax driver
		  0 = /dev/gfax0	GammaLink channel 0
		  1 = /dev/gfax1	GammaLink channel 1
		    ...

159 char	RESERVED

159 block	RESERVED

160 char	General Purpose Instrument Bus (GPIB)
		  0 = /dev/gpib0	First GPIB bus
		  1 = /dev/gpib1	Second GPIB bus
		    ...

160 block       Carmel 8-port SATA Disks on First Controller
		  0 = /dev/carmel/0     SATA disk 0 whole disk
		  1 = /dev/carmel/0p1   SATA disk 0 partition 1
		    ...
		 31 = /dev/carmel/0p31  SATA disk 0 partition 31

		 32 = /dev/carmel/1     SATA disk 1 whole disk
		 64 = /dev/carmel/2     SATA disk 2 whole disk
		    ...
		224 = /dev/carmel/7     SATA disk 7 whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 31.

161 char	IrCOMM devices (IrDA serial/parallel emulation)
		  0 = /dev/ircomm0	First IrCOMM device
		  1 = /dev/ircomm1	Second IrCOMM device
		    ...
		 16 = /dev/irlpt0	First IrLPT device
		 17 = /dev/irlpt1	Second IrLPT device
		    ...

161 block       Carmel 8-port SATA Disks on Second Controller
		  0 = /dev/carmel/8     SATA disk 8 whole disk
		  1 = /dev/carmel/8p1   SATA disk 8 partition 1
		    ...
		 31 = /dev/carmel/8p31  SATA disk 8 partition 31

		 32 = /dev/carmel/9     SATA disk 9 whole disk
		 64 = /dev/carmel/10    SATA disk 10 whole disk
		    ...
		224 = /dev/carmel/15    SATA disk 15 whole disk

		Partitions are handled in the same way as for IDE
		disks (see major number 3) except that the limit on
		partitions is 31.

162 char	Raw block device interface
		  0 = /dev/rawctl	Raw I/O control device
		  1 = /dev/raw/raw1	First raw I/O device
		  2 = /dev/raw/raw2	Second raw I/O device
		    ...
                 max minor number of raw device is set by kernel config
                 MAX_RAW_DEVS or raw module parameter 'max_raw_devs'

163 char

164 char	Chase Research AT/PCI-Fast serial card
		  0 = /dev/ttyCH0	AT/PCI-Fast board 0, port 0
		    ...
		 15 = /dev/ttyCH15	AT/PCI-Fast board 0, port 15
		 16 = /dev/ttyCH16	AT/PCI-Fast board 1, port 0
		    ...
		 31 = /dev/ttyCH31	AT/PCI-Fast board 1, port 15
		 32 = /dev/ttyCH32	AT/PCI-Fast board 2, port 0
		    ...
		 47 = /dev/ttyCH47	AT/PCI-Fast board 2, port 15
		 48 = /dev/ttyCH48	AT/PCI-Fast board 3, port 0
		    ...
		 63 = /dev/ttyCH63	AT/PCI-Fast board 3, port 15

165 char	Chase Research AT/PCI-Fast serial card - alternate devices
		  0 = /dev/cuch0	Callout device for ttyCH0
		    ...
		 63 = /dev/cuch63	Callout device for ttyCH63

166 char	ACM USB modems
		  0 = /dev/ttyACM0	First ACM modem
		  1 = /dev/ttyACM1	Second ACM modem
		    ...

167 char	ACM USB modems - alternate devices
		  0 = /dev/cuacm0	Callout device for ttyACM0
		  1 = /dev/cuacm1	Callout device for ttyACM1
		    ...

168 char	Eracom CSA7000 PCI encryption adaptor
		  0 = /dev/ecsa0	First CSA7000
		  1 = /dev/ecsa1	Second CSA7000
		    ...

169 char	Eracom CSA8000 PCI encryption adaptor
		  0 = /dev/ecsa8-0	First CSA8000
		  1 = /dev/ecsa8-1	Second CSA8000
		    ...

170 char	AMI MegaRAC remote access controller
		  0 = /dev/megarac0	First MegaRAC card
		  1 = /dev/megarac1	Second MegaRAC card
		    ...

171 char	Reserved for IEEE 1394 (Firewire)

172 char	Moxa Intellio serial card
		  0 = /dev/ttyMX0	First Moxa port
		  1 = /dev/ttyMX1	Second Moxa port
		    ...
		127 = /dev/ttyMX127	128th Moxa port
		128 = /dev/moxactl	Moxa control port

173 char	Moxa Intellio serial card - alternate devices
		  0 = /dev/cumx0	Callout device for ttyMX0
		  1 = /dev/cumx1	Callout device for ttyMX1
		    ...
		127 = /dev/cumx127	Callout device for ttyMX127

174 char	SmartIO serial card
		  0 = /dev/ttySI0	First SmartIO port
		  1 = /dev/ttySI1	Second SmartIO port
		    ...

175 char	SmartIO serial card - alternate devices
		  0 = /dev/cusi0	Callout device for ttySI0
		  1 = /dev/cusi1	Callout device for ttySI1
		    ...

176 char	nCipher nFast PCI crypto accelerator
		  0 = /dev/nfastpci0	First nFast PCI device
		  1 = /dev/nfastpci1	First nFast PCI device
		    ...

177 char	TI PCILynx memory spaces
		  0 = /dev/pcilynx/aux0	 AUX space of first PCILynx card
		    ...
		 15 = /dev/pcilynx/aux15 AUX space of 16th PCILynx card
		 16 = /dev/pcilynx/rom0	 ROM space of first PCILynx card
		    ...
		 31 = /dev/pcilynx/rom15 ROM space of 16th PCILynx card
		 32 = /dev/pcilynx/ram0	 RAM space of first PCILynx card
		    ...
		 47 = /dev/pcilynx/ram15 RAM space of 16th PCILynx card

178 char	Giganet cLAN1xxx virtual interface adapter
		  0 = /dev/clanvi0	First cLAN adapter
		  1 = /dev/clanvi1	Second cLAN adapter
		    ...

179 block       MMC block devices
		  0 = /dev/mmcblk0      First SD/MMC card
		  1 = /dev/mmcblk0p1    First partition on first MMC card
		  8 = /dev/mmcblk1      Second SD/MMC card
		    ...

		The start of next SD/MMC card can be configured with
		CONFIG_MMC_BLOCK_MINORS, or overridden at boot/modprobe
		time using the mmcblk.perdev_minors option. That would
		bump the offset between each card to be the configured
		value instead of the default 8.

179 char	CCube DVXChip-based PCI products
		  0 = /dev/dvxirq0	First DVX device
		  1 = /dev/dvxirq1	Second DVX device
		    ...

180 char	USB devices
		  0 = /dev/usb/lp0	First USB printer
		    ...
		 15 = /dev/usb/lp15	16th USB printer
		 48 = /dev/usb/scanner0	First USB scanner
		    ...
		 63 = /dev/usb/scanner15 16th USB scanner
		 64 = /dev/usb/rio500	Diamond Rio 500
		 65 = /dev/usb/usblcd	USBLCD Interface (info@usblcd.de)
		 66 = /dev/usb/cpad0	Synaptics cPad (mouse/LCD)
		 96 = /dev/usb/hiddev0	1st USB HID device
		    ...
		111 = /dev/usb/hiddev15	16th USB HID device
		112 = /dev/usb/auer0	1st auerswald ISDN device
		    ...
		127 = /dev/usb/auer15	16th auerswald ISDN device
		128 = /dev/usb/brlvgr0	First Braille Voyager device
		    ...
		131 = /dev/usb/brlvgr3	Fourth Braille Voyager device
		132 = /dev/usb/idmouse	ID Mouse (fingerprint scanner) device
		133 = /dev/usb/sisusbvga1	First SiSUSB VGA device
		    ...
		140 = /dev/usb/sisusbvga8	Eighth SISUSB VGA device
		144 = /dev/usb/lcd	USB LCD device
		160 = /dev/usb/legousbtower0	1st USB Legotower device
		    ...
		175 = /dev/usb/legousbtower15	16th USB Legotower device
		176 = /dev/usb/usbtmc1	First USB TMC device
		   ...
		191 = /dev/usb/usbtmc16	16th USB TMC device
		192 = /dev/usb/yurex1	First USB Yurex device
		   ...
		209 = /dev/usb/yurex16	16th USB Yurex device

180 block	USB block devices
		  0 = /dev/uba		First USB block device
		  8 = /dev/ubb		Second USB block device
		 16 = /dev/ubc		Third USB block device
 		    ...

181 char	Conrad Electronic parallel port radio clocks
		  0 = /dev/pcfclock0	First Conrad radio clock
		  1 = /dev/pcfclock1	Second Conrad radio clock
		    ...

182 char	Picture Elements THR2 binarizer
		  0 = /dev/pethr0	First THR2 board
		  1 = /dev/pethr1	Second THR2 board
		    ...

183 char	SST 5136-DN DeviceNet interface
		  0 = /dev/ss5136dn0	First DeviceNet interface
		  1 = /dev/ss5136dn1	Second DeviceNet interface
		    ...

		This device used to be assigned to major number 144.
		It had to be moved due to an unfortunate conflict.

184 char	Picture Elements' video simulator/sender
		  0 = /dev/pevss0	First sender board
		  1 = /dev/pevss1	Second sender board
		    ...

185 char	InterMezzo high availability file system
		  0 = /dev/intermezzo0	First cache manager
		  1 = /dev/intermezzo1	Second cache manager
		    ...

		See http://web.archive.org/web/20080115195241/
		http://inter-mezzo.org/index.html

186 char	Object-based storage control device
		  0 = /dev/obd0		First obd control device
		  1 = /dev/obd1		Second obd control device
		    ...

		See ftp://ftp.lustre.org/pub/obd for code and information.

187 char	DESkey hardware encryption device
		  0 = /dev/deskey0	First DES key
		  1 = /dev/deskey1	Second DES key
		    ...

188 char	USB serial converters
		  0 = /dev/ttyUSB0	First USB serial converter
		  1 = /dev/ttyUSB1	Second USB serial converter
		    ...

189 char	USB serial converters - alternate devices
		  0 = /dev/cuusb0	Callout device for ttyUSB0
		  1 = /dev/cuusb1	Callout device for ttyUSB1
		    ...

190 char	Kansas City tracker/tuner card
		  0 = /dev/kctt0	First KCT/T card
		  1 = /dev/kctt1	Second KCT/T card
		    ...

191 char	Reserved for PCMCIA

192 char	Kernel profiling interface
		  0 = /dev/profile	Profiling control device
		  1 = /dev/profile0	Profiling device for CPU 0
		  2 = /dev/profile1	Profiling device for CPU 1
		    ...

193 char	Kernel event-tracing interface
		  0 = /dev/trace	Tracing control device
		  1 = /dev/trace0	Tracing device for CPU 0
		  2 = /dev/trace1	Tracing device for CPU 1
		    ...

194 char	linVideoStreams (LINVS)
		  0 = /dev/mvideo/status0	Video compression status
		  1 = /dev/mvideo/stream0	Video stream
		  2 = /dev/mvideo/frame0	Single compressed frame
		  3 = /dev/mvideo/rawframe0	Raw uncompressed frame
		  4 = /dev/mvideo/codec0	Direct codec access
		  5 = /dev/mvideo/video4linux0	Video4Linux compatibility

		 16 = /dev/mvideo/status1	Second device
		    ...
		 32 = /dev/mvideo/status2	Third device
		    ...
		    ...
		240 = /dev/mvideo/status15	16th device
		    ...

195 char	Nvidia graphics devices
		  0 = /dev/nvidia0		First Nvidia card
		  1 = /dev/nvidia1		Second Nvidia card
		    ...
		255 = /dev/nvidiactl		Nvidia card control device

196 char	Tormenta T1 card
		  0 = /dev/tor/0		Master control channel for all cards
		  1 = /dev/tor/1		First DS0
		  2 = /dev/tor/2		Second DS0
		    ...
		 48 = /dev/tor/48		48th DS0
		 49 = /dev/tor/49		First pseudo-channel
		 50 = /dev/tor/50		Second pseudo-channel
		    ...

197 char	OpenTNF tracing facility
		  0 = /dev/tnf/t0		Trace 0 data extraction
		  1 = /dev/tnf/t1		Trace 1 data extraction
		    ...
		128 = /dev/tnf/status		Tracing facility status
		130 = /dev/tnf/trace		Tracing device

198 char	Total Impact TPMP2 quad coprocessor PCI card
		  0 = /dev/tpmp2/0		First card
		  1 = /dev/tpmp2/1		Second card
		    ...

199 char	Veritas volume manager (VxVM) volumes
		  0 = /dev/vx/rdsk/*/*		First volume
		  1 = /dev/vx/rdsk/*/*		Second volume
		    ...

199 block	Veritas volume manager (VxVM) volumes
		  0 = /dev/vx/dsk/*/*		First volume
		  1 = /dev/vx/dsk/*/*		Second volume
		    ...

		The namespace in these directories is maintained by
		the user space VxVM software.

200 char	Veritas VxVM configuration interface
                  0 = /dev/vx/config		Configuration access node
                  1 = /dev/vx/trace		Volume i/o trace access node
                  2 = /dev/vx/iod		Volume i/o daemon access node
                  3 = /dev/vx/info		Volume information access node
                  4 = /dev/vx/task		Volume tasks access node
                  5 = /dev/vx/taskmon		Volume tasks monitor daemon

201 char	Veritas VxVM dynamic multipathing driver
		  0 = /dev/vx/rdmp/*		First multipath device
		  1 = /dev/vx/rdmp/*		Second multipath device
		    ...
201 block	Veritas VxVM dynamic multipathing driver
		  0 = /dev/vx/dmp/*		First multipath device
		  1 = /dev/vx/dmp/*		Second multipath device
		    ...

		The namespace in these directories is maintained by
		the user space VxVM software.

202 char	CPU model-specific registers
		  0 = /dev/cpu/0/msr		MSRs on CPU 0
		  1 = /dev/cpu/1/msr		MSRs on CPU 1
		    ...

202 block	Xen Virtual Block Device
		  0 = /dev/xvda       First Xen VBD whole disk
		  16 = /dev/xvdb      Second Xen VBD whole disk
		  32 = /dev/xvdc      Third Xen VBD whole disk
		    ...
		  240 = /dev/xvdp     Sixteenth Xen VBD whole disk

                Partitions are handled in the same way as for IDE
                disks (see major number 3) except that the limit on
                partitions is 15.

203 char	CPU CPUID information
		  0 = /dev/cpu/0/cpuid		CPUID on CPU 0
		  1 = /dev/cpu/1/cpuid		CPUID on CPU 1
		    ...

204 char	Low-density serial ports
		  0 = /dev/ttyLU0		LinkUp Systems L72xx UART - port 0
		  1 = /dev/ttyLU1		LinkUp Systems L72xx UART - port 1
		  2 = /dev/ttyLU2		LinkUp Systems L72xx UART - port 2
		  3 = /dev/ttyLU3		LinkUp Systems L72xx UART - port 3
		  4 = /dev/ttyFB0		Intel Footbridge (ARM)
		  5 = /dev/ttySA0		StrongARM builtin serial port 0
		  6 = /dev/ttySA1		StrongARM builtin serial port 1
		  7 = /dev/ttySA2		StrongARM builtin serial port 2
		  8 = /dev/ttySC0		SCI serial port (SuperH) - port 0
		  9 = /dev/ttySC1		SCI serial port (SuperH) - port 1
		 10 = /dev/ttySC2		SCI serial port (SuperH) - port 2
		 11 = /dev/ttySC3		SCI serial port (SuperH) - port 3
		 12 = /dev/ttyFW0		Firmware console - port 0
		 13 = /dev/ttyFW1		Firmware console - port 1
		 14 = /dev/ttyFW2		Firmware console - port 2
		 15 = /dev/ttyFW3		Firmware console - port 3
		 16 = /dev/ttyAM0		ARM "AMBA" serial port 0
		    ...
		 31 = /dev/ttyAM15		ARM "AMBA" serial port 15
		 32 = /dev/ttyDB0		DataBooster serial port 0
		    ...
		 39 = /dev/ttyDB7		DataBooster serial port 7
		 40 = /dev/ttySG0		SGI Altix console port
		 41 = /dev/ttySMX0		Motorola i.MX - port 0
		 42 = /dev/ttySMX1		Motorola i.MX - port 1
		 43 = /dev/ttySMX2		Motorola i.MX - port 2
		 44 = /dev/ttyMM0		Marvell MPSC - port 0
		 45 = /dev/ttyMM1		Marvell MPSC - port 1
		 46 = /dev/ttyCPM0		PPC CPM (SCC or SMC) - port 0
		    ...
		 47 = /dev/ttyCPM5		PPC CPM (SCC or SMC) - port 5
		 50 = /dev/ttyIOC0		Altix serial card
		    ...
		 81 = /dev/ttyIOC31		Altix serial card
		 82 = /dev/ttyVR0		NEC VR4100 series SIU
		 83 = /dev/ttyVR1		NEC VR4100 series DSIU
		 84 = /dev/ttyIOC84		Altix ioc4 serial card
		    ...
		 115 = /dev/ttyIOC115		Altix ioc4 serial card
		 116 = /dev/ttySIOC0		Altix ioc3 serial card
		    ...
		 147 = /dev/ttySIOC31		Altix ioc3 serial card
		 148 = /dev/ttyPSC0		PPC PSC - port 0
		    ...
		 153 = /dev/ttyPSC5		PPC PSC - port 5
		 154 = /dev/ttyAT0		ATMEL serial port 0
		    ...
		 169 = /dev/ttyAT15		ATMEL serial port 15
		 170 = /dev/ttyNX0		Hilscher netX serial port 0
		    ...
		 185 = /dev/ttyNX15		Hilscher netX serial port 15
		 186 = /dev/ttyJ0		JTAG1 DCC protocol based serial port emulation
		 187 = /dev/ttyUL0		Xilinx uartlite - port 0
		    ...
		 190 = /dev/ttyUL3		Xilinx uartlite - port 3
		 191 = /dev/xvc0		Xen virtual console - port 0
		 192 = /dev/ttyPZ0		pmac_zilog - port 0
		    ...
		 195 = /dev/ttyPZ3		pmac_zilog - port 3
		 196 = /dev/ttyTX0		TX39/49 serial port 0
		    ...
		 204 = /dev/ttyTX7		TX39/49 serial port 7
		 205 = /dev/ttySC0		SC26xx serial port 0
		 206 = /dev/ttySC1		SC26xx serial port 1
		 207 = /dev/ttySC2		SC26xx serial port 2
		 208 = /dev/ttySC3		SC26xx serial port 3
		 209 = /dev/ttyMAX0		MAX3100 serial port 0
		 210 = /dev/ttyMAX1		MAX3100 serial port 1
		 211 = /dev/ttyMAX2		MAX3100 serial port 2
		 212 = /dev/ttyMAX3		MAX3100 serial port 3

205 char	Low-density serial ports (alternate device)
		  0 = /dev/culu0		Callout device for ttyLU0
		  1 = /dev/culu1		Callout device for ttyLU1
		  2 = /dev/culu2		Callout device for ttyLU2
		  3 = /dev/culu3		Callout device for ttyLU3
		  4 = /dev/cufb0		Callout device for ttyFB0
		  5 = /dev/cusa0		Callout device for ttySA0
		  6 = /dev/cusa1		Callout device for ttySA1
		  7 = /dev/cusa2		Callout device for ttySA2
		  8 = /dev/cusc0		Callout device for ttySC0
		  9 = /dev/cusc1		Callout device for ttySC1
		 10 = /dev/cusc2		Callout device for ttySC2
		 11 = /dev/cusc3		Callout device for ttySC3
		 12 = /dev/cufw0		Callout device for ttyFW0
		 13 = /dev/cufw1		Callout device for ttyFW1
		 14 = /dev/cufw2		Callout device for ttyFW2
		 15 = /dev/cufw3		Callout device for ttyFW3
		 16 = /dev/cuam0		Callout device for ttyAM0
		    ...
		 31 = /dev/cuam15		Callout device for ttyAM15
		 32 = /dev/cudb0		Callout device for ttyDB0
		    ...
		 39 = /dev/cudb7		Callout device for ttyDB7
		 40 = /dev/cusg0		Callout device for ttySG0
		 41 = /dev/ttycusmx0		Callout device for ttySMX0
		 42 = /dev/ttycusmx1		Callout device for ttySMX1
		 43 = /dev/ttycusmx2		Callout device for ttySMX2
		 46 = /dev/cucpm0		Callout device for ttyCPM0
		    ...
		 49 = /dev/cucpm5		Callout device for ttyCPM5
		 50 = /dev/cuioc40		Callout device for ttyIOC40
		    ...
		 81 = /dev/cuioc431		Callout device for ttyIOC431
		 82 = /dev/cuvr0		Callout device for ttyVR0
		 83 = /dev/cuvr1		Callout device for ttyVR1

206 char	OnStream SC-x0 tape devices
		  0 = /dev/osst0		First OnStream SCSI tape, mode 0
		  1 = /dev/osst1		Second OnStream SCSI tape, mode 0
		    ...
		 32 = /dev/osst0l		First OnStream SCSI tape, mode 1
		 33 = /dev/osst1l		Second OnStream SCSI tape, mode 1
		    ...
		 64 = /dev/osst0m		First OnStream SCSI tape, mode 2
		 65 = /dev/osst1m		Second OnStream SCSI tape, mode 2
		    ...
		 96 = /dev/osst0a		First OnStream SCSI tape, mode 3
		 97 = /dev/osst1a		Second OnStream SCSI tape, mode 3
		    ...
		128 = /dev/nosst0		No rewind version of /dev/osst0
		129 = /dev/nosst1		No rewind version of /dev/osst1
		    ...
		160 = /dev/nosst0l		No rewind version of /dev/osst0l
		161 = /dev/nosst1l		No rewind version of /dev/osst1l
		    ...
		192 = /dev/nosst0m		No rewind version of /dev/osst0m
		193 = /dev/nosst1m		No rewind version of /dev/osst1m
		    ...
		224 = /dev/nosst0a		No rewind version of /dev/osst0a
		225 = /dev/nosst1a		No rewind version of /dev/osst1a
		    ...

		The OnStream SC-x0 SCSI tapes do not support the
		standard SCSI SASD command set and therefore need
		their own driver "osst". Note that the IDE, USB (and
		maybe ParPort) versions may be driven via ide-scsi or
		usb-storage SCSI emulation and this osst device and
		driver as well.  The ADR-x0 drives are QIC-157
		compliant and don't need osst.

207 char	Compaq ProLiant health feature indicate
		  0 = /dev/cpqhealth/cpqw	Redirector interface
		  1 = /dev/cpqhealth/crom	EISA CROM
		  2 = /dev/cpqhealth/cdt	Data Table
		  3 = /dev/cpqhealth/cevt	Event Log
		  4 = /dev/cpqhealth/casr	Automatic Server Recovery
		  5 = /dev/cpqhealth/cecc	ECC Memory
		  6 = /dev/cpqhealth/cmca	Machine Check Architecture
		  7 = /dev/cpqhealth/ccsm	Deprecated CDT
		  8 = /dev/cpqhealth/cnmi	NMI Handling
		  9 = /dev/cpqhealth/css	Sideshow Management
		 10 = /dev/cpqhealth/cram	CMOS interface
		 11 = /dev/cpqhealth/cpci	PCI IRQ interface

208 char	User space serial ports
		  0 = /dev/ttyU0		First user space serial port
		  1 = /dev/ttyU1		Second user space serial port
		    ...

209 char	User space serial ports (alternate devices)
		  0 = /dev/cuu0			Callout device for ttyU0
		  1 = /dev/cuu1			Callout device for ttyU1
		    ...

210 char	SBE, Inc. sync/async serial card
		  0 = /dev/sbei/wxcfg0		Configuration device for board 0
		  1 = /dev/sbei/dld0		Download device for board 0
		  2 = /dev/sbei/wan00		WAN device, port 0, board 0
		  3 = /dev/sbei/wan01		WAN device, port 1, board 0
		  4 = /dev/sbei/wan02		WAN device, port 2, board 0
		  5 = /dev/sbei/wan03		WAN device, port 3, board 0
		  6 = /dev/sbei/wanc00		WAN clone device, port 0, board 0
		  7 = /dev/sbei/wanc01		WAN clone device, port 1, board 0
		  8 = /dev/sbei/wanc02		WAN clone device, port 2, board 0
		  9 = /dev/sbei/wanc03		WAN clone device, port 3, board 0
		 10 = /dev/sbei/wxcfg1		Configuration device for board 1
		 11 = /dev/sbei/dld1		Download device for board 1
		 12 = /dev/sbei/wan10		WAN device, port 0, board 1
		 13 = /dev/sbei/wan11		WAN device, port 1, board 1
		 14 = /dev/sbei/wan12		WAN device, port 2, board 1
		 15 = /dev/sbei/wan13		WAN device, port 3, board 1
		 16 = /dev/sbei/wanc10		WAN clone device, port 0, board 1
		 17 = /dev/sbei/wanc11		WAN clone device, port 1, board 1
		 18 = /dev/sbei/wanc12		WAN clone device, port 2, board 1
		 19 = /dev/sbei/wanc13		WAN clone device, port 3, board 1
		    ...

		Yes, each board is really spaced 10 (decimal) apart.

211 char	Addinum CPCI1500 digital I/O card
		  0 = /dev/addinum/cpci1500/0	First CPCI1500 card
		  1 = /dev/addinum/cpci1500/1	Second CPCI1500 card
		    ...

212 char	LinuxTV.org DVB driver subsystem
		  0 = /dev/dvb/adapter0/video0    first video decoder of first card
		  1 = /dev/dvb/adapter0/audio0    first audio decoder of first card
		  2 = /dev/dvb/adapter0/sec0      (obsolete/unused)
		  3 = /dev/dvb/adapter0/frontend0 first frontend device of first card
		  4 = /dev/dvb/adapter0/demux0    first demux device of first card
		  5 = /dev/dvb/adapter0/dvr0      first digital video recoder device of first card
		  6 = /dev/dvb/adapter0/ca0       first common access port of first card
		  7 = /dev/dvb/adapter0/net0      first network device of first card
		  8 = /dev/dvb/adapter0/osd0      first on-screen-display device of first card
		  9 = /dev/dvb/adapter0/video1    second video decoder of first card
		    ...
		 64 = /dev/dvb/adapter1/video0    first video decoder of second card
		    ...
		128 = /dev/dvb/adapter2/video0    first video decoder of third card
		    ...
		196 = /dev/dvb/adapter3/video0    first video decoder of fourth card

216 char	Bluetooth RFCOMM TTY devices
		  0 = /dev/rfcomm0		First Bluetooth RFCOMM TTY device
		  1 = /dev/rfcomm1		Second Bluetooth RFCOMM TTY device
		    ...

217 char	Bluetooth RFCOMM TTY devices (alternate devices)
		  0 = /dev/curf0		Callout device for rfcomm0
		  1 = /dev/curf1		Callout device for rfcomm1
		    ...

218 char	The Logical Company bus Unibus/Qbus adapters
		  0 = /dev/logicalco/bci/0	First bus adapter
		  1 = /dev/logicalco/bci/1	First bus adapter
		    ...

219 char	The Logical Company DCI-1300 digital I/O card
		  0 = /dev/logicalco/dci1300/0	First DCI-1300 card
		  1 = /dev/logicalco/dci1300/1	Second DCI-1300 card
		    ...

220 char	Myricom Myrinet "GM" board
		  0 = /dev/myricom/gm0		First Myrinet GM board
		  1 = /dev/myricom/gmp0		First board "root access"
		  2 = /dev/myricom/gm1		Second Myrinet GM board
		  3 = /dev/myricom/gmp1		Second board "root access"
		    ...

221 char	VME bus
		  0 = /dev/bus/vme/m0		First master image
		  1 = /dev/bus/vme/m1		Second master image
		  2 = /dev/bus/vme/m2		Third master image
		  3 = /dev/bus/vme/m3		Fourth master image
		  4 = /dev/bus/vme/s0		First slave image
		  5 = /dev/bus/vme/s1		Second slave image
		  6 = /dev/bus/vme/s2		Third slave image
		  7 = /dev/bus/vme/s3		Fourth slave image
		  8 = /dev/bus/vme/ctl		Control

		It is expected that all VME bus drivers will use the
		same interface.  For interface documentation see
		http://www.vmelinux.org/.

224 char	A2232 serial card
		  0 = /dev/ttyY0		First A2232 port
		  1 = /dev/ttyY1		Second A2232 port
		    ...

225 char	A2232 serial card (alternate devices)
		  0 = /dev/cuy0			Callout device for ttyY0
		  1 = /dev/cuy1			Callout device for ttyY1
		    ...

226 char	Direct Rendering Infrastructure (DRI)
		  0 = /dev/dri/card0		First graphics card
		  1 = /dev/dri/card1		Second graphics card
		    ...

227 char	IBM 3270 terminal Unix tty access
		  1 = /dev/3270/tty1		First 3270 terminal
		  2 = /dev/3270/tty2		Seconds 3270 terminal
		    ...

228 char	IBM 3270 terminal block-mode access
		  0 = /dev/3270/tub		Controlling interface
		  1 = /dev/3270/tub1		First 3270 terminal
		  2 = /dev/3270/tub2		Second 3270 terminal
		    ...

229 char	IBM iSeries/pSeries virtual console
		  0 = /dev/hvc0			First console port
		  1 = /dev/hvc1			Second console port
		    ...

230 char	IBM iSeries virtual tape
		  0 = /dev/iseries/vt0		First virtual tape, mode 0
		  1 = /dev/iseries/vt1		Second virtual tape, mode 0
		    ...
		 32 = /dev/iseries/vt0l		First virtual tape, mode 1
		 33 = /dev/iseries/vt1l		Second virtual tape, mode 1
		    ...
		 64 = /dev/iseries/vt0m		First virtual tape, mode 2
		 65 = /dev/iseries/vt1m		Second virtual tape, mode 2
		    ...
		 96 = /dev/iseries/vt0a		First virtual tape, mode 3
		 97 = /dev/iseries/vt1a		Second virtual tape, mode 3
		      ...
		128 = /dev/iseries/nvt0		First virtual tape, mode 0, no rewind
		129 = /dev/iseries/nvt1		Second virtual tape, mode 0, no rewind
		    ...
		160 = /dev/iseries/nvt0l	First virtual tape, mode 1, no rewind
		161 = /dev/iseries/nvt1l	Second virtual tape, mode 1, no rewind
		    ...
		192 = /dev/iseries/nvt0m	First virtual tape, mode 2, no rewind
		193 = /dev/iseries/nvt1m	Second virtual tape, mode 2, no rewind
		    ...
		224 = /dev/iseries/nvt0a	First virtual tape, mode 3, no rewind
		225 = /dev/iseries/nvt1a	Second virtual tape, mode 3, no rewind
		    ...

		"No rewind" refers to the omission of the default
		automatic rewind on device close.  The MTREW or MTOFFL
		ioctl()'s can be used to rewind the tape regardless of
		the device used to access it.

231 char	InfiniBand
		0 = /dev/infiniband/umad0
		1 = /dev/infiniband/umad1
		  ...
		63 = /dev/infiniband/umad63    63rd InfiniBandMad device
		64 = /dev/infiniband/issm0     First InfiniBand IsSM device
		65 = /dev/infiniband/issm1     Second InfiniBand IsSM device
		  ...
		127 = /dev/infiniband/issm63    63rd InfiniBand IsSM device
		128 = /dev/infiniband/uverbs0   First InfiniBand verbs device
		129 = /dev/infiniband/uverbs1   Second InfiniBand verbs device
		  ...
		159 = /dev/infiniband/uverbs31  31st InfiniBand verbs device

232 char	Biometric Devices
		0 = /dev/biometric/sensor0/fingerprint	first fingerprint sensor on first device
		1 = /dev/biometric/sensor0/iris		first iris sensor on first device
		2 = /dev/biometric/sensor0/retina	first retina sensor on first device
		3 = /dev/biometric/sensor0/voiceprint	first voiceprint sensor on first device
		4 = /dev/biometric/sensor0/facial	first facial sensor on first device
		5 = /dev/biometric/sensor0/hand		first hand sensor on first device
		  ...
		10 = /dev/biometric/sensor1/fingerprint	first fingerprint sensor on second device
		  ...
		20 = /dev/biometric/sensor2/fingerprint	first fingerprint sensor on third device
		  ...

233 char	PathScale InfiniPath interconnect
		0 = /dev/ipath        Primary device for programs (any unit)
		1 = /dev/ipath0       Access specifically to unit 0
		2 = /dev/ipath1       Access specifically to unit 1
		  ...
		4 = /dev/ipath3       Access specifically to unit 3
		129 = /dev/ipath_sma    Device used by Subnet Management Agent
		130 = /dev/ipath_diag   Device used by diagnostics programs

234-239		UNASSIGNED

240-254 char	LOCAL/EXPERIMENTAL USE

240-254 block	LOCAL/EXPERIMENTAL USE
		Allocated for local/experimental use.  For devices not
		assigned official numbers, these ranges should be
		used in order to avoid conflicting with future assignments.

255 char	RESERVED

255 block	RESERVED

		This major is reserved to assist the expansion to a
		larger number space.  No device nodes with this major
		should ever be created on the filesystem.
		(This is probably not true anymore, but I'll leave it
		for now /Torben)

---LARGE MAJORS!!!!!---

256 char	Equinox SST multi-port serial boards
		   0 = /dev/ttyEQ0	First serial port on first Equinox SST board
		 127 = /dev/ttyEQ127	Last serial port on first Equinox SST board
		 128 = /dev/ttyEQ128	First serial port on second Equinox SST board
		  ...
		1027 = /dev/ttyEQ1027	Last serial port on eighth Equinox SST board

256 block	Resident Flash Disk Flash Translation Layer
		  0 = /dev/rfda		First RFD FTL layer
		 16 = /dev/rfdb		Second RFD FTL layer
		  ...
		240 = /dev/rfdp		16th RFD FTL layer

257 char	Phoenix Technologies Cryptographic Services Driver
		  0 = /dev/ptlsec	Crypto Services Driver

257 block	SSFDC Flash Translation Layer filesystem
		  0 = /dev/ssfdca	First SSFDC layer
		  8 = /dev/ssfdcb	Second SSFDC layer
		 16 = /dev/ssfdcc	Third SSFDC layer
		 24 = /dev/ssfdcd	4th SSFDC layer
		 32 = /dev/ssfdce	5th SSFDC layer
		 40 = /dev/ssfdcf	6th SSFDC layer
		 48 = /dev/ssfdcg	7th SSFDC layer
		 56 = /dev/ssfdch	8th SSFDC layer

258 block	ROM/Flash read-only translation layer
		  0 = /dev/blockrom0	First ROM card's translation layer interface
		  1 = /dev/blockrom1	Second ROM card's translation layer interface
		  ...

259 block	Block Extended Major
		  Used dynamically to hold additional partition minor
		  numbers and allow large numbers of partitions per device

259 char	FPGA configuration interfaces
		  0 = /dev/icap0	First Xilinx internal configuration
		  1 = /dev/icap1	Second Xilinx internal configuration

260 char	OSD (Object-based-device) SCSI Device
		  0 = /dev/osd0		First OSD Device
		  1 = /dev/osd1		Second OSD Device
		  ...
		  255 = /dev/osd255	256th OSD Device

 ****	ADDITIONAL /dev DIRECTORY ENTRIES

This section details additional entries that should or may exist in
the /dev directory.  It is preferred that symbolic links use the same
form (absolute or relative) as is indicated here.  Links are
classified as "hard" or "symbolic" depending on the preferred type of
link; if possible, the indicated type of link should be used.


	Compulsory links

These links should exist on all systems:

/dev/fd		/proc/self/fd	symbolic	File descriptors
/dev/stdin	fd/0		symbolic	stdin file descriptor
/dev/stdout	fd/1		symbolic	stdout file descriptor
/dev/stderr	fd/2		symbolic	stderr file descriptor
/dev/nfsd	socksys		symbolic	Required by iBCS-2
/dev/X0R	null		symbolic	Required by iBCS-2

Note: /dev/X0R is <letter X>-<digit 0>-<letter R>.

	Recommended links

It is recommended that these links exist on all systems:

/dev/core	/proc/kcore	symbolic	Backward compatibility
/dev/ramdisk	ram0		symbolic	Backward compatibility
/dev/ftape	qft0		symbolic	Backward compatibility
/dev/bttv0	video0		symbolic	Backward compatibility
/dev/radio	radio0		symbolic	Backward compatibility
/dev/i2o*	/dev/i2o/*	symbolic	Backward compatibility
/dev/scd?	sr?		hard		Alternate SCSI CD-ROM name

	Locally defined links

The following links may be established locally to conform to the
configuration of the system.  This is merely a tabulation of existing
practice, and does not constitute a recommendation.  However, if they
exist, they should have the following uses.

/dev/mouse	mouse port	symbolic	Current mouse device
/dev/tape	tape device	symbolic	Current tape device
/dev/cdrom	CD-ROM device	symbolic	Current CD-ROM device
/dev/cdwriter	CD-writer	symbolic	Current CD-writer device
/dev/scanner	scanner		symbolic	Current scanner device
/dev/modem	modem port	symbolic	Current dialout device
/dev/root	root device	symbolic	Current root filesystem
/dev/swap	swap device	symbolic	Current swap device

/dev/modem should not be used for a modem which supports dialin as
well as dialout, as it tends to cause lock file problems.  If it
exists, /dev/modem should point to the appropriate primary TTY device
(the use of the alternate callout devices is deprecated).

For SCSI devices, /dev/tape and /dev/cdrom should point to the
``cooked'' devices (/dev/st* and /dev/sr*, respectively), whereas
/dev/cdwriter and /dev/scanner should point to the appropriate generic
SCSI devices (/dev/sg*).

/dev/mouse may point to a primary serial TTY device, a hardware mouse
device, or a socket for a mouse driver program (e.g. /dev/gpmdata).

	Sockets and pipes

Non-transient sockets and named pipes may exist in /dev.  Common entries are:

/dev/printer	socket		lpd local socket
/dev/log	socket		syslog local socket
/dev/gpmdata	socket		gpm mouse multiplexer

	Mount points

The following names are reserved for mounting special filesystems
under /dev.  These special filesystems provide kernel interfaces that
cannot be provided with standard device nodes.

/dev/pts	devpts		PTY slave filesystem
/dev/shm	tmpfs		POSIX shared memory maintenance access

 ****	TERMINAL DEVICES

Terminal, or TTY devices are a special class of character devices.  A
terminal device is any device that could act as a controlling terminal
for a session; this includes virtual consoles, serial ports, and
pseudoterminals (PTYs).

All terminal devices share a common set of capabilities known as line
disciplines; these include the common terminal line discipline as well
as SLIP and PPP modes.

All terminal devices are named similarly; this section explains the
naming and use of the various types of TTYs.  Note that the naming
conventions include several historical warts; some of these are
Linux-specific, some were inherited from other systems, and some
reflect Linux outgrowing a borrowed convention.

A hash mark (#) in a device name is used here to indicate a decimal
number without leading zeroes.

	Virtual consoles and the console device

Virtual consoles are full-screen terminal displays on the system video
monitor.  Virtual consoles are named /dev/tty#, with numbering
starting at /dev/tty1; /dev/tty0 is the current virtual console.
/dev/tty0 is the device that should be used to access the system video
card on those architectures for which the frame buffer devices
(/dev/fb*) are not applicable.	Do not use /dev/console
for this purpose.

The console device, /dev/console, is the device to which system
messages should be sent, and on which logins should be permitted in
single-user mode.  Starting with Linux 2.1.71, /dev/console is managed
by the kernel; for previous versions it should be a symbolic link to
either /dev/tty0, a specific virtual console such as /dev/tty1, or to
a serial port primary (tty*, not cu*) device, depending on the
configuration of the system.

	Serial ports

Serial ports are RS-232 serial ports and any device which simulates
one, either in hardware (such as internal modems) or in software (such
as the ISDN driver.)  Under Linux, each serial ports has two device
names, the primary or callin device and the alternate or callout one.
Each kind of device is indicated by a different letter.	 For any
letter X, the names of the devices are /dev/ttyX# and /dev/cux#,
respectively; for historical reasons, /dev/ttyS# and /dev/ttyC#
correspond to /dev/cua# and /dev/cub#.	In the future, it should be
expected that multiple letters will be used; all letters will be upper
case for the "tty" device (e.g. /dev/ttyDP#) and lower case for the
"cu" device (e.g. /dev/cudp#).

The names /dev/ttyQ# and /dev/cuq# are reserved for local use.

The alternate devices provide for kernel-based exclusion and somewhat
different defaults than the primary devices.  Their main purpose is to
allow the use of serial ports with programs with no inherent or broken
support for serial ports.  Their use is deprecated, and they may be
removed from a future version of Linux.

Arbitration of serial ports is provided by the use of lock files with
the names /var/lock/LCK..ttyX#.	 The contents of the lock file should
be the PID of the locking process as an ASCII number.

It is common practice to install links such as /dev/modem
which point to serial ports.  In order to ensure proper locking in the
presence of these links, it is recommended that software chase
symlinks and lock all possible names; additionally, it is recommended
that a lock file be installed with the corresponding alternate
device.	 In order to avoid deadlocks, it is recommended that the locks
are acquired in the following order, and released in the reverse:

	1. The symbolic link name, if any (/var/lock/LCK..modem)
	2. The "tty" name (/var/lock/LCK..ttyS2)
	3. The alternate device name (/var/lock/LCK..cua2)

In the case of nested symbolic links, the lock files should be
installed in the order the symlinks are resolved.

Under no circumstances should an application hold a lock while waiting
for another to be released.  In addition, applications which attempt
to create lock files for the corresponding alternate device names
should take into account the possibility of being used on a non-serial
port TTY, for which no alternate device would exist.

	Pseudoterminals (PTYs)

Pseudoterminals, or PTYs, are used to create login sessions or provide
other capabilities requiring a TTY line discipline (including SLIP or
PPP capability) to arbitrary data-generation processes.	 Each PTY has
a master side, named /dev/pty[p-za-e][0-9a-f], and a slave side, named
/dev/tty[p-za-e][0-9a-f].  The kernel arbitrates the use of PTYs by
allowing each master side to be opened only once.

Once the master side has been opened, the corresponding slave device
can be used in the same manner as any TTY device.  The master and
slave devices are connected by the kernel, generating the equivalent
of a bidirectional pipe with TTY capabilities.

Recent versions of the Linux kernels and GNU libc contain support for
the System V/Unix98 naming scheme for PTYs, which assigns a common
device, /dev/ptmx, to all the masters (opening it will automatically
give you a previously unassigned PTY) and a subdirectory, /dev/pts,
for the slaves; the slaves are named with decimal integers (/dev/pts/#
in our notation).  This removes the problem of exhausting the
namespace and enables the kernel to automatically create the device
nodes for the slaves on demand using the "devpts" filesystem.

Digital Signature Verification API

CONTENTS

1. Introduction
2. API
3. User-space utilities


1. Introduction

Digital signature verification API provides a method to verify digital signature.
Currently digital signatures are used by the IMA/EVM integrity protection subsystem.

Digital signature verification is implemented using cut-down kernel port of
GnuPG multi-precision integers (MPI) library. The kernel port provides
memory allocation errors handling, has been refactored according to kernel
coding style, and checkpatch.pl reported errors and warnings have been fixed.

Public key and signature consist of header and MPIs.

struct pubkey_hdr {
	uint8_t		version;	/* key format version */
	time_t		timestamp;	/* key made, always 0 for now */
	uint8_t		algo;
	uint8_t		nmpi;
	char		mpi[0];
} __packed;

struct signature_hdr {
	uint8_t		version;	/* signature format version */
	time_t		timestamp;	/* signature made */
	uint8_t		algo;
	uint8_t		hash;
	uint8_t		keyid[8];
	uint8_t		nmpi;
	char		mpi[0];
} __packed;

keyid equals to SHA1[12-19] over the total key content.
Signature header is used as an input to generate a signature.
Such approach insures that key or signature header could not be changed.
It protects timestamp from been changed and can be used for rollback
protection.

2. API

API currently includes only 1 function:

	digsig_verify() - digital signature verification with public key


/**
 * digsig_verify() - digital signature verification with public key
 * @keyring:	keyring to search key in
 * @sig:	digital signature
 * @sigen:	length of the signature
 * @data:	data
 * @datalen:	length of the data
 * @return:	0 on success, -EINVAL otherwise
 *
 * Verifies data integrity against digital signature.
 * Currently only RSA is supported.
 * Normally hash of the content is used as a data for this function.
 *
 */
int digsig_verify(struct key *keyring, const char *sig, int siglen,
						const char *data, int datalen);

3. User-space utilities

The signing and key management utilities evm-utils provide functionality
to generate signatures, to load keys into the kernel keyring.
Keys can be in PEM or converted to the kernel format.
When the key is added to the kernel keyring, the keyid defines the name
of the key: 5D2B05FC633EE3E8 in the example bellow.

Here is example output of the keyctl utility.

$ keyctl show
Session Keyring
       -3 --alswrv      0     0  keyring: _ses
603976250 --alswrv      0    -1   \_ keyring: _uid.0
817777377 --alswrv      0     0       \_ user: kmk
891974900 --alswrv      0     0       \_ encrypted: evm-key
170323636 --alswrv      0     0       \_ keyring: _module
548221616 --alswrv      0     0       \_ keyring: _ima
128198054 --alswrv      0     0       \_ keyring: _evm

$ keyctl list 128198054
1 key in keyring:
620789745: --alswrv     0     0 user: 5D2B05FC633EE3E8


Dmitry Kasatkin
06.10.2011
		     Dynamic DMA mapping Guide
		     =========================

		 David S. Miller <davem@redhat.com>
		 Richard Henderson <rth@cygnus.com>
		  Jakub Jelinek <jakub@redhat.com>

This is a guide to device driver writers on how to use the DMA API
with example pseudo-code.  For a concise description of the API, see
DMA-API.txt.

                       CPU and DMA addresses

There are several kinds of addresses involved in the DMA API, and it's
important to understand the differences.

The kernel normally uses virtual addresses.  Any address returned by
kmalloc(), vmalloc(), and similar interfaces is a virtual address and can
be stored in a "void *".

The virtual memory system (TLB, page tables, etc.) translates virtual
addresses to CPU physical addresses, which are stored as "phys_addr_t" or
"resource_size_t".  The kernel manages device resources like registers as
physical addresses.  These are the addresses in /proc/iomem.  The physical
address is not directly useful to a driver; it must use ioremap() to map
the space and produce a virtual address.

I/O devices use a third kind of address: a "bus address" or "DMA address".
If a device has registers at an MMIO address, or if it performs DMA to read
or write system memory, the addresses used by the device are bus addresses.
In some systems, bus addresses are identical to CPU physical addresses, but
in general they are not.  IOMMUs and host bridges can produce arbitrary
mappings between physical and bus addresses.

Here's a picture and some examples:

               CPU                  CPU                  Bus
             Virtual              Physical             Address
             Address              Address               Space
              Space                Space

            +-------+             +------+             +------+
            |       |             |MMIO  |   Offset    |      |
            |       |  Virtual    |Space |   applied   |      |
          C +-------+ --------> B +------+ ----------> +------+ A
            |       |  mapping    |      |   by host   |      |
  +-----+   |       |             |      |   bridge    |      |   +--------+
  |     |   |       |             +------+             |      |   |        |
  | CPU |   |       |             | RAM  |             |      |   | Device |
  |     |   |       |             |      |             |      |   |        |
  +-----+   +-------+             +------+             +------+   +--------+
            |       |  Virtual    |Buffer|   Mapping   |      |
          X +-------+ --------> Y +------+ <---------- +------+ Z
            |       |  mapping    | RAM  |   by IOMMU
            |       |             |      |
            |       |             |      |
            +-------+             +------+

During the enumeration process, the kernel learns about I/O devices and
their MMIO space and the host bridges that connect them to the system.  For
example, if a PCI device has a BAR, the kernel reads the bus address (A)
from the BAR and converts it to a CPU physical address (B).  The address B
is stored in a struct resource and usually exposed via /proc/iomem.  When a
driver claims a device, it typically uses ioremap() to map physical address
B at a virtual address (C).  It can then use, e.g., ioread32(C), to access
the device registers at bus address A.

If the device supports DMA, the driver sets up a buffer using kmalloc() or
a similar interface, which returns a virtual address (X).  The virtual
memory system maps X to a physical address (Y) in system RAM.  The driver
can use virtual address X to access the buffer, but the device itself
cannot because DMA doesn't go through the CPU virtual memory system.

In some simple systems, the device can do DMA directly to physical address
Y.  But in many others, there is IOMMU hardware that translates bus
addresses to physical addresses, e.g., it translates Z to Y.  This is part
of the reason for the DMA API: the driver can give a virtual address X to
an interface like dma_map_single(), which sets up any required IOMMU
mapping and returns the bus address Z.  The driver then tells the device to
do DMA to Z, and the IOMMU maps it to the buffer at address Y in system
RAM.

So that Linux can use the dynamic DMA mapping, it needs some help from the
drivers, namely it has to take into account that DMA addresses should be
mapped only for the time they are actually used and unmapped after the DMA
transfer.

The following API will work of course even on platforms where no such
hardware exists.

Note that the DMA API works with any bus independent of the underlying
microprocessor architecture. You should use the DMA API rather than the
bus-specific DMA API, i.e., use the dma_map_*() interfaces rather than the
pci_map_*() interfaces.

First of all, you should make sure

#include <linux/dma-mapping.h>

is in your driver, which provides the definition of dma_addr_t.  This type
can hold any valid DMA or bus address for the platform and should be used
everywhere you hold a DMA address returned from the DMA mapping functions.

			 What memory is DMA'able?

The first piece of information you must know is what kernel memory can
be used with the DMA mapping facilities.  There has been an unwritten
set of rules regarding this, and this text is an attempt to finally
write them down.

If you acquired your memory via the page allocator
(i.e. __get_free_page*()) or the generic memory allocators
(i.e. kmalloc() or kmem_cache_alloc()) then you may DMA to/from
that memory using the addresses returned from those routines.

This means specifically that you may _not_ use the memory/addresses
returned from vmalloc() for DMA.  It is possible to DMA to the
_underlying_ memory mapped into a vmalloc() area, but this requires
walking page tables to get the physical addresses, and then
translating each of those pages back to a kernel address using
something like __va().  [ EDIT: Update this when we integrate
Gerd Knorr's generic code which does this. ]

This rule also means that you may use neither kernel image addresses
(items in data/text/bss segments), nor module image addresses, nor
stack addresses for DMA.  These could all be mapped somewhere entirely
different than the rest of physical memory.  Even if those classes of
memory could physically work with DMA, you'd need to ensure the I/O
buffers were cacheline-aligned.  Without that, you'd see cacheline
sharing problems (data corruption) on CPUs with DMA-incoherent caches.
(The CPU could write to one word, DMA would write to a different one
in the same cache line, and one of them could be overwritten.)

Also, this means that you cannot take the return of a kmap()
call and DMA to/from that.  This is similar to vmalloc().

What about block I/O and networking buffers?  The block I/O and
networking subsystems make sure that the buffers they use are valid
for you to DMA from/to.

			DMA addressing limitations

Does your device have any DMA addressing limitations?  For example, is
your device only capable of driving the low order 24-bits of address?
If so, you need to inform the kernel of this fact.

By default, the kernel assumes that your device can address the full
32-bits.  For a 64-bit capable device, this needs to be increased.
And for a device with limitations, as discussed in the previous
paragraph, it needs to be decreased.

Special note about PCI: PCI-X specification requires PCI-X devices to
support 64-bit addressing (DAC) for all transactions.  And at least
one platform (SGI SN2) requires 64-bit consistent allocations to
operate correctly when the IO bus is in PCI-X mode.

For correct operation, you must interrogate the kernel in your device
probe routine to see if the DMA controller on the machine can properly
support the DMA addressing limitation your device has.  It is good
style to do this even if your device holds the default setting,
because this shows that you did think about these issues wrt. your
device.

The query is performed via a call to dma_set_mask_and_coherent():

	int dma_set_mask_and_coherent(struct device *dev, u64 mask);

which will query the mask for both streaming and coherent APIs together.
If you have some special requirements, then the following two separate
queries can be used instead:

	The query for streaming mappings is performed via a call to
	dma_set_mask():

		int dma_set_mask(struct device *dev, u64 mask);

	The query for consistent allocations is performed via a call
	to dma_set_coherent_mask():

		int dma_set_coherent_mask(struct device *dev, u64 mask);

Here, dev is a pointer to the device struct of your device, and mask
is a bit mask describing which bits of an address your device
supports.  It returns zero if your card can perform DMA properly on
the machine given the address mask you provided.  In general, the
device struct of your device is embedded in the bus-specific device
struct of your device.  For example, &pdev->dev is a pointer to the
device struct of a PCI device (pdev is a pointer to the PCI device
struct of your device).

If it returns non-zero, your device cannot perform DMA properly on
this platform, and attempting to do so will result in undefined
behavior.  You must either use a different mask, or not use DMA.

This means that in the failure case, you have three options:

1) Use another DMA mask, if possible (see below).
2) Use some non-DMA mode for data transfer, if possible.
3) Ignore this device and do not initialize it.

It is recommended that your driver print a kernel KERN_WARNING message
when you end up performing either #2 or #3.  In this manner, if a user
of your driver reports that performance is bad or that the device is not
even detected, you can ask them for the kernel messages to find out
exactly why.

The standard 32-bit addressing device would do something like this:

	if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32))) {
		dev_warn(dev, "mydev: No suitable DMA available\n");
		goto ignore_this_device;
	}

Another common scenario is a 64-bit capable device.  The approach here
is to try for 64-bit addressing, but back down to a 32-bit mask that
should not fail.  The kernel may fail the 64-bit mask not because the
platform is not capable of 64-bit addressing.  Rather, it may fail in
this case simply because 32-bit addressing is done more efficiently
than 64-bit addressing.  For example, Sparc64 PCI SAC addressing is
more efficient than DAC addressing.

Here is how you would handle a 64-bit capable device which can drive
all 64-bits when accessing streaming DMA:

	int using_dac;

	if (!dma_set_mask(dev, DMA_BIT_MASK(64))) {
		using_dac = 1;
	} else if (!dma_set_mask(dev, DMA_BIT_MASK(32))) {
		using_dac = 0;
	} else {
		dev_warn(dev, "mydev: No suitable DMA available\n");
		goto ignore_this_device;
	}

If a card is capable of using 64-bit consistent allocations as well,
the case would look like this:

	int using_dac, consistent_using_dac;

	if (!dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64))) {
		using_dac = 1;
	   	consistent_using_dac = 1;
	} else if (!dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32))) {
		using_dac = 0;
		consistent_using_dac = 0;
	} else {
		dev_warn(dev, "mydev: No suitable DMA available\n");
		goto ignore_this_device;
	}

The coherent mask will always be able to set the same or a smaller mask as
the streaming mask. However for the rare case that a device driver only
uses consistent allocations, one would have to check the return value from
dma_set_coherent_mask().

Finally, if your device can only drive the low 24-bits of
address you might do something like:

	if (dma_set_mask(dev, DMA_BIT_MASK(24))) {
		dev_warn(dev, "mydev: 24-bit DMA addressing not available\n");
		goto ignore_this_device;
	}

When dma_set_mask() or dma_set_mask_and_coherent() is successful, and
returns zero, the kernel saves away this mask you have provided.  The
kernel will use this information later when you make DMA mappings.

There is a case which we are aware of at this time, which is worth
mentioning in this documentation.  If your device supports multiple
functions (for example a sound card provides playback and record
functions) and the various different functions have _different_
DMA addressing limitations, you may wish to probe each mask and
only provide the functionality which the machine can handle.  It
is important that the last call to dma_set_mask() be for the
most specific mask.

Here is pseudo-code showing how this might be done:

	#define PLAYBACK_ADDRESS_BITS	DMA_BIT_MASK(32)
	#define RECORD_ADDRESS_BITS	DMA_BIT_MASK(24)

	struct my_sound_card *card;
	struct device *dev;

	...
	if (!dma_set_mask(dev, PLAYBACK_ADDRESS_BITS)) {
		card->playback_enabled = 1;
	} else {
		card->playback_enabled = 0;
		dev_warn(dev, "%s: Playback disabled due to DMA limitations\n",
		       card->name);
	}
	if (!dma_set_mask(dev, RECORD_ADDRESS_BITS)) {
		card->record_enabled = 1;
	} else {
		card->record_enabled = 0;
		dev_warn(dev, "%s: Record disabled due to DMA limitations\n",
		       card->name);
	}

A sound card was used as an example here because this genre of PCI
devices seems to be littered with ISA chips given a PCI front end,
and thus retaining the 16MB DMA addressing limitations of ISA.

			Types of DMA mappings

There are two types of DMA mappings:

- Consistent DMA mappings which are usually mapped at driver
  initialization, unmapped at the end and for which the hardware should
  guarantee that the device and the CPU can access the data
  in parallel and will see updates made by each other without any
  explicit software flushing.

  Think of "consistent" as "synchronous" or "coherent".

  The current default is to return consistent memory in the low 32
  bits of the bus space.  However, for future compatibility you should
  set the consistent mask even if this default is fine for your
  driver.

  Good examples of what to use consistent mappings for are:

	- Network card DMA ring descriptors.
	- SCSI adapter mailbox command data structures.
	- Device firmware microcode executed out of
	  main memory.

  The invariant these examples all require is that any CPU store
  to memory is immediately visible to the device, and vice
  versa.  Consistent mappings guarantee this.

  IMPORTANT: Consistent DMA memory does not preclude the usage of
             proper memory barriers.  The CPU may reorder stores to
	     consistent memory just as it may normal memory.  Example:
	     if it is important for the device to see the first word
	     of a descriptor updated before the second, you must do
	     something like:

		desc->word0 = address;
		wmb();
		desc->word1 = DESC_VALID;

             in order to get correct behavior on all platforms.

	     Also, on some platforms your driver may need to flush CPU write
	     buffers in much the same way as it needs to flush write buffers
	     found in PCI bridges (such as by reading a register's value
	     after writing it).

- Streaming DMA mappings which are usually mapped for one DMA
  transfer, unmapped right after it (unless you use dma_sync_* below)
  and for which hardware can optimize for sequential accesses.

  This of "streaming" as "asynchronous" or "outside the coherency
  domain".

  Good examples of what to use streaming mappings for are:

	- Networking buffers transmitted/received by a device.
	- Filesystem buffers written/read by a SCSI device.

  The interfaces for using this type of mapping were designed in
  such a way that an implementation can make whatever performance
  optimizations the hardware allows.  To this end, when using
  such mappings you must be explicit about what you want to happen.

Neither type of DMA mapping has alignment restrictions that come from
the underlying bus, although some devices may have such restrictions.
Also, systems with caches that aren't DMA-coherent will work better
when the underlying buffers don't share cache lines with other data.


		 Using Consistent DMA mappings.

To allocate and map large (PAGE_SIZE or so) consistent DMA regions,
you should do:

	dma_addr_t dma_handle;

	cpu_addr = dma_alloc_coherent(dev, size, &dma_handle, gfp);

where device is a struct device *. This may be called in interrupt
context with the GFP_ATOMIC flag.

Size is the length of the region you want to allocate, in bytes.

This routine will allocate RAM for that region, so it acts similarly to
__get_free_pages() (but takes size instead of a page order).  If your
driver needs regions sized smaller than a page, you may prefer using
the dma_pool interface, described below.

The consistent DMA mapping interfaces, for non-NULL dev, will by
default return a DMA address which is 32-bit addressable.  Even if the
device indicates (via DMA mask) that it may address the upper 32-bits,
consistent allocation will only return > 32-bit addresses for DMA if
the consistent DMA mask has been explicitly changed via
dma_set_coherent_mask().  This is true of the dma_pool interface as
well.

dma_alloc_coherent() returns two values: the virtual address which you
can use to access it from the CPU and dma_handle which you pass to the
card.

The CPU virtual address and the DMA bus address are both
guaranteed to be aligned to the smallest PAGE_SIZE order which
is greater than or equal to the requested size.  This invariant
exists (for example) to guarantee that if you allocate a chunk
which is smaller than or equal to 64 kilobytes, the extent of the
buffer you receive will not cross a 64K boundary.

To unmap and free such a DMA region, you call:

	dma_free_coherent(dev, size, cpu_addr, dma_handle);

where dev, size are the same as in the above call and cpu_addr and
dma_handle are the values dma_alloc_coherent() returned to you.
This function may not be called in interrupt context.

If your driver needs lots of smaller memory regions, you can write
custom code to subdivide pages returned by dma_alloc_coherent(),
or you can use the dma_pool API to do that.  A dma_pool is like
a kmem_cache, but it uses dma_alloc_coherent(), not __get_free_pages().
Also, it understands common hardware constraints for alignment,
like queue heads needing to be aligned on N byte boundaries.

Create a dma_pool like this:

	struct dma_pool *pool;

	pool = dma_pool_create(name, dev, size, align, boundary);

The "name" is for diagnostics (like a kmem_cache name); dev and size
are as above.  The device's hardware alignment requirement for this
type of data is "align" (which is expressed in bytes, and must be a
power of two).  If your device has no boundary crossing restrictions,
pass 0 for boundary; passing 4096 says memory allocated from this pool
must not cross 4KByte boundaries (but at that time it may be better to
use dma_alloc_coherent() directly instead).

Allocate memory from a DMA pool like this:

	cpu_addr = dma_pool_alloc(pool, flags, &dma_handle);

flags are GFP_KERNEL if blocking is permitted (not in_interrupt nor
holding SMP locks), GFP_ATOMIC otherwise.  Like dma_alloc_coherent(),
this returns two values, cpu_addr and dma_handle.

Free memory that was allocated from a dma_pool like this:

	dma_pool_free(pool, cpu_addr, dma_handle);

where pool is what you passed to dma_pool_alloc(), and cpu_addr and
dma_handle are the values dma_pool_alloc() returned. This function
may be called in interrupt context.

Destroy a dma_pool by calling:

	dma_pool_destroy(pool);

Make sure you've called dma_pool_free() for all memory allocated
from a pool before you destroy the pool. This function may not
be called in interrupt context.

			DMA Direction

The interfaces described in subsequent portions of this document
take a DMA direction argument, which is an integer and takes on
one of the following values:

 DMA_BIDIRECTIONAL
 DMA_TO_DEVICE
 DMA_FROM_DEVICE
 DMA_NONE

You should provide the exact DMA direction if you know it.

DMA_TO_DEVICE means "from main memory to the device"
DMA_FROM_DEVICE means "from the device to main memory"
It is the direction in which the data moves during the DMA
transfer.

You are _strongly_ encouraged to specify this as precisely
as you possibly can.

If you absolutely cannot know the direction of the DMA transfer,
specify DMA_BIDIRECTIONAL.  It means that the DMA can go in
either direction.  The platform guarantees that you may legally
specify this, and that it will work, but this may be at the
cost of performance for example.

The value DMA_NONE is to be used for debugging.  One can
hold this in a data structure before you come to know the
precise direction, and this will help catch cases where your
direction tracking logic has failed to set things up properly.

Another advantage of specifying this value precisely (outside of
potential platform-specific optimizations of such) is for debugging.
Some platforms actually have a write permission boolean which DMA
mappings can be marked with, much like page protections in the user
program address space.  Such platforms can and do report errors in the
kernel logs when the DMA controller hardware detects violation of the
permission setting.

Only streaming mappings specify a direction, consistent mappings
implicitly have a direction attribute setting of
DMA_BIDIRECTIONAL.

The SCSI subsystem tells you the direction to use in the
'sc_data_direction' member of the SCSI command your driver is
working on.

For Networking drivers, it's a rather simple affair.  For transmit
packets, map/unmap them with the DMA_TO_DEVICE direction
specifier.  For receive packets, just the opposite, map/unmap them
with the DMA_FROM_DEVICE direction specifier.

		  Using Streaming DMA mappings

The streaming DMA mapping routines can be called from interrupt
context.  There are two versions of each map/unmap, one which will
map/unmap a single memory region, and one which will map/unmap a
scatterlist.

To map a single region, you do:

	struct device *dev = &my_dev->dev;
	dma_addr_t dma_handle;
	void *addr = buffer->ptr;
	size_t size = buffer->len;

	dma_handle = dma_map_single(dev, addr, size, direction);
	if (dma_mapping_error(dma_handle)) {
		/*
		 * reduce current DMA mapping usage,
		 * delay and try again later or
		 * reset driver.
		 */
		goto map_error_handling;
	}

and to unmap it:

	dma_unmap_single(dev, dma_handle, size, direction);

You should call dma_mapping_error() as dma_map_single() could fail and return
error. Not all DMA implementations support the dma_mapping_error() interface.
However, it is a good practice to call dma_mapping_error() interface, which
will invoke the generic mapping error check interface. Doing so will ensure
that the mapping code will work correctly on all DMA implementations without
any dependency on the specifics of the underlying implementation. Using the
returned address without checking for errors could result in failures ranging
from panics to silent data corruption. A couple of examples of incorrect ways
to check for errors that make assumptions about the underlying DMA
implementation are as follows and these are applicable to dma_map_page() as
well.

Incorrect example 1:
	dma_addr_t dma_handle;

	dma_handle = dma_map_single(dev, addr, size, direction);
	if ((dma_handle & 0xffff != 0) || (dma_handle >= 0x1000000)) {
		goto map_error;
	}

Incorrect example 2:
	dma_addr_t dma_handle;

	dma_handle = dma_map_single(dev, addr, size, direction);
	if (dma_handle == DMA_ERROR_CODE) {
		goto map_error;
	}

You should call dma_unmap_single() when the DMA activity is finished, e.g.,
from the interrupt which told you that the DMA transfer is done.

Using CPU pointers like this for single mappings has a disadvantage:
you cannot reference HIGHMEM memory in this way.  Thus, there is a
map/unmap interface pair akin to dma_{map,unmap}_single().  These
interfaces deal with page/offset pairs instead of CPU pointers.
Specifically:

	struct device *dev = &my_dev->dev;
	dma_addr_t dma_handle;
	struct page *page = buffer->page;
	unsigned long offset = buffer->offset;
	size_t size = buffer->len;

	dma_handle = dma_map_page(dev, page, offset, size, direction);
	if (dma_mapping_error(dma_handle)) {
		/*
		 * reduce current DMA mapping usage,
		 * delay and try again later or
		 * reset driver.
		 */
		goto map_error_handling;
	}

	...

	dma_unmap_page(dev, dma_handle, size, direction);

Here, "offset" means byte offset within the given page.

You should call dma_mapping_error() as dma_map_page() could fail and return
error as outlined under the dma_map_single() discussion.

You should call dma_unmap_page() when the DMA activity is finished, e.g.,
from the interrupt which told you that the DMA transfer is done.

With scatterlists, you map a region gathered from several regions by:

	int i, count = dma_map_sg(dev, sglist, nents, direction);
	struct scatterlist *sg;

	for_each_sg(sglist, sg, count, i) {
		hw_address[i] = sg_dma_address(sg);
		hw_len[i] = sg_dma_len(sg);
	}

where nents is the number of entries in the sglist.

The implementation is free to merge several consecutive sglist entries
into one (e.g. if DMA mapping is done with PAGE_SIZE granularity, any
consecutive sglist entries can be merged into one provided the first one
ends and the second one starts on a page boundary - in fact this is a huge
advantage for cards which either cannot do scatter-gather or have very
limited number of scatter-gather entries) and returns the actual number
of sg entries it mapped them to. On failure 0 is returned.

Then you should loop count times (note: this can be less than nents times)
and use sg_dma_address() and sg_dma_len() macros where you previously
accessed sg->address and sg->length as shown above.

To unmap a scatterlist, just call:

	dma_unmap_sg(dev, sglist, nents, direction);

Again, make sure DMA activity has already finished.

PLEASE NOTE:  The 'nents' argument to the dma_unmap_sg call must be
              the _same_ one you passed into the dma_map_sg call,
	      it should _NOT_ be the 'count' value _returned_ from the
              dma_map_sg call.

Every dma_map_{single,sg}() call should have its dma_unmap_{single,sg}()
counterpart, because the bus address space is a shared resource and
you could render the machine unusable by consuming all bus addresses.

If you need to use the same streaming DMA region multiple times and touch
the data in between the DMA transfers, the buffer needs to be synced
properly in order for the CPU and device to see the most up-to-date and
correct copy of the DMA buffer.

So, firstly, just map it with dma_map_{single,sg}(), and after each DMA
transfer call either:

	dma_sync_single_for_cpu(dev, dma_handle, size, direction);

or:

	dma_sync_sg_for_cpu(dev, sglist, nents, direction);

as appropriate.

Then, if you wish to let the device get at the DMA area again,
finish accessing the data with the CPU, and then before actually
giving the buffer to the hardware call either:

	dma_sync_single_for_device(dev, dma_handle, size, direction);

or:

	dma_sync_sg_for_device(dev, sglist, nents, direction);

as appropriate.

After the last DMA transfer call one of the DMA unmap routines
dma_unmap_{single,sg}(). If you don't touch the data from the first
dma_map_*() call till dma_unmap_*(), then you don't have to call the
dma_sync_*() routines at all.

Here is pseudo code which shows a situation in which you would need
to use the dma_sync_*() interfaces.

	my_card_setup_receive_buffer(struct my_card *cp, char *buffer, int len)
	{
		dma_addr_t mapping;

		mapping = dma_map_single(cp->dev, buffer, len, DMA_FROM_DEVICE);
		if (dma_mapping_error(dma_handle)) {
			/*
			 * reduce current DMA mapping usage,
			 * delay and try again later or
			 * reset driver.
			 */
			goto map_error_handling;
		}

		cp->rx_buf = buffer;
		cp->rx_len = len;
		cp->rx_dma = mapping;

		give_rx_buf_to_card(cp);
	}

	...

	my_card_interrupt_handler(int irq, void *devid, struct pt_regs *regs)
	{
		struct my_card *cp = devid;

		...
		if (read_card_status(cp) == RX_BUF_TRANSFERRED) {
			struct my_card_header *hp;

			/* Examine the header to see if we wish
			 * to accept the data.  But synchronize
			 * the DMA transfer with the CPU first
			 * so that we see updated contents.
			 */
			dma_sync_single_for_cpu(&cp->dev, cp->rx_dma,
						cp->rx_len,
						DMA_FROM_DEVICE);

			/* Now it is safe to examine the buffer. */
			hp = (struct my_card_header *) cp->rx_buf;
			if (header_is_ok(hp)) {
				dma_unmap_single(&cp->dev, cp->rx_dma, cp->rx_len,
						 DMA_FROM_DEVICE);
				pass_to_upper_layers(cp->rx_buf);
				make_and_setup_new_rx_buf(cp);
			} else {
				/* CPU should not write to
				 * DMA_FROM_DEVICE-mapped area,
				 * so dma_sync_single_for_device() is
				 * not needed here. It would be required
				 * for DMA_BIDIRECTIONAL mapping if
				 * the memory was modified.
				 */
				give_rx_buf_to_card(cp);
			}
		}
	}

Drivers converted fully to this interface should not use virt_to_bus() any
longer, nor should they use bus_to_virt(). Some drivers have to be changed a
little bit, because there is no longer an equivalent to bus_to_virt() in the
dynamic DMA mapping scheme - you have to always store the DMA addresses
returned by the dma_alloc_coherent(), dma_pool_alloc(), and dma_map_single()
calls (dma_map_sg() stores them in the scatterlist itself if the platform
supports dynamic DMA mapping in hardware) in your driver structures and/or
in the card registers.

All drivers should be using these interfaces with no exceptions.  It
is planned to completely remove virt_to_bus() and bus_to_virt() as
they are entirely deprecated.  Some ports already do not provide these
as it is impossible to correctly support them.

			Handling Errors

DMA address space is limited on some architectures and an allocation
failure can be determined by:

- checking if dma_alloc_coherent() returns NULL or dma_map_sg returns 0

- checking the dma_addr_t returned from dma_map_single() and dma_map_page()
  by using dma_mapping_error():

	dma_addr_t dma_handle;

	dma_handle = dma_map_single(dev, addr, size, direction);
	if (dma_mapping_error(dev, dma_handle)) {
		/*
		 * reduce current DMA mapping usage,
		 * delay and try again later or
		 * reset driver.
		 */
		goto map_error_handling;
	}

- unmap pages that are already mapped, when mapping error occurs in the middle
  of a multiple page mapping attempt. These example are applicable to
  dma_map_page() as well.

Example 1:
	dma_addr_t dma_handle1;
	dma_addr_t dma_handle2;

	dma_handle1 = dma_map_single(dev, addr, size, direction);
	if (dma_mapping_error(dev, dma_handle1)) {
		/*
		 * reduce current DMA mapping usage,
		 * delay and try again later or
		 * reset driver.
		 */
		goto map_error_handling1;
	}
	dma_handle2 = dma_map_single(dev, addr, size, direction);
	if (dma_mapping_error(dev, dma_handle2)) {
		/*
		 * reduce current DMA mapping usage,
		 * delay and try again later or
		 * reset driver.
		 */
		goto map_error_handling2;
	}

	...

	map_error_handling2:
		dma_unmap_single(dma_handle1);
	map_error_handling1:

Example 2: (if buffers are allocated in a loop, unmap all mapped buffers when
	    mapping error is detected in the middle)

	dma_addr_t dma_addr;
	dma_addr_t array[DMA_BUFFERS];
	int save_index = 0;

	for (i = 0; i < DMA_BUFFERS; i++) {

		...

		dma_addr = dma_map_single(dev, addr, size, direction);
		if (dma_mapping_error(dev, dma_addr)) {
			/*
			 * reduce current DMA mapping usage,
			 * delay and try again later or
			 * reset driver.
			 */
			goto map_error_handling;
		}
		array[i].dma_addr = dma_addr;
		save_index++;
	}

	...

	map_error_handling:

	for (i = 0; i < save_index; i++) {

		...

		dma_unmap_single(array[i].dma_addr);
	}

Networking drivers must call dev_kfree_skb() to free the socket buffer
and return NETDEV_TX_OK if the DMA mapping fails on the transmit hook
(ndo_start_xmit). This means that the socket buffer is just dropped in
the failure case.

SCSI drivers must return SCSI_MLQUEUE_HOST_BUSY if the DMA mapping
fails in the queuecommand hook. This means that the SCSI subsystem
passes the command to the driver again later.

		Optimizing Unmap State Space Consumption

On many platforms, dma_unmap_{single,page}() is simply a nop.
Therefore, keeping track of the mapping address and length is a waste
of space.  Instead of filling your drivers up with ifdefs and the like
to "work around" this (which would defeat the whole purpose of a
portable API) the following facilities are provided.

Actually, instead of describing the macros one by one, we'll
transform some example code.

1) Use DEFINE_DMA_UNMAP_{ADDR,LEN} in state saving structures.
   Example, before:

	struct ring_state {
		struct sk_buff *skb;
		dma_addr_t mapping;
		__u32 len;
	};

   after:

	struct ring_state {
		struct sk_buff *skb;
		DEFINE_DMA_UNMAP_ADDR(mapping);
		DEFINE_DMA_UNMAP_LEN(len);
	};

2) Use dma_unmap_{addr,len}_set() to set these values.
   Example, before:

	ringp->mapping = FOO;
	ringp->len = BAR;

   after:

	dma_unmap_addr_set(ringp, mapping, FOO);
	dma_unmap_len_set(ringp, len, BAR);

3) Use dma_unmap_{addr,len}() to access these values.
   Example, before:

	dma_unmap_single(dev, ringp->mapping, ringp->len,
			 DMA_FROM_DEVICE);

   after:

	dma_unmap_single(dev,
			 dma_unmap_addr(ringp, mapping),
			 dma_unmap_len(ringp, len),
			 DMA_FROM_DEVICE);

It really should be self-explanatory.  We treat the ADDR and LEN
separately, because it is possible for an implementation to only
need the address in order to perform the unmap operation.

			Platform Issues

If you are just writing drivers for Linux and do not maintain
an architecture port for the kernel, you can safely skip down
to "Closing".

1) Struct scatterlist requirements.

   Don't invent the architecture specific struct scatterlist; just use
   <asm-generic/scatterlist.h>. You need to enable
   CONFIG_NEED_SG_DMA_LENGTH if the architecture supports IOMMUs
   (including software IOMMU).

2) ARCH_DMA_MINALIGN

   Architectures must ensure that kmalloc'ed buffer is
   DMA-safe. Drivers and subsystems depend on it. If an architecture
   isn't fully DMA-coherent (i.e. hardware doesn't ensure that data in
   the CPU cache is identical to data in main memory),
   ARCH_DMA_MINALIGN must be set so that the memory allocator
   makes sure that kmalloc'ed buffer doesn't share a cache line with
   the others. See arch/arm/include/asm/cache.h as an example.

   Note that ARCH_DMA_MINALIGN is about DMA memory alignment
   constraints. You don't need to worry about the architecture data
   alignment constraints (e.g. the alignment constraints about 64-bit
   objects).

3) Supporting multiple types of IOMMUs

   If your architecture needs to support multiple types of IOMMUs, you
   can use include/linux/asm-generic/dma-mapping-common.h. It's a
   library to support the DMA API with multiple types of IOMMUs. Lots
   of architectures (x86, powerpc, sh, alpha, ia64, microblaze and
   sparc) use it. Choose one to see how it can be used. If you need to
   support multiple types of IOMMUs in a single system, the example of
   x86 or powerpc helps.

			   Closing

This document, and the API itself, would not be in its current
form without the feedback and suggestions from numerous individuals.
We would like to specifically mention, in no particular order, the
following people:

	Russell King <rmk@arm.linux.org.uk>
	Leo Dagum <dagum@barrel.engr.sgi.com>
	Ralf Baechle <ralf@oss.sgi.com>
	Grant Grundler <grundler@cup.hp.com>
	Jay Estabrook <Jay.Estabrook@compaq.com>
	Thomas Sailer <sailer@ife.ee.ethz.ch>
	Andrea Arcangeli <andrea@suse.de>
	Jens Axboe <jens.axboe@oracle.com>
	David Mosberger-Tang <davidm@hpl.hp.com>
               Dynamic DMA mapping using the generic device
               ============================================

        James E.J. Bottomley <James.Bottomley@HansenPartnership.com>

This document describes the DMA API.  For a more gentle introduction
of the API (and actual examples), see Documentation/DMA-API-HOWTO.txt.

This API is split into two pieces.  Part I describes the basic API.
Part II describes extensions for supporting non-consistent memory
machines.  Unless you know that your driver absolutely has to support
non-consistent platforms (this is usually only legacy platforms) you
should only use the API described in part I.

Part I - dma_ API
-------------------------------------

To get the dma_ API, you must #include <linux/dma-mapping.h>.  This
provides dma_addr_t and the interfaces described below.

A dma_addr_t can hold any valid DMA or bus address for the platform.  It
can be given to a device to use as a DMA source or target.  A CPU cannot
reference a dma_addr_t directly because there may be translation between
its physical address space and the bus address space.

Part Ia - Using large DMA-coherent buffers
------------------------------------------

void *
dma_alloc_coherent(struct device *dev, size_t size,
			     dma_addr_t *dma_handle, gfp_t flag)

Consistent memory is memory for which a write by either the device or
the processor can immediately be read by the processor or device
without having to worry about caching effects.  (You may however need
to make sure to flush the processor's write buffers before telling
devices to read that memory.)

This routine allocates a region of <size> bytes of consistent memory.

It returns a pointer to the allocated region (in the processor's virtual
address space) or NULL if the allocation failed.

It also returns a <dma_handle> which may be cast to an unsigned integer the
same width as the bus and given to the device as the bus address base of
the region.

Note: consistent memory can be expensive on some platforms, and the
minimum allocation length may be as big as a page, so you should
consolidate your requests for consistent memory as much as possible.
The simplest way to do that is to use the dma_pool calls (see below).

The flag parameter (dma_alloc_coherent() only) allows the caller to
specify the GFP_ flags (see kmalloc()) for the allocation (the
implementation may choose to ignore flags that affect the location of
the returned memory, like GFP_DMA).

void *
dma_zalloc_coherent(struct device *dev, size_t size,
			     dma_addr_t *dma_handle, gfp_t flag)

Wraps dma_alloc_coherent() and also zeroes the returned memory if the
allocation attempt succeeded.

void
dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
			   dma_addr_t dma_handle)

Free a region of consistent memory you previously allocated.  dev,
size and dma_handle must all be the same as those passed into
dma_alloc_coherent().  cpu_addr must be the virtual address returned by
the dma_alloc_coherent().

Note that unlike their sibling allocation calls, these routines
may only be called with IRQs enabled.


Part Ib - Using small DMA-coherent buffers
------------------------------------------

To get this part of the dma_ API, you must #include <linux/dmapool.h>

Many drivers need lots of small DMA-coherent memory regions for DMA
descriptors or I/O buffers.  Rather than allocating in units of a page
or more using dma_alloc_coherent(), you can use DMA pools.  These work
much like a struct kmem_cache, except that they use the DMA-coherent allocator,
not __get_free_pages().  Also, they understand common hardware constraints
for alignment, like queue heads needing to be aligned on N-byte boundaries.


	struct dma_pool *
	dma_pool_create(const char *name, struct device *dev,
			size_t size, size_t align, size_t alloc);

dma_pool_create() initializes a pool of DMA-coherent buffers
for use with a given device.  It must be called in a context which
can sleep.

The "name" is for diagnostics (like a struct kmem_cache name); dev and size
are like what you'd pass to dma_alloc_coherent().  The device's hardware
alignment requirement for this type of data is "align" (which is expressed
in bytes, and must be a power of two).  If your device has no boundary
crossing restrictions, pass 0 for alloc; passing 4096 says memory allocated
from this pool must not cross 4KByte boundaries.


	void *dma_pool_alloc(struct dma_pool *pool, gfp_t gfp_flags,
			dma_addr_t *dma_handle);

This allocates memory from the pool; the returned memory will meet the
size and alignment requirements specified at creation time.  Pass
GFP_ATOMIC to prevent blocking, or if it's permitted (not
in_interrupt, not holding SMP locks), pass GFP_KERNEL to allow
blocking.  Like dma_alloc_coherent(), this returns two values:  an
address usable by the CPU, and the DMA address usable by the pool's
device.


	void dma_pool_free(struct dma_pool *pool, void *vaddr,
			dma_addr_t addr);

This puts memory back into the pool.  The pool is what was passed to
dma_pool_alloc(); the CPU (vaddr) and DMA addresses are what
were returned when that routine allocated the memory being freed.


	void dma_pool_destroy(struct dma_pool *pool);

dma_pool_destroy() frees the resources of the pool.  It must be
called in a context which can sleep.  Make sure you've freed all allocated
memory back to the pool before you destroy it.


Part Ic - DMA addressing limitations
------------------------------------

int
dma_supported(struct device *dev, u64 mask)

Checks to see if the device can support DMA to the memory described by
mask.

Returns: 1 if it can and 0 if it can't.

Notes: This routine merely tests to see if the mask is possible.  It
won't change the current mask settings.  It is more intended as an
internal API for use by the platform than an external API for use by
driver writers.

int
dma_set_mask_and_coherent(struct device *dev, u64 mask)

Checks to see if the mask is possible and updates the device
streaming and coherent DMA mask parameters if it is.

Returns: 0 if successful and a negative error if not.

int
dma_set_mask(struct device *dev, u64 mask)

Checks to see if the mask is possible and updates the device
parameters if it is.

Returns: 0 if successful and a negative error if not.

int
dma_set_coherent_mask(struct device *dev, u64 mask)

Checks to see if the mask is possible and updates the device
parameters if it is.

Returns: 0 if successful and a negative error if not.

u64
dma_get_required_mask(struct device *dev)

This API returns the mask that the platform requires to
operate efficiently.  Usually this means the returned mask
is the minimum required to cover all of memory.  Examining the
required mask gives drivers with variable descriptor sizes the
opportunity to use smaller descriptors as necessary.

Requesting the required mask does not alter the current mask.  If you
wish to take advantage of it, you should issue a dma_set_mask()
call to set the mask to the value returned.


Part Id - Streaming DMA mappings
--------------------------------

dma_addr_t
dma_map_single(struct device *dev, void *cpu_addr, size_t size,
		      enum dma_data_direction direction)

Maps a piece of processor virtual memory so it can be accessed by the
device and returns the bus address of the memory.

The direction for both APIs may be converted freely by casting.
However the dma_ API uses a strongly typed enumerator for its
direction:

DMA_NONE		no direction (used for debugging)
DMA_TO_DEVICE		data is going from the memory to the device
DMA_FROM_DEVICE		data is coming from the device to the memory
DMA_BIDIRECTIONAL	direction isn't known

Notes:  Not all memory regions in a machine can be mapped by this API.
Further, contiguous kernel virtual space may not be contiguous as
physical memory.  Since this API does not provide any scatter/gather
capability, it will fail if the user tries to map a non-physically
contiguous piece of memory.  For this reason, memory to be mapped by
this API should be obtained from sources which guarantee it to be
physically contiguous (like kmalloc).

Further, the bus address of the memory must be within the
dma_mask of the device (the dma_mask is a bit mask of the
addressable region for the device, i.e., if the bus address of
the memory ANDed with the dma_mask is still equal to the bus
address, then the device can perform DMA to the memory).  To
ensure that the memory allocated by kmalloc is within the dma_mask,
the driver may specify various platform-dependent flags to restrict
the bus address range of the allocation (e.g., on x86, GFP_DMA
guarantees to be within the first 16MB of available bus addresses,
as required by ISA devices).

Note also that the above constraints on physical contiguity and
dma_mask may not apply if the platform has an IOMMU (a device which
maps an I/O bus address to a physical memory address).  However, to be
portable, device driver writers may *not* assume that such an IOMMU
exists.

Warnings:  Memory coherency operates at a granularity called the cache
line width.  In order for memory mapped by this API to operate
correctly, the mapped region must begin exactly on a cache line
boundary and end exactly on one (to prevent two separately mapped
regions from sharing a single cache line).  Since the cache line size
may not be known at compile time, the API will not enforce this
requirement.  Therefore, it is recommended that driver writers who
don't take special care to determine the cache line size at run time
only map virtual regions that begin and end on page boundaries (which
are guaranteed also to be cache line boundaries).

DMA_TO_DEVICE synchronisation must be done after the last modification
of the memory region by the software and before it is handed off to
the driver.  Once this primitive is used, memory covered by this
primitive should be treated as read-only by the device.  If the device
may write to it at any point, it should be DMA_BIDIRECTIONAL (see
below).

DMA_FROM_DEVICE synchronisation must be done before the driver
accesses data that may be changed by the device.  This memory should
be treated as read-only by the driver.  If the driver needs to write
to it at any point, it should be DMA_BIDIRECTIONAL (see below).

DMA_BIDIRECTIONAL requires special handling: it means that the driver
isn't sure if the memory was modified before being handed off to the
device and also isn't sure if the device will also modify it.  Thus,
you must always sync bidirectional memory twice: once before the
memory is handed off to the device (to make sure all memory changes
are flushed from the processor) and once before the data may be
accessed after being used by the device (to make sure any processor
cache lines are updated with data that the device may have changed).

void
dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
		 enum dma_data_direction direction)

Unmaps the region previously mapped.  All the parameters passed in
must be identical to those passed in (and returned) by the mapping
API.

dma_addr_t
dma_map_page(struct device *dev, struct page *page,
		    unsigned long offset, size_t size,
		    enum dma_data_direction direction)
void
dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
	       enum dma_data_direction direction)

API for mapping and unmapping for pages.  All the notes and warnings
for the other mapping APIs apply here.  Also, although the <offset>
and <size> parameters are provided to do partial page mapping, it is
recommended that you never use these unless you really know what the
cache width is.

int
dma_mapping_error(struct device *dev, dma_addr_t dma_addr)

In some circumstances dma_map_single() and dma_map_page() will fail to create
a mapping. A driver can check for these errors by testing the returned
DMA address with dma_mapping_error(). A non-zero return value means the mapping
could not be created and the driver should take appropriate action (e.g.
reduce current DMA mapping usage or delay and try again later).

	int
	dma_map_sg(struct device *dev, struct scatterlist *sg,
		int nents, enum dma_data_direction direction)

Returns: the number of bus address segments mapped (this may be shorter
than <nents> passed in if some elements of the scatter/gather list are
physically or virtually adjacent and an IOMMU maps them with a single
entry).

Please note that the sg cannot be mapped again if it has been mapped once.
The mapping process is allowed to destroy information in the sg.

As with the other mapping interfaces, dma_map_sg() can fail. When it
does, 0 is returned and a driver must take appropriate action. It is
critical that the driver do something, in the case of a block driver
aborting the request or even oopsing is better than doing nothing and
corrupting the filesystem.

With scatterlists, you use the resulting mapping like this:

	int i, count = dma_map_sg(dev, sglist, nents, direction);
	struct scatterlist *sg;

	for_each_sg(sglist, sg, count, i) {
		hw_address[i] = sg_dma_address(sg);
		hw_len[i] = sg_dma_len(sg);
	}

where nents is the number of entries in the sglist.

The implementation is free to merge several consecutive sglist entries
into one (e.g. with an IOMMU, or if several pages just happen to be
physically contiguous) and returns the actual number of sg entries it
mapped them to. On failure 0, is returned.

Then you should loop count times (note: this can be less than nents times)
and use sg_dma_address() and sg_dma_len() macros where you previously
accessed sg->address and sg->length as shown above.

	void
	dma_unmap_sg(struct device *dev, struct scatterlist *sg,
		int nhwentries, enum dma_data_direction direction)

Unmap the previously mapped scatter/gather list.  All the parameters
must be the same as those and passed in to the scatter/gather mapping
API.

Note: <nents> must be the number you passed in, *not* the number of
bus address entries returned.

void
dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
			enum dma_data_direction direction)
void
dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle, size_t size,
			   enum dma_data_direction direction)
void
dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
		    enum dma_data_direction direction)
void
dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
		       enum dma_data_direction direction)

Synchronise a single contiguous or scatter/gather mapping for the CPU
and device. With the sync_sg API, all the parameters must be the same
as those passed into the single mapping API. With the sync_single API,
you can use dma_handle and size parameters that aren't identical to
those passed into the single mapping API to do a partial sync.

Notes:  You must do this:

- Before reading values that have been written by DMA from the device
  (use the DMA_FROM_DEVICE direction)
- After writing values that will be written to the device using DMA
  (use the DMA_TO_DEVICE) direction
- before *and* after handing memory to the device if the memory is
  DMA_BIDIRECTIONAL

See also dma_map_single().

dma_addr_t
dma_map_single_attrs(struct device *dev, void *cpu_addr, size_t size,
		     enum dma_data_direction dir,
		     struct dma_attrs *attrs)

void
dma_unmap_single_attrs(struct device *dev, dma_addr_t dma_addr,
		       size_t size, enum dma_data_direction dir,
		       struct dma_attrs *attrs)

int
dma_map_sg_attrs(struct device *dev, struct scatterlist *sgl,
		 int nents, enum dma_data_direction dir,
		 struct dma_attrs *attrs)

void
dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sgl,
		   int nents, enum dma_data_direction dir,
		   struct dma_attrs *attrs)

The four functions above are just like the counterpart functions
without the _attrs suffixes, except that they pass an optional
struct dma_attrs*.

struct dma_attrs encapsulates a set of "DMA attributes". For the
definition of struct dma_attrs see linux/dma-attrs.h.

The interpretation of DMA attributes is architecture-specific, and
each attribute should be documented in Documentation/DMA-attributes.txt.

If struct dma_attrs* is NULL, the semantics of each of these
functions is identical to those of the corresponding function
without the _attrs suffix. As a result dma_map_single_attrs()
can generally replace dma_map_single(), etc.

As an example of the use of the *_attrs functions, here's how
you could pass an attribute DMA_ATTR_FOO when mapping memory
for DMA:

#include <linux/dma-attrs.h>
/* DMA_ATTR_FOO should be defined in linux/dma-attrs.h and
 * documented in Documentation/DMA-attributes.txt */
...

	DEFINE_DMA_ATTRS(attrs);
	dma_set_attr(DMA_ATTR_FOO, &attrs);
	....
	n = dma_map_sg_attrs(dev, sg, nents, DMA_TO_DEVICE, &attr);
	....

Architectures that care about DMA_ATTR_FOO would check for its
presence in their implementations of the mapping and unmapping
routines, e.g.:

void whizco_dma_map_sg_attrs(struct device *dev, dma_addr_t dma_addr,
			     size_t size, enum dma_data_direction dir,
			     struct dma_attrs *attrs)
{
	....
	int foo =  dma_get_attr(DMA_ATTR_FOO, attrs);
	....
	if (foo)
		/* twizzle the frobnozzle */
	....


Part II - Advanced dma_ usage
-----------------------------

Warning: These pieces of the DMA API should not be used in the
majority of cases, since they cater for unlikely corner cases that
don't belong in usual drivers.

If you don't understand how cache line coherency works between a
processor and an I/O device, you should not be using this part of the
API at all.

void *
dma_alloc_noncoherent(struct device *dev, size_t size,
			       dma_addr_t *dma_handle, gfp_t flag)

Identical to dma_alloc_coherent() except that the platform will
choose to return either consistent or non-consistent memory as it sees
fit.  By using this API, you are guaranteeing to the platform that you
have all the correct and necessary sync points for this memory in the
driver should it choose to return non-consistent memory.

Note: where the platform can return consistent memory, it will
guarantee that the sync points become nops.

Warning:  Handling non-consistent memory is a real pain.  You should
only use this API if you positively know your driver will be
required to work on one of the rare (usually non-PCI) architectures
that simply cannot make consistent memory.

void
dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr,
			      dma_addr_t dma_handle)

Free memory allocated by the nonconsistent API.  All parameters must
be identical to those passed in (and returned by
dma_alloc_noncoherent()).

int
dma_get_cache_alignment(void)

Returns the processor cache alignment.  This is the absolute minimum
alignment *and* width that you must observe when either mapping
memory or doing partial flushes.

Notes: This API may return a number *larger* than the actual cache
line, but it will guarantee that one or more cache lines fit exactly
into the width returned by this call.  It will also always be a power
of two for easy alignment.

void
dma_cache_sync(struct device *dev, void *vaddr, size_t size,
	       enum dma_data_direction direction)

Do a partial sync of memory that was allocated by
dma_alloc_noncoherent(), starting at virtual address vaddr and
continuing on for size.  Again, you *must* observe the cache line
boundaries when doing this.

int
dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
			    dma_addr_t device_addr, size_t size, int
			    flags)

Declare region of memory to be handed out by dma_alloc_coherent() when
it's asked for coherent memory for this device.

phys_addr is the CPU physical address to which the memory is currently
assigned (this will be ioremapped so the CPU can access the region).

device_addr is the bus address the device needs to be programmed
with to actually address this memory (this will be handed out as the
dma_addr_t in dma_alloc_coherent()).

size is the size of the area (must be multiples of PAGE_SIZE).

flags can be ORed together and are:

DMA_MEMORY_MAP - request that the memory returned from
dma_alloc_coherent() be directly writable.

DMA_MEMORY_IO - request that the memory returned from
dma_alloc_coherent() be addressable using read()/write()/memcpy_toio() etc.

One or both of these flags must be present.

DMA_MEMORY_INCLUDES_CHILDREN - make the declared memory be allocated by
dma_alloc_coherent of any child devices of this one (for memory residing
on a bridge).

DMA_MEMORY_EXCLUSIVE - only allocate memory from the declared regions. 
Do not allow dma_alloc_coherent() to fall back to system memory when
it's out of memory in the declared region.

The return value will be either DMA_MEMORY_MAP or DMA_MEMORY_IO and
must correspond to a passed in flag (i.e. no returning DMA_MEMORY_IO
if only DMA_MEMORY_MAP were passed in) for success or zero for
failure.

Note, for DMA_MEMORY_IO returns, all subsequent memory returned by
dma_alloc_coherent() may no longer be accessed directly, but instead
must be accessed using the correct bus functions.  If your driver
isn't prepared to handle this contingency, it should not specify
DMA_MEMORY_IO in the input flags.

As a simplification for the platforms, only *one* such region of
memory may be declared per device.

For reasons of efficiency, most platforms choose to track the declared
region only at the granularity of a page.  For smaller allocations,
you should use the dma_pool() API.

void
dma_release_declared_memory(struct device *dev)

Remove the memory region previously declared from the system.  This
API performs *no* in-use checking for this region and will return
unconditionally having removed all the required structures.  It is the
driver's job to ensure that no parts of this memory region are
currently in use.

void *
dma_mark_declared_memory_occupied(struct device *dev,
				  dma_addr_t device_addr, size_t size)

This is used to occupy specific regions of the declared space
(dma_alloc_coherent() will hand out the first free region it finds).

device_addr is the *device* address of the region requested.

size is the size (and should be a page-sized multiple).

The return value will be either a pointer to the processor virtual
address of the memory, or an error (via PTR_ERR()) if any part of the
region is occupied.

Part III - Debug drivers use of the DMA-API
-------------------------------------------

The DMA-API as described above has some constraints. DMA addresses must be
released with the corresponding function with the same size for example. With
the advent of hardware IOMMUs it becomes more and more important that drivers
do not violate those constraints. In the worst case such a violation can
result in data corruption up to destroyed filesystems.

To debug drivers and find bugs in the usage of the DMA-API checking code can
be compiled into the kernel which will tell the developer about those
violations. If your architecture supports it you can select the "Enable
debugging of DMA-API usage" option in your kernel configuration. Enabling this
option has a performance impact. Do not enable it in production kernels.

If you boot the resulting kernel will contain code which does some bookkeeping
about what DMA memory was allocated for which device. If this code detects an
error it prints a warning message with some details into your kernel log. An
example warning message may look like this:

------------[ cut here ]------------
WARNING: at /data2/repos/linux-2.6-iommu/lib/dma-debug.c:448
	check_unmap+0x203/0x490()
Hardware name:
forcedeth 0000:00:08.0: DMA-API: device driver frees DMA memory with wrong
	function [device address=0x00000000640444be] [size=66 bytes] [mapped as
single] [unmapped as page]
Modules linked in: nfsd exportfs bridge stp llc r8169
Pid: 0, comm: swapper Tainted: G        W  2.6.28-dmatest-09289-g8bb99c0 #1
Call Trace:
 <IRQ>  [<ffffffff80240b22>] warn_slowpath+0xf2/0x130
 [<ffffffff80647b70>] _spin_unlock+0x10/0x30
 [<ffffffff80537e75>] usb_hcd_link_urb_to_ep+0x75/0xc0
 [<ffffffff80647c22>] _spin_unlock_irqrestore+0x12/0x40
 [<ffffffff8055347f>] ohci_urb_enqueue+0x19f/0x7c0
 [<ffffffff80252f96>] queue_work+0x56/0x60
 [<ffffffff80237e10>] enqueue_task_fair+0x20/0x50
 [<ffffffff80539279>] usb_hcd_submit_urb+0x379/0xbc0
 [<ffffffff803b78c3>] cpumask_next_and+0x23/0x40
 [<ffffffff80235177>] find_busiest_group+0x207/0x8a0
 [<ffffffff8064784f>] _spin_lock_irqsave+0x1f/0x50
 [<ffffffff803c7ea3>] check_unmap+0x203/0x490
 [<ffffffff803c8259>] debug_dma_unmap_page+0x49/0x50
 [<ffffffff80485f26>] nv_tx_done_optimized+0xc6/0x2c0
 [<ffffffff80486c13>] nv_nic_irq_optimized+0x73/0x2b0
 [<ffffffff8026df84>] handle_IRQ_event+0x34/0x70
 [<ffffffff8026ffe9>] handle_edge_irq+0xc9/0x150
 [<ffffffff8020e3ab>] do_IRQ+0xcb/0x1c0
 [<ffffffff8020c093>] ret_from_intr+0x0/0xa
 <EOI> <4>---[ end trace f6435a98e2a38c0e ]---

The driver developer can find the driver and the device including a stacktrace
of the DMA-API call which caused this warning.

Per default only the first error will result in a warning message. All other
errors will only silently counted. This limitation exist to prevent the code
from flooding your kernel log. To support debugging a device driver this can
be disabled via debugfs. See the debugfs interface documentation below for
details.

The debugfs directory for the DMA-API debugging code is called dma-api/. In
this directory the following files can currently be found:

	dma-api/all_errors	This file contains a numeric value. If this
				value is not equal to zero the debugging code
				will print a warning for every error it finds
				into the kernel log. Be careful with this
				option, as it can easily flood your logs.

	dma-api/disabled	This read-only file contains the character 'Y'
				if the debugging code is disabled. This can
				happen when it runs out of memory or if it was
				disabled at boot time

	dma-api/error_count	This file is read-only and shows the total
				numbers of errors found.

	dma-api/num_errors	The number in this file shows how many
				warnings will be printed to the kernel log
				before it stops. This number is initialized to
				one at system boot and be set by writing into
				this file

	dma-api/min_free_entries
				This read-only file can be read to get the
				minimum number of free dma_debug_entries the
				allocator has ever seen. If this value goes
				down to zero the code will disable itself
				because it is not longer reliable.

	dma-api/num_free_entries
				The current number of free dma_debug_entries
				in the allocator.

	dma-api/driver-filter
				You can write a name of a driver into this file
				to limit the debug output to requests from that
				particular driver. Write an empty string to
				that file to disable the filter and see
				all errors again.

If you have this code compiled into your kernel it will be enabled by default.
If you want to boot without the bookkeeping anyway you can provide
'dma_debug=off' as a boot parameter. This will disable DMA-API debugging.
Notice that you can not enable it again at runtime. You have to reboot to do
so.

If you want to see debug messages only for a special device driver you can
specify the dma_debug_driver=<drivername> parameter. This will enable the
driver filter at boot time. The debug code will only print errors for that
driver afterwards. This filter can be disabled or changed later using debugfs.

When the code disables itself at runtime this is most likely because it ran
out of dma_debug_entries. These entries are preallocated at boot. The number
of preallocated entries is defined per architecture. If it is too low for you
boot with 'dma_debug_entries=<your_desired_number>' to overwrite the
architectural default.

void debug_dmap_mapping_error(struct device *dev, dma_addr_t dma_addr);

dma-debug interface debug_dma_mapping_error() to debug drivers that fail
to check DMA mapping errors on addresses returned by dma_map_single() and
dma_map_page() interfaces. This interface clears a flag set by
debug_dma_map_page() to indicate that dma_mapping_error() has been called by
the driver. When driver does unmap, debug_dma_unmap() checks the flag and if
this flag is still set, prints warning message that includes call trace that
leads up to the unmap. This interface can be called from dma_mapping_error()
routines to enable DMA mapping error check debugging.

			DMA attributes
			==============

This document describes the semantics of the DMA attributes that are
defined in linux/dma-attrs.h.

DMA_ATTR_WRITE_BARRIER
----------------------

DMA_ATTR_WRITE_BARRIER is a (write) barrier attribute for DMA.  DMA
to a memory region with the DMA_ATTR_WRITE_BARRIER attribute forces
all pending DMA writes to complete, and thus provides a mechanism to
strictly order DMA from a device across all intervening busses and
bridges.  This barrier is not specific to a particular type of
interconnect, it applies to the system as a whole, and so its
implementation must account for the idiosyncrasies of the system all
the way from the DMA device to memory.

As an example of a situation where DMA_ATTR_WRITE_BARRIER would be
useful, suppose that a device does a DMA write to indicate that data is
ready and available in memory.  The DMA of the "completion indication"
could race with data DMA.  Mapping the memory used for completion
indications with DMA_ATTR_WRITE_BARRIER would prevent the race.

DMA_ATTR_WEAK_ORDERING
----------------------

DMA_ATTR_WEAK_ORDERING specifies that reads and writes to the mapping
may be weakly ordered, that is that reads and writes may pass each other.

Since it is optional for platforms to implement DMA_ATTR_WEAK_ORDERING,
those that do not will simply ignore the attribute and exhibit default
behavior.

DMA_ATTR_WRITE_COMBINE
----------------------

DMA_ATTR_WRITE_COMBINE specifies that writes to the mapping may be
buffered to improve performance.

Since it is optional for platforms to implement DMA_ATTR_WRITE_COMBINE,
those that do not will simply ignore the attribute and exhibit default
behavior.

DMA_ATTR_NON_CONSISTENT
-----------------------

DMA_ATTR_NON_CONSISTENT lets the platform to choose to return either
consistent or non-consistent memory as it sees fit.  By using this API,
you are guaranteeing to the platform that you have all the correct and
necessary sync points for this memory in the driver.

DMA_ATTR_NO_KERNEL_MAPPING
--------------------------

DMA_ATTR_NO_KERNEL_MAPPING lets the platform to avoid creating a kernel
virtual mapping for the allocated buffer. On some architectures creating
such mapping is non-trivial task and consumes very limited resources
(like kernel virtual address space or dma consistent address space).
Buffers allocated with this attribute can be only passed to user space
by calling dma_mmap_attrs(). By using this API, you are guaranteeing
that you won't dereference the pointer returned by dma_alloc_attr(). You
can treat it as a cookie that must be passed to dma_mmap_attrs() and
dma_free_attrs(). Make sure that both of these also get this attribute
set on each call.

Since it is optional for platforms to implement
DMA_ATTR_NO_KERNEL_MAPPING, those that do not will simply ignore the
attribute and exhibit default behavior.

DMA_ATTR_SKIP_CPU_SYNC
----------------------

By default dma_map_{single,page,sg} functions family transfer a given
buffer from CPU domain to device domain. Some advanced use cases might
require sharing a buffer between more than one device. This requires
having a mapping created separately for each device and is usually
performed by calling dma_map_{single,page,sg} function more than once
for the given buffer with device pointer to each device taking part in
the buffer sharing. The first call transfers a buffer from 'CPU' domain
to 'device' domain, what synchronizes CPU caches for the given region
(usually it means that the cache has been flushed or invalidated
depending on the dma direction). However, next calls to
dma_map_{single,page,sg}() for other devices will perform exactly the
same synchronization operation on the CPU cache. CPU cache synchronization
might be a time consuming operation, especially if the buffers are
large, so it is highly recommended to avoid it if possible.
DMA_ATTR_SKIP_CPU_SYNC allows platform code to skip synchronization of
the CPU cache for the given buffer assuming that it has been already
transferred to 'device' domain. This attribute can be also used for
dma_unmap_{single,page,sg} functions family to force buffer to stay in
device domain after releasing a mapping for it. Use this attribute with
care!

DMA_ATTR_FORCE_CONTIGUOUS
-------------------------

By default DMA-mapping subsystem is allowed to assemble the buffer
allocated by dma_alloc_attrs() function from individual pages if it can
be mapped as contiguous chunk into device dma address space. By
specifying this attribute the allocated buffer is forced to be contiguous
also in physical memory.
                    DMA Buffer Sharing API Guide
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~

                            Sumit Semwal
                <sumit dot semwal at linaro dot org>
                 <sumit dot semwal at ti dot com>

This document serves as a guide to device-driver writers on what is the dma-buf
buffer sharing API, how to use it for exporting and using shared buffers.

Any device driver which wishes to be a part of DMA buffer sharing, can do so as
either the 'exporter' of buffers, or the 'user' of buffers.

Say a driver A wants to use buffers created by driver B, then we call B as the
exporter, and A as buffer-user.

The exporter
- implements and manages operations[1] for the buffer
- allows other users to share the buffer by using dma_buf sharing APIs,
- manages the details of buffer allocation,
- decides about the actual backing storage where this allocation happens,
- takes care of any migration of scatterlist - for all (shared) users of this
   buffer,

The buffer-user
- is one of (many) sharing users of the buffer.
- doesn't need to worry about how the buffer is allocated, or where.
- needs a mechanism to get access to the scatterlist that makes up this buffer
   in memory, mapped into its own address space, so it can access the same area
   of memory.

dma-buf operations for device dma only
--------------------------------------

The dma_buf buffer sharing API usage contains the following steps:

1. Exporter announces that it wishes to export a buffer
2. Userspace gets the file descriptor associated with the exported buffer, and
   passes it around to potential buffer-users based on use case
3. Each buffer-user 'connects' itself to the buffer
4. When needed, buffer-user requests access to the buffer from exporter
5. When finished with its use, the buffer-user notifies end-of-DMA to exporter
6. when buffer-user is done using this buffer completely, it 'disconnects'
   itself from the buffer.


1. Exporter's announcement of buffer export

   The buffer exporter announces its wish to export a buffer. In this, it
   connects its own private buffer data, provides implementation for operations
   that can be performed on the exported dma_buf, and flags for the file
   associated with this buffer.

   Interface:
      struct dma_buf *dma_buf_export_named(void *priv, struct dma_buf_ops *ops,
				     size_t size, int flags,
				     const char *exp_name)

   If this succeeds, dma_buf_export allocates a dma_buf structure, and returns a
   pointer to the same. It also associates an anonymous file with this buffer,
   so it can be exported. On failure to allocate the dma_buf object, it returns
   NULL.

   'exp_name' is the name of exporter - to facilitate information while
   debugging.

   Exporting modules which do not wish to provide any specific name may use the
   helper define 'dma_buf_export()', with the same arguments as above, but
   without the last argument; a KBUILD_MODNAME pre-processor directive will be
   inserted in place of 'exp_name' instead.

2. Userspace gets a handle to pass around to potential buffer-users

   Userspace entity requests for a file-descriptor (fd) which is a handle to the
   anonymous file associated with the buffer. It can then share the fd with other
   drivers and/or processes.

   Interface:
      int dma_buf_fd(struct dma_buf *dmabuf)

   This API installs an fd for the anonymous file associated with this buffer;
   returns either 'fd', or error.

3. Each buffer-user 'connects' itself to the buffer

   Each buffer-user now gets a reference to the buffer, using the fd passed to
   it.

   Interface:
      struct dma_buf *dma_buf_get(int fd)

   This API will return a reference to the dma_buf, and increment refcount for
   it.

   After this, the buffer-user needs to attach its device with the buffer, which
   helps the exporter to know of device buffer constraints.

   Interface:
      struct dma_buf_attachment *dma_buf_attach(struct dma_buf *dmabuf,
                                                struct device *dev)

   This API returns reference to an attachment structure, which is then used
   for scatterlist operations. It will optionally call the 'attach' dma_buf
   operation, if provided by the exporter.

   The dma-buf sharing framework does the bookkeeping bits related to managing
   the list of all attachments to a buffer.

Until this stage, the buffer-exporter has the option to choose not to actually
allocate the backing storage for this buffer, but wait for the first buffer-user
to request use of buffer for allocation.


4. When needed, buffer-user requests access to the buffer

   Whenever a buffer-user wants to use the buffer for any DMA, it asks for
   access to the buffer using dma_buf_map_attachment API. At least one attach to
   the buffer must have happened before map_dma_buf can be called.

   Interface:
      struct sg_table * dma_buf_map_attachment(struct dma_buf_attachment *,
                                         enum dma_data_direction);

   This is a wrapper to dma_buf->ops->map_dma_buf operation, which hides the
   "dma_buf->ops->" indirection from the users of this interface.

   In struct dma_buf_ops, map_dma_buf is defined as
      struct sg_table * (*map_dma_buf)(struct dma_buf_attachment *,
                                                enum dma_data_direction);

   It is one of the buffer operations that must be implemented by the exporter.
   It should return the sg_table containing scatterlist for this buffer, mapped
   into caller's address space.

   If this is being called for the first time, the exporter can now choose to
   scan through the list of attachments for this buffer, collate the requirements
   of the attached devices, and choose an appropriate backing storage for the
   buffer.

   Based on enum dma_data_direction, it might be possible to have multiple users
   accessing at the same time (for reading, maybe), or any other kind of sharing
   that the exporter might wish to make available to buffer-users.

   map_dma_buf() operation can return -EINTR if it is interrupted by a signal.


5. When finished, the buffer-user notifies end-of-DMA to exporter

   Once the DMA for the current buffer-user is over, it signals 'end-of-DMA' to
   the exporter using the dma_buf_unmap_attachment API.

   Interface:
      void dma_buf_unmap_attachment(struct dma_buf_attachment *,
                                    struct sg_table *);

   This is a wrapper to dma_buf->ops->unmap_dma_buf() operation, which hides the
   "dma_buf->ops->" indirection from the users of this interface.

   In struct dma_buf_ops, unmap_dma_buf is defined as
      void (*unmap_dma_buf)(struct dma_buf_attachment *, struct sg_table *);

   unmap_dma_buf signifies the end-of-DMA for the attachment provided. Like
   map_dma_buf, this API also must be implemented by the exporter.


6. when buffer-user is done using this buffer, it 'disconnects' itself from the
   buffer.

   After the buffer-user has no more interest in using this buffer, it should
   disconnect itself from the buffer:

   - it first detaches itself from the buffer.

   Interface:
      void dma_buf_detach(struct dma_buf *dmabuf,
                          struct dma_buf_attachment *dmabuf_attach);

   This API removes the attachment from the list in dmabuf, and optionally calls
   dma_buf->ops->detach(), if provided by exporter, for any housekeeping bits.

   - Then, the buffer-user returns the buffer reference to exporter.

   Interface:
     void dma_buf_put(struct dma_buf *dmabuf);

   This API then reduces the refcount for this buffer.

   If, as a result of this call, the refcount becomes 0, the 'release' file
   operation related to this fd is called. It calls the dmabuf->ops->release()
   operation in turn, and frees the memory allocated for dmabuf when exported.

NOTES:
- Importance of attach-detach and {map,unmap}_dma_buf operation pairs
   The attach-detach calls allow the exporter to figure out backing-storage
   constraints for the currently-interested devices. This allows preferential
   allocation, and/or migration of pages across different types of storage
   available, if possible.

   Bracketing of DMA access with {map,unmap}_dma_buf operations is essential
   to allow just-in-time backing of storage, and migration mid-way through a
   use-case.

- Migration of backing storage if needed
   If after
   - at least one map_dma_buf has happened,
   - and the backing storage has been allocated for this buffer,
   another new buffer-user intends to attach itself to this buffer, it might
   be allowed, if possible for the exporter.

   In case it is allowed by the exporter:
    if the new buffer-user has stricter 'backing-storage constraints', and the
    exporter can handle these constraints, the exporter can just stall on the
    map_dma_buf until all outstanding access is completed (as signalled by
    unmap_dma_buf).
    Once all users have finished accessing and have unmapped this buffer, the
    exporter could potentially move the buffer to the stricter backing-storage,
    and then allow further {map,unmap}_dma_buf operations from any buffer-user
    from the migrated backing-storage.

   If the exporter cannot fulfill the backing-storage constraints of the new
   buffer-user device as requested, dma_buf_attach() would return an error to
   denote non-compatibility of the new buffer-sharing request with the current
   buffer.

   If the exporter chooses not to allow an attach() operation once a
   map_dma_buf() API has been called, it simply returns an error.

Kernel cpu access to a dma-buf buffer object
--------------------------------------------

The motivation to allow cpu access from the kernel to a dma-buf object from the
importers side are:
- fallback operations, e.g. if the devices is connected to a usb bus and the
  kernel needs to shuffle the data around first before sending it away.
- full transparency for existing users on the importer side, i.e. userspace
  should not notice the difference between a normal object from that subsystem
  and an imported one backed by a dma-buf. This is really important for drm
  opengl drivers that expect to still use all the existing upload/download
  paths.

Access to a dma_buf from the kernel context involves three steps:

1. Prepare access, which invalidate any necessary caches and make the object
   available for cpu access.
2. Access the object page-by-page with the dma_buf map apis
3. Finish access, which will flush any necessary cpu caches and free reserved
   resources.

1. Prepare access

   Before an importer can access a dma_buf object with the cpu from the kernel
   context, it needs to notify the exporter of the access that is about to
   happen.

   Interface:
      int dma_buf_begin_cpu_access(struct dma_buf *dmabuf,
				   size_t start, size_t len,
				   enum dma_data_direction direction)

   This allows the exporter to ensure that the memory is actually available for
   cpu access - the exporter might need to allocate or swap-in and pin the
   backing storage. The exporter also needs to ensure that cpu access is
   coherent for the given range and access direction. The range and access
   direction can be used by the exporter to optimize the cache flushing, i.e.
   access outside of the range or with a different direction (read instead of
   write) might return stale or even bogus data (e.g. when the exporter needs to
   copy the data to temporary storage).

   This step might fail, e.g. in oom conditions.

2. Accessing the buffer

   To support dma_buf objects residing in highmem cpu access is page-based using
   an api similar to kmap. Accessing a dma_buf is done in aligned chunks of
   PAGE_SIZE size. Before accessing a chunk it needs to be mapped, which returns
   a pointer in kernel virtual address space. Afterwards the chunk needs to be
   unmapped again. There is no limit on how often a given chunk can be mapped
   and unmapped, i.e. the importer does not need to call begin_cpu_access again
   before mapping the same chunk again.

   Interfaces:
      void *dma_buf_kmap(struct dma_buf *, unsigned long);
      void dma_buf_kunmap(struct dma_buf *, unsigned long, void *);

   There are also atomic variants of these interfaces. Like for kmap they
   facilitate non-blocking fast-paths. Neither the importer nor the exporter (in
   the callback) is allowed to block when using these.

   Interfaces:
      void *dma_buf_kmap_atomic(struct dma_buf *, unsigned long);
      void dma_buf_kunmap_atomic(struct dma_buf *, unsigned long, void *);

   For importers all the restrictions of using kmap apply, like the limited
   supply of kmap_atomic slots. Hence an importer shall only hold onto at most 2
   atomic dma_buf kmaps at the same time (in any given process context).

   dma_buf kmap calls outside of the range specified in begin_cpu_access are
   undefined. If the range is not PAGE_SIZE aligned, kmap needs to succeed on
   the partial chunks at the beginning and end but may return stale or bogus
   data outside of the range (in these partial chunks).

   Note that these calls need to always succeed. The exporter needs to complete
   any preparations that might fail in begin_cpu_access.

   For some cases the overhead of kmap can be too high, a vmap interface
   is introduced. This interface should be used very carefully, as vmalloc
   space is a limited resources on many architectures.

   Interfaces:
      void *dma_buf_vmap(struct dma_buf *dmabuf)
      void dma_buf_vunmap(struct dma_buf *dmabuf, void *vaddr)

   The vmap call can fail if there is no vmap support in the exporter, or if it
   runs out of vmalloc space. Fallback to kmap should be implemented. Note that
   the dma-buf layer keeps a reference count for all vmap access and calls down
   into the exporter's vmap function only when no vmapping exists, and only
   unmaps it once. Protection against concurrent vmap/vunmap calls is provided
   by taking the dma_buf->lock mutex.

3. Finish access

   When the importer is done accessing the range specified in begin_cpu_access,
   it needs to announce this to the exporter (to facilitate cache flushing and
   unpinning of any pinned resources). The result of any dma_buf kmap calls
   after end_cpu_access is undefined.

   Interface:
      void dma_buf_end_cpu_access(struct dma_buf *dma_buf,
				  size_t start, size_t len,
				  enum dma_data_direction dir);


Direct Userspace Access/mmap Support
------------------------------------

Being able to mmap an export dma-buf buffer object has 2 main use-cases:
- CPU fallback processing in a pipeline and
- supporting existing mmap interfaces in importers.

1. CPU fallback processing in a pipeline

   In many processing pipelines it is sometimes required that the cpu can access
   the data in a dma-buf (e.g. for thumbnail creation, snapshots, ...). To avoid
   the need to handle this specially in userspace frameworks for buffer sharing
   it's ideal if the dma_buf fd itself can be used to access the backing storage
   from userspace using mmap.

   Furthermore Android's ION framework already supports this (and is otherwise
   rather similar to dma-buf from a userspace consumer side with using fds as
   handles, too). So it's beneficial to support this in a similar fashion on
   dma-buf to have a good transition path for existing Android userspace.

   No special interfaces, userspace simply calls mmap on the dma-buf fd.

2. Supporting existing mmap interfaces in importers

   Similar to the motivation for kernel cpu access it is again important that
   the userspace code of a given importing subsystem can use the same interfaces
   with a imported dma-buf buffer object as with a native buffer object. This is
   especially important for drm where the userspace part of contemporary OpenGL,
   X, and other drivers is huge, and reworking them to use a different way to
   mmap a buffer rather invasive.

   The assumption in the current dma-buf interfaces is that redirecting the
   initial mmap is all that's needed. A survey of some of the existing
   subsystems shows that no driver seems to do any nefarious thing like syncing
   up with outstanding asynchronous processing on the device or allocating
   special resources at fault time. So hopefully this is good enough, since
   adding interfaces to intercept pagefaults and allow pte shootdowns would
   increase the complexity quite a bit.

   Interface:
      int dma_buf_mmap(struct dma_buf *, struct vm_area_struct *,
		       unsigned long);

   If the importing subsystem simply provides a special-purpose mmap call to set
   up a mapping in userspace, calling do_mmap with dma_buf->file will equally
   achieve that for a dma-buf object.

3. Implementation notes for exporters

   Because dma-buf buffers have invariant size over their lifetime, the dma-buf
   core checks whether a vma is too large and rejects such mappings. The
   exporter hence does not need to duplicate this check.

   Because existing importing subsystems might presume coherent mappings for
   userspace, the exporter needs to set up a coherent mapping. If that's not
   possible, it needs to fake coherency by manually shooting down ptes when
   leaving the cpu domain and flushing caches at fault time. Note that all the
   dma_buf files share the same anon inode, hence the exporter needs to replace
   the dma_buf file stored in vma->vm_file with it's own if pte shootdown is
   required. This is because the kernel uses the underlying inode's address_space
   for vma tracking (and hence pte tracking at shootdown time with
   unmap_mapping_range).

   If the above shootdown dance turns out to be too expensive in certain
   scenarios, we can extend dma-buf with a more explicit cache tracking scheme
   for userspace mappings. But the current assumption is that using mmap is
   always a slower path, so some inefficiencies should be acceptable.

   Exporters that shoot down mappings (for any reasons) shall not do any
   synchronization at fault time with outstanding device operations.
   Synchronization is an orthogonal issue to sharing the backing storage of a
   buffer and hence should not be handled by dma-buf itself. This is explicitly
   mentioned here because many people seem to want something like this, but if
   different exporters handle this differently, buffer sharing can fail in
   interesting ways depending upong the exporter (if userspace starts depending
   upon this implicit synchronization).

Other Interfaces Exposed to Userspace on the dma-buf FD
------------------------------------------------------

- Since kernel 3.12 the dma-buf FD supports the llseek system call, but only
  with offset=0 and whence=SEEK_END|SEEK_SET. SEEK_SET is supported to allow
  the usual size discover pattern size = SEEK_END(0); SEEK_SET(0). Every other
  llseek operation will report -EINVAL.

  If llseek on dma-buf FDs isn't support the kernel will report -ESPIPE for all
  cases. Userspace can use this to detect support for discovering the dma-buf
  size using llseek.

Miscellaneous notes
-------------------

- Any exporters or users of the dma-buf buffer sharing framework must have
  a 'select DMA_SHARED_BUFFER' in their respective Kconfigs.

- In order to avoid fd leaks on exec, the FD_CLOEXEC flag must be set
  on the file descriptor.  This is not just a resource leak, but a
  potential security hole.  It could give the newly exec'd application
  access to buffers, via the leaked fd, to which it should otherwise
  not be permitted access.

  The problem with doing this via a separate fcntl() call, versus doing it
  atomically when the fd is created, is that this is inherently racy in a
  multi-threaded app[3].  The issue is made worse when it is library code
  opening/creating the file descriptor, as the application may not even be
  aware of the fd's.

  To avoid this problem, userspace must have a way to request O_CLOEXEC
  flag be set when the dma-buf fd is created.  So any API provided by
  the exporting driver to create a dmabuf fd must provide a way to let
  userspace control setting of O_CLOEXEC flag passed in to dma_buf_fd().

- If an exporter needs to manually flush caches and hence needs to fake
  coherency for mmap support, it needs to be able to zap all the ptes pointing
  at the backing storage. Now linux mm needs a struct address_space associated
  with the struct file stored in vma->vm_file to do that with the function
  unmap_mapping_range. But the dma_buf framework only backs every dma_buf fd
  with the anon_file struct file, i.e. all dma_bufs share the same file.

  Hence exporters need to setup their own file (and address_space) association
  by setting vma->vm_file and adjusting vma->vm_pgoff in the dma_buf mmap
  callback. In the specific case of a gem driver the exporter could use the
  shmem file already provided by gem (and set vm_pgoff = 0). Exporters can then
  zap ptes by unmapping the corresponding range of the struct address_space
  associated with their own file.

References:
[1] struct dma_buf_ops in include/linux/dma-buf.h
[2] All interfaces mentioned above defined in include/linux/dma-buf.h
[3] https://lwn.net/Articles/236486/
			DMA Engine API Guide
			====================

		 Vinod Koul <vinod dot koul at intel.com>

NOTE: For DMA Engine usage in async_tx please see:
	Documentation/crypto/async-tx-api.txt


Below is a guide to device driver writers on how to use the Slave-DMA API of the
DMA Engine. This is applicable only for slave DMA usage only.

The slave DMA usage consists of following steps:
1. Allocate a DMA slave channel
2. Set slave and controller specific parameters
3. Get a descriptor for transaction
4. Submit the transaction
5. Issue pending requests and wait for callback notification

1. Allocate a DMA slave channel

   Channel allocation is slightly different in the slave DMA context,
   client drivers typically need a channel from a particular DMA
   controller only and even in some cases a specific channel is desired.
   To request a channel dma_request_channel() API is used.

   Interface:
	struct dma_chan *dma_request_channel(dma_cap_mask_t mask,
			dma_filter_fn filter_fn,
			void *filter_param);
   where dma_filter_fn is defined as:
	typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);

   The 'filter_fn' parameter is optional, but highly recommended for
   slave and cyclic channels as they typically need to obtain a specific
   DMA channel.

   When the optional 'filter_fn' parameter is NULL, dma_request_channel()
   simply returns the first channel that satisfies the capability mask.

   Otherwise, the 'filter_fn' routine will be called once for each free
   channel which has a capability in 'mask'.  'filter_fn' is expected to
   return 'true' when the desired DMA channel is found.

   A channel allocated via this interface is exclusive to the caller,
   until dma_release_channel() is called.

2. Set slave and controller specific parameters

   Next step is always to pass some specific information to the DMA
   driver.  Most of the generic information which a slave DMA can use
   is in struct dma_slave_config.  This allows the clients to specify
   DMA direction, DMA addresses, bus widths, DMA burst lengths etc
   for the peripheral.

   If some DMA controllers have more parameters to be sent then they
   should try to embed struct dma_slave_config in their controller
   specific structure. That gives flexibility to client to pass more
   parameters, if required.

   Interface:
	int dmaengine_slave_config(struct dma_chan *chan,
				  struct dma_slave_config *config)

   Please see the dma_slave_config structure definition in dmaengine.h
   for a detailed explanation of the struct members.  Please note
   that the 'direction' member will be going away as it duplicates the
   direction given in the prepare call.

3. Get a descriptor for transaction

   For slave usage the various modes of slave transfers supported by the
   DMA-engine are:

   slave_sg	- DMA a list of scatter gather buffers from/to a peripheral
   dma_cyclic	- Perform a cyclic DMA operation from/to a peripheral till the
		  operation is explicitly stopped.
   interleaved_dma - This is common to Slave as well as M2M clients. For slave
		 address of devices' fifo could be already known to the driver.
		 Various types of operations could be expressed by setting
		 appropriate values to the 'dma_interleaved_template' members.

   A non-NULL return of this transfer API represents a "descriptor" for
   the given transaction.

   Interface:
	struct dma_async_tx_descriptor *(*chan->device->device_prep_slave_sg)(
		struct dma_chan *chan, struct scatterlist *sgl,
		unsigned int sg_len, enum dma_data_direction direction,
		unsigned long flags);

	struct dma_async_tx_descriptor *(*chan->device->device_prep_dma_cyclic)(
		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
		size_t period_len, enum dma_data_direction direction);

	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
		struct dma_chan *chan, struct dma_interleaved_template *xt,
		unsigned long flags);

   The peripheral driver is expected to have mapped the scatterlist for
   the DMA operation prior to calling device_prep_slave_sg, and must
   keep the scatterlist mapped until the DMA operation has completed.
   The scatterlist must be mapped using the DMA struct device.  So,
   normal setup should look like this:

	nr_sg = dma_map_sg(chan->device->dev, sgl, sg_len);
	if (nr_sg == 0)
		/* error */

	desc = chan->device->device_prep_slave_sg(chan, sgl, nr_sg,
			direction, flags);

   Once a descriptor has been obtained, the callback information can be
   added and the descriptor must then be submitted.  Some DMA engine
   drivers may hold a spinlock between a successful preparation and
   submission so it is important that these two operations are closely
   paired.

   Note:
	Although the async_tx API specifies that completion callback
	routines cannot submit any new operations, this is not the
	case for slave/cyclic DMA.

	For slave DMA, the subsequent transaction may not be available
	for submission prior to callback function being invoked, so
	slave DMA callbacks are permitted to prepare and submit a new
	transaction.

	For cyclic DMA, a callback function may wish to terminate the
	DMA via dmaengine_terminate_all().

	Therefore, it is important that DMA engine drivers drop any
	locks before calling the callback function which may cause a
	deadlock.

	Note that callbacks will always be invoked from the DMA
	engines tasklet, never from interrupt context.

4. Submit the transaction

   Once the descriptor has been prepared and the callback information
   added, it must be placed on the DMA engine drivers pending queue.

   Interface:
	dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc)

   This returns a cookie can be used to check the progress of DMA engine
   activity via other DMA engine calls not covered in this document.

   dmaengine_submit() will not start the DMA operation, it merely adds
   it to the pending queue.  For this, see step 5, dma_async_issue_pending.

5. Issue pending DMA requests and wait for callback notification

   The transactions in the pending queue can be activated by calling the
   issue_pending API. If channel is idle then the first transaction in
   queue is started and subsequent ones queued up.

   On completion of each DMA operation, the next in queue is started and
   a tasklet triggered. The tasklet will then call the client driver
   completion callback routine for notification, if set.

   Interface:
	void dma_async_issue_pending(struct dma_chan *chan);

Further APIs:

1. int dmaengine_terminate_all(struct dma_chan *chan)

   This causes all activity for the DMA channel to be stopped, and may
   discard data in the DMA FIFO which hasn't been fully transferred.
   No callback functions will be called for any incomplete transfers.

2. int dmaengine_pause(struct dma_chan *chan)

   This pauses activity on the DMA channel without data loss.

3. int dmaengine_resume(struct dma_chan *chan)

   Resume a previously paused DMA channel.  It is invalid to resume a
   channel which is not currently paused.

4. enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,
        dma_cookie_t cookie, dma_cookie_t *last, dma_cookie_t *used)

   This can be used to check the status of the channel.  Please see
   the documentation in include/linux/dmaengine.h for a more complete
   description of this API.

   This can be used in conjunction with dma_async_is_complete() and
   the cookie returned from 'descriptor->submit()' to check for
   completion of a specific DMA transaction.

   Note:
	Not all DMA engine drivers can return reliable information for
	a running DMA channel.  It is recommended that DMA engine users
	pause or stop (via dmaengine_terminate_all) the channel before
	using this API.
                        DMA with ISA and LPC devices
                        ============================

                      Pierre Ossman <drzeus@drzeus.cx>

This document describes how to do DMA transfers using the old ISA DMA
controller. Even though ISA is more or less dead today the LPC bus
uses the same DMA system so it will be around for quite some time.

Part I - Headers and dependencies
---------------------------------

To do ISA style DMA you need to include two headers:

#include <linux/dma-mapping.h>
#include <asm/dma.h>

The first is the generic DMA API used to convert virtual addresses to
bus addresses (see Documentation/DMA-API.txt for details).

The second contains the routines specific to ISA DMA transfers. Since
this is not present on all platforms make sure you construct your
Kconfig to be dependent on ISA_DMA_API (not ISA) so that nobody tries
to build your driver on unsupported platforms.

Part II - Buffer allocation
---------------------------

The ISA DMA controller has some very strict requirements on which
memory it can access so extra care must be taken when allocating
buffers.

(You usually need a special buffer for DMA transfers instead of
transferring directly to and from your normal data structures.)

The DMA-able address space is the lowest 16 MB of _physical_ memory.
Also the transfer block may not cross page boundaries (which are 64
or 128 KiB depending on which channel you use).

In order to allocate a piece of memory that satisfies all these
requirements you pass the flag GFP_DMA to kmalloc.

Unfortunately the memory available for ISA DMA is scarce so unless you
allocate the memory during boot-up it's a good idea to also pass
__GFP_REPEAT and __GFP_NOWARN to make the allocater try a bit harder.

(This scarcity also means that you should allocate the buffer as
early as possible and not release it until the driver is unloaded.)

Part III - Address translation
------------------------------

To translate the virtual address to a bus address, use the normal DMA
API. Do _not_ use isa_virt_to_phys() even though it does the same
thing. The reason for this is that the function isa_virt_to_phys()
will require a Kconfig dependency to ISA, not just ISA_DMA_API which
is really all you need. Remember that even though the DMA controller
has its origins in ISA it is used elsewhere.

Note: x86_64 had a broken DMA API when it came to ISA but has since
been fixed. If your arch has problems then fix the DMA API instead of
reverting to the ISA functions.

Part IV - Channels
------------------

A normal ISA DMA controller has 8 channels. The lower four are for
8-bit transfers and the upper four are for 16-bit transfers.

(Actually the DMA controller is really two separate controllers where
channel 4 is used to give DMA access for the second controller (0-3).
This means that of the four 16-bits channels only three are usable.)

You allocate these in a similar fashion as all basic resources:

extern int request_dma(unsigned int dmanr, const char * device_id);
extern void free_dma(unsigned int dmanr);

The ability to use 16-bit or 8-bit transfers is _not_ up to you as a
driver author but depends on what the hardware supports. Check your
specs or test different channels.

Part V - Transfer data
----------------------

Now for the good stuff, the actual DMA transfer. :)

Before you use any ISA DMA routines you need to claim the DMA lock
using claim_dma_lock(). The reason is that some DMA operations are
not atomic so only one driver may fiddle with the registers at a
time.

The first time you use the DMA controller you should call
clear_dma_ff(). This clears an internal register in the DMA
controller that is used for the non-atomic operations. As long as you
(and everyone else) uses the locking functions then you only need to
reset this once.

Next, you tell the controller in which direction you intend to do the
transfer using set_dma_mode(). Currently you have the options
DMA_MODE_READ and DMA_MODE_WRITE.

Set the address from where the transfer should start (this needs to
be 16-bit aligned for 16-bit transfers) and how many bytes to
transfer. Note that it's _bytes_. The DMA routines will do all the
required translation to values that the DMA controller understands.

The final step is enabling the DMA channel and releasing the DMA
lock.

Once the DMA transfer is finished (or timed out) you should disable
the channel again. You should also check get_dma_residue() to make
sure that all data has been transferred.

Example:

int flags, residue;

flags = claim_dma_lock();

clear_dma_ff();

set_dma_mode(channel, DMA_MODE_WRITE);
set_dma_addr(channel, phys_addr);
set_dma_count(channel, num_bytes);

dma_enable(channel);

release_dma_lock(flags);

while (!device_done());

flags = claim_dma_lock();

dma_disable(channel);

residue = dma_get_residue(channel);
if (residue != 0)
	printk(KERN_ERR "driver: Incomplete DMA transfer!"
		" %d bytes left!\n", residue);

release_dma_lock(flags);

Part VI - Suspend/resume
------------------------

It is the driver's responsibility to make sure that the machine isn't
suspended while a DMA transfer is in progress. Also, all DMA settings
are lost when the system suspends so if your driver relies on the DMA
controller being in a certain state then you have to restore these
registers upon resume.
				DMA Test Guide
				==============

		Andy Shevchenko <andriy.shevchenko@linux.intel.com>

This small document introduces how to test DMA drivers using dmatest module.

	Part 1 - How to build the test module

The menuconfig contains an option that could be found by following path:
	Device Drivers -> DMA Engine support -> DMA Test client

In the configuration file the option called CONFIG_DMATEST. The dmatest could
be built as module or inside kernel. Let's consider those cases.

	Part 2 - When dmatest is built as a module...

Example of usage:
	% modprobe dmatest channel=dma0chan0 timeout=2000 iterations=1 run=1

...or:
	% modprobe dmatest
	% echo dma0chan0 > /sys/module/dmatest/parameters/channel
	% echo 2000 > /sys/module/dmatest/parameters/timeout
	% echo 1 > /sys/module/dmatest/parameters/iterations
	% echo 1 > /sys/module/dmatest/parameters/run

...or on the kernel command line:

	dmatest.channel=dma0chan0 dmatest.timeout=2000 dmatest.iterations=1 dmatest.run=1

Hint: available channel list could be extracted by running the following
command:
	% ls -1 /sys/class/dma/

Once started a message like "dmatest: Started 1 threads using dma0chan0" is
emitted.  After that only test failure messages are reported until the test
stops.

Note that running a new test will not stop any in progress test.

The following command returns the state of the test.
	% cat /sys/module/dmatest/parameters/run

To wait for test completion userpace can poll 'run' until it is false, or use
the wait parameter.  Specifying 'wait=1' when loading the module causes module
initialization to pause until a test run has completed, while reading
/sys/module/dmatest/parameters/wait waits for any running test to complete
before returning.  For example, the following scripts wait for 42 tests
to complete before exiting.  Note that if 'iterations' is set to 'infinite' then
waiting is disabled.

Example:
	% modprobe dmatest run=1 iterations=42 wait=1
	% modprobe -r dmatest
...or:
	% modprobe dmatest run=1 iterations=42
	% cat /sys/module/dmatest/parameters/wait
	% modprobe -r dmatest

	Part 3 - When built-in in the kernel...

The module parameters that is supplied to the kernel command line will be used
for the first performed test. After user gets a control, the test could be
re-run with the same or different parameters. For the details see the above
section "Part 2 - When dmatest is built as a module..."

In both cases the module parameters are used as the actual values for the test
case. You always could check them at run-time by running
	% grep -H . /sys/module/dmatest/parameters/*

	Part 4 - Gathering the test results

Test results are printed to the kernel log buffer with the format:

"dmatest: result <channel>: <test id>: '<error msg>' with src_off=<val> dst_off=<val> len=<val> (<err code>)"

Example of output:
	% dmesg | tail -n 1
	dmatest: result dma0chan0-copy0: #1: No errors with src_off=0x7bf dst_off=0x8ad len=0x3fea (0)

The message format is unified across the different types of errors. A number in
the parens represents additional information, e.g. error code, error counter,
or status.  A test thread also emits a summary line at completion listing the
number of tests executed, number that failed, and a result code.

Example:
	% dmesg | tail -n 1
	dmatest: dma0chan0-copy0: summary 1 test, 0 failures 1000 iops 100000 KB/s (0)

The details of a data miscompare error are also emitted, but do not follow the
above format.

Introduction
============

This document describes how to use the dynamic debug (dyndbg) feature.

Dynamic debug is designed to allow you to dynamically enable/disable
kernel code to obtain additional kernel information.  Currently, if
CONFIG_DYNAMIC_DEBUG is set, then all pr_debug()/dev_dbg() and
print_hex_dump_debug()/print_hex_dump_bytes() calls can be dynamically
enabled per-callsite.

If CONFIG_DYNAMIC_DEBUG is not set, print_hex_dump_debug() is just
shortcut for print_hex_dump(KERN_DEBUG).

For print_hex_dump_debug()/print_hex_dump_bytes(), format string is
its 'prefix_str' argument, if it is constant string; or "hexdump"
in case 'prefix_str' is build dynamically.

Dynamic debug has even more useful features:

 * Simple query language allows turning on and off debugging
   statements by matching any combination of 0 or 1 of:

   - source filename
   - function name
   - line number (including ranges of line numbers)
   - module name
   - format string

 * Provides a debugfs control file: <debugfs>/dynamic_debug/control
   which can be read to display the complete list of known debug
   statements, to help guide you

Controlling dynamic debug Behaviour
===================================

The behaviour of pr_debug()/dev_dbg()s are controlled via writing to a
control file in the 'debugfs' filesystem. Thus, you must first mount
the debugfs filesystem, in order to make use of this feature.
Subsequently, we refer to the control file as:
<debugfs>/dynamic_debug/control. For example, if you want to enable
printing from source file 'svcsock.c', line 1603 you simply do:

nullarbor:~ # echo 'file svcsock.c line 1603 +p' >
				<debugfs>/dynamic_debug/control

If you make a mistake with the syntax, the write will fail thus:

nullarbor:~ # echo 'file svcsock.c wtf 1 +p' >
				<debugfs>/dynamic_debug/control
-bash: echo: write error: Invalid argument

Viewing Dynamic Debug Behaviour
===========================

You can view the currently configured behaviour of all the debug
statements via:

nullarbor:~ # cat <debugfs>/dynamic_debug/control
# filename:lineno [module]function flags format
/usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svc_rdma.c:323 [svcxprt_rdma]svc_rdma_cleanup =_ "SVCRDMA Module Removed, deregister RPC RDMA transport\012"
/usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svc_rdma.c:341 [svcxprt_rdma]svc_rdma_init =_ "\011max_inline       : %d\012"
/usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svc_rdma.c:340 [svcxprt_rdma]svc_rdma_init =_ "\011sq_depth         : %d\012"
/usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svc_rdma.c:338 [svcxprt_rdma]svc_rdma_init =_ "\011max_requests     : %d\012"
...


You can also apply standard Unix text manipulation filters to this
data, e.g.

nullarbor:~ # grep -i rdma <debugfs>/dynamic_debug/control  | wc -l
62

nullarbor:~ # grep -i tcp <debugfs>/dynamic_debug/control | wc -l
42

The third column shows the currently enabled flags for each debug
statement callsite (see below for definitions of the flags).  The
default value, with no flags enabled, is "=_".  So you can view all
the debug statement callsites with any non-default flags:

nullarbor:~ # awk '$3 != "=_"' <debugfs>/dynamic_debug/control
# filename:lineno [module]function flags format
/usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svcsock.c:1603 [sunrpc]svc_send p "svc_process: st_sendto returned %d\012"


Command Language Reference
==========================

At the lexical level, a command comprises a sequence of words separated
by spaces or tabs.  So these are all equivalent:

nullarbor:~ # echo -c 'file svcsock.c line 1603 +p' >
				<debugfs>/dynamic_debug/control
nullarbor:~ # echo -c '  file   svcsock.c     line  1603 +p  ' >
				<debugfs>/dynamic_debug/control
nullarbor:~ # echo -n 'file svcsock.c line 1603 +p' >
				<debugfs>/dynamic_debug/control

Command submissions are bounded by a write() system call.
Multiple commands can be written together, separated by ';' or '\n'.

  ~# echo "func pnpacpi_get_resources +p; func pnp_assign_mem +p" \
     > <debugfs>/dynamic_debug/control

If your query set is big, you can batch them too:

  ~# cat query-batch-file > <debugfs>/dynamic_debug/control

A another way is to use wildcard. The match rule support '*' (matches
zero or more characters) and '?' (matches exactly one character).For
example, you can match all usb drivers:

  ~# echo "file drivers/usb/* +p" > <debugfs>/dynamic_debug/control

At the syntactical level, a command comprises a sequence of match
specifications, followed by a flags change specification.

command ::= match-spec* flags-spec

The match-spec's are used to choose a subset of the known pr_debug()
callsites to which to apply the flags-spec.  Think of them as a query
with implicit ANDs between each pair.  Note that an empty list of
match-specs will select all debug statement callsites.

A match specification comprises a keyword, which controls the
attribute of the callsite to be compared, and a value to compare
against.  Possible keywords are:

match-spec ::= 'func' string |
	       'file' string |
	       'module' string |
	       'format' string |
	       'line' line-range

line-range ::= lineno |
	       '-'lineno |
	       lineno'-' |
	       lineno'-'lineno
// Note: line-range cannot contain space, e.g.
// "1-30" is valid range but "1 - 30" is not.

lineno ::= unsigned-int

The meanings of each keyword are:

func
    The given string is compared against the function name
    of each callsite.  Example:

    func svc_tcp_accept

file
    The given string is compared against either the full pathname, the
    src-root relative pathname, or the basename of the source file of
    each callsite.  Examples:

    file svcsock.c
    file kernel/freezer.c
    file /usr/src/packages/BUILD/sgi-enhancednfs-1.4/default/net/sunrpc/svcsock.c

module
    The given string is compared against the module name
    of each callsite.  The module name is the string as
    seen in "lsmod", i.e. without the directory or the .ko
    suffix and with '-' changed to '_'.  Examples:

    module sunrpc
    module nfsd

format
    The given string is searched for in the dynamic debug format
    string.  Note that the string does not need to match the
    entire format, only some part.  Whitespace and other
    special characters can be escaped using C octal character
    escape \ooo notation, e.g. the space character is \040.
    Alternatively, the string can be enclosed in double quote
    characters (") or single quote characters (').
    Examples:

    format svcrdma:	    // many of the NFS/RDMA server pr_debugs
    format readahead	    // some pr_debugs in the readahead cache
    format nfsd:\040SETATTR // one way to match a format with whitespace
    format "nfsd: SETATTR"  // a neater way to match a format with whitespace
    format 'nfsd: SETATTR'  // yet another way to match a format with whitespace

line
    The given line number or range of line numbers is compared
    against the line number of each pr_debug() callsite.  A single
    line number matches the callsite line number exactly.  A
    range of line numbers matches any callsite between the first
    and last line number inclusive.  An empty first number means
    the first line in the file, an empty line number means the
    last number in the file.  Examples:

    line 1603	    // exactly line 1603
    line 1600-1605  // the six lines from line 1600 to line 1605
    line -1605	    // the 1605 lines from line 1 to line 1605
    line 1600-	    // all lines from line 1600 to the end of the file

The flags specification comprises a change operation followed
by one or more flag characters.  The change operation is one
of the characters:

  -    remove the given flags
  +    add the given flags
  =    set the flags to the given flags

The flags are:

  p    enables the pr_debug() callsite.
  f    Include the function name in the printed message
  l    Include line number in the printed message
  m    Include module name in the printed message
  t    Include thread ID in messages not generated from interrupt context
  _    No flags are set. (Or'd with others on input)

For print_hex_dump_debug() and print_hex_dump_bytes(), only 'p' flag
have meaning, other flags ignored.

For display, the flags are preceded by '='
(mnemonic: what the flags are currently equal to).

Note the regexp ^[-+=][flmpt_]+$ matches a flags specification.
To clear all flags at once, use "=_" or "-flmpt".


Debug messages during Boot Process
==================================

To activate debug messages for core code and built-in modules during
the boot process, even before userspace and debugfs exists, use
dyndbg="QUERY", module.dyndbg="QUERY", or ddebug_query="QUERY"
(ddebug_query is obsoleted by dyndbg, and deprecated).  QUERY follows
the syntax described above, but must not exceed 1023 characters.  Your
bootloader may impose lower limits.

These dyndbg params are processed just after the ddebug tables are
processed, as part of the arch_initcall.  Thus you can enable debug
messages in all code run after this arch_initcall via this boot
parameter.

On an x86 system for example ACPI enablement is a subsys_initcall and
   dyndbg="file ec.c +p"
will show early Embedded Controller transactions during ACPI setup if
your machine (typically a laptop) has an Embedded Controller.
PCI (or other devices) initialization also is a hot candidate for using
this boot parameter for debugging purposes.

If foo module is not built-in, foo.dyndbg will still be processed at
boot time, without effect, but will be reprocessed when module is
loaded later.  dyndbg_query= and bare dyndbg= are only processed at
boot.


Debug Messages at Module Initialization Time
============================================

When "modprobe foo" is called, modprobe scans /proc/cmdline for
foo.params, strips "foo.", and passes them to the kernel along with
params given in modprobe args or /etc/modprob.d/*.conf files,
in the following order:

1. # parameters given via /etc/modprobe.d/*.conf
   options foo dyndbg=+pt
   options foo dyndbg # defaults to +p

2. # foo.dyndbg as given in boot args, "foo." is stripped and passed
   foo.dyndbg=" func bar +p; func buz +mp"

3. # args to modprobe
   modprobe foo dyndbg==pmf # override previous settings

These dyndbg queries are applied in order, with last having final say.
This allows boot args to override or modify those from /etc/modprobe.d
(sensible, since 1 is system wide, 2 is kernel or boot specific), and
modprobe args to override both.

In the foo.dyndbg="QUERY" form, the query must exclude "module foo".
"foo" is extracted from the param-name, and applied to each query in
"QUERY", and only 1 match-spec of each type is allowed.

The dyndbg option is a "fake" module parameter, which means:

- modules do not need to define it explicitly
- every module gets it tacitly, whether they use pr_debug or not
- it doesn't appear in /sys/module/$module/parameters/
  To see it, grep the control file, or inspect /proc/cmdline.

For CONFIG_DYNAMIC_DEBUG kernels, any settings given at boot-time (or
enabled by -DDEBUG flag during compilation) can be disabled later via
the sysfs interface if the debug messages are no longer needed:

   echo "module module_name -p" > <debugfs>/dynamic_debug/control

Examples
========

// enable the message at line 1603 of file svcsock.c
nullarbor:~ # echo -n 'file svcsock.c line 1603 +p' >
				<debugfs>/dynamic_debug/control

// enable all the messages in file svcsock.c
nullarbor:~ # echo -n 'file svcsock.c +p' >
				<debugfs>/dynamic_debug/control

// enable all the messages in the NFS server module
nullarbor:~ # echo -n 'module nfsd +p' >
				<debugfs>/dynamic_debug/control

// enable all 12 messages in the function svc_process()
nullarbor:~ # echo -n 'func svc_process +p' >
				<debugfs>/dynamic_debug/control

// disable all 12 messages in the function svc_process()
nullarbor:~ # echo -n 'func svc_process -p' >
				<debugfs>/dynamic_debug/control

// enable messages for NFS calls READ, READLINK, READDIR and READDIR+.
nullarbor:~ # echo -n 'format "nfsd: READ" +p' >
				<debugfs>/dynamic_debug/control

// enable messages in files of which the paths include string "usb"
nullarbor:~ # echo -n '*usb* +p' > <debugfs>/dynamic_debug/control

// enable all messages
nullarbor:~ # echo -n '+p' > <debugfs>/dynamic_debug/control

// add module, function to all enabled messages
nullarbor:~ # echo -n '+mf' > <debugfs>/dynamic_debug/control

// boot-args example, with newlines and comments for readability
Kernel command line: ...
  // see whats going on in dyndbg=value processing
  dynamic_debug.verbose=1
  // enable pr_debugs in 2 builtins, #cmt is stripped
  dyndbg="module params +p #cmt ; module sys +p"
  // enable pr_debugs in 2 functions in a module loaded later
  pc87360.dyndbg="func pc87360_init_device +p; func pc87360_find +p"


EDAC - Error Detection And Correction

Written by Doug Thompson <dougthompson@xmission.com>
7 Dec 2005
17 Jul 2007	Updated

(c) Mauro Carvalho Chehab
05 Aug 2009	Nehalem interface

EDAC is maintained and written by:

	Doug Thompson, Dave Jiang, Dave Peterson et al,
	original author: Thayne Harbaugh,

Contact:
	website:	bluesmoke.sourceforge.net
	mailing list:	bluesmoke-devel@lists.sourceforge.net

"bluesmoke" was the name for this device driver when it was "out-of-tree"
and maintained at sourceforge.net.  When it was pushed into 2.6.16 for the
first time, it was renamed to 'EDAC'.

The bluesmoke project at sourceforge.net is now utilized as a 'staging area'
for EDAC development, before it is sent upstream to kernel.org

At the bluesmoke/EDAC project site is a series of quilt patches against
recent kernels, stored in a SVN repository. For easier downloading, there
is also a tarball snapshot available.

============================================================================
EDAC PURPOSE

The 'edac' kernel module goal is to detect and report errors that occur
within the computer system running under linux.

MEMORY

In the initial release, memory Correctable Errors (CE) and Uncorrectable
Errors (UE) are the primary errors being harvested. These types of errors
are harvested by the 'edac_mc' class of device.

Detecting CE events, then harvesting those events and reporting them,
CAN be a predictor of future UE events.  With CE events, the system can
continue to operate, but with less safety. Preventive maintenance and
proactive part replacement of memory DIMMs exhibiting CEs can reduce
the likelihood of the dreaded UE events and system 'panics'.

NON-MEMORY

A new feature for EDAC, the edac_device class of device, was added in
the 2.6.23 version of the kernel.

This new device type allows for non-memory type of ECC hardware detectors
to have their states harvested and presented to userspace via the sysfs
interface.

Some architectures have ECC detectors for L1, L2 and L3 caches, along with DMA
engines, fabric switches, main data path switches, interconnections,
and various other hardware data paths. If the hardware reports it, then
a edac_device device probably can be constructed to harvest and present
that to userspace.


PCI BUS SCANNING

In addition, PCI Bus Parity and SERR Errors are scanned for on PCI devices
in order to determine if errors are occurring on data transfers.

The presence of PCI Parity errors must be examined with a grain of salt.
There are several add-in adapters that do NOT follow the PCI specification
with regards to Parity generation and reporting. The specification says
the vendor should tie the parity status bits to 0 if they do not intend
to generate parity.  Some vendors do not do this, and thus the parity bit
can "float" giving false positives.

In the kernel there is a PCI device attribute located in sysfs that is
checked by the EDAC PCI scanning code. If that attribute is set,
PCI parity/error scanning is skipped for that device. The attribute
is:

	broken_parity_status

as is located in /sys/devices/pci<XXX>/0000:XX:YY.Z directories for
PCI devices.

FUTURE HARDWARE SCANNING

EDAC will have future error detectors that will be integrated with
EDAC or added to it, in the following list:

	MCE	Machine Check Exception
	MCA	Machine Check Architecture
	NMI	NMI notification of ECC errors
	MSRs 	Machine Specific Register error cases
	and other mechanisms.

These errors are usually bus errors, ECC errors, thermal throttling
and the like.


============================================================================
EDAC VERSIONING

EDAC is composed of a "core" module (edac_core.ko) and several Memory
Controller (MC) driver modules. On a given system, the CORE
is loaded and one MC driver will be loaded. Both the CORE and
the MC driver (or edac_device driver) have individual versions that reflect
current release level of their respective modules.

Thus, to "report" on what version a system is running, one must report both
the CORE's and the MC driver's versions.


LOADING

If 'edac' was statically linked with the kernel then no loading is
necessary.  If 'edac' was built as modules then simply modprobe the
'edac' pieces that you need.  You should be able to modprobe
hardware-specific modules and have the dependencies load the necessary core
modules.

Example:

$> modprobe amd76x_edac

loads both the amd76x_edac.ko memory controller module and the edac_mc.ko
core module.


============================================================================
EDAC sysfs INTERFACE

EDAC presents a 'sysfs' interface for control, reporting and attribute
reporting purposes.

EDAC lives in the /sys/devices/system/edac directory.

Within this directory there currently reside 2 'edac' components:

	mc	memory controller(s) system
	pci	PCI control and status system


============================================================================
Memory Controller (mc) Model

First a background on the memory controller's model abstracted in EDAC.
Each 'mc' device controls a set of DIMM memory modules. These modules are
laid out in a Chip-Select Row (csrowX) and Channel table (chX). There can
be multiple csrows and multiple channels.

Memory controllers allow for several csrows, with 8 csrows being a typical value.
Yet, the actual number of csrows depends on the electrical "loading"
of a given motherboard, memory controller and DIMM characteristics.

Dual channels allows for 128 bit data transfers to the CPU from memory.
Some newer chipsets allow for more than 2 channels, like Fully Buffered DIMMs
(FB-DIMMs). The following example will assume 2 channels:


		Channel 0	Channel 1
	===================================
	csrow0	| DIMM_A0	| DIMM_B0 |
	csrow1	| DIMM_A0	| DIMM_B0 |
	===================================

	===================================
	csrow2	| DIMM_A1	| DIMM_B1 |
	csrow3	| DIMM_A1	| DIMM_B1 |
	===================================

In the above example table there are 4 physical slots on the motherboard
for memory DIMMs:

	DIMM_A0
	DIMM_B0
	DIMM_A1
	DIMM_B1

Labels for these slots are usually silk screened on the motherboard. Slots
labeled 'A' are channel 0 in this example. Slots labeled 'B'
are channel 1. Notice that there are two csrows possible on a
physical DIMM. These csrows are allocated their csrow assignment
based on the slot into which the memory DIMM is placed. Thus, when 1 DIMM
is placed in each Channel, the csrows cross both DIMMs.

Memory DIMMs come single or dual "ranked". A rank is a populated csrow.
Thus, 2 single ranked DIMMs, placed in slots DIMM_A0 and DIMM_B0 above
will have 1 csrow, csrow0. csrow1 will be empty. On the other hand,
when 2 dual ranked DIMMs are similarly placed, then both csrow0 and
csrow1 will be populated. The pattern repeats itself for csrow2 and
csrow3.

The representation of the above is reflected in the directory tree
in EDAC's sysfs interface. Starting in directory
/sys/devices/system/edac/mc each memory controller will be represented
by its own 'mcX' directory, where 'X' is the index of the MC.


	..../edac/mc/
		   |
		   |->mc0
		   |->mc1
		   |->mc2
		   ....

Under each 'mcX' directory each 'csrowX' is again represented by a
'csrowX', where 'X' is the csrow index:


	.../mc/mc0/
		|
		|->csrow0
		|->csrow2
		|->csrow3
		....

Notice that there is no csrow1, which indicates that csrow0 is
composed of a single ranked DIMMs. This should also apply in both
Channels, in order to have dual-channel mode be operational. Since
both csrow2 and csrow3 are populated, this indicates a dual ranked
set of DIMMs for channels 0 and 1.


Within each of the 'mcX' and 'csrowX' directories are several
EDAC control and attribute files.

============================================================================
'mcX' DIRECTORIES


In 'mcX' directories are EDAC control and attribute files for
this 'X' instance of the memory controllers.

For a description of the sysfs API, please see:
	Documentation/ABI/testing/sysfs/devices-edac


============================================================================
'csrowX' DIRECTORIES

When CONFIG_EDAC_LEGACY_SYSFS is enabled, the sysfs will contain the
csrowX directories. As this API doesn't work properly for Rambus, FB-DIMMs
and modern Intel Memory Controllers, this is being deprecated in favor
of dimmX directories.

In the 'csrowX' directories are EDAC control and attribute files for
this 'X' instance of csrow:


Total Uncorrectable Errors count attribute file:

	'ue_count'

	This attribute file displays the total count of uncorrectable
	errors that have occurred on this csrow. If panic_on_ue is set
	this counter will not have a chance to increment, since EDAC
	will panic the system.


Total Correctable Errors count attribute file:

	'ce_count'

	This attribute file displays the total count of correctable
	errors that have occurred on this csrow. This
	count is very important to examine. CEs provide early
	indications that a DIMM is beginning to fail. This count
	field should be monitored for non-zero values and report
	such information to the system administrator.


Total memory managed by this csrow attribute file:

	'size_mb'

	This attribute file displays, in count of megabytes, of memory
	that this csrow contains.


Memory Type attribute file:

	'mem_type'

	This attribute file will display what type of memory is currently
	on this csrow. Normally, either buffered or unbuffered memory.
	Examples:
		Registered-DDR
		Unbuffered-DDR


EDAC Mode of operation attribute file:

	'edac_mode'

	This attribute file will display what type of Error detection
	and correction is being utilized.


Device type attribute file:

	'dev_type'

	This attribute file will display what type of DRAM device is
	being utilized on this DIMM.
	Examples:
		x1
		x2
		x4
		x8


Channel 0 CE Count attribute file:

	'ch0_ce_count'

	This attribute file will display the count of CEs on this
	DIMM located in channel 0.


Channel 0 UE Count attribute file:

	'ch0_ue_count'

	This attribute file will display the count of UEs on this
	DIMM located in channel 0.


Channel 0 DIMM Label control file:

	'ch0_dimm_label'

	This control file allows this DIMM to have a label assigned
	to it. With this label in the module, when errors occur
	the output can provide the DIMM label in the system log.
	This becomes vital for panic events to isolate the
	cause of the UE event.

	DIMM Labels must be assigned after booting, with information
	that correctly identifies the physical slot with its
	silk screen label. This information is currently very
	motherboard specific and determination of this information
	must occur in userland at this time.


Channel 1 CE Count attribute file:

	'ch1_ce_count'

	This attribute file will display the count of CEs on this
	DIMM located in channel 1.


Channel 1 UE Count attribute file:

	'ch1_ue_count'

	This attribute file will display the count of UEs on this
	DIMM located in channel 0.


Channel 1 DIMM Label control file:

	'ch1_dimm_label'

	This control file allows this DIMM to have a label assigned
	to it. With this label in the module, when errors occur
	the output can provide the DIMM label in the system log.
	This becomes vital for panic events to isolate the
	cause of the UE event.

	DIMM Labels must be assigned after booting, with information
	that correctly identifies the physical slot with its
	silk screen label. This information is currently very
	motherboard specific and determination of this information
	must occur in userland at this time.

============================================================================
SYSTEM LOGGING

If logging for UEs and CEs are enabled then system logs will have
error notices indicating errors that have been detected:

EDAC MC0: CE page 0x283, offset 0xce0, grain 8, syndrome 0x6ec3, row 0,
channel 1 "DIMM_B1": amd76x_edac

EDAC MC0: CE page 0x1e5, offset 0xfb0, grain 8, syndrome 0xb741, row 0,
channel 1 "DIMM_B1": amd76x_edac


The structure of the message is:
	the memory controller			(MC0)
	Error type				(CE)
	memory page				(0x283)
	offset in the page			(0xce0)
	the byte granularity 			(grain 8)
		or resolution of the error
	the error syndrome			(0xb741)
	memory row				(row 0)
	memory channel				(channel 1)
	DIMM label, if set prior		(DIMM B1
	and then an optional, driver-specific message that may
		have additional information.

Both UEs and CEs with no info will lack all but memory controller,
error type, a notice of "no info" and then an optional,
driver-specific error message.


============================================================================
PCI Bus Parity Detection


On Header Type 00 devices the primary status is looked at
for any parity error regardless of whether Parity is enabled on the
device.  (The spec indicates parity is generated in some cases).
On Header Type 01 bridges, the secondary status register is also
looked at to see if parity occurred on the bus on the other side of
the bridge.


SYSFS CONFIGURATION

Under /sys/devices/system/edac/pci are control and attribute files as follows:


Enable/Disable PCI Parity checking control file:

	'check_pci_parity'


	This control file enables or disables the PCI Bus Parity scanning
	operation. Writing a 1 to this file enables the scanning. Writing
	a 0 to this file disables the scanning.

	Enable:
	echo "1" >/sys/devices/system/edac/pci/check_pci_parity

	Disable:
	echo "0" >/sys/devices/system/edac/pci/check_pci_parity


Parity Count:

	'pci_parity_count'

	This attribute file will display the number of parity errors that
	have been detected.


============================================================================
MODULE PARAMETERS

Panic on UE control file:

	'edac_mc_panic_on_ue'

	An uncorrectable error will cause a machine panic.  This is usually
	desirable.  It is a bad idea to continue when an uncorrectable error
	occurs - it is indeterminate what was uncorrected and the operating
	system context might be so mangled that continuing will lead to further
	corruption. If the kernel has MCE configured, then EDAC will never
	notice the UE.

	LOAD TIME: module/kernel parameter: edac_mc_panic_on_ue=[0|1]

	RUN TIME:  echo "1" > /sys/module/edac_core/parameters/edac_mc_panic_on_ue


Log UE control file:

	'edac_mc_log_ue'

	Generate kernel messages describing uncorrectable errors.  These errors
	are reported through the system message log system.  UE statistics
	will be accumulated even when UE logging is disabled.

	LOAD TIME: module/kernel parameter: edac_mc_log_ue=[0|1]

	RUN TIME: echo "1" > /sys/module/edac_core/parameters/edac_mc_log_ue


Log CE control file:

	'edac_mc_log_ce'

	Generate kernel messages describing correctable errors.  These
	errors are reported through the system message log system.
	CE statistics will be accumulated even when CE logging is disabled.

	LOAD TIME: module/kernel parameter: edac_mc_log_ce=[0|1]

	RUN TIME: echo "1" > /sys/module/edac_core/parameters/edac_mc_log_ce


Polling period control file:

	'edac_mc_poll_msec'

	The time period, in milliseconds, for polling for error information.
	Too small a value wastes resources.  Too large a value might delay
	necessary handling of errors and might loose valuable information for
	locating the error.  1000 milliseconds (once each second) is the current
	default. Systems which require all the bandwidth they can get, may
	increase this.

	LOAD TIME: module/kernel parameter: edac_mc_poll_msec=[0|1]

	RUN TIME: echo "1000" > /sys/module/edac_core/parameters/edac_mc_poll_msec


Panic on PCI PARITY Error:

	'panic_on_pci_parity'


	This control files enables or disables panicking when a parity
	error has been detected.


	module/kernel parameter: edac_panic_on_pci_pe=[0|1]

	Enable:
	echo "1" > /sys/module/edac_core/parameters/edac_panic_on_pci_pe

	Disable:
	echo "0" > /sys/module/edac_core/parameters/edac_panic_on_pci_pe



=======================================================================


EDAC_DEVICE type of device

In the header file, edac_core.h, there is a series of edac_device structures
and APIs for the EDAC_DEVICE.

User space access to an edac_device is through the sysfs interface.

At the location /sys/devices/system/edac (sysfs) new edac_device devices will
appear.

There is a three level tree beneath the above 'edac' directory. For example,
the 'test_device_edac' device (found at the bluesmoke.sourceforget.net website)
installs itself as:

	/sys/devices/systm/edac/test-instance

in this directory are various controls, a symlink and one or more 'instance'
directories.

The standard default controls are:

	log_ce		boolean to log CE events
	log_ue		boolean to log UE events
	panic_on_ue	boolean to 'panic' the system if an UE is encountered
			(default off, can be set true via startup script)
	poll_msec	time period between POLL cycles for events

The test_device_edac device adds at least one of its own custom control:

	test_bits	which in the current test driver does nothing but
			show how it is installed. A ported driver can
			add one or more such controls and/or attributes
			for specific uses.
			One out-of-tree driver uses controls here to allow
			for ERROR INJECTION operations to hardware
			injection registers

The symlink points to the 'struct dev' that is registered for this edac_device.

INSTANCES

One or more instance directories are present. For the 'test_device_edac' case:

	test-instance0


In this directory there are two default counter attributes, which are totals of
counter in deeper subdirectories.

	ce_count	total of CE events of subdirectories
	ue_count	total of UE events of subdirectories

BLOCKS

At the lowest directory level is the 'block' directory. There can be 0, 1
or more blocks specified in each instance.

	test-block0


In this directory the default attributes are:

	ce_count	which is counter of CE events for this 'block'
			of hardware being monitored
	ue_count	which is counter of UE events for this 'block'
			of hardware being monitored


The 'test_device_edac' device adds 4 attributes and 1 control:

	test-block-bits-0	for every POLL cycle this counter
				is incremented
	test-block-bits-1	every 10 cycles, this counter is bumped once,
				and test-block-bits-0 is set to 0
	test-block-bits-2	every 100 cycles, this counter is bumped once,
				and test-block-bits-1 is set to 0
	test-block-bits-3	every 1000 cycles, this counter is bumped once,
				and test-block-bits-2 is set to 0


	reset-counters		writing ANY thing to this control will
				reset all the above counters.


Use of the 'test_device_edac' driver should any others to create their own
unique drivers for their hardware systems.

The 'test_device_edac' sample driver is located at the
bluesmoke.sourceforge.net project site for EDAC.

=======================================================================
NEHALEM USAGE OF EDAC APIs

This chapter documents some EXPERIMENTAL mappings for EDAC API to handle
Nehalem EDAC driver. They will likely be changed on future versions
of the driver.

Due to the way Nehalem exports Memory Controller data, some adjustments
were done at i7core_edac driver. This chapter will cover those differences

1) On Nehalem, there are one Memory Controller per Quick Patch Interconnect
   (QPI). At the driver, the term "socket" means one QPI. This is
   associated with a physical CPU socket.

   Each MC have 3 physical read channels, 3 physical write channels and
   3 logic channels. The driver currently sees it as just 3 channels.
   Each channel can have up to 3 DIMMs.

   The minimum known unity is DIMMs. There are no information about csrows.
   As EDAC API maps the minimum unity is csrows, the driver sequencially
   maps channel/dimm into different csrows.

   For example, supposing the following layout:
	Ch0 phy rd0, wr0 (0x063f4031): 2 ranks, UDIMMs
	  dimm 0 1024 Mb offset: 0, bank: 8, rank: 1, row: 0x4000, col: 0x400
	  dimm 1 1024 Mb offset: 4, bank: 8, rank: 1, row: 0x4000, col: 0x400
        Ch1 phy rd1, wr1 (0x063f4031): 2 ranks, UDIMMs
	  dimm 0 1024 Mb offset: 0, bank: 8, rank: 1, row: 0x4000, col: 0x400
	Ch2 phy rd3, wr3 (0x063f4031): 2 ranks, UDIMMs
	  dimm 0 1024 Mb offset: 0, bank: 8, rank: 1, row: 0x4000, col: 0x400
   The driver will map it as:
	csrow0: channel 0, dimm0
	csrow1: channel 0, dimm1
	csrow2: channel 1, dimm0
	csrow3: channel 2, dimm0

exports one
   DIMM per csrow.

   Each QPI is exported as a different memory controller.

2) Nehalem MC has the hability to generate errors. The driver implements this
   functionality via some error injection nodes:

   For injecting a memory error, there are some sysfs nodes, under
   /sys/devices/system/edac/mc/mc?/:

   inject_addrmatch/*:
      Controls the error injection mask register. It is possible to specify
      several characteristics of the address to match an error code:
         dimm = the affected dimm. Numbers are relative to a channel;
         rank = the memory rank;
         channel = the channel that will generate an error;
         bank = the affected bank;
         page = the page address;
         column (or col) = the address column.
      each of the above values can be set to "any" to match any valid value.

      At driver init, all values are set to any.

      For example, to generate an error at rank 1 of dimm 2, for any channel,
      any bank, any page, any column:
		echo 2 >/sys/devices/system/edac/mc/mc0/inject_addrmatch/dimm
		echo 1 >/sys/devices/system/edac/mc/mc0/inject_addrmatch/rank

	To return to the default behaviour of matching any, you can do:
		echo any >/sys/devices/system/edac/mc/mc0/inject_addrmatch/dimm
		echo any >/sys/devices/system/edac/mc/mc0/inject_addrmatch/rank

   inject_eccmask:
       specifies what bits will have troubles,

   inject_section:
       specifies what ECC cache section will get the error:
		3 for both
		2 for the highest
		1 for the lowest

   inject_type:
       specifies the type of error, being a combination of the following bits:
		bit 0 - repeat
		bit 1 - ecc
		bit 2 - parity

       inject_enable starts the error generation when something different
       than 0 is written.

   All inject vars can be read. root permission is needed for write.

   Datasheet states that the error will only be generated after a write on an
   address that matches inject_addrmatch. It seems, however, that reading will
   also produce an error.

   For example, the following code will generate an error for any write access
   at socket 0, on any DIMM/address on channel 2:

   echo 2 >/sys/devices/system/edac/mc/mc0/inject_addrmatch/channel
   echo 2 >/sys/devices/system/edac/mc/mc0/inject_type
   echo 64 >/sys/devices/system/edac/mc/mc0/inject_eccmask
   echo 3 >/sys/devices/system/edac/mc/mc0/inject_section
   echo 1 >/sys/devices/system/edac/mc/mc0/inject_enable
   dd if=/dev/mem of=/dev/null seek=16k bs=4k count=1 >& /dev/null

   For socket 1, it is needed to replace "mc0" by "mc1" at the above
   commands.

   The generated error message will look like:

   EDAC MC0: UE row 0, channel-a= 0 channel-b= 0 labels "-": NON_FATAL (addr = 0x0075b980, socket=0, Dimm=0, Channel=2, syndrome=0x00000040, count=1, Err=8c0000400001009f:4000080482 (read error: read ECC error))

3) Nehalem specific Corrected Error memory counters

   Nehalem have some registers to count memory errors. The driver uses those
   registers to report Corrected Errors on devices with Registered Dimms.

   However, those counters don't work with Unregistered Dimms. As the chipset
   offers some counters that also work with UDIMMS (but with a worse level of
   granularity than the default ones), the driver exposes those registers for
   UDIMM memories.

   They can be read by looking at the contents of all_channel_counts/

   $ for i in /sys/devices/system/edac/mc/mc0/all_channel_counts/*; do echo $i; cat $i; done
	/sys/devices/system/edac/mc/mc0/all_channel_counts/udimm0
	0
	/sys/devices/system/edac/mc/mc0/all_channel_counts/udimm1
	0
	/sys/devices/system/edac/mc/mc0/all_channel_counts/udimm2
	0

   What happens here is that errors on different csrows, but at the same
   dimm number will increment the same counter.
   So, in this memory mapping:
	csrow0: channel 0, dimm0
	csrow1: channel 0, dimm1
	csrow2: channel 1, dimm0
	csrow3: channel 2, dimm0
   The hardware will increment udimm0 for an error at the first dimm at either
	csrow0, csrow2  or csrow3;
   The hardware will increment udimm1 for an error at the second dimm at either
	csrow0, csrow2  or csrow3;
   The hardware will increment udimm2 for an error at the third dimm at either
	csrow0, csrow2  or csrow3;

4) Standard error counters

   The standard error counters are generated when an mcelog error is received
   by the driver. Since, with udimm, this is counted by software, it is
   possible that some errors could be lost. With rdimm's, they displays the
   contents of the registers
			  The EFI Boot Stub
		     ---------------------------

On the x86 and ARM platforms, a kernel zImage/bzImage can masquerade
as a PE/COFF image, thereby convincing EFI firmware loaders to load
it as an EFI executable. The code that modifies the bzImage header,
along with the EFI-specific entry point that the firmware loader
jumps to are collectively known as the "EFI boot stub", and live in
arch/x86/boot/header.S and arch/x86/boot/compressed/eboot.c,
respectively. For ARM the EFI stub is implemented in
arch/arm/boot/compressed/efi-header.S and
arch/arm/boot/compressed/efi-stub.c. EFI stub code that is shared
between architectures is in drivers/firmware/efi/efi-stub-helper.c.

For arm64, there is no compressed kernel support, so the Image itself
masquerades as a PE/COFF image and the EFI stub is linked into the
kernel. The arm64 EFI stub lives in arch/arm64/kernel/efi-entry.S
and arch/arm64/kernel/efi-stub.c.

By using the EFI boot stub it's possible to boot a Linux kernel
without the use of a conventional EFI boot loader, such as grub or
elilo. Since the EFI boot stub performs the jobs of a boot loader, in
a certain sense it *IS* the boot loader.

The EFI boot stub is enabled with the CONFIG_EFI_STUB kernel option.


**** How to install bzImage.efi

The bzImage located in arch/x86/boot/bzImage must be copied to the EFI
System Partition (ESP) and renamed with the extension ".efi". Without
the extension the EFI firmware loader will refuse to execute it. It's
not possible to execute bzImage.efi from the usual Linux file systems
because EFI firmware doesn't have support for them. For ARM the
arch/arm/boot/zImage should be copied to the system partition, and it
may not need to be renamed. Similarly for arm64, arch/arm64/boot/Image
should be copied but not necessarily renamed.


**** Passing kernel parameters from the EFI shell

Arguments to the kernel can be passed after bzImage.efi, e.g.

	fs0:> bzImage.efi console=ttyS0 root=/dev/sda4


**** The "initrd=" option

Like most boot loaders, the EFI stub allows the user to specify
multiple initrd files using the "initrd=" option. This is the only EFI
stub-specific command line parameter, everything else is passed to the
kernel when it boots.

The path to the initrd file must be an absolute path from the
beginning of the ESP, relative path names do not work. Also, the path
is an EFI-style path and directory elements must be separated with
backslashes (\). For example, given the following directory layout,

fs0:>
	Kernels\
			bzImage.efi
			initrd-large.img

	Ramdisks\
			initrd-small.img
			initrd-medium.img

to boot with the initrd-large.img file if the current working
directory is fs0:\Kernels, the following command must be used,

	fs0:\Kernels> bzImage.efi initrd=\Kernels\initrd-large.img

Notice how bzImage.efi can be specified with a relative path. That's
because the image we're executing is interpreted by the EFI shell,
which understands relative paths, whereas the rest of the command line
is passed to bzImage.efi.


**** The "dtb=" option

For the ARM and arm64 architectures, we also need to be able to provide a
device tree to the kernel. This is done with the "dtb=" command line option,
and is processed in the same manner as the "initrd=" option that is
described above.
EISA bus support (Marc Zyngier <maz@wild-wind.fr.eu.org>)

This document groups random notes about porting EISA drivers to the
new EISA/sysfs API.

Starting from version 2.5.59, the EISA bus is almost given the same
status as other much more mainstream busses such as PCI or USB. This
has been possible through sysfs, which defines a nice enough set of
abstractions to manage busses, devices and drivers.

Although the new API is quite simple to use, converting existing
drivers to the new infrastructure is not an easy task (mostly because
detection code is generally also used to probe ISA cards). Moreover,
most EISA drivers are among the oldest Linux drivers so, as you can
imagine, some dust has settled here over the years.

The EISA infrastructure is made up of three parts :

    - The bus code implements most of the generic code. It is shared
    among all the architectures that the EISA code runs on. It
    implements bus probing (detecting EISA cards available on the bus),
    allocates I/O resources, allows fancy naming through sysfs, and
    offers interfaces for driver to register.

    - The bus root driver implements the glue between the bus hardware
    and the generic bus code. It is responsible for discovering the
    device implementing the bus, and setting it up to be latter probed
    by the bus code. This can go from something as simple as reserving
    an I/O region on x86, to the rather more complex, like the hppa
    EISA code. This is the part to implement in order to have EISA
    running on an "new" platform.

    - The driver offers the bus a list of devices that it manages, and
    implements the necessary callbacks to probe and release devices
    whenever told to.

Every function/structure below lives in <linux/eisa.h>, which depends
heavily on <linux/device.h>.

** Bus root driver :

int eisa_root_register (struct eisa_root_device *root);

The eisa_root_register function is used to declare a device as the
root of an EISA bus. The eisa_root_device structure holds a reference
to this device, as well as some parameters for probing purposes.

struct eisa_root_device {
	struct device   *dev;	 /* Pointer to bridge device */
	struct resource *res;
	unsigned long    bus_base_addr;
	int		 slots;  /* Max slot number */
	int		 force_probe; /* Probe even when no slot 0 */
	u64		 dma_mask; /* from bridge device */
	int              bus_nr; /* Set by eisa_root_register */
	struct resource  eisa_root_res;	/* ditto */
};

node          : used for eisa_root_register internal purpose
dev           : pointer to the root device
res           : root device I/O resource
bus_base_addr : slot 0 address on this bus
slots	      : max slot number to probe
force_probe   : Probe even when slot 0 is empty (no EISA mainboard)
dma_mask      : Default DMA mask. Usually the bridge device dma_mask.
bus_nr	      : unique bus id, set by eisa_root_register

** Driver :

int eisa_driver_register (struct eisa_driver *edrv);
void eisa_driver_unregister (struct eisa_driver *edrv);

Clear enough ?

struct eisa_device_id {
        char sig[EISA_SIG_LEN];
	unsigned long driver_data;
};

struct eisa_driver {
        const struct eisa_device_id *id_table;
        struct device_driver         driver;
};

id_table	: an array of NULL terminated EISA id strings,
		  followed by an empty string. Each string can
		  optionally be paired with a driver-dependent value
		  (driver_data).

driver		: a generic driver, such as described in
		  Documentation/driver-model/driver.txt. Only .name,
		  .probe and .remove members are mandatory.

An example is the 3c59x driver :

static struct eisa_device_id vortex_eisa_ids[] = {
	{ "TCM5920", EISA_3C592_OFFSET },
	{ "TCM5970", EISA_3C597_OFFSET },
	{ "" }
};

static struct eisa_driver vortex_eisa_driver = {
	.id_table = vortex_eisa_ids,
	.driver   = {
		.name    = "3c59x",
		.probe   = vortex_eisa_probe,
		.remove  = vortex_eisa_remove
	}
};

** Device :

The sysfs framework calls .probe and .remove functions upon device
discovery and removal (note that the .remove function is only called
when driver is built as a module).

Both functions are passed a pointer to a 'struct device', which is
encapsulated in a 'struct eisa_device' described as follows :

struct eisa_device {
        struct eisa_device_id id;
        int                   slot;
	int                   state;
	unsigned long         base_addr;
	struct resource       res[EISA_MAX_RESOURCES];
	u64                   dma_mask;
        struct device         dev; /* generic device */
};

id	: EISA id, as read from device. id.driver_data is set from the
	  matching driver EISA id.
slot	: slot number which the device was detected on
state   : set of flags indicating the state of the device. Current
	  flags are EISA_CONFIG_ENABLED and EISA_CONFIG_FORCED.
res	: set of four 256 bytes I/O regions allocated to this device
dma_mask: DMA mask set from the parent device.
dev	: generic device (see Documentation/driver-model/device.txt)

You can get the 'struct eisa_device' from 'struct device' using the
'to_eisa_device' macro.

** Misc stuff :

void eisa_set_drvdata (struct eisa_device *edev, void *data);

Stores data into the device's driver_data area.

void *eisa_get_drvdata (struct eisa_device *edev):

Gets the pointer previously stored into the device's driver_data area.

int eisa_get_region_index (void *addr);

Returns the region number (0 <= x < EISA_MAX_RESOURCES) of a given
address.

** Kernel parameters :

eisa_bus.enable_dev :

A comma-separated list of slots to be enabled, even if the firmware
set the card as disabled. The driver must be able to properly
initialize the device in such conditions.

eisa_bus.disable_dev :

A comma-separated list of slots to be enabled, even if the firmware
set the card as enabled. The driver won't be called to handle this
device.

virtual_root.force_probe :

Force the probing code to probe EISA slots even when it cannot find an
EISA compliant mainboard (nothing appears on slot 0). Defaults to 0
(don't force), and set to 1 (force probing) when either
CONFIG_ALPHA_JENSEN or CONFIG_EISA_VLB_PRIMING are set.

** Random notes :

Converting an EISA driver to the new API mostly involves *deleting*
code (since probing is now in the core EISA code). Unfortunately, most
drivers share their probing routine between ISA, and EISA. Special
care must be taken when ripping out the EISA code, so other busses
won't suffer from these surgical strikes...

You *must not* expect any EISA device to be detected when returning
from eisa_driver_register, since the chances are that the bus has not
yet been probed. In fact, that's what happens most of the time (the
bus root driver usually kicks in rather late in the boot process).
Unfortunately, most drivers are doing the probing by themselves, and
expect to have explored the whole machine when they exit their probe
routine.

For example, switching your favorite EISA SCSI card to the "hotplug"
model is "the right thing"(tm).

** Thanks :

I'd like to thank the following people for their help :
- Xavier Benigni for lending me a wonderful Alpha Jensen,
- James Bottomley, Jeff Garzik for getting this stuff into the kernel,
- Andries Brouwer for contributing numerous EISA ids,
- Catrin Jones for coping with far too many machines at home.
Email clients info for Linux
======================================================================

General Preferences
----------------------------------------------------------------------
Patches for the Linux kernel are submitted via email, preferably as
inline text in the body of the email.  Some maintainers accept
attachments, but then the attachments should have content-type
"text/plain".  However, attachments are generally frowned upon because
it makes quoting portions of the patch more difficult in the patch
review process.

Email clients that are used for Linux kernel patches should send the
patch text untouched.  For example, they should not modify or delete tabs
or spaces, even at the beginning or end of lines.

Don't send patches with "format=flowed".  This can cause unexpected
and unwanted line breaks.

Don't let your email client do automatic word wrapping for you.
This can also corrupt your patch.

Email clients should not modify the character set encoding of the text.
Emailed patches should be in ASCII or UTF-8 encoding only.
If you configure your email client to send emails with UTF-8 encoding,
you avoid some possible charset problems.

Email clients should generate and maintain References: or In-Reply-To:
headers so that mail threading is not broken.

Copy-and-paste (or cut-and-paste) usually does not work for patches
because tabs are converted to spaces.  Using xclipboard, xclip, and/or
xcutsel may work, but it's best to test this for yourself or just avoid
copy-and-paste.

Don't use PGP/GPG signatures in mail that contains patches.
This breaks many scripts that read and apply the patches.
(This should be fixable.)

It's a good idea to send a patch to yourself, save the received message,
and successfully apply it with 'patch' before sending patches to Linux
mailing lists.


Some email client (MUA) hints
----------------------------------------------------------------------
Here are some specific MUA configuration hints for editing and sending
patches for the Linux kernel.  These are not meant to be complete
software package configuration summaries.

Legend:
TUI = text-based user interface
GUI = graphical user interface

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Alpine (TUI)

Config options:
In the "Sending Preferences" section:

- "Do Not Send Flowed Text" must be enabled
- "Strip Whitespace Before Sending" must be disabled

When composing the message, the cursor should be placed where the patch
should appear, and then pressing CTRL-R let you specify the patch file
to insert into the message.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Evolution (GUI)

Some people use this successfully for patches.

When composing mail select: Preformat
  from Format->Heading->Preformatted (Ctrl-7)
  or the toolbar

Then use:
  Insert->Text File... (Alt-n x)
to insert the patch.

You can also "diff -Nru old.c new.c | xclip", select Preformat, then
paste with the middle button.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Kmail (GUI)

Some people use Kmail successfully for patches.

The default setting of not composing in HTML is appropriate; do not
enable it.

When composing an email, under options, uncheck "word wrap". The only
disadvantage is any text you type in the email will not be word-wrapped
so you will have to manually word wrap text before the patch. The easiest
way around this is to compose your email with word wrap enabled, then save
it as a draft. Once you pull it up again from your drafts it is now hard
word-wrapped and you can uncheck "word wrap" without losing the existing
wrapping.

At the bottom of your email, put the commonly-used patch delimiter before
inserting your patch:  three hyphens (---).

Then from the "Message" menu item, select insert file and choose your patch.
As an added bonus you can customise the message creation toolbar menu
and put the "insert file" icon there.

Make the composer window wide enough so that no lines wrap. As of
KMail 1.13.5 (KDE 4.5.4), KMail will apply word wrapping when sending
the email if the lines wrap in the composer window. Having word wrapping
disabled in the Options menu isn't enough. Thus, if your patch has very
long lines, you must make the composer window very wide before sending
the email. See: https://bugs.kde.org/show_bug.cgi?id=174034

You can safely GPG sign attachments, but inlined text is preferred for
patches so do not GPG sign them.  Signing patches that have been inserted
as inlined text will make them tricky to extract from their 7-bit encoding.

If you absolutely must send patches as attachments instead of inlining
them as text, right click on the attachment and select properties, and
highlight "Suggest automatic display" to make the attachment inlined to
make it more viewable.

When saving patches that are sent as inlined text, select the email that
contains the patch from the message list pane, right click and select
"save as".  You can use the whole email unmodified as a patch if it was
properly composed.  There is no option currently to save the email when you
are actually viewing it in its own window -- there has been a request filed
at kmail's bugzilla and hopefully this will be addressed.  Emails are saved
as read-write for user only so you will have to chmod them to make them
group and world readable if you copy them elsewhere.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Lotus Notes (GUI)

Run away from it.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Mutt (TUI)

Plenty of Linux developers use mutt, so it must work pretty well.

Mutt doesn't come with an editor, so whatever editor you use should be
used in a way that there are no automatic linebreaks.  Most editors have
an "insert file" option that inserts the contents of a file unaltered.

To use 'vim' with mutt:
  set editor="vi"

  If using xclip, type the command
  :set paste
  before middle button or shift-insert or use
  :r filename

if you want to include the patch inline.
(a)ttach works fine without "set paste".

Config options:
It should work with default settings.
However, it's a good idea to set the "send_charset" to:
  set send_charset="us-ascii:utf-8"

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Pine (TUI)

Pine has had some whitespace truncation issues in the past, but these
should all be fixed now.

Use alpine (pine's successor) if you can.

Config options:
- quell-flowed-text is needed for recent versions
- the "no-strip-whitespace-before-send" option is needed


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sylpheed (GUI)

- Works well for inlining text (or using attachments).
- Allows use of an external editor.
- Is slow on large folders.
- Won't do TLS SMTP auth over a non-SSL connection.
- Has a helpful ruler bar in the compose window.
- Adding addresses to address book doesn't understand the display name
  properly.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Thunderbird (GUI)

Thunderbird is an Outlook clone that likes to mangle text, but there are ways
to coerce it into behaving.

- Allows use of an external editor:
  The easiest thing to do with Thunderbird and patches is to use an
  "external editor" extension and then just use your favorite $EDITOR
  for reading/merging patches into the body text.  To do this, download
  and install the extension, then add a button for it using
  View->Toolbars->Customize... and finally just click on it when in the
  Compose dialog.

To beat some sense out of the internal editor, do this:

- Edit your Thunderbird config settings so that it won't use format=flowed.
  Go to "edit->preferences->advanced->config editor" to bring up the
  thunderbird's registry editor.

- Set "mailnews.send_plaintext_flowed" to "false"

- Set "mailnews.wraplength" from "72" to "0"

- "View" > "Message Body As" > "Plain Text"

- "View" > "Character Encoding" > "Unicode (UTF-8)"

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
TkRat (GUI)

Works.  Use "Insert file..." or external editor.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Gmail (Web GUI)

Does not work for sending patches.

Gmail web client converts tabs to spaces automatically.

At the same time it wraps lines every 78 chars with CRLF style line breaks
although tab2space problem can be solved with external editor.

Another problem is that Gmail will base64-encode any message that has a
non-ASCII character. That includes things like European names.

                                ###
Using flexible arrays in the kernel
Last updated for 2.6.32
Jonathan Corbet <corbet@lwn.net>

Large contiguous memory allocations can be unreliable in the Linux kernel.
Kernel programmers will sometimes respond to this problem by allocating
pages with vmalloc().  This solution not ideal, though.  On 32-bit systems,
memory from vmalloc() must be mapped into a relatively small address space;
it's easy to run out.  On SMP systems, the page table changes required by
vmalloc() allocations can require expensive cross-processor interrupts on
all CPUs.  And, on all systems, use of space in the vmalloc() range
increases pressure on the translation lookaside buffer (TLB), reducing the
performance of the system.

In many cases, the need for memory from vmalloc() can be eliminated by
piecing together an array from smaller parts; the flexible array library
exists to make this task easier.

A flexible array holds an arbitrary (within limits) number of fixed-sized
objects, accessed via an integer index.  Sparse arrays are handled
reasonably well.  Only single-page allocations are made, so memory
allocation failures should be relatively rare.  The down sides are that the
arrays cannot be indexed directly, individual object size cannot exceed the
system page size, and putting data into a flexible array requires a copy
operation.  It's also worth noting that flexible arrays do no internal
locking at all; if concurrent access to an array is possible, then the
caller must arrange for appropriate mutual exclusion.

The creation of a flexible array is done with:

    #include <linux/flex_array.h>

    struct flex_array *flex_array_alloc(int element_size,
					unsigned int total,
					gfp_t flags);

The individual object size is provided by element_size, while total is the
maximum number of objects which can be stored in the array.  The flags
argument is passed directly to the internal memory allocation calls.  With
the current code, using flags to ask for high memory is likely to lead to
notably unpleasant side effects.

It is also possible to define flexible arrays at compile time with:

    DEFINE_FLEX_ARRAY(name, element_size, total);

This macro will result in a definition of an array with the given name; the
element size and total will be checked for validity at compile time.

Storing data into a flexible array is accomplished with a call to:

    int flex_array_put(struct flex_array *array, unsigned int element_nr,
    		       void *src, gfp_t flags);

This call will copy the data from src into the array, in the position
indicated by element_nr (which must be less than the maximum specified when
the array was created).  If any memory allocations must be performed, flags
will be used.  The return value is zero on success, a negative error code
otherwise.

There might possibly be a need to store data into a flexible array while
running in some sort of atomic context; in this situation, sleeping in the
memory allocator would be a bad thing.  That can be avoided by using
GFP_ATOMIC for the flags value, but, often, there is a better way.  The
trick is to ensure that any needed memory allocations are done before
entering atomic context, using:

    int flex_array_prealloc(struct flex_array *array, unsigned int start,
			    unsigned int nr_elements, gfp_t flags);

This function will ensure that memory for the elements indexed in the range
defined by start and nr_elements has been allocated.  Thereafter, a
flex_array_put() call on an element in that range is guaranteed not to
block.

Getting data back out of the array is done with:

    void *flex_array_get(struct flex_array *fa, unsigned int element_nr);

The return value is a pointer to the data element, or NULL if that
particular element has never been allocated.

Note that it is possible to get back a valid pointer for an element which
has never been stored in the array.  Memory for array elements is allocated
one page at a time; a single allocation could provide memory for several
adjacent elements.  Flexible array elements are normally initialized to the
value FLEX_ARRAY_FREE (defined as 0x6c in <linux/poison.h>), so errors
involving that number probably result from use of unstored array entries.
Note that, if array elements are allocated with __GFP_ZERO, they will be
initialized to zero and this poisoning will not happen.

Individual elements in the array can be cleared with:

    int flex_array_clear(struct flex_array *array, unsigned int element_nr);

This function will set the given element to FLEX_ARRAY_FREE and return
zero.  If storage for the indicated element is not allocated for the array,
flex_array_clear() will return -EINVAL instead.  Note that clearing an
element does not release the storage associated with it; to reduce the
allocated size of an array, call:

    int flex_array_shrink(struct flex_array *array);

The return value will be the number of pages of memory actually freed.
This function works by scanning the array for pages containing nothing but
FLEX_ARRAY_FREE bytes, so (1) it can be expensive, and (2) it will not work
if the array's pages are allocated with __GFP_ZERO.

It is possible to remove all elements of an array with a call to:

    void flex_array_free_parts(struct flex_array *array);

This call frees all elements, but leaves the array itself in place.
Freeing the entire array is done with:

    void flex_array_free(struct flex_array *array);

As of this writing, there are no users of flexible arrays in the mainline
kernel.  The functions described here are also not exported to modules;
that will probably be fixed when somebody comes up with a need for it.
Futex Requeue PI
----------------

Requeueing of tasks from a non-PI futex to a PI futex requires
special handling in order to ensure the underlying rt_mutex is never
left without an owner if it has waiters; doing so would break the PI
boosting logic [see rt-mutex-desgin.txt] For the purposes of
brevity, this action will be referred to as "requeue_pi" throughout
this document.  Priority inheritance is abbreviated throughout as
"PI".

Motivation
----------

Without requeue_pi, the glibc implementation of
pthread_cond_broadcast() must resort to waking all the tasks waiting
on a pthread_condvar and letting them try to sort out which task
gets to run first in classic thundering-herd formation.  An ideal
implementation would wake the highest-priority waiter, and leave the
rest to the natural wakeup inherent in unlocking the mutex
associated with the condvar.

Consider the simplified glibc calls:

/* caller must lock mutex */
pthread_cond_wait(cond, mutex)
{
	lock(cond->__data.__lock);
	unlock(mutex);
	do {
	   unlock(cond->__data.__lock);
	   futex_wait(cond->__data.__futex);
	   lock(cond->__data.__lock);
	} while(...)
	unlock(cond->__data.__lock);
	lock(mutex);
}

pthread_cond_broadcast(cond)
{
	lock(cond->__data.__lock);
	unlock(cond->__data.__lock);
	futex_requeue(cond->data.__futex, cond->mutex);
}

Once pthread_cond_broadcast() requeues the tasks, the cond->mutex
has waiters. Note that pthread_cond_wait() attempts to lock the
mutex only after it has returned to user space.  This will leave the
underlying rt_mutex with waiters, and no owner, breaking the
previously mentioned PI-boosting algorithms.

In order to support PI-aware pthread_condvar's, the kernel needs to
be able to requeue tasks to PI futexes.  This support implies that
upon a successful futex_wait system call, the caller would return to
user space already holding the PI futex.  The glibc implementation
would be modified as follows:


/* caller must lock mutex */
pthread_cond_wait_pi(cond, mutex)
{
	lock(cond->__data.__lock);
	unlock(mutex);
	do {
	   unlock(cond->__data.__lock);
	   futex_wait_requeue_pi(cond->__data.__futex);
	   lock(cond->__data.__lock);
	} while(...)
	unlock(cond->__data.__lock);
        /* the kernel acquired the mutex for us */
}

pthread_cond_broadcast_pi(cond)
{
	lock(cond->__data.__lock);
	unlock(cond->__data.__lock);
	futex_requeue_pi(cond->data.__futex, cond->mutex);
}

The actual glibc implementation will likely test for PI and make the
necessary changes inside the existing calls rather than creating new
calls for the PI cases.  Similar changes are needed for
pthread_cond_timedwait() and pthread_cond_signal().

Implementation
--------------

In order to ensure the rt_mutex has an owner if it has waiters, it
is necessary for both the requeue code, as well as the waiting code,
to be able to acquire the rt_mutex before returning to user space.
The requeue code cannot simply wake the waiter and leave it to
acquire the rt_mutex as it would open a race window between the
requeue call returning to user space and the waiter waking and
starting to run.  This is especially true in the uncontended case.

The solution involves two new rt_mutex helper routines,
rt_mutex_start_proxy_lock() and rt_mutex_finish_proxy_lock(), which
allow the requeue code to acquire an uncontended rt_mutex on behalf
of the waiter and to enqueue the waiter on a contended rt_mutex.
Two new system calls provide the kernel<->user interface to
requeue_pi: FUTEX_WAIT_REQUEUE_PI and FUTEX_REQUEUE_CMP_PI.

FUTEX_WAIT_REQUEUE_PI is called by the waiter (pthread_cond_wait()
and pthread_cond_timedwait()) to block on the initial futex and wait
to be requeued to a PI-aware futex.  The implementation is the
result of a high-speed collision between futex_wait() and
futex_lock_pi(), with some extra logic to check for the additional
wake-up scenarios.

FUTEX_REQUEUE_CMP_PI is called by the waker
(pthread_cond_broadcast() and pthread_cond_signal()) to requeue and
possibly wake the waiting tasks. Internally, this system call is
still handled by futex_requeue (by passing requeue_pi=1).  Before
requeueing, futex_requeue() attempts to acquire the requeue target
PI futex on behalf of the top waiter.  If it can, this waiter is
woken.  futex_requeue() then proceeds to requeue the remaining
nr_wake+nr_requeue tasks to the PI futex, calling
rt_mutex_start_proxy_lock() prior to each requeue to prepare the
task as a waiter on the underlying rt_mutex.  It is possible that
the lock can be acquired at this stage as well, if so, the next
waiter is woken to finish the acquisition of the lock.

FUTEX_REQUEUE_PI accepts nr_wake and nr_requeue as arguments, but
their sum is all that really matters.  futex_requeue() will wake or
requeue up to nr_wake + nr_requeue tasks.  It will wake only as many
tasks as it can acquire the lock for, which in the majority of cases
should be 0 as good programming practice dictates that the caller of
either pthread_cond_broadcast() or pthread_cond_signal() acquire the
mutex prior to making the call. FUTEX_REQUEUE_PI requires that
nr_wake=1.  nr_requeue should be INT_MAX for broadcast and 0 for
signal.
Using gcov with the Linux kernel
================================

1. Introduction
2. Preparation
3. Customization
4. Files
5. Modules
6. Separated build and test machines
7. Troubleshooting
Appendix A: sample script: gather_on_build.sh
Appendix B: sample script: gather_on_test.sh


1. Introduction
===============

gcov profiling kernel support enables the use of GCC's coverage testing
tool gcov [1] with the Linux kernel. Coverage data of a running kernel
is exported in gcov-compatible format via the "gcov" debugfs directory.
To get coverage data for a specific file, change to the kernel build
directory and use gcov with the -o option as follows (requires root):

# cd /tmp/linux-out
# gcov -o /sys/kernel/debug/gcov/tmp/linux-out/kernel spinlock.c

This will create source code files annotated with execution counts
in the current directory. In addition, graphical gcov front-ends such
as lcov [2] can be used to automate the process of collecting data
for the entire kernel and provide coverage overviews in HTML format.

Possible uses:

* debugging (has this line been reached at all?)
* test improvement (how do I change my test to cover these lines?)
* minimizing kernel configurations (do I need this option if the
  associated code is never run?)

--

[1] http://gcc.gnu.org/onlinedocs/gcc/Gcov.html
[2] http://ltp.sourceforge.net/coverage/lcov.php


2. Preparation
==============

Configure the kernel with:

        CONFIG_DEBUG_FS=y
        CONFIG_GCOV_KERNEL=y

select the gcc's gcov format, default is autodetect based on gcc version:

        CONFIG_GCOV_FORMAT_AUTODETECT=y

and to get coverage data for the entire kernel:

        CONFIG_GCOV_PROFILE_ALL=y

Note that kernels compiled with profiling flags will be significantly
larger and run slower. Also CONFIG_GCOV_PROFILE_ALL may not be supported
on all architectures.

Profiling data will only become accessible once debugfs has been
mounted:

        mount -t debugfs none /sys/kernel/debug


3. Customization
================

To enable profiling for specific files or directories, add a line
similar to the following to the respective kernel Makefile:

        For a single file (e.g. main.o):
                GCOV_PROFILE_main.o := y

        For all files in one directory:
                GCOV_PROFILE := y

To exclude files from being profiled even when CONFIG_GCOV_PROFILE_ALL
is specified, use:

                GCOV_PROFILE_main.o := n
        and:
                GCOV_PROFILE := n

Only files which are linked to the main kernel image or are compiled as
kernel modules are supported by this mechanism.


4. Files
========

The gcov kernel support creates the following files in debugfs:

        /sys/kernel/debug/gcov
                Parent directory for all gcov-related files.

        /sys/kernel/debug/gcov/reset
                Global reset file: resets all coverage data to zero when
                written to.

        /sys/kernel/debug/gcov/path/to/compile/dir/file.gcda
                The actual gcov data file as understood by the gcov
                tool. Resets file coverage data to zero when written to.

        /sys/kernel/debug/gcov/path/to/compile/dir/file.gcno
                Symbolic link to a static data file required by the gcov
                tool. This file is generated by gcc when compiling with
                option -ftest-coverage.


5. Modules
==========

Kernel modules may contain cleanup code which is only run during
module unload time. The gcov mechanism provides a means to collect
coverage data for such code by keeping a copy of the data associated
with the unloaded module. This data remains available through debugfs.
Once the module is loaded again, the associated coverage counters are
initialized with the data from its previous instantiation.

This behavior can be deactivated by specifying the gcov_persist kernel
parameter:

        gcov_persist=0

At run-time, a user can also choose to discard data for an unloaded
module by writing to its data file or the global reset file.


6. Separated build and test machines
====================================

The gcov kernel profiling infrastructure is designed to work out-of-the
box for setups where kernels are built and run on the same machine. In
cases where the kernel runs on a separate machine, special preparations
must be made, depending on where the gcov tool is used:

a) gcov is run on the TEST machine

The gcov tool version on the test machine must be compatible with the
gcc version used for kernel build. Also the following files need to be
copied from build to test machine:

from the source tree:
  - all C source files + headers

from the build tree:
  - all C source files + headers
  - all .gcda and .gcno files
  - all links to directories

It is important to note that these files need to be placed into the
exact same file system location on the test machine as on the build
machine. If any of the path components is symbolic link, the actual
directory needs to be used instead (due to make's CURDIR handling).

b) gcov is run on the BUILD machine

The following files need to be copied after each test case from test
to build machine:

from the gcov directory in sysfs:
  - all .gcda files
  - all links to .gcno files

These files can be copied to any location on the build machine. gcov
must then be called with the -o option pointing to that directory.

Example directory setup on the build machine:

  /tmp/linux:    kernel source tree
  /tmp/out:      kernel build directory as specified by make O=
  /tmp/coverage: location of the files copied from the test machine

  [user@build] cd /tmp/out
  [user@build] gcov -o /tmp/coverage/tmp/out/init main.c


7. Troubleshooting
==================

Problem:  Compilation aborts during linker step.
Cause:    Profiling flags are specified for source files which are not
          linked to the main kernel or which are linked by a custom
          linker procedure.
Solution: Exclude affected source files from profiling by specifying
          GCOV_PROFILE := n or GCOV_PROFILE_basename.o := n in the
          corresponding Makefile.

Problem:  Files copied from sysfs appear empty or incomplete.
Cause:    Due to the way seq_file works, some tools such as cp or tar
          may not correctly copy files from sysfs.
Solution: Use 'cat' to read .gcda files and 'cp -d' to copy links.
          Alternatively use the mechanism shown in Appendix B.


Appendix A: gather_on_build.sh
==============================

Sample script to gather coverage meta files on the build machine
(see 6a):
#!/bin/bash

KSRC=$1
KOBJ=$2
DEST=$3

if [ -z "$KSRC" ] || [ -z "$KOBJ" ] || [ -z "$DEST" ]; then
  echo "Usage: $0 <ksrc directory> <kobj directory> <output.tar.gz>" >&2
  exit 1
fi

KSRC=$(cd $KSRC; printf "all:\n\t@echo \${CURDIR}\n" | make -f -)
KOBJ=$(cd $KOBJ; printf "all:\n\t@echo \${CURDIR}\n" | make -f -)

find $KSRC $KOBJ \( -name '*.gcno' -o -name '*.[ch]' -o -type l \) -a \
                 -perm /u+r,g+r | tar cfz $DEST -P -T -

if [ $? -eq 0 ] ; then
  echo "$DEST successfully created, copy to test system and unpack with:"
  echo "  tar xfz $DEST -P"
else
  echo "Could not create file $DEST"
fi


Appendix B: gather_on_test.sh
=============================

Sample script to gather coverage data files on the test machine
(see 6b):

#!/bin/bash -e

DEST=$1
GCDA=/sys/kernel/debug/gcov

if [ -z "$DEST" ] ; then
  echo "Usage: $0 <output.tar.gz>" >&2
  exit 1
fi

TEMPDIR=$(mktemp -d)
echo Collecting data..
find $GCDA -type d -exec mkdir -p $TEMPDIR/\{\} \;
find $GCDA -name '*.gcda' -exec sh -c 'cat < $0 > '$TEMPDIR'/$0' {} \;
find $GCDA -name '*.gcno' -exec sh -c 'cp -d $0 '$TEMPDIR'/$0' {} \;
tar czf $DEST -C $TEMPDIR sys
rm -rf $TEMPDIR

echo "$DEST successfully created, copy to build system and unpack with:"
echo "  tar xfz $DEST"
Notes on the change from 16-bit UIDs to 32-bit UIDs:

- kernel code MUST take into account __kernel_uid_t and __kernel_uid32_t
  when communicating between user and kernel space in an ioctl or data
  structure.

- kernel code should use uid_t and gid_t in kernel-private structures and
  code.

What's left to be done for 32-bit UIDs on all Linux architectures:

- Disk quotas have an interesting limitation that is not related to the
  maximum UID/GID. They are limited by the maximum file size on the
  underlying filesystem, because quota records are written at offsets
  corresponding to the UID in question.
  Further investigation is needed to see if the quota system can cope
  properly with huge UIDs. If it can deal with 64-bit file offsets on all 
  architectures, this should not be a problem.

- Decide whether or not to keep backwards compatibility with the system
  accounting file, or if we should break it as the comments suggest
  (currently, the old 16-bit UID and GID are still written to disk, and
  part of the former pad space is used to store separate 32-bit UID and
  GID)

- Need to validate that OS emulation calls the 16-bit UID
  compatibility syscalls, if the OS being emulated used 16-bit UIDs, or
  uses the 32-bit UID system calls properly otherwise.

  This affects at least:
	iBCS on Intel

	sparc32 emulation on sparc64
	(need to support whatever new 32-bit UID system calls are added to
	sparc32)

- Validate that all filesystems behave properly.

  At present, 32-bit UIDs _should_ work for:
	ext2
	ufs
	isofs
	nfs
	coda
	udf

  Ioctl() fixups have been made for:
	ncpfs
	smbfs

  Filesystems with simple fixups to prevent 16-bit UID wraparound:
	minix
	sysv
	qnx4

  Other filesystems have not been checked yet.

- The ncpfs and smpfs filesystems cannot presently use 32-bit UIDs in
  all ioctl()s. Some new ioctl()s have been added with 32-bit UIDs, but
  more are needed. (as well as new user<->kernel data structures)

- The ELF core dump format only supports 16-bit UIDs on arm, i386, m68k,
  sh, and sparc32. Fixing this is probably not that important, but would
  require adding a new ELF section.

- The ioctl()s used to control the in-kernel NFS server only support
  16-bit UIDs on arm, i386, m68k, sh, and sparc32.

- make sure that the UID mapping feature of AX25 networking works properly
  (it should be safe because it's always used a 32-bit integer to
  communicate between user and kernel)


Chris Wing
wingc@umich.edu

last updated: January 11, 2000
HSI - High-speed Synchronous Serial Interface

1. Introduction
~~~~~~~~~~~~~~~

High Speed Syncronous Interface (HSI) is a fullduplex, low latency protocol,
that is optimized for die-level interconnect between an Application Processor
and a Baseband chipset. It has been specified by the MIPI alliance in 2003 and
implemented by multiple vendors since then.

The HSI interface supports full duplex communication over multiple channels
(typically 8) and is capable of reaching speeds up to 200 Mbit/s.

The serial protocol uses two signals, DATA and FLAG as combined data and clock
signals and an additional READY signal for flow control. An additional WAKE
signal can be used to wakeup the chips from standby modes. The signals are
commonly prefixed by AC for signals going from the application die to the
cellular die and CA for signals going the other way around.

+------------+                                 +---------------+
|  Cellular  |                                 |  Application  |
|    Die     |                                 |      Die      |
|            | - - - - - - CAWAKE - - - - - - >|               |
|           T|------------ CADATA ------------>|R              |
|           X|------------ CAFLAG ------------>|X              |
|            |<----------- ACREADY ------------|               |
|            |                                 |               |
|            |                                 |               |
|            |< - - - - -  ACWAKE - - - - - - -|               |
|           R|<----------- ACDATA -------------|T              |
|           X|<----------- ACFLAG -------------|X              |
|            |------------ CAREADY ----------->|               |
|            |                                 |               |
|            |                                 |               |
+------------+                                 +---------------+

2. HSI Subsystem in Linux
~~~~~~~~~~~~~~~~~~~~~~~~~

In the Linux kernel the hsi subsystem is supposed to be used for HSI devices.
The hsi subsystem contains drivers for hsi controllers including support for
multi-port controllers and provides a generic API for using the HSI ports.

It also contains HSI client drivers, which make use of the generic API to
implement a protocol used on the HSI interface. These client drivers can
use an arbitrary number of channels.

3. hsi-char Device
~~~~~~~~~~~~~~~~~~

Each port automatically registers a generic client driver called hsi_char,
which provides a charecter device for userspace representing the HSI port.
It can be used to communicate via HSI from userspace. Userspace may
configure the hsi_char device using the following ioctl commands:

* HSC_RESET:
 - flush the HSI port

* HSC_SET_PM
 - enable or disable the client.

* HSC_SEND_BREAK
 - send break

* HSC_SET_RX
 - set RX configuration

* HSC_GET_RX
 - get RX configuration

* HSC_SET_TX
 - set TX configuration

* HSC_GET_TX
 - get TX configuration
Introduction:

	The hw_random framework is software that makes use of a
	special hardware feature on your CPU or motherboard,
	a Random Number Generator (RNG).  The software has two parts:
	a core providing the /dev/hw_random character device and its
	sysfs support, plus a hardware-specific driver that plugs
	into that core.

	To make the most effective use of these mechanisms, you
	should download the support software as well.  Download the
	latest version of the "rng-tools" package from the
	hw_random driver's official Web site:

		http://sourceforge.net/projects/gkernel/

	Those tools use /dev/hw_random to fill the kernel entropy pool,
	which is used internally and exported by the /dev/urandom and
	/dev/random special files.

Theory of operation:

	CHARACTER DEVICE.  Using the standard open()
	and read() system calls, you can read random data from
	the hardware RNG device.  This data is NOT CHECKED by any
	fitness tests, and could potentially be bogus (if the
	hardware is faulty or has been tampered with).  Data is only
	output if the hardware "has-data" flag is set, but nevertheless
	a security-conscious person would run fitness tests on the
	data before assuming it is truly random.

	The rng-tools package uses such tests in "rngd", and lets you
	run them by hand with a "rngtest" utility.

	/dev/hw_random is char device major 10, minor 183.

	CLASS DEVICE.  There is a /sys/class/misc/hw_random node with
	two unique attributes, "rng_available" and "rng_current".  The
	"rng_available" attribute lists the hardware-specific drivers
	available, while "rng_current" lists the one which is currently
	connected to /dev/hw_random.  If your system has more than one
	RNG available, you may change the one used by writing a name from
	the list in "rng_available" into "rng_current".

==========================================================================

	Hardware driver for Intel/AMD/VIA Random Number Generators (RNG)
	Copyright 2000,2001 Jeff Garzik <jgarzik@pobox.com>
	Copyright 2000,2001 Philipp Rumpf <prumpf@mandrakesoft.com>


About the Intel RNG hardware, from the firmware hub datasheet:

	The Firmware Hub integrates a Random Number Generator (RNG)
	using thermal noise generated from inherently random quantum
	mechanical properties of silicon. When not generating new random
	bits the RNG circuitry will enter a low power state. Intel will
	provide a binary software driver to give third party software
	access to our RNG for use as a security feature. At this time,
	the RNG is only to be used with a system in an OS-present state.

Intel RNG Driver notes:

	* FIXME: support poll(2)

	NOTE: request_mem_region was removed, for three reasons:
	1) Only one RNG is supported by this driver, 2) The location
	used by the RNG is a fixed location in MMIO-addressable memory,
	3) users with properly working BIOS e820 handling will always
	have the region in which the RNG is located reserved, so
	request_mem_region calls always fail for proper setups.
	However, for people who use mem=XX, BIOS e820 information is
	-not- in /proc/iomem, and request_mem_region(RNG_ADDR) can
	succeed.

Driver details:

	Based on:
	Intel 82802AB/82802AC Firmware Hub (FWH) Datasheet
		May 1999 Order Number: 290658-002 R

	Intel 82802 Firmware Hub: Random Number Generator
	Programmer's Reference Manual
		December 1999 Order Number: 298029-001 R

	Intel 82802 Firmware HUB Random Number Generator Driver
	Copyright (c) 2000 Matt Sottek <msottek@quiknet.com>

	Special thanks to Matt Sottek.  I did the "guts", he
	did the "brains" and all the testing.
Hardware Spinlock Framework

1. Introduction

Hardware spinlock modules provide hardware assistance for synchronization
and mutual exclusion between heterogeneous processors and those not operating
under a single, shared operating system.

For example, OMAP4 has dual Cortex-A9, dual Cortex-M3 and a C64x+ DSP,
each of which is running a different Operating System (the master, A9,
is usually running Linux and the slave processors, the M3 and the DSP,
are running some flavor of RTOS).

A generic hwspinlock framework allows platform-independent drivers to use
the hwspinlock device in order to access data structures that are shared
between remote processors, that otherwise have no alternative mechanism
to accomplish synchronization and mutual exclusion operations.

This is necessary, for example, for Inter-processor communications:
on OMAP4, cpu-intensive multimedia tasks are offloaded by the host to the
remote M3 and/or C64x+ slave processors (by an IPC subsystem called Syslink).

To achieve fast message-based communications, a minimal kernel support
is needed to deliver messages arriving from a remote processor to the
appropriate user process.

This communication is based on simple data structures that is shared between
the remote processors, and access to it is synchronized using the hwspinlock
module (remote processor directly places new messages in this shared data
structure).

A common hwspinlock interface makes it possible to have generic, platform-
independent, drivers.

2. User API

  struct hwspinlock *hwspin_lock_request(void);
   - dynamically assign an hwspinlock and return its address, or NULL
     in case an unused hwspinlock isn't available. Users of this
     API will usually want to communicate the lock's id to the remote core
     before it can be used to achieve synchronization.
     Should be called from a process context (might sleep).

  struct hwspinlock *hwspin_lock_request_specific(unsigned int id);
   - assign a specific hwspinlock id and return its address, or NULL
     if that hwspinlock is already in use. Usually board code will
     be calling this function in order to reserve specific hwspinlock
     ids for predefined purposes.
     Should be called from a process context (might sleep).

  int hwspin_lock_free(struct hwspinlock *hwlock);
   - free a previously-assigned hwspinlock; returns 0 on success, or an
     appropriate error code on failure (e.g. -EINVAL if the hwspinlock
     is already free).
     Should be called from a process context (might sleep).

  int hwspin_lock_timeout(struct hwspinlock *hwlock, unsigned int timeout);
   - lock a previously-assigned hwspinlock with a timeout limit (specified in
     msecs). If the hwspinlock is already taken, the function will busy loop
     waiting for it to be released, but give up when the timeout elapses.
     Upon a successful return from this function, preemption is disabled so
     the caller must not sleep, and is advised to release the hwspinlock as
     soon as possible, in order to minimize remote cores polling on the
     hardware interconnect.
     Returns 0 when successful and an appropriate error code otherwise (most
     notably -ETIMEDOUT if the hwspinlock is still busy after timeout msecs).
     The function will never sleep.

  int hwspin_lock_timeout_irq(struct hwspinlock *hwlock, unsigned int timeout);
   - lock a previously-assigned hwspinlock with a timeout limit (specified in
     msecs). If the hwspinlock is already taken, the function will busy loop
     waiting for it to be released, but give up when the timeout elapses.
     Upon a successful return from this function, preemption and the local
     interrupts are disabled, so the caller must not sleep, and is advised to
     release the hwspinlock as soon as possible.
     Returns 0 when successful and an appropriate error code otherwise (most
     notably -ETIMEDOUT if the hwspinlock is still busy after timeout msecs).
     The function will never sleep.

  int hwspin_lock_timeout_irqsave(struct hwspinlock *hwlock, unsigned int to,
							unsigned long *flags);
   - lock a previously-assigned hwspinlock with a timeout limit (specified in
     msecs). If the hwspinlock is already taken, the function will busy loop
     waiting for it to be released, but give up when the timeout elapses.
     Upon a successful return from this function, preemption is disabled,
     local interrupts are disabled and their previous state is saved at the
     given flags placeholder. The caller must not sleep, and is advised to
     release the hwspinlock as soon as possible.
     Returns 0 when successful and an appropriate error code otherwise (most
     notably -ETIMEDOUT if the hwspinlock is still busy after timeout msecs).
     The function will never sleep.

  int hwspin_trylock(struct hwspinlock *hwlock);
   - attempt to lock a previously-assigned hwspinlock, but immediately fail if
     it is already taken.
     Upon a successful return from this function, preemption is disabled so
     caller must not sleep, and is advised to release the hwspinlock as soon as
     possible, in order to minimize remote cores polling on the hardware
     interconnect.
     Returns 0 on success and an appropriate error code otherwise (most
     notably -EBUSY if the hwspinlock was already taken).
     The function will never sleep.

  int hwspin_trylock_irq(struct hwspinlock *hwlock);
   - attempt to lock a previously-assigned hwspinlock, but immediately fail if
     it is already taken.
     Upon a successful return from this function, preemption and the local
     interrupts are disabled so caller must not sleep, and is advised to
     release the hwspinlock as soon as possible.
     Returns 0 on success and an appropriate error code otherwise (most
     notably -EBUSY if the hwspinlock was already taken).
     The function will never sleep.

  int hwspin_trylock_irqsave(struct hwspinlock *hwlock, unsigned long *flags);
   - attempt to lock a previously-assigned hwspinlock, but immediately fail if
     it is already taken.
     Upon a successful return from this function, preemption is disabled,
     the local interrupts are disabled and their previous state is saved
     at the given flags placeholder. The caller must not sleep, and is advised
     to release the hwspinlock as soon as possible.
     Returns 0 on success and an appropriate error code otherwise (most
     notably -EBUSY if the hwspinlock was already taken).
     The function will never sleep.

  void hwspin_unlock(struct hwspinlock *hwlock);
   - unlock a previously-locked hwspinlock. Always succeed, and can be called
     from any context (the function never sleeps). Note: code should _never_
     unlock an hwspinlock which is already unlocked (there is no protection
     against this).

  void hwspin_unlock_irq(struct hwspinlock *hwlock);
   - unlock a previously-locked hwspinlock and enable local interrupts.
     The caller should _never_ unlock an hwspinlock which is already unlocked.
     Doing so is considered a bug (there is no protection against this).
     Upon a successful return from this function, preemption and local
     interrupts are enabled. This function will never sleep.

  void
  hwspin_unlock_irqrestore(struct hwspinlock *hwlock, unsigned long *flags);
   - unlock a previously-locked hwspinlock.
     The caller should _never_ unlock an hwspinlock which is already unlocked.
     Doing so is considered a bug (there is no protection against this).
     Upon a successful return from this function, preemption is reenabled,
     and the state of the local interrupts is restored to the state saved at
     the given flags. This function will never sleep.

  int hwspin_lock_get_id(struct hwspinlock *hwlock);
   - retrieve id number of a given hwspinlock. This is needed when an
     hwspinlock is dynamically assigned: before it can be used to achieve
     mutual exclusion with a remote cpu, the id number should be communicated
     to the remote task with which we want to synchronize.
     Returns the hwspinlock id number, or -EINVAL if hwlock is null.

3. Typical usage

#include <linux/hwspinlock.h>
#include <linux/err.h>

int hwspinlock_example1(void)
{
	struct hwspinlock *hwlock;
	int ret;

	/* dynamically assign a hwspinlock */
	hwlock = hwspin_lock_request();
	if (!hwlock)
		...

	id = hwspin_lock_get_id(hwlock);
	/* probably need to communicate id to a remote processor now */

	/* take the lock, spin for 1 sec if it's already taken */
	ret = hwspin_lock_timeout(hwlock, 1000);
	if (ret)
		...

	/*
	 * we took the lock, do our thing now, but do NOT sleep
	 */

	/* release the lock */
	hwspin_unlock(hwlock);

	/* free the lock */
	ret = hwspin_lock_free(hwlock);
	if (ret)
		...

	return ret;
}

int hwspinlock_example2(void)
{
	struct hwspinlock *hwlock;
	int ret;

	/*
	 * assign a specific hwspinlock id - this should be called early
	 * by board init code.
	 */
	hwlock = hwspin_lock_request_specific(PREDEFINED_LOCK_ID);
	if (!hwlock)
		...

	/* try to take it, but don't spin on it */
	ret = hwspin_trylock(hwlock);
	if (!ret) {
		pr_info("lock is already taken\n");
		return -EBUSY;
	}

	/*
	 * we took the lock, do our thing now, but do NOT sleep
	 */

	/* release the lock */
	hwspin_unlock(hwlock);

	/* free the lock */
	ret = hwspin_lock_free(hwlock);
	if (ret)
		...

	return ret;
}


4. API for implementors

  int hwspin_lock_register(struct hwspinlock_device *bank, struct device *dev,
		const struct hwspinlock_ops *ops, int base_id, int num_locks);
   - to be called from the underlying platform-specific implementation, in
     order to register a new hwspinlock device (which is usually a bank of
     numerous locks). Should be called from a process context (this function
     might sleep).
     Returns 0 on success, or appropriate error code on failure.

  int hwspin_lock_unregister(struct hwspinlock_device *bank);
   - to be called from the underlying vendor-specific implementation, in order
     to unregister an hwspinlock device (which is usually a bank of numerous
     locks).
     Should be called from a process context (this function might sleep).
     Returns the address of hwspinlock on success, or NULL on error (e.g.
     if the hwspinlock is still in use).

5. Important structs

struct hwspinlock_device is a device which usually contains a bank
of hardware locks. It is registered by the underlying hwspinlock
implementation using the hwspin_lock_register() API.

/**
 * struct hwspinlock_device - a device which usually spans numerous hwspinlocks
 * @dev: underlying device, will be used to invoke runtime PM api
 * @ops: platform-specific hwspinlock handlers
 * @base_id: id index of the first lock in this device
 * @num_locks: number of locks in this device
 * @lock: dynamically allocated array of 'struct hwspinlock'
 */
struct hwspinlock_device {
	struct device *dev;
	const struct hwspinlock_ops *ops;
	int base_id;
	int num_locks;
	struct hwspinlock lock[0];
};

struct hwspinlock_device contains an array of hwspinlock structs, each
of which represents a single hardware lock:

/**
 * struct hwspinlock - this struct represents a single hwspinlock instance
 * @bank: the hwspinlock_device structure which owns this lock
 * @lock: initialized and used by hwspinlock core
 * @priv: private data, owned by the underlying platform-specific hwspinlock drv
 */
struct hwspinlock {
	struct hwspinlock_device *bank;
	spinlock_t lock;
	void *priv;
};

When registering a bank of locks, the hwspinlock driver only needs to
set the priv members of the locks. The rest of the members are set and
initialized by the hwspinlock core itself.

6. Implementation callbacks

There are three possible callbacks defined in 'struct hwspinlock_ops':

struct hwspinlock_ops {
	int (*trylock)(struct hwspinlock *lock);
	void (*unlock)(struct hwspinlock *lock);
	void (*relax)(struct hwspinlock *lock);
};

The first two callbacks are mandatory:

The ->trylock() callback should make a single attempt to take the lock, and
return 0 on failure and 1 on success. This callback may _not_ sleep.

The ->unlock() callback releases the lock. It always succeed, and it, too,
may _not_ sleep.

The ->relax() callback is optional. It is called by hwspinlock core while
spinning on a lock, and can be used by the underlying implementation to force
a delay between two successive invocations of ->trylock(). It may _not_ sleep.
Using the initial RAM disk (initrd)
===================================

Written 1996,2000 by Werner Almesberger <werner.almesberger@epfl.ch> and
                     Hans Lermen <lermen@fgan.de>


initrd provides the capability to load a RAM disk by the boot loader.
This RAM disk can then be mounted as the root file system and programs
can be run from it. Afterwards, a new root file system can be mounted
from a different device. The previous root (from initrd) is then moved
to a directory and can be subsequently unmounted.

initrd is mainly designed to allow system startup to occur in two phases,
where the kernel comes up with a minimum set of compiled-in drivers, and
where additional modules are loaded from initrd.

This document gives a brief overview of the use of initrd. A more detailed
discussion of the boot process can be found in [1].


Operation
---------

When using initrd, the system typically boots as follows:

  1) the boot loader loads the kernel and the initial RAM disk
  2) the kernel converts initrd into a "normal" RAM disk and
     frees the memory used by initrd
  3) if the root device is not /dev/ram0, the old (deprecated)
     change_root procedure is followed. see the "Obsolete root change
     mechanism" section below.
  4) root device is mounted. if it is /dev/ram0, the initrd image is
     then mounted as root
  5) /sbin/init is executed (this can be any valid executable, including
     shell scripts; it is run with uid 0 and can do basically everything
     init can do).
  6) init mounts the "real" root file system
  7) init places the root file system at the root directory using the
     pivot_root system call
  8) init execs the /sbin/init on the new root filesystem, performing
     the usual boot sequence
  9) the initrd file system is removed

Note that changing the root directory does not involve unmounting it.
It is therefore possible to leave processes running on initrd during that
procedure. Also note that file systems mounted under initrd continue to
be accessible.


Boot command-line options
-------------------------

initrd adds the following new options:

  initrd=<path>    (e.g. LOADLIN)

    Loads the specified file as the initial RAM disk. When using LILO, you
    have to specify the RAM disk image file in /etc/lilo.conf, using the
    INITRD configuration variable.

  noinitrd

    initrd data is preserved but it is not converted to a RAM disk and
    the "normal" root file system is mounted. initrd data can be read
    from /dev/initrd. Note that the data in initrd can have any structure
    in this case and doesn't necessarily have to be a file system image.
    This option is used mainly for debugging.

    Note: /dev/initrd is read-only and it can only be used once. As soon
    as the last process has closed it, all data is freed and /dev/initrd
    can't be opened anymore.

  root=/dev/ram0

    initrd is mounted as root, and the normal boot procedure is followed,
    with the RAM disk mounted as root.

Compressed cpio images
----------------------

Recent kernels have support for populating a ramdisk from a compressed cpio
archive. On such systems, the creation of a ramdisk image doesn't need to
involve special block devices or loopbacks; you merely create a directory on
disk with the desired initrd content, cd to that directory, and run (as an
example):

find . | cpio --quiet -H newc -o | gzip -9 -n > /boot/imagefile.img

Examining the contents of an existing image file is just as simple:

mkdir /tmp/imagefile
cd /tmp/imagefile
gzip -cd /boot/imagefile.img | cpio -imd --quiet

Installation
------------

First, a directory for the initrd file system has to be created on the
"normal" root file system, e.g.

# mkdir /initrd

The name is not relevant. More details can be found on the pivot_root(2)
man page.

If the root file system is created during the boot procedure (i.e. if
you're building an install floppy), the root file system creation
procedure should create the /initrd directory.

If initrd will not be mounted in some cases, its content is still
accessible if the following device has been created:

# mknod /dev/initrd b 1 250 
# chmod 400 /dev/initrd

Second, the kernel has to be compiled with RAM disk support and with
support for the initial RAM disk enabled. Also, at least all components
needed to execute programs from initrd (e.g. executable format and file
system) must be compiled into the kernel.

Third, you have to create the RAM disk image. This is done by creating a
file system on a block device, copying files to it as needed, and then
copying the content of the block device to the initrd file. With recent
kernels, at least three types of devices are suitable for that:

 - a floppy disk (works everywhere but it's painfully slow)
 - a RAM disk (fast, but allocates physical memory)
 - a loopback device (the most elegant solution)

We'll describe the loopback device method:

 1) make sure loopback block devices are configured into the kernel
 2) create an empty file system of the appropriate size, e.g.
    # dd if=/dev/zero of=initrd bs=300k count=1
    # mke2fs -F -m0 initrd
    (if space is critical, you may want to use the Minix FS instead of Ext2)
 3) mount the file system, e.g.
    # mount -t ext2 -o loop initrd /mnt
 4) create the console device:
    # mkdir /mnt/dev
    # mknod /mnt/dev/console c 5 1
 5) copy all the files that are needed to properly use the initrd
    environment. Don't forget the most important file, /sbin/init
    Note that /sbin/init's permissions must include "x" (execute).
 6) correct operation the initrd environment can frequently be tested
    even without rebooting with the command
    # chroot /mnt /sbin/init
    This is of course limited to initrds that do not interfere with the
    general system state (e.g. by reconfiguring network interfaces,
    overwriting mounted devices, trying to start already running demons,
    etc. Note however that it is usually possible to use pivot_root in
    such a chroot'ed initrd environment.)
 7) unmount the file system
    # umount /mnt
 8) the initrd is now in the file "initrd". Optionally, it can now be
    compressed
    # gzip -9 initrd

For experimenting with initrd, you may want to take a rescue floppy and
only add a symbolic link from /sbin/init to /bin/sh. Alternatively, you
can try the experimental newlib environment [2] to create a small
initrd.

Finally, you have to boot the kernel and load initrd. Almost all Linux
boot loaders support initrd. Since the boot process is still compatible
with an older mechanism, the following boot command line parameters
have to be given:

  root=/dev/ram0 rw

(rw is only necessary if writing to the initrd file system.)

With LOADLIN, you simply execute

     LOADLIN <kernel> initrd=<disk_image>
e.g. LOADLIN C:\LINUX\BZIMAGE initrd=C:\LINUX\INITRD.GZ root=/dev/ram0 rw

With LILO, you add the option INITRD=<path> to either the global section
or to the section of the respective kernel in /etc/lilo.conf, and pass
the options using APPEND, e.g.

  image = /bzImage
    initrd = /boot/initrd.gz
    append = "root=/dev/ram0 rw"

and run /sbin/lilo

For other boot loaders, please refer to the respective documentation.

Now you can boot and enjoy using initrd.


Changing the root device
------------------------

When finished with its duties, init typically changes the root device
and proceeds with starting the Linux system on the "real" root device.

The procedure involves the following steps:
 - mounting the new root file system
 - turning it into the root file system
 - removing all accesses to the old (initrd) root file system
 - unmounting the initrd file system and de-allocating the RAM disk

Mounting the new root file system is easy: it just needs to be mounted on
a directory under the current root. Example:

# mkdir /new-root
# mount -o ro /dev/hda1 /new-root

The root change is accomplished with the pivot_root system call, which
is also available via the pivot_root utility (see pivot_root(8) man
page; pivot_root is distributed with util-linux version 2.10h or higher
[3]). pivot_root moves the current root to a directory under the new
root, and puts the new root at its place. The directory for the old root
must exist before calling pivot_root. Example:

# cd /new-root
# mkdir initrd
# pivot_root . initrd

Now, the init process may still access the old root via its
executable, shared libraries, standard input/output/error, and its
current root directory. All these references are dropped by the
following command:

# exec chroot . what-follows <dev/console >dev/console 2>&1

Where what-follows is a program under the new root, e.g. /sbin/init
If the new root file system will be used with udev and has no valid
/dev directory, udev must be initialized before invoking chroot in order
to provide /dev/console.

Note: implementation details of pivot_root may change with time. In order
to ensure compatibility, the following points should be observed:

 - before calling pivot_root, the current directory of the invoking
   process should point to the new root directory
 - use . as the first argument, and the _relative_ path of the directory
   for the old root as the second argument
 - a chroot program must be available under the old and the new root
 - chroot to the new root afterwards
 - use relative paths for dev/console in the exec command

Now, the initrd can be unmounted and the memory allocated by the RAM
disk can be freed:

# umount /initrd
# blockdev --flushbufs /dev/ram0

It is also possible to use initrd with an NFS-mounted root, see the
pivot_root(8) man page for details.


Usage scenarios
---------------

The main motivation for implementing initrd was to allow for modular
kernel configuration at system installation. The procedure would work
as follows:

  1) system boots from floppy or other media with a minimal kernel
     (e.g. support for RAM disks, initrd, a.out, and the Ext2 FS) and
     loads initrd
  2) /sbin/init determines what is needed to (1) mount the "real" root FS
     (i.e. device type, device drivers, file system) and (2) the
     distribution media (e.g. CD-ROM, network, tape, ...). This can be
     done by asking the user, by auto-probing, or by using a hybrid
     approach.
  3) /sbin/init loads the necessary kernel modules
  4) /sbin/init creates and populates the root file system (this doesn't
     have to be a very usable system yet)
  5) /sbin/init invokes pivot_root to change the root file system and
     execs - via chroot - a program that continues the installation
  6) the boot loader is installed
  7) the boot loader is configured to load an initrd with the set of
     modules that was used to bring up the system (e.g. /initrd can be
     modified, then unmounted, and finally, the image is written from
     /dev/ram0 or /dev/rd/0 to a file)
  8) now the system is bootable and additional installation tasks can be
     performed

The key role of initrd here is to re-use the configuration data during
normal system operation without requiring the use of a bloated "generic"
kernel or re-compiling or re-linking the kernel.

A second scenario is for installations where Linux runs on systems with
different hardware configurations in a single administrative domain. In
such cases, it is desirable to generate only a small set of kernels
(ideally only one) and to keep the system-specific part of configuration
information as small as possible. In this case, a common initrd could be
generated with all the necessary modules. Then, only /sbin/init or a file
read by it would have to be different.

A third scenario is more convenient recovery disks, because information
like the location of the root FS partition doesn't have to be provided at
boot time, but the system loaded from initrd can invoke a user-friendly
dialog and it can also perform some sanity checks (or even some form of
auto-detection).

Last not least, CD-ROM distributors may use it for better installation
from CD, e.g. by using a boot floppy and bootstrapping a bigger RAM disk
via initrd from CD; or by booting via a loader like LOADLIN or directly
from the CD-ROM, and loading the RAM disk from CD without need of
floppies. 


Obsolete root change mechanism
------------------------------

The following mechanism was used before the introduction of pivot_root.
Current kernels still support it, but you should _not_ rely on its
continued availability.

It works by mounting the "real" root device (i.e. the one set with rdev
in the kernel image or with root=... at the boot command line) as the
root file system when linuxrc exits. The initrd file system is then
unmounted, or, if it is still busy, moved to a directory /initrd, if
such a directory exists on the new root file system.

In order to use this mechanism, you do not have to specify the boot
command options root, init, or rw. (If specified, they will affect
the real root file system, not the initrd environment.)
  
If /proc is mounted, the "real" root device can be changed from within
linuxrc by writing the number of the new root FS device to the special
file /proc/sys/kernel/real-root-dev, e.g.

  # echo 0x301 >/proc/sys/kernel/real-root-dev

Note that the mechanism is incompatible with NFS and similar file
systems.

This old, deprecated mechanism is commonly called "change_root", while
the new, supported mechanism is called "pivot_root".


Mixed change_root and pivot_root mechanism
------------------------------------------

In case you did not want to use root=/dev/ram0 to trigger the pivot_root
mechanism, you may create both /linuxrc and /sbin/init in your initrd image.

/linuxrc would contain only the following:

#! /bin/sh
mount -n -t proc proc /proc
echo 0x0100 >/proc/sys/kernel/real-root-dev
umount -n /proc

Once linuxrc exited, the kernel would mount again your initrd as root,
this time executing /sbin/init. Again, it would be the duty of this init
to build the right environment (maybe using the root= device passed on
the cmdline) before the final execution of the real /sbin/init.


Resources
---------

[1] Almesberger, Werner; "Booting Linux: The History and the Future"
    http://www.almesberger.net/cv/papers/ols2k-9.ps.gz
[2] newlib package (experimental), with initrd example
    http://sources.redhat.com/newlib/
[3] util-linux: Miscellaneous utilities for Linux
    http://www.kernel.org/pub/linux/utils/util-linux/
Explaining the dreaded "No init found." boot hang message
=========================================================

OK, so you've got this pretty unintuitive message (currently located
in init/main.c) and are wondering what the H*** went wrong.
Some high-level reasons for failure (listed roughly in order of execution)
to load the init binary are:
A) Unable to mount root FS
B) init binary doesn't exist on rootfs
C) broken console device
D) binary exists but dependencies not available
E) binary cannot be loaded

Detailed explanations:
0) Set "debug" kernel parameter (in bootloader config file or CONFIG_CMDLINE)
   to get more detailed kernel messages.
A) make sure you have the correct root FS type
   (and root= kernel parameter points to the correct partition),
   required drivers such as storage hardware (such as SCSI or USB!)
   and filesystem (ext3, jffs2 etc.) are builtin (alternatively as modules,
   to be pre-loaded by an initrd)
C) Possibly a conflict in console= setup --> initial console unavailable.
   E.g. some serial consoles are unreliable due to serial IRQ issues (e.g.
   missing interrupt-based configuration).
   Try using a different console= device or e.g. netconsole= .
D) e.g. required library dependencies of the init binary such as
   /lib/ld-linux.so.2 missing or broken. Use readelf -d <INIT>|grep NEEDED
   to find out which libraries are required.
E) make sure the binary's architecture matches your hardware.
   E.g. i386 vs. x86_64 mismatch, or trying to load x86 on ARM hardware.
   In case you tried loading a non-binary file here (shell script?),
   you should make sure that the script specifies an interpreter in its shebang
   header line (#!/...) that is fully working (including its library
   dependencies). And before tackling scripts, better first test a simple
   non-script binary such as /bin/sh and confirm its successful execution.
   To find out more, add code to init/main.c to display kernel_execve()s
   return values.

Please extend this explanation whenever you find new failure causes
(after all loading the init binary is a CRITICAL and hard transition step
which needs to be made as painless as possible), then submit patch to LKML.
Further TODOs:
- Implement the various run_init_process() invocations via a struct array
  which can then store the kernel_execve() result value and on failure
  log it all by iterating over _all_ results (very important usability fix).
- try to make the implementation itself more helpful in general,
  e.g. by providing additional error messages at affected places.

Andreas Mohr <andi at lisas period de>
Linux IOMMU Support
===================

The architecture spec can be obtained from the below location.

http://www.intel.com/technology/virtualization/

This guide gives a quick cheat sheet for some basic understanding.

Some Keywords

DMAR - DMA remapping
DRHD - DMA Engine Reporting Structure
RMRR - Reserved memory Region Reporting Structure
ZLR  - Zero length reads from PCI devices
IOVA - IO Virtual address.

Basic stuff
-----------

ACPI enumerates and lists the different DMA engines in the platform, and
device scope relationships between PCI devices and which DMA engine  controls
them.

What is RMRR?
-------------

There are some devices the BIOS controls, for e.g USB devices to perform
PS2 emulation. The regions of memory used for these devices are marked
reserved in the e820 map. When we turn on DMA translation, DMA to those
regions will fail. Hence BIOS uses RMRR to specify these regions along with
devices that need to access these regions. OS is expected to setup
unity mappings for these regions for these devices to access these regions.

How is IOVA generated?
---------------------

Well behaved drivers call pci_map_*() calls before sending command to device
that needs to perform DMA. Once DMA is completed and mapping is no longer
required, device performs a pci_unmap_*() calls to unmap the region.

The Intel IOMMU driver allocates a virtual address per domain. Each PCIE
device has its own domain (hence protection). Devices under p2p bridges
share the virtual address with all devices under the p2p bridge due to
transaction id aliasing for p2p bridges.

IOVA generation is pretty generic. We used the same technique as vmalloc()
but these are not global address spaces, but separate for each domain.
Different DMA engines may support different number of domains.

We also allocate guard pages with each mapping, so we can attempt to catch
any overflow that might happen.


Graphics Problems?
------------------
If you encounter issues with graphics devices, you can try adding
option intel_iommu=igfx_off to turn off the integrated graphics engine.
If this fixes anything, please ensure you file a bug reporting the problem.

Some exceptions to IOVA
-----------------------
Interrupt ranges are not address translated, (0xfee00000 - 0xfeefffff).
The same is true for peer to peer transactions. Hence we reserve the
address from PCI MMIO ranges so they are not allocated for IOVA addresses.


Fault reporting
---------------
When errors are reported, the DMA engine signals via an interrupt. The fault
reason and device that caused it with fault reason is printed on console.

See below for sample.


Boot Message Sample
-------------------

Something like this gets printed indicating presence of DMAR tables
in ACPI.

ACPI: DMAR (v001 A M I  OEMDMAR  0x00000001 MSFT 0x00000097) @ 0x000000007f5b5ef0

When DMAR is being processed and initialized by ACPI, prints DMAR locations
and any RMRR's processed.

ACPI DMAR:Host address width 36
ACPI DMAR:DRHD (flags: 0x00000000)base: 0x00000000fed90000
ACPI DMAR:DRHD (flags: 0x00000000)base: 0x00000000fed91000
ACPI DMAR:DRHD (flags: 0x00000001)base: 0x00000000fed93000
ACPI DMAR:RMRR base: 0x00000000000ed000 end: 0x00000000000effff
ACPI DMAR:RMRR base: 0x000000007f600000 end: 0x000000007fffffff

When DMAR is enabled for use, you will notice..

PCI-DMA: Using DMAR IOMMU

Fault reporting
---------------

DMAR:[DMA Write] Request device [00:02.0] fault addr 6df084000
DMAR:[fault reason 05] PTE Write access is not set
DMAR:[DMA Write] Request device [00:02.0] fault addr 6df084000
DMAR:[fault reason 05] PTE Write access is not set

TBD
----

- For compatibility testing, could use unity map domain for all devices, just
  provide a 1-1 for all useful memory under a single domain for all devices.
- API for paravirt ops for abstracting functionality for VMM folks.
Intel(R) TXT Overview:
=====================

Intel's technology for safer computing, Intel(R) Trusted Execution
Technology (Intel(R) TXT), defines platform-level enhancements that
provide the building blocks for creating trusted platforms.

Intel TXT was formerly known by the code name LaGrande Technology (LT).

Intel TXT in Brief:
o  Provides dynamic root of trust for measurement (DRTM)
o  Data protection in case of improper shutdown
o  Measurement and verification of launched environment

Intel TXT is part of the vPro(TM) brand and is also available some
non-vPro systems.  It is currently available on desktop systems
based on the Q35, X38, Q45, and Q43 Express chipsets (e.g. Dell
Optiplex 755, HP dc7800, etc.) and mobile systems based on the GM45,
PM45, and GS45 Express chipsets.

For more information, see http://www.intel.com/technology/security/.
This site also has a link to the Intel TXT MLE Developers Manual,
which has been updated for the new released platforms.

Intel TXT has been presented at various events over the past few
years, some of which are:
      LinuxTAG 2008:
          http://www.linuxtag.org/2008/en/conf/events/vp-donnerstag.html
      TRUST2008:
          http://www.trust-conference.eu/downloads/Keynote-Speakers/
          3_David-Grawrock_The-Front-Door-of-Trusted-Computing.pdf
      IDF, Shanghai:
          http://www.prcidf.com.cn/index_en.html
      IDFs 2006, 2007 (I'm not sure if/where they are online)

Trusted Boot Project Overview:
=============================

Trusted Boot (tboot) is an open source, pre-kernel/VMM module that
uses Intel TXT to perform a measured and verified launch of an OS
kernel/VMM.

It is hosted on SourceForge at http://sourceforge.net/projects/tboot.
The mercurial source repo is available at http://www.bughost.org/
repos.hg/tboot.hg.

Tboot currently supports launching Xen (open source VMM/hypervisor
w/ TXT support since v3.2), and now Linux kernels.


Value Proposition for Linux or "Why should you care?"
=====================================================

While there are many products and technologies that attempt to
measure or protect the integrity of a running kernel, they all
assume the kernel is "good" to begin with.  The Integrity
Measurement Architecture (IMA) and Linux Integrity Module interface
are examples of such solutions.

To get trust in the initial kernel without using Intel TXT, a
static root of trust must be used.  This bases trust in BIOS
starting at system reset and requires measurement of all code
executed between system reset through the completion of the kernel
boot as well as data objects used by that code.  In the case of a
Linux kernel, this means all of BIOS, any option ROMs, the
bootloader and the boot config.  In practice, this is a lot of
code/data, much of which is subject to change from boot to boot
(e.g. changing NICs may change option ROMs).  Without reference
hashes, these measurement changes are difficult to assess or
confirm as benign.  This process also does not provide DMA
protection, memory configuration/alias checks and locks, crash
protection, or policy support.

By using the hardware-based root of trust that Intel TXT provides,
many of these issues can be mitigated.  Specifically: many
pre-launch components can be removed from the trust chain, DMA
protection is provided to all launched components, a large number
of platform configuration checks are performed and values locked,
protection is provided for any data in the event of an improper
shutdown, and there is support for policy-based execution/verification.
This provides a more stable measurement and a higher assurance of
system configuration and initial state than would be otherwise
possible.  Since the tboot project is open source, source code for
almost all parts of the trust chain is available (excepting SMM and
Intel-provided firmware).

How Does it Work?
=================

o  Tboot is an executable that is launched by the bootloader as
   the "kernel" (the binary the bootloader executes).
o  It performs all of the work necessary to determine if the
   platform supports Intel TXT and, if so, executes the GETSEC[SENTER]
   processor instruction that initiates the dynamic root of trust.
   -  If tboot determines that the system does not support Intel TXT
      or is not configured correctly (e.g. the SINIT AC Module was
      incorrect), it will directly launch the kernel with no changes
      to any state.
   -  Tboot will output various information about its progress to the
      terminal, serial port, and/or an in-memory log; the output
      locations can be configured with a command line switch.
o  The GETSEC[SENTER] instruction will return control to tboot and
   tboot then verifies certain aspects of the environment (e.g. TPM NV
   lock, e820 table does not have invalid entries, etc.).
o  It will wake the APs from the special sleep state the GETSEC[SENTER]
   instruction had put them in and place them into a wait-for-SIPI
   state.
   -  Because the processors will not respond to an INIT or SIPI when
      in the TXT environment, it is necessary to create a small VT-x
      guest for the APs.  When they run in this guest, they will
      simply wait for the INIT-SIPI-SIPI sequence, which will cause
      VMEXITs, and then disable VT and jump to the SIPI vector.  This
      approach seemed like a better choice than having to insert
      special code into the kernel's MP wakeup sequence.
o  Tboot then applies an (optional) user-defined launch policy to
   verify the kernel and initrd.
   -  This policy is rooted in TPM NV and is described in the tboot
      project.  The tboot project also contains code for tools to
      create and provision the policy.
   -  Policies are completely under user control and if not present
      then any kernel will be launched.
   -  Policy action is flexible and can include halting on failures
      or simply logging them and continuing.
o  Tboot adjusts the e820 table provided by the bootloader to reserve
   its own location in memory as well as to reserve certain other
   TXT-related regions.
o  As part of its launch, tboot DMA protects all of RAM (using the
   VT-d PMRs).  Thus, the kernel must be booted with 'intel_iommu=on'
   in order to remove this blanket protection and use VT-d's
   page-level protection.
o  Tboot will populate a shared page with some data about itself and
   pass this to the Linux kernel as it transfers control.
   -  The location of the shared page is passed via the boot_params
      struct as a physical address.
o  The kernel will look for the tboot shared page address and, if it
   exists, map it.
o  As one of the checks/protections provided by TXT, it makes a copy
   of the VT-d DMARs in a DMA-protected region of memory and verifies
   them for correctness.  The VT-d code will detect if the kernel was
   launched with tboot and use this copy instead of the one in the
   ACPI table.
o  At this point, tboot and TXT are out of the picture until a
   shutdown (S<n>)
o  In order to put a system into any of the sleep states after a TXT
   launch, TXT must first be exited.  This is to prevent attacks that
   attempt to crash the system to gain control on reboot and steal
   data left in memory.
   -  The kernel will perform all of its sleep preparation and
      populate the shared page with the ACPI data needed to put the
      platform in the desired sleep state.
   -  Then the kernel jumps into tboot via the vector specified in the
      shared page.
   -  Tboot will clean up the environment and disable TXT, then use the
      kernel-provided ACPI information to actually place the platform
      into the desired sleep state.
   -  In the case of S3, tboot will also register itself as the resume
      vector.  This is necessary because it must re-establish the
      measured environment upon resume.  Once the TXT environment
      has been restored, it will restore the TPM PCRs and then
      transfer control back to the kernel's S3 resume vector.
      In order to preserve system integrity across S3, the kernel
      provides tboot with a set of memory ranges (RAM and RESERVED_KERN
      in the e820 table, but not any memory that BIOS might alter over
      the S3 transition) that tboot will calculate a MAC (message
      authentication code) over and then seal with the TPM. On resume
      and once the measured environment has been re-established, tboot
      will re-calculate the MAC and verify it against the sealed value.
      Tboot's policy determines what happens if the verification fails.
      Note that the c/s 194 of tboot which has the new MAC code supports
      this.

That's pretty much it for TXT support.


Configuring the System:
======================

This code works with 32bit, 32bit PAE, and 64bit (x86_64) kernels.

In BIOS, the user must enable:  TPM, TXT, VT-x, VT-d.  Not all BIOSes
allow these to be individually enabled/disabled and the screens in
which to find them are BIOS-specific.

grub.conf needs to be modified as follows:
        title Linux 2.6.29-tip w/ tboot
          root (hd0,0)
                kernel /tboot.gz logging=serial,vga,memory
                module /vmlinuz-2.6.29-tip intel_iommu=on ro
                       root=LABEL=/ rhgb console=ttyS0,115200 3
                module /initrd-2.6.29-tip.img
                module /Q35_SINIT_17.BIN

The kernel option for enabling Intel TXT support is found under the
Security top-level menu and is called "Enable Intel(R) Trusted
Execution Technology (TXT)".  It is considered EXPERIMENTAL and
depends on the generic x86 support (to allow maximum flexibility in
kernel build options), since the tboot code will detect whether the
platform actually supports Intel TXT and thus whether any of the
kernel code is executed.

The Q35_SINIT_17.BIN file is what Intel TXT refers to as an
Authenticated Code Module.  It is specific to the chipset in the
system and can also be found on the Trusted Boot site.  It is an
(unencrypted) module signed by Intel that is used as part of the
DRTM process to verify and configure the system.  It is signed
because it operates at a higher privilege level in the system than
any other macrocode and its correct operation is critical to the
establishment of the DRTM.  The process for determining the correct
SINIT ACM for a system is documented in the SINIT-guide.txt file
that is on the tboot SourceForge site under the SINIT ACM downloads.
The io_mapping functions in linux/io-mapping.h provide an abstraction for
efficiently mapping small regions of an I/O device to the CPU. The initial
usage is to support the large graphics aperture on 32-bit processors where
ioremap_wc cannot be used to statically map the entire aperture to the CPU
as it would consume too much of the kernel address space.

A mapping object is created during driver initialization using

	struct io_mapping *io_mapping_create_wc(unsigned long base,
						unsigned long size)

		'base' is the bus address of the region to be made
		mappable, while 'size' indicates how large a mapping region to
		enable. Both are in bytes.

		This _wc variant provides a mapping which may only be used
		with the io_mapping_map_atomic_wc or io_mapping_map_wc.

With this mapping object, individual pages can be mapped either atomically
or not, depending on the necessary scheduling environment. Of course, atomic
maps are more efficient:

	void *io_mapping_map_atomic_wc(struct io_mapping *mapping,
				       unsigned long offset)

		'offset' is the offset within the defined mapping region.
		Accessing addresses beyond the region specified in the
		creation function yields undefined results. Using an offset
		which is not page aligned yields an undefined result. The
		return value points to a single page in CPU address space.

		This _wc variant returns a write-combining map to the
		page and may only be used with mappings created by
		io_mapping_create_wc

		Note that the task may not sleep while holding this page
		mapped.

	void io_mapping_unmap_atomic(void *vaddr)

		'vaddr' must be the value returned by the last
		io_mapping_map_atomic_wc call. This unmaps the specified
		page and allows the task to sleep once again.

If you need to sleep while holding the lock, you can use the non-atomic
variant, although they may be significantly slower.

	void *io_mapping_map_wc(struct io_mapping *mapping,
				unsigned long offset)

		This works like io_mapping_map_atomic_wc except it allows
		the task to sleep while holding the page mapped.

	void io_mapping_unmap(void *vaddr)

		This works like io_mapping_unmap_atomic, except it is used
		for pages mapped with io_mapping_map_wc.

At driver close time, the io_mapping object must be freed:

	void io_mapping_free(struct io_mapping *mapping)

Current Implementation:

The initial implementation of these functions uses existing mapping
mechanisms and so provides only an abstraction layer and no new
functionality.

On 64-bit processors, io_mapping_create_wc calls ioremap_wc for the whole
range, creating a permanent kernel-visible mapping to the resource. The
map_atomic and map functions add the requested offset to the base of the
virtual address returned by ioremap_wc.

On 32-bit processors with HIGHMEM defined, io_mapping_map_atomic_wc uses
kmap_atomic_pfn to map the specified page in an atomic fashion;
kmap_atomic_pfn isn't really supposed to be used with device pages, but it
provides an efficient mapping for this usage.

On 32-bit processors without HIGHMEM defined, io_mapping_map_atomic_wc and
io_mapping_map_wc both use ioremap_wc, a terribly inefficient function which
performs an IPI to inform all processors about the new mapping. This results
in a significant performance penalty.
On some platforms, so-called memory-mapped I/O is weakly ordered.  On such
platforms, driver writers are responsible for ensuring that I/O writes to
memory-mapped addresses on their device arrive in the order intended.  This is
typically done by reading a 'safe' device or bridge register, causing the I/O
chipset to flush pending writes to the device before any reads are posted.  A
driver would usually use this technique immediately prior to the exit of a
critical section of code protected by spinlocks.  This would ensure that
subsequent writes to I/O space arrived only after all prior writes (much like a
memory barrier op, mb(), only with respect to I/O).

A more concrete example from a hypothetical device driver:

        ...
CPU A:  spin_lock_irqsave(&dev_lock, flags)
CPU A:  val = readl(my_status);
CPU A:  ...
CPU A:  writel(newval, ring_ptr);
CPU A:  spin_unlock_irqrestore(&dev_lock, flags)
        ...
CPU B:  spin_lock_irqsave(&dev_lock, flags)
CPU B:  val = readl(my_status);
CPU B:  ...
CPU B:  writel(newval2, ring_ptr);
CPU B:  spin_unlock_irqrestore(&dev_lock, flags)
        ...

In the case above, the device may receive newval2 before it receives newval,
which could cause problems.  Fixing it is easy enough though:

        ...
CPU A:  spin_lock_irqsave(&dev_lock, flags)
CPU A:  val = readl(my_status);
CPU A:  ...
CPU A:  writel(newval, ring_ptr);
CPU A:  (void)readl(safe_register); /* maybe a config register? */
CPU A:  spin_unlock_irqrestore(&dev_lock, flags)
        ...
CPU B:  spin_lock_irqsave(&dev_lock, flags)
CPU B:  val = readl(my_status);
CPU B:  ...
CPU B:  writel(newval2, ring_ptr);
CPU B:  (void)readl(safe_register); /* maybe a config register? */
CPU B:  spin_unlock_irqrestore(&dev_lock, flags)

Here, the reads from safe_register will cause the I/O chipset to flush any
pending writes before actually posting the read to the chipset, preventing
possible data corruption.
I/O statistics fields
---------------

Since 2.4.20 (and some versions before, with patches), and 2.5.45,
more extensive disk statistics have been introduced to help measure disk
activity. Tools such as sar and iostat typically interpret these and do
the work for you, but in case you are interested in creating your own
tools, the fields are explained here.

In 2.4 now, the information is found as additional fields in
/proc/partitions.  In 2.6, the same information is found in two
places: one is in the file /proc/diskstats, and the other is within
the sysfs file system, which must be mounted in order to obtain
the information. Throughout this document we'll assume that sysfs
is mounted on /sys, although of course it may be mounted anywhere.
Both /proc/diskstats and sysfs use the same source for the information
and so should not differ.

Here are examples of these different formats:

2.4:
   3     0   39082680 hda 446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160
   3     1    9221278 hda1 35486 0 35496 38030 0 0 0 0 0 38030 38030


2.6 sysfs:
   446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160
   35486    38030    38030    38030

2.6 diskstats:
   3    0   hda 446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160
   3    1   hda1 35486 38030 38030 38030

On 2.4 you might execute "grep 'hda ' /proc/partitions". On 2.6, you have
a choice of "cat /sys/block/hda/stat" or "grep 'hda ' /proc/diskstats".
The advantage of one over the other is that the sysfs choice works well
if you are watching a known, small set of disks.  /proc/diskstats may
be a better choice if you are watching a large number of disks because
you'll avoid the overhead of 50, 100, or 500 or more opens/closes with
each snapshot of your disk statistics.

In 2.4, the statistics fields are those after the device name. In
the above example, the first field of statistics would be 446216.
By contrast, in 2.6 if you look at /sys/block/hda/stat, you'll
find just the eleven fields, beginning with 446216.  If you look at
/proc/diskstats, the eleven fields will be preceded by the major and
minor device numbers, and device name.  Each of these formats provides
eleven fields of statistics, each meaning exactly the same things.
All fields except field 9 are cumulative since boot.  Field 9 should
go to zero as I/Os complete; all others only increase (unless they
overflow and wrap).  Yes, these are (32-bit or 64-bit) unsigned long
(native word size) numbers, and on a very busy or long-lived system they
may wrap. Applications should be prepared to deal with that; unless
your observations are measured in large numbers of minutes or hours,
they should not wrap twice before you notice them.

Each set of stats only applies to the indicated device; if you want
system-wide stats you'll have to find all the devices and sum them all up.

Field  1 -- # of reads completed
    This is the total number of reads completed successfully.
Field  2 -- # of reads merged, field 6 -- # of writes merged
    Reads and writes which are adjacent to each other may be merged for
    efficiency.  Thus two 4K reads may become one 8K read before it is
    ultimately handed to the disk, and so it will be counted (and queued)
    as only one I/O.  This field lets you know how often this was done.
Field  3 -- # of sectors read
    This is the total number of sectors read successfully.
Field  4 -- # of milliseconds spent reading
    This is the total number of milliseconds spent by all reads (as
    measured from __make_request() to end_that_request_last()).
Field  5 -- # of writes completed
    This is the total number of writes completed successfully.
Field  6 -- # of writes merged
    See the description of field 2.
Field  7 -- # of sectors written
    This is the total number of sectors written successfully.
Field  8 -- # of milliseconds spent writing
    This is the total number of milliseconds spent by all writes (as
    measured from __make_request() to end_that_request_last()).
Field  9 -- # of I/Os currently in progress
    The only field that should go to zero. Incremented as requests are
    given to appropriate struct request_queue and decremented as they finish.
Field 10 -- # of milliseconds spent doing I/Os
    This field increases so long as field 9 is nonzero.
Field 11 -- weighted # of milliseconds spent doing I/Os
    This field is incremented at each I/O start, I/O completion, I/O
    merge, or read of these stats by the number of I/Os in progress
    (field 9) times the number of milliseconds spent doing I/O since the
    last update of this field.  This can provide an easy measure of both
    I/O completion time and the backlog that may be accumulating.


To avoid introducing performance bottlenecks, no locks are held while
modifying these counters.  This implies that minor inaccuracies may be
introduced when changes collide, so (for instance) adding up all the
read I/Os issued per partition should equal those made to the disks ...
but due to the lack of locking it may only be very close.

In 2.6, there are counters for each CPU, which make the lack of locking
almost a non-issue.  When the statistics are read, the per-CPU counters
are summed (possibly overflowing the unsigned long variable they are
summed to) and the result given to the user.  There is no convenient
user interface for accessing the per-CPU counters themselves.

Disks vs Partitions
-------------------

There were significant changes between 2.4 and 2.6 in the I/O subsystem.
As a result, some statistic information disappeared. The translation from
a disk address relative to a partition to the disk address relative to
the host disk happens much earlier.  All merges and timings now happen
at the disk level rather than at both the disk and partition level as
in 2.4.  Consequently, you'll see a different statistics output on 2.6 for
partitions from that for disks.  There are only *four* fields available
for partitions on 2.6 machines.  This is reflected in the examples above.

Field  1 -- # of reads issued
    This is the total number of reads issued to this partition.
Field  2 -- # of sectors read
    This is the total number of sectors requested to be read from this
    partition.
Field  3 -- # of writes issued
    This is the total number of writes issued to this partition.
Field  4 -- # of sectors written
    This is the total number of sectors requested to be written to
    this partition.

Note that since the address is translated to a disk-relative one, and no
record of the partition-relative address is kept, the subsequent success
or failure of the read cannot be attributed to the partition.  In other
words, the number of reads for partitions is counted slightly before time
of queuing for partitions, and at completion for whole disks.  This is
a subtle distinction that is probably uninteresting for most cases.

More significant is the error induced by counting the numbers of
reads/writes before merges for partitions and after for disks. Since a
typical workload usually contains a lot of successive and adjacent requests,
the number of reads/writes issued can be several times higher than the
number of reads/writes completed.

In 2.6.25, the full statistic set is again available for partitions and
disk and partition statistics are consistent again. Since we still don't
keep record of the partition-relative address, an operation is attributed to
the partition which contains the first sector of the request after the
eventual merges. As requests can be merged across partition, this could lead
to some (probably insignificant) inaccuracy.

Additional notes
----------------

In 2.6, sysfs is not mounted by default.  If your distribution of
Linux hasn't added it already, here's the line you'll want to add to
your /etc/fstab:

none /sys sysfs defaults 0 0


In 2.6, all disk statistics were removed from /proc/stat.  In 2.4, they
appear in both /proc/partitions and /proc/stat, although the ones in
/proc/stat take a very different format from those in /proc/partitions
(see proc(5), if your system has it.)

-- ricklind@us.ibm.com

                          The Linux IPMI Driver
			  ---------------------
			      Corey Minyard
			  <minyard@mvista.com>
			    <minyard@acm.org>

The Intelligent Platform Management Interface, or IPMI, is a
standard for controlling intelligent devices that monitor a system.
It provides for dynamic discovery of sensors in the system and the
ability to monitor the sensors and be informed when the sensor's
values change or go outside certain boundaries.  It also has a
standardized database for field-replaceable units (FRUs) and a watchdog
timer.

To use this, you need an interface to an IPMI controller in your
system (called a Baseboard Management Controller, or BMC) and
management software that can use the IPMI system.

This document describes how to use the IPMI driver for Linux.  If you
are not familiar with IPMI itself, see the web site at
http://www.intel.com/design/servers/ipmi/index.htm.  IPMI is a big
subject and I can't cover it all here!

Configuration
-------------

The Linux IPMI driver is modular, which means you have to pick several
things to have it work right depending on your hardware.  Most of
these are available in the 'Character Devices' menu then the IPMI
menu.

No matter what, you must pick 'IPMI top-level message handler' to use
IPMI.  What you do beyond that depends on your needs and hardware.

The message handler does not provide any user-level interfaces.
Kernel code (like the watchdog) can still use it.  If you need access
from userland, you need to select 'Device interface for IPMI' if you
want access through a device driver.

The driver interface depends on your hardware.  If your system
properly provides the SMBIOS info for IPMI, the driver will detect it
and just work.  If you have a board with a standard interface (These
will generally be either "KCS", "SMIC", or "BT", consult your hardware
manual), choose the 'IPMI SI handler' option.

You should generally enable ACPI on your system, as systems with IPMI
can have ACPI tables describing them.

If you have a standard interface and the board manufacturer has done
their job correctly, the IPMI controller should be automatically
detected (via ACPI or SMBIOS tables) and should just work.  Sadly,
many boards do not have this information.  The driver attempts
standard defaults, but they may not work.  If you fall into this
situation, you need to read the section below named 'The SI Driver'.

IPMI defines a standard watchdog timer.  You can enable this with the
'IPMI Watchdog Timer' config option.  If you compile the driver into
the kernel, then via a kernel command-line option you can have the
watchdog timer start as soon as it initializes.  It also have a lot
of other options, see the 'Watchdog' section below for more details.
Note that you can also have the watchdog continue to run if it is
closed (by default it is disabled on close).  Go into the 'Watchdog
Cards' menu, enable 'Watchdog Timer Support', and enable the option
'Disable watchdog shutdown on close'.

IPMI systems can often be powered off using IPMI commands.  Select
'IPMI Poweroff' to do this.  The driver will auto-detect if the system
can be powered off by IPMI.  It is safe to enable this even if your
system doesn't support this option.  This works on ATCA systems, the
Radisys CPI1 card, and any IPMI system that supports standard chassis
management commands.

If you want the driver to put an event into the event log on a panic,
enable the 'Generate a panic event to all BMCs on a panic' option.  If
you want the whole panic string put into the event log using OEM
events, enable the 'Generate OEM events containing the panic string'
option.

Basic Design
------------

The Linux IPMI driver is designed to be very modular and flexible, you
only need to take the pieces you need and you can use it in many
different ways.  Because of that, it's broken into many chunks of
code.  These chunks (by module name) are:

ipmi_msghandler - This is the central piece of software for the IPMI
system.  It handles all messages, message timing, and responses.  The
IPMI users tie into this, and the IPMI physical interfaces (called
System Management Interfaces, or SMIs) also tie in here.  This
provides the kernelland interface for IPMI, but does not provide an
interface for use by application processes.

ipmi_devintf - This provides a userland IOCTL interface for the IPMI
driver, each open file for this device ties in to the message handler
as an IPMI user.

ipmi_si - A driver for various system interfaces.  This supports KCS,
SMIC, and BT interfaces.

ipmi_watchdog - IPMI requires systems to have a very capable watchdog
timer.  This driver implements the standard Linux watchdog timer
interface on top of the IPMI message handler.

ipmi_poweroff - Some systems support the ability to be turned off via
IPMI commands.

These are all individually selectable via configuration options.

Note that the KCS-only interface has been removed.  The af_ipmi driver
is no longer supported and has been removed because it was impossible
to do 32 bit emulation on 64-bit kernels with it.

Much documentation for the interface is in the include files.  The
IPMI include files are:

net/af_ipmi.h - Contains the socket interface.

linux/ipmi.h - Contains the user interface and IOCTL interface for IPMI.

linux/ipmi_smi.h - Contains the interface for system management interfaces
(things that interface to IPMI controllers) to use.

linux/ipmi_msgdefs.h - General definitions for base IPMI messaging.


Addressing
----------

The IPMI addressing works much like IP addresses, you have an overlay
to handle the different address types.  The overlay is:

  struct ipmi_addr
  {
	int   addr_type;
	short channel;
	char  data[IPMI_MAX_ADDR_SIZE];
  };

The addr_type determines what the address really is.  The driver
currently understands two different types of addresses.

"System Interface" addresses are defined as:

  struct ipmi_system_interface_addr
  {
	int   addr_type;
	short channel;
  };

and the type is IPMI_SYSTEM_INTERFACE_ADDR_TYPE.  This is used for talking
straight to the BMC on the current card.  The channel must be
IPMI_BMC_CHANNEL.

Messages that are destined to go out on the IPMB bus use the
IPMI_IPMB_ADDR_TYPE address type.  The format is

  struct ipmi_ipmb_addr
  {
	int           addr_type;
	short         channel;
	unsigned char slave_addr;
	unsigned char lun;
  };

The "channel" here is generally zero, but some devices support more
than one channel, it corresponds to the channel as defined in the IPMI
spec.


Messages
--------

Messages are defined as:

struct ipmi_msg
{
	unsigned char netfn;
	unsigned char lun;
	unsigned char cmd;
	unsigned char *data;
	int           data_len;
};

The driver takes care of adding/stripping the header information.  The
data portion is just the data to be send (do NOT put addressing info
here) or the response.  Note that the completion code of a response is
the first item in "data", it is not stripped out because that is how
all the messages are defined in the spec (and thus makes counting the
offsets a little easier :-).

When using the IOCTL interface from userland, you must provide a block
of data for "data", fill it, and set data_len to the length of the
block of data, even when receiving messages.  Otherwise the driver
will have no place to put the message.

Messages coming up from the message handler in kernelland will come in
as:

  struct ipmi_recv_msg
  {
	struct list_head link;

	/* The type of message as defined in the "Receive Types"
           defines above. */
	int         recv_type;

	ipmi_user_t      *user;
	struct ipmi_addr addr;
	long             msgid;
	struct ipmi_msg  msg;

	/* Call this when done with the message.  It will presumably free
	   the message and do any other necessary cleanup. */
	void (*done)(struct ipmi_recv_msg *msg);

	/* Place-holder for the data, don't make any assumptions about
	   the size or existence of this, since it may change. */
	unsigned char   msg_data[IPMI_MAX_MSG_LENGTH];
  };

You should look at the receive type and handle the message
appropriately.


The Upper Layer Interface (Message Handler)
-------------------------------------------

The upper layer of the interface provides the users with a consistent
view of the IPMI interfaces.  It allows multiple SMI interfaces to be
addressed (because some boards actually have multiple BMCs on them)
and the user should not have to care what type of SMI is below them.


Creating the User

To user the message handler, you must first create a user using
ipmi_create_user.  The interface number specifies which SMI you want
to connect to, and you must supply callback functions to be called
when data comes in.  The callback function can run at interrupt level,
so be careful using the callbacks.  This also allows to you pass in a
piece of data, the handler_data, that will be passed back to you on
all calls.

Once you are done, call ipmi_destroy_user() to get rid of the user.

From userland, opening the device automatically creates a user, and
closing the device automatically destroys the user.


Messaging

To send a message from kernel-land, the ipmi_request() call does
pretty much all message handling.  Most of the parameter are
self-explanatory.  However, it takes a "msgid" parameter.  This is NOT
the sequence number of messages.  It is simply a long value that is
passed back when the response for the message is returned.  You may
use it for anything you like.

Responses come back in the function pointed to by the ipmi_recv_hndl
field of the "handler" that you passed in to ipmi_create_user().
Remember again, these may be running at interrupt level.  Remember to
look at the receive type, too.

From userland, you fill out an ipmi_req_t structure and use the
IPMICTL_SEND_COMMAND ioctl.  For incoming stuff, you can use select()
or poll() to wait for messages to come in.  However, you cannot use
read() to get them, you must call the IPMICTL_RECEIVE_MSG with the
ipmi_recv_t structure to actually get the message.  Remember that you
must supply a pointer to a block of data in the msg.data field, and
you must fill in the msg.data_len field with the size of the data.
This gives the receiver a place to actually put the message.

If the message cannot fit into the data you provide, you will get an
EMSGSIZE error and the driver will leave the data in the receive
queue.  If you want to get it and have it truncate the message, us
the IPMICTL_RECEIVE_MSG_TRUNC ioctl.

When you send a command (which is defined by the lowest-order bit of
the netfn per the IPMI spec) on the IPMB bus, the driver will
automatically assign the sequence number to the command and save the
command.  If the response is not receive in the IPMI-specified 5
seconds, it will generate a response automatically saying the command
timed out.  If an unsolicited response comes in (if it was after 5
seconds, for instance), that response will be ignored.

In kernelland, after you receive a message and are done with it, you
MUST call ipmi_free_recv_msg() on it, or you will leak messages.  Note
that you should NEVER mess with the "done" field of a message, that is
required to properly clean up the message.

Note that when sending, there is an ipmi_request_supply_msgs() call
that lets you supply the smi and receive message.  This is useful for
pieces of code that need to work even if the system is out of buffers
(the watchdog timer uses this, for instance).  You supply your own
buffer and own free routines.  This is not recommended for normal use,
though, since it is tricky to manage your own buffers.


Events and Incoming Commands

The driver takes care of polling for IPMI events and receiving
commands (commands are messages that are not responses, they are
commands that other things on the IPMB bus have sent you).  To receive
these, you must register for them, they will not automatically be sent
to you.

To receive events, you must call ipmi_set_gets_events() and set the
"val" to non-zero.  Any events that have been received by the driver
since startup will immediately be delivered to the first user that
registers for events.  After that, if multiple users are registered
for events, they will all receive all events that come in.

For receiving commands, you have to individually register commands you
want to receive.  Call ipmi_register_for_cmd() and supply the netfn
and command name for each command you want to receive.  You also
specify a bitmask of the channels you want to receive the command from
(or use IPMI_CHAN_ALL for all channels if you don't care).  Only one
user may be registered for each netfn/cmd/channel, but different users
may register for different commands, or the same command if the
channel bitmasks do not overlap.

From userland, equivalent IOCTLs are provided to do these functions.


The Lower Layer (SMI) Interface
-------------------------------

As mentioned before, multiple SMI interfaces may be registered to the
message handler, each of these is assigned an interface number when
they register with the message handler.  They are generally assigned
in the order they register, although if an SMI unregisters and then
another one registers, all bets are off.

The ipmi_smi.h defines the interface for management interfaces, see
that for more details.


The SI Driver
-------------

The SI driver allows up to 4 KCS or SMIC interfaces to be configured
in the system.  By default, scan the ACPI tables for interfaces, and
if it doesn't find any the driver will attempt to register one KCS
interface at the spec-specified I/O port 0xca2 without interrupts.
You can change this at module load time (for a module) with:

  modprobe ipmi_si.o type=<type1>,<type2>....
       ports=<port1>,<port2>... addrs=<addr1>,<addr2>...
       irqs=<irq1>,<irq2>...
       regspacings=<sp1>,<sp2>,... regsizes=<size1>,<size2>,...
       regshifts=<shift1>,<shift2>,...
       slave_addrs=<addr1>,<addr2>,...
       force_kipmid=<enable1>,<enable2>,...
       kipmid_max_busy_us=<ustime1>,<ustime2>,...
       unload_when_empty=[0|1]
       trydefaults=[0|1] trydmi=[0|1] tryacpi=[0|1]
       tryplatform=[0|1] trypci=[0|1]

Each of these except try... items is a list, the first item for the
first interface, second item for the second interface, etc.

The si_type may be either "kcs", "smic", or "bt".  If you leave it blank, it
defaults to "kcs".

If you specify addrs as non-zero for an interface, the driver will
use the memory address given as the address of the device.  This
overrides si_ports.

If you specify ports as non-zero for an interface, the driver will
use the I/O port given as the device address.

If you specify irqs as non-zero for an interface, the driver will
attempt to use the given interrupt for the device.

trydefaults sets whether the standard IPMI interface at 0xca2 and
any interfaces specified by ACPE are tried.  By default, the driver
tries it, set this value to zero to turn this off.

The other try... items disable discovery by their corresponding
names.  These are all enabled by default, set them to zero to disable
them.  The tryplatform disables openfirmware.

The next three parameters have to do with register layout.  The
registers used by the interfaces may not appear at successive
locations and they may not be in 8-bit registers.  These parameters
allow the layout of the data in the registers to be more precisely
specified.

The regspacings parameter give the number of bytes between successive
register start addresses.  For instance, if the regspacing is set to 4
and the start address is 0xca2, then the address for the second
register would be 0xca6.  This defaults to 1.

The regsizes parameter gives the size of a register, in bytes.  The
data used by IPMI is 8-bits wide, but it may be inside a larger
register.  This parameter allows the read and write type to specified.
It may be 1, 2, 4, or 8.  The default is 1.

Since the register size may be larger than 32 bits, the IPMI data may not
be in the lower 8 bits.  The regshifts parameter give the amount to shift
the data to get to the actual IPMI data.

The slave_addrs specifies the IPMI address of the local BMC.  This is
usually 0x20 and the driver defaults to that, but in case it's not, it
can be specified when the driver starts up.

The force_ipmid parameter forcefully enables (if set to 1) or disables
(if set to 0) the kernel IPMI daemon.  Normally this is auto-detected
by the driver, but systems with broken interrupts might need an enable,
or users that don't want the daemon (don't need the performance, don't
want the CPU hit) can disable it.

If unload_when_empty is set to 1, the driver will be unloaded if it
doesn't find any interfaces or all the interfaces fail to work.  The
default is one.  Setting to 0 is useful with the hotmod, but is
obviously only useful for modules.

When compiled into the kernel, the parameters can be specified on the
kernel command line as:

  ipmi_si.type=<type1>,<type2>...
       ipmi_si.ports=<port1>,<port2>... ipmi_si.addrs=<addr1>,<addr2>...
       ipmi_si.irqs=<irq1>,<irq2>... ipmi_si.trydefaults=[0|1]
       ipmi_si.regspacings=<sp1>,<sp2>,...
       ipmi_si.regsizes=<size1>,<size2>,...
       ipmi_si.regshifts=<shift1>,<shift2>,...
       ipmi_si.slave_addrs=<addr1>,<addr2>,...
       ipmi_si.force_kipmid=<enable1>,<enable2>,...
       ipmi_si.kipmid_max_busy_us=<ustime1>,<ustime2>,...

It works the same as the module parameters of the same names.

By default, the driver will attempt to detect any device specified by
ACPI, and if none of those then a KCS device at the spec-specified
0xca2.  If you want to turn this off, set the "trydefaults" option to
false.

If your IPMI interface does not support interrupts and is a KCS or
SMIC interface, the IPMI driver will start a kernel thread for the
interface to help speed things up.  This is a low-priority kernel
thread that constantly polls the IPMI driver while an IPMI operation
is in progress.  The force_kipmid module parameter will all the user to
force this thread on or off.  If you force it off and don't have
interrupts, the driver will run VERY slowly.  Don't blame me,
these interfaces suck.

Unfortunately, this thread can use a lot of CPU depending on the
interface's performance.  This can waste a lot of CPU and cause
various issues with detecting idle CPU and using extra power.  To
avoid this, the kipmid_max_busy_us sets the maximum amount of time, in
microseconds, that kipmid will spin before sleeping for a tick.  This
value sets a balance between performance and CPU waste and needs to be
tuned to your needs.  Maybe, someday, auto-tuning will be added, but
that's not a simple thing and even the auto-tuning would need to be
tuned to the user's desired performance.

The driver supports a hot add and remove of interfaces.  This way,
interfaces can be added or removed after the kernel is up and running.
This is done using /sys/modules/ipmi_si/parameters/hotmod, which is a
write-only parameter.  You write a string to this interface.  The string
has the format:
   <op1>[:op2[:op3...]]
The "op"s are:
   add|remove,kcs|bt|smic,mem|i/o,<address>[,<opt1>[,<opt2>[,...]]]
You can specify more than one interface on the line.  The "opt"s are:
   rsp=<regspacing>
   rsi=<regsize>
   rsh=<regshift>
   irq=<irq>
   ipmb=<ipmb slave addr>
and these have the same meanings as discussed above.  Note that you
can also use this on the kernel command line for a more compact format
for specifying an interface.  Note that when removing an interface,
only the first three parameters (si type, address type, and address)
are used for the comparison.  Any options are ignored for removing.


Other Pieces
------------

Get the detailed info related with the IPMI device
--------------------------------------------------

Some users need more detailed information about a device, like where
the address came from or the raw base device for the IPMI interface.
You can use the IPMI smi_watcher to catch the IPMI interfaces as they
come or go, and to grab the information, you can use the function
ipmi_get_smi_info(), which returns the following structure:

struct ipmi_smi_info {
	enum ipmi_addr_src addr_src;
	struct device *dev;
	union {
		struct {
			void *acpi_handle;
		} acpi_info;
	} addr_info;
};

Currently special info for only for SI_ACPI address sources is
returned.  Others may be added as necessary.

Note that the dev pointer is included in the above structure, and
assuming ipmi_smi_get_info returns success, you must call put_device
on the dev pointer.


Watchdog
--------

A watchdog timer is provided that implements the Linux-standard
watchdog timer interface.  It has three module parameters that can be
used to control it:

  modprobe ipmi_watchdog timeout=<t> pretimeout=<t> action=<action type>
      preaction=<preaction type> preop=<preop type> start_now=x
      nowayout=x ifnum_to_use=n

ifnum_to_use specifies which interface the watchdog timer should use.
The default is -1, which means to pick the first one registered.

The timeout is the number of seconds to the action, and the pretimeout
is the amount of seconds before the reset that the pre-timeout panic will
occur (if pretimeout is zero, then pretimeout will not be enabled).  Note
that the pretimeout is the time before the final timeout.  So if the
timeout is 50 seconds and the pretimeout is 10 seconds, then the pretimeout
will occur in 40 second (10 seconds before the timeout).

The action may be "reset", "power_cycle", or "power_off", and
specifies what to do when the timer times out, and defaults to
"reset".

The preaction may be "pre_smi" for an indication through the SMI
interface, "pre_int" for an indication through the SMI with an
interrupts, and "pre_nmi" for a NMI on a preaction.  This is how
the driver is informed of the pretimeout.

The preop may be set to "preop_none" for no operation on a pretimeout,
"preop_panic" to set the preoperation to panic, or "preop_give_data"
to provide data to read from the watchdog device when the pretimeout
occurs.  A "pre_nmi" setting CANNOT be used with "preop_give_data"
because you can't do data operations from an NMI.

When preop is set to "preop_give_data", one byte comes ready to read
on the device when the pretimeout occurs.  Select and fasync work on
the device, as well.

If start_now is set to 1, the watchdog timer will start running as
soon as the driver is loaded.

If nowayout is set to 1, the watchdog timer will not stop when the
watchdog device is closed.  The default value of nowayout is true
if the CONFIG_WATCHDOG_NOWAYOUT option is enabled, or false if not.

When compiled into the kernel, the kernel command line is available
for configuring the watchdog:

  ipmi_watchdog.timeout=<t> ipmi_watchdog.pretimeout=<t>
	ipmi_watchdog.action=<action type>
	ipmi_watchdog.preaction=<preaction type>
	ipmi_watchdog.preop=<preop type>
	ipmi_watchdog.start_now=x
	ipmi_watchdog.nowayout=x

The options are the same as the module parameter options.

The watchdog will panic and start a 120 second reset timeout if it
gets a pre-action.  During a panic or a reboot, the watchdog will
start a 120 timer if it is running to make sure the reboot occurs.

Note that if you use the NMI preaction for the watchdog, you MUST NOT
use the nmi watchdog.  There is no reasonable way to tell if an NMI
comes from the IPMI controller, so it must assume that if it gets an
otherwise unhandled NMI, it must be from IPMI and it will panic
immediately.

Once you open the watchdog timer, you must write a 'V' character to the
device to close it, or the timer will not stop.  This is a new semantic
for the driver, but makes it consistent with the rest of the watchdog
drivers in Linux.


Panic Timeouts
--------------

The OpenIPMI driver supports the ability to put semi-custom and custom
events in the system event log if a panic occurs.  if you enable the
'Generate a panic event to all BMCs on a panic' option, you will get
one event on a panic in a standard IPMI event format.  If you enable
the 'Generate OEM events containing the panic string' option, you will
also get a bunch of OEM events holding the panic string.


The field settings of the events are:
* Generator ID: 0x21 (kernel)
* EvM Rev: 0x03 (this event is formatting in IPMI 1.0 format)
* Sensor Type: 0x20 (OS critical stop sensor)
* Sensor #: The first byte of the panic string (0 if no panic string)
* Event Dir | Event Type: 0x6f (Assertion, sensor-specific event info)
* Event Data 1: 0xa1 (Runtime stop in OEM bytes 2 and 3)
* Event data 2: second byte of panic string
* Event data 3: third byte of panic string
See the IPMI spec for the details of the event layout.  This event is
always sent to the local management controller.  It will handle routing
the message to the right place

Other OEM events have the following format:
Record ID (bytes 0-1): Set by the SEL.
Record type (byte 2): 0xf0 (OEM non-timestamped)
byte 3: The slave address of the card saving the panic
byte 4: A sequence number (starting at zero)
The rest of the bytes (11 bytes) are the panic string.  If the panic string
is longer than 11 bytes, multiple messages will be sent with increasing
sequence numbers.

Because you cannot send OEM events using the standard interface, this
function will attempt to find an SEL and add the events there.  It
will first query the capabilities of the local management controller.
If it has an SEL, then they will be stored in the SEL of the local
management controller.  If not, and the local management controller is
an event generator, the event receiver from the local management
controller will be queried and the events sent to the SEL on that
device.  Otherwise, the events go nowhere since there is nowhere to
send them.


Poweroff
--------

If the poweroff capability is selected, the IPMI driver will install
a shutdown function into the standard poweroff function pointer.  This
is in the ipmi_poweroff module.  When the system requests a powerdown,
it will send the proper IPMI commands to do this.  This is supported on
several platforms.

There is a module parameter named "poweroff_powercycle" that may
either be zero (do a power down) or non-zero (do a power cycle, power
the system off, then power it on in a few seconds).  Setting
ipmi_poweroff.poweroff_control=x will do the same thing on the kernel
command line.  The parameter is also available via the proc filesystem
in /proc/sys/dev/ipmi/poweroff_powercycle.  Note that if the system
does not support power cycling, it will always do the power off.

The "ifnum_to_use" parameter specifies which interface the poweroff
code should use.  The default is -1, which means to pick the first one
registered.

Note that if you have ACPI enabled, the system will prefer using ACPI to
power off.
ChangeLog:
	Started by Ingo Molnar <mingo@redhat.com>
	Update by Max Krasnyansky <maxk@qualcomm.com>

SMP IRQ affinity

/proc/irq/IRQ#/smp_affinity and /proc/irq/IRQ#/smp_affinity_list specify
which target CPUs are permitted for a given IRQ source.  It's a bitmask
(smp_affinity) or cpu list (smp_affinity_list) of allowed CPUs.  It's not
allowed to turn off all CPUs, and if an IRQ controller does not support
IRQ affinity then the value will not change from the default of all cpus.

/proc/irq/default_smp_affinity specifies default affinity mask that applies
to all non-active IRQs. Once IRQ is allocated/activated its affinity bitmask
will be set to the default mask. It can then be changed as described above.
Default mask is 0xffffffff.

Here is an example of restricting IRQ44 (eth1) to CPU0-3 then restricting
it to CPU4-7 (this is an 8-CPU SMP box):

[root@moon 44]# cd /proc/irq/44
[root@moon 44]# cat smp_affinity
ffffffff

[root@moon 44]# echo 0f > smp_affinity
[root@moon 44]# cat smp_affinity
0000000f
[root@moon 44]# ping -f h
PING hell (195.4.7.3): 56 data bytes
...
--- hell ping statistics ---
6029 packets transmitted, 6027 packets received, 0% packet loss
round-trip min/avg/max = 0.1/0.1/0.4 ms
[root@moon 44]# cat /proc/interrupts | grep 'CPU\|44:'
           CPU0       CPU1       CPU2       CPU3      CPU4       CPU5        CPU6       CPU7
 44:       1068       1785       1785       1783         0          0           0         0    IO-APIC-level  eth1

As can be seen from the line above IRQ44 was delivered only to the first four
processors (0-3).
Now lets restrict that IRQ to CPU(4-7).

[root@moon 44]# echo f0 > smp_affinity
[root@moon 44]# cat smp_affinity
000000f0
[root@moon 44]# ping -f h
PING hell (195.4.7.3): 56 data bytes
..
--- hell ping statistics ---
2779 packets transmitted, 2777 packets received, 0% packet loss
round-trip min/avg/max = 0.1/0.5/585.4 ms
[root@moon 44]# cat /proc/interrupts |  'CPU\|44:'
           CPU0       CPU1       CPU2       CPU3      CPU4       CPU5        CPU6       CPU7
 44:       1068       1785       1785       1783      1784       1069        1070       1069   IO-APIC-level  eth1

This time around IRQ44 was delivered only to the last four processors.
i.e counters for the CPU0-3 did not change.

Here is an example of limiting that same irq (44) to cpus 1024 to 1031:

[root@moon 44]# echo 1024-1031 > smp_affinity_list
[root@moon 44]# cat smp_affinity_list
1024-1031

Note that to do this with a bitmask would require 32 bitmasks of zero
to follow the pertinent one.
irq_domain interrupt number mapping library

The current design of the Linux kernel uses a single large number
space where each separate IRQ source is assigned a different number.
This is simple when there is only one interrupt controller, but in
systems with multiple interrupt controllers the kernel must ensure
that each one gets assigned non-overlapping allocations of Linux
IRQ numbers.

The number of interrupt controllers registered as unique irqchips
show a rising tendency: for example subdrivers of different kinds
such as GPIO controllers avoid reimplementing identical callback
mechanisms as the IRQ core system by modelling their interrupt
handlers as irqchips, i.e. in effect cascading interrupt controllers.

Here the interrupt number loose all kind of correspondence to
hardware interrupt numbers: whereas in the past, IRQ numbers could
be chosen so they matched the hardware IRQ line into the root
interrupt controller (i.e. the component actually fireing the
interrupt line to the CPU) nowadays this number is just a number.

For this reason we need a mechanism to separate controller-local
interrupt numbers, called hardware irq's, from Linux IRQ numbers.

The irq_alloc_desc*() and irq_free_desc*() APIs provide allocation of
irq numbers, but they don't provide any support for reverse mapping of
the controller-local IRQ (hwirq) number into the Linux IRQ number
space.

The irq_domain library adds mapping between hwirq and IRQ numbers on
top of the irq_alloc_desc*() API.  An irq_domain to manage mapping is
preferred over interrupt controller drivers open coding their own
reverse mapping scheme.

irq_domain also implements translation from Device Tree interrupt
specifiers to hwirq numbers, and can be easily extended to support
other IRQ topology data sources.

=== irq_domain usage ===
An interrupt controller driver creates and registers an irq_domain by
calling one of the irq_domain_add_*() functions (each mapping method
has a different allocator function, more on that later).  The function
will return a pointer to the irq_domain on success.  The caller must
provide the allocator function with an irq_domain_ops structure.

In most cases, the irq_domain will begin empty without any mappings
between hwirq and IRQ numbers.  Mappings are added to the irq_domain
by calling irq_create_mapping() which accepts the irq_domain and a
hwirq number as arguments.  If a mapping for the hwirq doesn't already
exist then it will allocate a new Linux irq_desc, associate it with
the hwirq, and call the .map() callback so the driver can perform any
required hardware setup.

When an interrupt is received, irq_find_mapping() function should
be used to find the Linux IRQ number from the hwirq number.

The irq_create_mapping() function must be called *atleast once*
before any call to irq_find_mapping(), lest the descriptor will not
be allocated.

If the driver has the Linux IRQ number or the irq_data pointer, and
needs to know the associated hwirq number (such as in the irq_chip
callbacks) then it can be directly obtained from irq_data->hwirq.

=== Types of irq_domain mappings ===
There are several mechanisms available for reverse mapping from hwirq
to Linux irq, and each mechanism uses a different allocation function.
Which reverse map type should be used depends on the use case.  Each
of the reverse map types are described below:

==== Linear ====
irq_domain_add_linear()

The linear reverse map maintains a fixed size table indexed by the
hwirq number.  When a hwirq is mapped, an irq_desc is allocated for
the hwirq, and the IRQ number is stored in the table.

The Linear map is a good choice when the maximum number of hwirqs is
fixed and a relatively small number (~ < 256).  The advantages of this
map are fixed time lookup for IRQ numbers, and irq_descs are only
allocated for in-use IRQs.  The disadvantage is that the table must be
as large as the largest possible hwirq number.

The majority of drivers should use the linear map.

==== Tree ====
irq_domain_add_tree()

The irq_domain maintains a radix tree map from hwirq numbers to Linux
IRQs.  When an hwirq is mapped, an irq_desc is allocated and the
hwirq is used as the lookup key for the radix tree.

The tree map is a good choice if the hwirq number can be very large
since it doesn't need to allocate a table as large as the largest
hwirq number.  The disadvantage is that hwirq to IRQ number lookup is
dependent on how many entries are in the table.

Very few drivers should need this mapping.  At the moment, powerpc
iseries is the only user.

==== No Map ===-
irq_domain_add_nomap()

The No Map mapping is to be used when the hwirq number is
programmable in the hardware.  In this case it is best to program the
Linux IRQ number into the hardware itself so that no mapping is
required.  Calling irq_create_direct_mapping() will allocate a Linux
IRQ number and call the .map() callback so that driver can program the
Linux IRQ number into the hardware.

Most drivers cannot use this mapping.

==== Legacy ====
irq_domain_add_simple()
irq_domain_add_legacy()
irq_domain_add_legacy_isa()

The Legacy mapping is a special case for drivers that already have a
range of irq_descs allocated for the hwirqs.  It is used when the
driver cannot be immediately converted to use the linear mapping.  For
example, many embedded system board support files use a set of #defines
for IRQ numbers that are passed to struct device registrations.  In that
case the Linux IRQ numbers cannot be dynamically assigned and the legacy
mapping should be used.

The legacy map assumes a contiguous range of IRQ numbers has already
been allocated for the controller and that the IRQ number can be
calculated by adding a fixed offset to the hwirq number, and
visa-versa.  The disadvantage is that it requires the interrupt
controller to manage IRQ allocations and it requires an irq_desc to be
allocated for every hwirq, even if it is unused.

The legacy map should only be used if fixed IRQ mappings must be
supported.  For example, ISA controllers would use the legacy map for
mapping Linux IRQs 0-15 so that existing ISA drivers get the correct IRQ
numbers.

Most users of legacy mappings should use irq_domain_add_simple() which
will use a legacy domain only if an IRQ range is supplied by the
system and will otherwise use a linear domain mapping. The semantics
of this call are such that if an IRQ range is specified then
descriptors will be allocated on-the-fly for it, and if no range is
specified it will fall through to irq_domain_add_linear() which means
*no* irq descriptors will be allocated.

A typical use case for simple domains is where an irqchip provider
is supporting both dynamic and static IRQ assignments.

In order to avoid ending up in a situation where a linear domain is
used and no descriptor gets allocated it is very important to make sure
that the driver using the simple domain call irq_create_mapping()
before any irq_find_mapping() since the latter will actually work
for the static IRQ assignment case.
IRQ-flags state tracing

started by Ingo Molnar <mingo@redhat.com>

the "irq-flags tracing" feature "traces" hardirq and softirq state, in
that it gives interested subsystems an opportunity to be notified of
every hardirqs-off/hardirqs-on, softirqs-off/softirqs-on event that
happens in the kernel.

CONFIG_TRACE_IRQFLAGS_SUPPORT is needed for CONFIG_PROVE_SPIN_LOCKING
and CONFIG_PROVE_RW_LOCKING to be offered by the generic lock debugging
code. Otherwise only CONFIG_PROVE_MUTEX_LOCKING and
CONFIG_PROVE_RWSEM_LOCKING will be offered on an architecture - these
are locking APIs that are not used in IRQ context. (the one exception
for rwsems is worked around)

architecture support for this is certainly not in the "trivial"
category, because lots of lowlevel assembly code deal with irq-flags
state changes. But an architecture can be irq-flags-tracing enabled in a
rather straightforward and risk-free manner.

Architectures that want to support this need to do a couple of
code-organizational changes first:

- add and enable TRACE_IRQFLAGS_SUPPORT in their arch level Kconfig file

and then a couple of functional changes are needed as well to implement
irq-flags-tracing support:

- in lowlevel entry code add (build-conditional) calls to the
  trace_hardirqs_off()/trace_hardirqs_on() functions. The lock validator
  closely guards whether the 'real' irq-flags matches the 'virtual'
  irq-flags state, and complains loudly (and turns itself off) if the
  two do not match. Usually most of the time for arch support for
  irq-flags-tracing is spent in this state: look at the lockdep
  complaint, try to figure out the assembly code we did not cover yet,
  fix and repeat. Once the system has booted up and works without a
  lockdep complaint in the irq-flags-tracing functions arch support is
  complete.
- if the architecture has non-maskable interrupts then those need to be
  excluded from the irq-tracing [and lock validation] mechanism via
  lockdep_off()/lockdep_on().

in general there is no risk from having an incomplete irq-flags-tracing
implementation in an architecture: lockdep will detect that and will
turn itself off. I.e. the lock validator will still be reliable. There
should be no crashes due to irq-tracing bugs. (except if the assembly
changes break other code by modifying conditions or registers that
shouldn't be)

What is an IRQ?

An IRQ is an interrupt request from a device.
Currently they can come in over a pin, or over a packet.
Several devices may be connected to the same pin thus
sharing an IRQ.

An IRQ number is a kernel identifier used to talk about a hardware
interrupt source.  Typically this is an index into the global irq_desc
array, but except for what linux/interrupt.h implements the details
are architecture specific.

An IRQ number is an enumeration of the possible interrupt sources on a
machine.  Typically what is enumerated is the number of input pins on
all of the interrupt controller in the system.  In the case of ISA
what is enumerated are the 16 input pins on the two i8259 interrupt
controllers.

Architectures can assign additional meaning to the IRQ numbers, and
are encouraged to in the case  where there is any manual configuration
of the hardware involved.  The ISA IRQs are a classic example of
assigning this kind of additional meaning.
ISA Plug & Play support by Jaroslav Kysela <perex@suse.cz>
==========================================================

Interface /proc/isapnp
======================

The interface has been removed. See pnp.txt for more details.

Interface /proc/bus/isapnp
==========================

This directory allows access to ISA PnP cards and logical devices.
The regular files contain the contents of ISA PnP registers for
a logical device.
               Java(tm) Binary Kernel Support for Linux v1.03
               ----------------------------------------------

Linux beats them ALL! While all other OS's are TALKING about direct
support of Java Binaries in the OS, Linux is doing it!

You can execute Java applications and Java Applets just like any
other program after you have done the following:

1) You MUST FIRST install the Java Developers Kit for Linux.
   The Java on Linux HOWTO gives the details on getting and
   installing this. This HOWTO can be found at:

	ftp://sunsite.unc.edu/pub/Linux/docs/HOWTO/Java-HOWTO

   You should also set up a reasonable CLASSPATH environment
   variable to use Java applications that make use of any
   nonstandard classes (not included in the same directory
   as the application itself).

2) You have to compile BINFMT_MISC either as a module or into
   the kernel (CONFIG_BINFMT_MISC) and set it up properly.
   If you choose to compile it as a module, you will have
   to insert it manually with modprobe/insmod, as kmod
   cannot easily be supported with binfmt_misc. 
   Read the file 'binfmt_misc.txt' in this directory to know
   more about the configuration process.

3) Add the following configuration items to binfmt_misc
   (you should really have read binfmt_misc.txt now):
   support for Java applications:
     ':Java:M::\xca\xfe\xba\xbe::/usr/local/bin/javawrapper:'
   support for executable Jar files:
     ':ExecutableJAR:E::jar::/usr/local/bin/jarwrapper:'
   support for Java Applets:
     ':Applet:E::html::/usr/bin/appletviewer:'
   or the following, if you want to be more selective:
     ':Applet:M::<!--applet::/usr/bin/appletviewer:'

   Of course you have to fix the path names. The path/file names given in this
   document match the Debian 2.1 system. (i.e. jdk installed in /usr,
   custom wrappers from this document in /usr/local)

   Note, that for the more selective applet support you have to modify
   existing html-files to contain <!--applet--> in the first line
   ('<' has to be the first character!) to let this work!

   For the compiled Java programs you need a wrapper script like the
   following (this is because Java is broken in case of the filename
   handling), again fix the path names, both in the script and in the
   above given configuration string.

   You, too, need the little program after the script. Compile like
   gcc -O2 -o javaclassname javaclassname.c
   and stick it to /usr/local/bin.

   Both the javawrapper shellscript and the javaclassname program
   were supplied by Colin J. Watson <cjw44@cam.ac.uk>.

====================== Cut here ===================
#!/bin/bash
# /usr/local/bin/javawrapper - the wrapper for binfmt_misc/java

if [ -z "$1" ]; then
	exec 1>&2
	echo Usage: $0 class-file
	exit 1
fi

CLASS=$1
FQCLASS=`/usr/local/bin/javaclassname $1`
FQCLASSN=`echo $FQCLASS | sed -e 's/^.*\.\([^.]*\)$/\1/'`
FQCLASSP=`echo $FQCLASS | sed -e 's-\.-/-g' -e 's-^[^/]*$--' -e 's-/[^/]*$--'`

# for example:
# CLASS=Test.class
# FQCLASS=foo.bar.Test
# FQCLASSN=Test
# FQCLASSP=foo/bar

unset CLASSBASE

declare -i LINKLEVEL=0

while :; do
	if [ "`basename $CLASS .class`" == "$FQCLASSN" ]; then
		# See if this directory works straight off
		cd -L `dirname $CLASS`
		CLASSDIR=$PWD
		cd $OLDPWD
		if echo $CLASSDIR | grep -q "$FQCLASSP$"; then
			CLASSBASE=`echo $CLASSDIR | sed -e "s.$FQCLASSP$.."`
			break;
		fi
		# Try dereferencing the directory name
		cd -P `dirname $CLASS`
		CLASSDIR=$PWD
		cd $OLDPWD
		if echo $CLASSDIR | grep -q "$FQCLASSP$"; then
			CLASSBASE=`echo $CLASSDIR | sed -e "s.$FQCLASSP$.."`
			break;
		fi
		# If no other possible filename exists
		if [ ! -L $CLASS ]; then
			exec 1>&2
			echo $0:
			echo "  $CLASS should be in a" \
			     "directory tree called $FQCLASSP"
			exit 1
		fi
	fi
	if [ ! -L $CLASS ]; then break; fi
	# Go down one more level of symbolic links
	let LINKLEVEL+=1
	if [ $LINKLEVEL -gt 5 ]; then
		exec 1>&2
		echo $0:
		echo "  Too many symbolic links encountered"
		exit 1
	fi
	CLASS=`ls --color=no -l $CLASS | sed -e 's/^.* \([^ ]*\)$/\1/'`
done

if [ -z "$CLASSBASE" ]; then
	if [ -z "$FQCLASSP" ]; then
		GOODNAME=$FQCLASSN.class
	else
		GOODNAME=$FQCLASSP/$FQCLASSN.class
	fi
	exec 1>&2
	echo $0:
	echo "  $FQCLASS should be in a file called $GOODNAME"
	exit 1
fi

if ! echo $CLASSPATH | grep -q "^\(.*:\)*$CLASSBASE\(:.*\)*"; then
	# class is not in CLASSPATH, so prepend dir of class to CLASSPATH
	if [ -z "${CLASSPATH}" ] ; then
		export CLASSPATH=$CLASSBASE
	else
		export CLASSPATH=$CLASSBASE:$CLASSPATH
	fi
fi

shift
/usr/bin/java $FQCLASS "$@"
====================== Cut here ===================


====================== Cut here ===================
/* javaclassname.c
 *
 * Extracts the class name from a Java class file; intended for use in a Java
 * wrapper of the type supported by the binfmt_misc option in the Linux kernel.
 *
 * Copyright (C) 1999 Colin J. Watson <cjw44@cam.ac.uk>.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */

#include <stdlib.h>
#include <stdio.h>
#include <stdarg.h>
#include <sys/types.h>

/* From Sun's Java VM Specification, as tag entries in the constant pool. */

#define CP_UTF8 1
#define CP_INTEGER 3
#define CP_FLOAT 4
#define CP_LONG 5
#define CP_DOUBLE 6
#define CP_CLASS 7
#define CP_STRING 8
#define CP_FIELDREF 9
#define CP_METHODREF 10
#define CP_INTERFACEMETHODREF 11
#define CP_NAMEANDTYPE 12
#define CP_METHODHANDLE 15
#define CP_METHODTYPE 16
#define CP_INVOKEDYNAMIC 18

/* Define some commonly used error messages */

#define seek_error() error("%s: Cannot seek\n", program)
#define corrupt_error() error("%s: Class file corrupt\n", program)
#define eof_error() error("%s: Unexpected end of file\n", program)
#define utf8_error() error("%s: Only ASCII 1-255 supported\n", program);

char *program;

long *pool;

u_int8_t read_8(FILE *classfile);
u_int16_t read_16(FILE *classfile);
void skip_constant(FILE *classfile, u_int16_t *cur);
void error(const char *format, ...);
int main(int argc, char **argv);

/* Reads in an unsigned 8-bit integer. */
u_int8_t read_8(FILE *classfile)
{
	int b = fgetc(classfile);
	if(b == EOF)
		eof_error();
	return (u_int8_t)b;
}

/* Reads in an unsigned 16-bit integer. */
u_int16_t read_16(FILE *classfile)
{
	int b1, b2;
	b1 = fgetc(classfile);
	if(b1 == EOF)
		eof_error();
	b2 = fgetc(classfile);
	if(b2 == EOF)
		eof_error();
	return (u_int16_t)((b1 << 8) | b2);
}

/* Reads in a value from the constant pool. */
void skip_constant(FILE *classfile, u_int16_t *cur)
{
	u_int16_t len;
	int seekerr = 1;
	pool[*cur] = ftell(classfile);
	switch(read_8(classfile))
	{
	case CP_UTF8:
		len = read_16(classfile);
		seekerr = fseek(classfile, len, SEEK_CUR);
		break;
	case CP_CLASS:
	case CP_STRING:
	case CP_METHODTYPE:
		seekerr = fseek(classfile, 2, SEEK_CUR);
		break;
	case CP_METHODHANDLE:
		seekerr = fseek(classfile, 3, SEEK_CUR);
		break;
	case CP_INTEGER:
	case CP_FLOAT:
	case CP_FIELDREF:
	case CP_METHODREF:
	case CP_INTERFACEMETHODREF:
	case CP_NAMEANDTYPE:
	case CP_INVOKEDYNAMIC:
		seekerr = fseek(classfile, 4, SEEK_CUR);
		break;
	case CP_LONG:
	case CP_DOUBLE:
		seekerr = fseek(classfile, 8, SEEK_CUR);
		++(*cur);
		break;
	default:
		corrupt_error();
	}
	if(seekerr)
		seek_error();
}

void error(const char *format, ...)
{
	va_list ap;
	va_start(ap, format);
	vfprintf(stderr, format, ap);
	va_end(ap);
	exit(1);
}

int main(int argc, char **argv)
{
	FILE *classfile;
	u_int16_t cp_count, i, this_class, classinfo_ptr;
	u_int8_t length;

	program = argv[0];

	if(!argv[1])
		error("%s: Missing input file\n", program);
	classfile = fopen(argv[1], "rb");
	if(!classfile)
		error("%s: Error opening %s\n", program, argv[1]);

	if(fseek(classfile, 8, SEEK_SET))  /* skip magic and version numbers */
		seek_error();
	cp_count = read_16(classfile);
	pool = calloc(cp_count, sizeof(long));
	if(!pool)
		error("%s: Out of memory for constant pool\n", program);

	for(i = 1; i < cp_count; ++i)
		skip_constant(classfile, &i);
	if(fseek(classfile, 2, SEEK_CUR))	/* skip access flags */
		seek_error();

	this_class = read_16(classfile);
	if(this_class < 1 || this_class >= cp_count)
		corrupt_error();
	if(!pool[this_class] || pool[this_class] == -1)
		corrupt_error();
	if(fseek(classfile, pool[this_class] + 1, SEEK_SET))
		seek_error();

	classinfo_ptr = read_16(classfile);
	if(classinfo_ptr < 1 || classinfo_ptr >= cp_count)
		corrupt_error();
	if(!pool[classinfo_ptr] || pool[classinfo_ptr] == -1)
		corrupt_error();
	if(fseek(classfile, pool[classinfo_ptr] + 1, SEEK_SET))
		seek_error();

	length = read_16(classfile);
	for(i = 0; i < length; ++i)
	{
		u_int8_t x = read_8(classfile);
		if((x & 0x80) || !x)
		{
			if((x & 0xE0) == 0xC0)
			{
				u_int8_t y = read_8(classfile);
				if((y & 0xC0) == 0x80)
				{
					int c = ((x & 0x1f) << 6) + (y & 0x3f);
					if(c) putchar(c);
					else utf8_error();
				}
				else utf8_error();
			}
			else utf8_error();
		}
		else if(x == '/') putchar('.');
		else putchar(x);
	}
	putchar('\n');
	free(pool);
	fclose(classfile);
	return 0;
}
====================== Cut here ===================


====================== Cut here ===================
#!/bin/bash
# /usr/local/java/bin/jarwrapper - the wrapper for binfmt_misc/jar

java -jar $1
====================== Cut here ===================


Now simply chmod +x the .class, .jar and/or .html files you want to execute.
To add a Java program to your path best put a symbolic link to the main
.class file into /usr/bin (or another place you like) omitting the .class
extension. The directory containing the original .class file will be
added to your CLASSPATH during execution.


To test your new setup, enter in the following simple Java app, and name
it "HelloWorld.java":

	class HelloWorld {
		public static void main(String args[]) {
			System.out.println("Hello World!");
		}
	}

Now compile the application with:
	javac HelloWorld.java

Set the executable permissions of the binary file, with:
	chmod 755 HelloWorld.class

And then execute it:
	./HelloWorld.class


To execute Java Jar files, simple chmod the *.jar files to include
the execution bit, then just do
       ./Application.jar


To execute Java Applets, simple chmod the *.html files to include
the execution bit, then just do
	./Applet.html


originally by Brian A. Lantz, brian@lantz.com
heavily edited for binfmt_misc by Richard Günther
new scripts by Colin J. Watson <cjw44@cam.ac.uk>
added executable Jar file support by Kurt Huwig <kurt@iku-netz.de>

kernel-doc nano-HOWTO
=====================

How to format kernel-doc comments
---------------------------------

In order to provide embedded, 'C' friendly, easy to maintain,
but consistent and extractable documentation of the functions and
data structures in the Linux kernel, the Linux kernel has adopted
a consistent style for documenting functions and their parameters,
and structures and their members.

The format for this documentation is called the kernel-doc format.
It is documented in this Documentation/kernel-doc-nano-HOWTO.txt file.

This style embeds the documentation within the source files, using
a few simple conventions.  The scripts/kernel-doc perl script, some
SGML templates in Documentation/DocBook, and other tools understand
these conventions, and are used to extract this embedded documentation
into various documents.

In order to provide good documentation of kernel functions and data
structures, please use the following conventions to format your
kernel-doc comments in Linux kernel source.

We definitely need kernel-doc formatted documentation for functions
that are exported to loadable modules using EXPORT_SYMBOL.

We also look to provide kernel-doc formatted documentation for
functions externally visible to other kernel files (not marked
"static").

We also recommend providing kernel-doc formatted documentation
for private (file "static") routines, for consistency of kernel
source code layout.  But this is lower priority and at the
discretion of the MAINTAINER of that kernel source file.

Data structures visible in kernel include files should also be
documented using kernel-doc formatted comments.

The opening comment mark "/**" is reserved for kernel-doc comments.
Only comments so marked will be considered by the kernel-doc scripts,
and any comment so marked must be in kernel-doc format.  Do not use
"/**" to be begin a comment block unless the comment block contains
kernel-doc formatted comments.  The closing comment marker for
kernel-doc comments can be either "*/" or "**/", but "*/" is
preferred in the Linux kernel tree.

Kernel-doc comments should be placed just before the function
or data structure being described.

Example kernel-doc function comment:

/**
 * foobar() - short function description of foobar
 * @arg1:	Describe the first argument to foobar.
 * @arg2:	Describe the second argument to foobar.
 *		One can provide multiple line descriptions
 *		for arguments.
 *
 * A longer description, with more discussion of the function foobar()
 * that might be useful to those using or modifying it.  Begins with
 * empty comment line, and may include additional embedded empty
 * comment lines.
 *
 * The longer description can have multiple paragraphs.
 *
 * Return: Describe the return value of foobar.
 */

The short description following the subject can span multiple lines
and ends with an @argument description, an empty line or the end of
the comment block.

The @argument descriptions must begin on the very next line following
this opening short function description line, with no intervening
empty comment lines.

If a function parameter is "..." (varargs), it should be listed in
kernel-doc notation as:
 * @...: description

The return value, if any, should be described in a dedicated section
named "Return".

Example kernel-doc data structure comment.

/**
 * struct blah - the basic blah structure
 * @mem1:	describe the first member of struct blah
 * @mem2:	describe the second member of struct blah,
 *		perhaps with more lines and words.
 *
 * Longer description of this structure.
 */

The kernel-doc function comments describe each parameter to the
function, in order, with the @name lines.

The kernel-doc data structure comments describe each structure member
in the data structure, with the @name lines.

The longer description formatting is "reflowed", losing your line
breaks.  So presenting carefully formatted lists within these
descriptions won't work so well; derived documentation will lose
the formatting.

See the section below "How to add extractable documentation to your
source files" for more details and notes on how to format kernel-doc
comments.

Components of the kernel-doc system
-----------------------------------

Many places in the source tree have extractable documentation in the
form of block comments above functions.  The components of this system
are:

- scripts/kernel-doc

  This is a perl script that hunts for the block comments and can mark
  them up directly into DocBook, man, text, and HTML. (No, not
  texinfo.)

- Documentation/DocBook/*.tmpl

  These are SGML template files, which are normal SGML files with
  special place-holders for where the extracted documentation should
  go.

- scripts/basic/docproc.c

  This is a program for converting SGML template files into SGML
  files. When a file is referenced it is searched for symbols
  exported (EXPORT_SYMBOL), to be able to distinguish between internal
  and external functions.
  It invokes kernel-doc, giving it the list of functions that
  are to be documented.
  Additionally it is used to scan the SGML template files to locate
  all the files referenced herein. This is used to generate dependency
  information as used by make.

- Makefile

  The targets 'xmldocs', 'psdocs', 'pdfdocs', and 'htmldocs' are used
  to build XML DocBook files, PostScript files, PDF files, and html files
  in Documentation/DocBook. The older target 'sgmldocs' is equivalent
  to 'xmldocs'.

- Documentation/DocBook/Makefile

  This is where C files are associated with SGML templates.


How to extract the documentation
--------------------------------

If you just want to read the ready-made books on the various
subsystems (see Documentation/DocBook/*.tmpl), just type 'make
psdocs', or 'make pdfdocs', or 'make htmldocs', depending on your
preference.  If you would rather read a different format, you can type
'make xmldocs' and then use DocBook tools to convert
Documentation/DocBook/*.xml to a format of your choice (for example,
'db2html ...' if 'make htmldocs' was not defined).

If you want to see man pages instead, you can do this:

$ cd linux
$ scripts/kernel-doc -man $(find -name '*.c') | split-man.pl /tmp/man
$ scripts/kernel-doc -man $(find -name '*.h') | split-man.pl /tmp/man

Here is split-man.pl:

-->
#!/usr/bin/perl

if ($#ARGV < 0) {
   die "where do I put the results?\n";
}

mkdir $ARGV[0],0777;
$state = 0;
while (<STDIN>) {
    if (/^\.TH \"[^\"]*\" 9 \"([^\"]*)\"/) {
	if ($state == 1) { close OUT }
	$state = 1;
	$fn = "$ARGV[0]/$1.9";
	print STDERR "Creating $fn\n";
	open OUT, ">$fn" or die "can't open $fn: $!\n";
	print OUT $_;
    } elsif ($state != 0) {
	print OUT $_;
    }
}

close OUT;
<--

If you just want to view the documentation for one function in one
file, you can do this:

$ scripts/kernel-doc -man -function fn file | nroff -man | less

or this:

$ scripts/kernel-doc -text -function fn file


How to add extractable documentation to your source files
---------------------------------------------------------

The format of the block comment is like this:

/**
 * function_name(:)? (- short description)?
(* @parameterx(space)*: (description of parameter x)?)*
(* a blank line)?
 * (Description:)? (Description of function)?
 * (section header: (section description)? )*
(*)?*/

All "description" text can span multiple lines, although the
function_name & its short description are traditionally on a single line.
Description text may also contain blank lines (i.e., lines that contain
only a "*").

"section header:" names must be unique per function (or struct,
union, typedef, enum).

Use the section header "Return" for sections describing the return value
of a function.

Avoid putting a spurious blank line after the function name, or else the
description will be repeated!

All descriptive text is further processed, scanning for the following special
patterns, which are highlighted appropriately.

'funcname()' - function
'$ENVVAR' - environment variable
'&struct_name' - name of a structure (up to two words including 'struct')
'@parameter' - name of a parameter
'%CONST' - name of a constant.

NOTE 1:  The multi-line descriptive text you provide does *not* recognize
line breaks, so if you try to format some text nicely, as in:

  Return:
    0 - cool
    1 - invalid arg
    2 - out of memory

this will all run together and produce:

  Return: 0 - cool 1 - invalid arg 2 - out of memory

NOTE 2:  If the descriptive text you provide has lines that begin with
some phrase followed by a colon, each of those phrases will be taken as
a new section heading, which means you should similarly try to avoid text
like:

  Return:
    0: cool
    1: invalid arg
    2: out of memory

every line of which would start a new section.  Again, probably not
what you were after.

Take a look around the source tree for examples.


kernel-doc for structs, unions, enums, and typedefs
---------------------------------------------------

Beside functions you can also write documentation for structs, unions,
enums and typedefs. Instead of the function name you must write the name
of the declaration;  the struct/union/enum/typedef must always precede
the name. Nesting of declarations is not supported.
Use the argument mechanism to document members or constants.

Inside a struct description, you can use the "private:" and "public:"
comment tags.  Structure fields that are inside a "private:" area
are not listed in the generated output documentation.  The "private:"
and "public:" tags must begin immediately following a "/*" comment
marker.  They may optionally include comments between the ":" and the
ending "*/" marker.

Example:

/**
 * struct my_struct - short description
 * @a: first member
 * @b: second member
 *
 * Longer description
 */
struct my_struct {
    int a;
    int b;
/* private: internal use only */
    int c;
};


Including documentation blocks in source files
----------------------------------------------

To facilitate having source code and comments close together, you can
include kernel-doc documentation blocks that are free-form comments
instead of being kernel-doc for functions, structures, unions,
enums, or typedefs.  This could be used for something like a
theory of operation for a driver or library code, for example.

This is done by using a DOC: section keyword with a section title.  E.g.:

/**
 * DOC: Theory of Operation
 *
 * The whizbang foobar is a dilly of a gizmo.  It can do whatever you
 * want it to do, at any time.  It reads your mind.  Here's how it works.
 *
 * foo bar splat
 *
 * The only drawback to this gizmo is that is can sometimes damage
 * hardware, software, or its subject(s).
 */

DOC: sections are used in SGML templates files as indicated below.


How to make new SGML template files
-----------------------------------

SGML template files (*.tmpl) are like normal SGML files, except that
they can contain escape sequences where extracted documentation should
be inserted.

!E<filename> is replaced by the documentation, in <filename>, for
functions that are exported using EXPORT_SYMBOL: the function list is
collected from files listed in Documentation/DocBook/Makefile.

!I<filename> is replaced by the documentation for functions that are
_not_ exported using EXPORT_SYMBOL.

!D<filename> is used to name additional files to search for functions
exported using EXPORT_SYMBOL.

!F<filename> <function [functions...]> is replaced by the
documentation, in <filename>, for the functions listed.

!P<filename> <section title> is replaced by the contents of the DOC:
section titled <section title> from <filename>.
Spaces are allowed in <section title>; do not quote the <section title>.

!C<filename> is replaced by nothing, but makes the tools check that
all DOC: sections and documented functions, symbols, etc. are used.
This makes sense to use when you use !F/!P only and want to verify
that all documentation is included.

Tim.
*/ <twaugh@redhat.com>

    Index of Documentation for People Interested in Writing and/or

                   Understanding the Linux Kernel.

          Juan-Mariano de Goyeneche <jmseyas@dit.upm.es>

/*
 * The latest version of this document may be found at:
 *   http://www.dit.upm.es/~jmseyas/linux/kernel/hackers-docs.html
 */

   The need for a document like this one became apparent in the
   linux-kernel mailing list as the same questions, asking for pointers
   to information, appeared again and again.
   
   Fortunately, as more and more people get to GNU/Linux, more and more
   get interested in the Kernel. But reading the sources is not always
   enough. It is easy to understand the code, but miss the concepts, the
   philosophy and design decisions behind this code.
   
   Unfortunately, not many documents are available for beginners to
   start. And, even if they exist, there was no "well-known" place which
   kept track of them. These lines try to cover this lack. All documents
   available on line known by the author are listed, while some reference
   books are also mentioned.
   
   PLEASE, if you know any paper not listed here or write a new document,
   send me an e-mail, and I'll include a reference to it here. Any
   corrections, ideas or comments are also welcomed.
   
   The papers that follow are listed in no particular order. All are
   cataloged with the following fields: the document's "Title", the
   "Author"/s, the "URL" where they can be found, some "Keywords" helpful
   when searching for specific topics, and a brief "Description" of the
   Document.
   
   Enjoy!
   
     ON-LINE DOCS:
       
     * Title: "Linux Device Drivers, Third Edition"
       Author: Jonathan Corbet, Alessandro Rubini, Greg Kroah-Hartman
       URL: http://lwn.net/Kernel/LDD3/
       Description: A 600-page book covering the (2.6.10) driver
       programming API and kernel hacking in general.  Available under the
       Creative Commons Attribution-ShareAlike 2.0 license.

     * Title: "The Linux Kernel"
       Author: David A. Rusling.
       URL: http://www.tldp.org/LDP/tlk/tlk.html
       Keywords: everything!, book.
       Description: On line, 200 pages book describing most aspects of
       the Linux Kernel. Probably, the first reference for beginners.
       Lots of illustrations explaining data structures use and
       relationships in the purest Richard W. Stevens' style. Contents:
       "1.-Hardware Basics, 2.-Software Basics, 3.-Memory Management,
       4.-Processes, 5.-Interprocess Communication Mechanisms, 6.-PCI,
       7.-Interrupts and Interrupt Handling, 8.-Device Drivers, 9.-The
       File system, 10.-Networks, 11.-Kernel Mechanisms, 12.-Modules,
       13.-The Linux Kernel Sources, A.-Linux Data Structures, B.-The
       Alpha AXP Processor, C.-Useful Web and FTP Sites, D.-The GNU
       General Public License, Glossary". In short: a must have.

     * Title: "Linux Device Drivers, 2nd Edition"
       Author: Alessandro Rubini and Jonathan Corbet.
       URL: http://www.xml.com/ldd/chapter/book/index.html
       Keywords: device drivers, modules, debugging, memory, hardware,
       interrupt handling, char drivers, block drivers, kmod, mmap, DMA,
       buses.
       Description: O'Reilly's popular book, now also on-line under the
       GNU Free Documentation License.
       Notes: You can also buy it in paper-form from O'Reilly. See below
       under BOOKS (Not on-line).

     * Title: "Conceptual Architecture of the Linux Kernel"
       Author: Ivan T. Bowman.
       URL: http://plg.uwaterloo.ca/
       Keywords: conceptual software architecture, extracted design,
       reverse engineering, system structure.
       Description: Conceptual software architecture of the Linux kernel,
       automatically extracted from the source code. Very detailed. Good
       figures. Gives good overall kernel understanding.

     * Title: "Concrete Architecture of the Linux Kernel"
       Author: Ivan T. Bowman, Saheem Siddiqi, and Meyer C. Tanuan.
       URL: http://plg.uwaterloo.ca/
       Keywords: concrete architecture, extracted design, reverse
       engineering, system structure, dependencies.
       Description: Concrete architecture of the Linux kernel,
       automatically extracted from the source code. Very detailed. Good
       figures. Gives good overall kernel understanding. This papers
       focus on lower details than its predecessor (files, variables...).

     * Title: "Linux as a Case Study: Its Extracted Software
       Architecture"
       Author: Ivan T. Bowman, Richard C. Holt and Neil V. Brewster.
       URL: http://plg.uwaterloo.ca/
       Keywords: software architecture, architecture recovery,
       redocumentation.
       Description: Paper appeared at ICSE'99, Los Angeles, May 16-22,
       1999. A mixture of the previous two documents from the same
       author.

     * Title: "Overview of the Virtual File System"
       Author: Richard Gooch.
       URL: http://www.mjmwired.net/kernel/Documentation/filesystems/vfs.txt
       Keywords: VFS, File System, mounting filesystems, opening files,
       dentries, dcache.
       Description: Brief introduction to the Linux Virtual File System.
       What is it, how it works, operations taken when opening a file or
       mounting a file system and description of important data
       structures explaining the purpose of each of their entries.

     * Title: "The Linux RAID-1, 4, 5 Code"
       Author: Ingo Molnar, Gadi Oxman and Miguel de Icaza.
       URL: http://www.linuxjournal.com/article.php?sid=2391
       Keywords: RAID, MD driver.
       Description: Linux Journal Kernel Korner article. Here is its
       abstract: "A description of the implementation of the RAID-1,
       RAID-4 and RAID-5 personalities of the MD device driver in the
       Linux kernel, providing users with high performance and reliable,
       secondary-storage capability using software".

     * Title: "Dynamic Kernels: Modularized Device Drivers"
       Author: Alessandro Rubini.
       URL: http://www.linuxjournal.com/article.php?sid=1219
       Keywords: device driver, module, loading/unloading modules,
       allocating resources.
       Description: Linux Journal Kernel Korner article. Here is its
       abstract: "This is the first of a series of four articles
       co-authored by Alessandro Rubini and Georg Zezchwitz which present
       a practical approach to writing Linux device drivers as kernel
       loadable modules. This installment presents an introduction to the
       topic, preparing the reader to understand next month's
       installment".

     * Title: "Dynamic Kernels: Discovery"
       Author: Alessandro Rubini.
       URL: http://www.linuxjournal.com/article.php?sid=1220
       Keywords: character driver, init_module, clean_up module,
       autodetection, mayor number, minor number, file operations,
       open(), close().
       Description: Linux Journal Kernel Korner article. Here is its
       abstract: "This article, the second of four, introduces part of
       the actual code to create custom module implementing a character
       device driver. It describes the code for module initialization and
       cleanup, as well as the open() and close() system calls".

     * Title: "The Devil's in the Details"
       Author: Georg v. Zezschwitz and Alessandro Rubini.
       URL: http://www.linuxjournal.com/article.php?sid=1221
       Keywords: read(), write(), select(), ioctl(), blocking/non
       blocking mode, interrupt handler.
       Description: Linux Journal Kernel Korner article. Here is its
       abstract: "This article, the third of four on writing character
       device drivers, introduces concepts of reading, writing, and using
       ioctl-calls".

     * Title: "Dissecting Interrupts and Browsing DMA"
       Author: Alessandro Rubini and Georg v. Zezschwitz.
       URL: http://www.linuxjournal.com/article.php?sid=1222
       Keywords: interrupts, irqs, DMA, bottom halves, task queues.
       Description: Linux Journal Kernel Korner article. Here is its
       abstract: "This is the fourth in a series of articles about
       writing character device drivers as loadable kernel modules. This
       month, we further investigate the field of interrupt handling.
       Though it is conceptually simple, practical limitations and
       constraints make this an ``interesting'' part of device driver
       writing, and several different facilities have been provided for
       different situations. We also investigate the complex topic of
       DMA".

     * Title: "Device Drivers Concluded"
       Author: Georg v. Zezschwitz.
       URL: http://www.linuxjournal.com/article.php?sid=1287
       Keywords: address spaces, pages, pagination, page management,
       demand loading, swapping, memory protection, memory mapping, mmap,
       virtual memory areas (VMAs), vremap, PCI.
       Description: Finally, the above turned out into a five articles
       series. This latest one's introduction reads: "This is the last of
       five articles about character device drivers. In this final
       section, Georg deals with memory mapping devices, beginning with
       an overall description of the Linux memory management concepts".

     * Title: "Network Buffers And Memory Management"
       Author: Alan Cox.
       URL: http://www.linuxjournal.com/article.php?sid=1312
       Keywords: sk_buffs, network devices, protocol/link layer
       variables, network devices flags, transmit, receive,
       configuration, multicast.
       Description: Linux Journal Kernel Korner. Here is the abstract:
       "Writing a network device driver for Linux is fundamentally
       simple---most of the complexity (other than talking to the
       hardware) involves managing network packets in memory".
       
     * Title: "Writing Linux Device Drivers"
       Author: Michael K. Johnson.
       URL: http://users.evitech.fi/~tk/rtos/writing_linux_device_d.html
       Keywords: files, VFS, file operations, kernel interface, character
       vs block devices, I/O access, hardware interrupts, DMA, access to
       user memory, memory allocation, timers.
       Description: Introductory 50-minutes (sic) tutorial on writing
       device drivers. 12 pages written by the same author of the "Kernel
       Hackers' Guide" which give a very good overview of the topic.
       
     * Title: "The Venus kernel interface"
       Author: Peter J. Braam.
       URL:
       http://www.coda.cs.cmu.edu/doc/html/kernel-venus-protocol.html
       Keywords: coda, filesystem, venus, cache manager.
       Description: "This document describes the communication between
       Venus and kernel level file system code needed for the operation
       of the Coda filesystem. This version document is meant to describe
       the current interface (version 1.0) as well as improvements we
       envisage".

     * Title: "Programming PCI-Devices under Linux"
       Author: Claus Schroeter.
       URL:
       ftp://ftp.llp.fu-berlin.de/pub/linux/LINUX-LAB/whitepapers/pcip.ps.gz
       Keywords: PCI, device, busmastering.
       Description: 6 pages tutorial on PCI programming under Linux.
       Gives the basic concepts on the architecture of the PCI subsystem,
       as long as basic functions and macros to read/write the devices
       and perform busmastering.

     * Title: "Writing Character Device Driver for Linux"
       Author: R. Baruch and C. Schroeter.
       URL:
       ftp://ftp.llp.fu-berlin.de/pub/linux/LINUX-LAB/whitepapers/drivers.ps.gz
       Keywords: character device drivers, I/O, signals, DMA, accessing
       ports in user space, kernel environment.
       Description: 68 pages paper on writing character drivers. A little
       bit old (1.993, 1.994) although still useful.

     * Title: "Design and Implementation of the Second Extended
       Filesystem"
       Author: Rémy Card, Theodore Ts'o, Stephen Tweedie.
       URL: http://web.mit.edu/tytso/www/linux/ext2intro.html
       Keywords: ext2, linux fs history, inode, directory, link, devices,
       VFS, physical structure, performance, benchmarks, ext2fs library,
       ext2fs tools, e2fsck.
       Description: Paper written by three of the top ext2 hackers.
       Covers Linux filesystems history, ext2 motivation, ext2 features,
       design, physical structure on disk, performance, benchmarks,
       e2fsck's passes description... A must read!
       Notes: This paper was first published in the Proceedings of the
       First Dutch International Symposium on Linux, ISBN 90-367-0385-9.

     * Title: "Analysis of the Ext2fs structure"
       Author: Louis-Dominique Dubeau.
       URL: http://www.nondot.org/sabre/os/files/FileSystems/ext2fs/
       Keywords: ext2, filesystem, ext2fs.
       Description: Description of ext2's blocks, directories, inodes,
       bitmaps, invariants...

     * Title: "Journaling the Linux ext2fs Filesystem"
       Author: Stephen C. Tweedie.
       URL:
       ftp://ftp.uk.linux.org/pub/linux/sct/fs/jfs/journal-design.ps.gz
       Keywords: ext3, journaling.
       Description: Excellent 8-pages paper explaining the journaling
       capabilities added to ext2 by the author, showing different
       problems faced and the alternatives chosen.

     * Title: "Kernel API changes from 2.0 to 2.2"
       Author: Richard Gooch.
       URL:
       http://www.linuxhq.com/guides/LKMPG/node28.html 
       Keywords: 2.2, changes.
       Description: Kernel functions/structures/variables which changed
       from 2.0.x to 2.2.x.

     * Title: "Kernel API changes from 2.2 to 2.4"
       Author: Richard Gooch.
       Keywords: 2.4, changes.
       Description: Kernel functions/structures/variables which changed
       from 2.2.x to 2.4.x.
       
     * Title: "Linux Kernel Module Programming Guide"
       Author: Ori Pomerantz.
       URL: http://tldp.org/LDP/lkmpg/2.6/html/index.html
       Keywords: modules, GPL book, /proc, ioctls, system calls,
       interrupt handlers .
       Description: Very nice 92 pages GPL book on the topic of modules
       programming. Lots of examples.
       
     * Title: "I/O Event Handling Under Linux"
       Author: Richard Gooch.
       Keywords: IO, I/O, select(2), poll(2), FDs, aio_read(2), readiness
       event queues.
       Description: From the Introduction: "I/O Event handling is about
       how your Operating System allows you to manage a large number of
       open files (file descriptors in UNIX/POSIX, or FDs) in your
       application. You want the OS to notify you when FDs become active
       (have data ready to be read or are ready for writing). Ideally you
       want a mechanism that is scalable. This means a large number of
       inactive FDs cost very little in memory and CPU time to manage".
       
     * Title: "The Kernel Hacking HOWTO"
       Author: Various Talented People, and Rusty.
       Location: in kernel tree, Documentation/DocBook/kernel-hacking.tmpl
       (must be built as "make {htmldocs | psdocs | pdfdocs})
       Keywords: HOWTO, kernel contexts, deadlock, locking, modules,
       symbols, return conventions.
       Description: From the Introduction: "Please understand that I
       never wanted to write this document, being grossly underqualified,
       but I always wanted to read it, and this was the only way. I
       simply explain some best practices, and give reading entry-points
       into the kernel sources. I avoid implementation details: that's
       what the code is for, and I ignore whole tracts of useful
       routines. This document assumes familiarity with C, and an
       understanding of what the kernel is, and how it is used. It was
       originally written for the 2.3 kernels, but nearly all of it
       applies to 2.2 too; 2.0 is slightly different".
       
     * Title: "Writing an ALSA Driver"
       Author: Takashi Iwai <tiwai@suse.de>
       URL: http://www.alsa-project.org/~iwai/writing-an-alsa-driver/index.html
       Keywords: ALSA, sound, soundcard, driver, lowlevel, hardware.
       Description: Advanced Linux Sound Architecture for developers,
       both at kernel and user-level sides. ALSA is the Linux kernel
       sound architecture in the 2.6 kernel version.
       
     * Title: "Programming Guide for Linux USB Device Drivers"
       Author: Detlef Fliegl.
       URL: http://usb.in.tum.de/usbdoc/
       Keywords: USB, universal serial bus.
       Description: A must-read. From the Preface: "This document should
       give detailed information about the current state of the USB
       subsystem and its API for USB device drivers. The first section
       will deal with the basics of USB devices. You will learn about
       different types of devices and their properties. Going into detail
       you will see how USB devices communicate on the bus. The second
       section gives an overview of the Linux USB subsystem [2] and the
       device driver framework. Then the API and its data structures will
       be explained step by step. The last section of this document
       contains a reference of all API calls and their return codes".
       Notes: Beware: the main page states: "This document may not be
       published, printed or used in excerpts without explicit permission
       of the author". Fortunately, it may still be read...

     * Title: "Linux Kernel Mailing List Glossary"
       Author: various
       URL: http://kernelnewbies.org/glossary/
       Keywords: glossary, terms, linux-kernel.
       Description: From the introduction: "This glossary is intended as
       a brief description of some of the acronyms and terms you may hear
       during discussion of the Linux kernel".
       
     * Title: "Linux Kernel Locking HOWTO"
       Author: Various Talented People, and Rusty.
       Location: in kernel tree, Documentation/DocBook/kernel-locking.tmpl
       (must be built as "make {htmldocs | psdocs | pdfdocs})
       Keywords: locks, locking, spinlock, semaphore, atomic, race
       condition, bottom halves, tasklets, softirqs.
       Description: The title says it all: document describing the
       locking system in the Linux Kernel either in uniprocessor or SMP
       systems.
       Notes: "It was originally written for the later (>2.3.47) 2.3
       kernels, but most of it applies to 2.2 too; 2.0 is slightly
       different". Freely redistributable under the conditions of the GNU
       General Public License.

     * Title: "Global spinlock list and usage"
       Author: Rick Lindsley.
       URL: http://lse.sourceforge.net/lockhier/global-spin-lock
       Keywords: spinlock.
       Description: This is an attempt to document both the existence and
       usage of the spinlocks in the Linux 2.4.5 kernel. Comprehensive
       list of spinlocks showing when they are used, which functions
       access them, how each lock is acquired, under what conditions it
       is held, whether interrupts can occur or not while it is held...

     * Title: "Porting Linux 2.0 Drivers To Linux 2.2: Changes and New
       Features "
       Author: Alan Cox.
       URL: http://www.linux-mag.com/1999-05/gear_01.html
       Keywords: ports, porting.
       Description: Article from Linux Magazine on porting from 2.0 to
       2.2 kernels.

     * Title: "Porting Device Drivers To Linux 2.2: part II"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/238 
       Keywords: ports, porting.
       Description: Second part on porting from 2.0 to 2.2 kernels.

     * Title: "How To Make Sure Your Driver Will Work On The Power
       Macintosh"
       Author: Paul Mackerras.
       URL: http://www.linux-mag.com/id/261
       Keywords: Mac, Power Macintosh, porting, drivers, compatibility.
       Description: The title says it all.

     * Title: "An Introduction to SCSI Drivers"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/284
       Keywords: SCSI, device, driver.
       Description: The title says it all.

     * Title: "Advanced SCSI Drivers And Other Tales"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/307
       Keywords: SCSI, device, driver, advanced.
       Description: The title says it all.

     * Title: "Writing Linux Mouse Drivers"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/330
       Keywords: mouse, driver, gpm.
       Description: The title says it all.

     * Title: "More on Mouse Drivers"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/356
       Keywords: mouse, driver, gpm, races, asynchronous I/O.
       Description: The title still says it all.

     * Title: "Writing Video4linux Radio Driver"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/381
       Keywords: video4linux, driver, radio, radio devices.
       Description: The title says it all.

     * Title: "Video4linux Drivers, Part 1: Video-Capture Device"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/406
       Keywords: video4linux, driver, video capture, capture devices,
       camera driver.
       Description: The title says it all.

     * Title: "Video4linux Drivers, Part 2: Video-capture Devices"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/429
       Keywords: video4linux, driver, video capture, capture devices,
       camera driver, control, query capabilities, capability, facility.
       Description: The title says it all.

     * Title: "PCI Management in Linux 2.2"
       Author: Alan Cox.
       URL: http://www.linux-mag.com/id/452
       Keywords: PCI, bus, bus-mastering.
       Description: The title says it all.

     * Title: "Linux 2.4 Kernel Internals"
       Author: Tigran Aivazian and Christoph Hellwig.
       URL: http://www.moses.uklinux.net/patches/lki.html
       Keywords: Linux, kernel, booting, SMB boot, VFS, page cache.
       Description: A little book used for a short training course.
       Covers building the kernel image, booting (including SMP bootup),
       process management, VFS and more.

     * Title: "Linux IP Networking. A Guide to the Implementation and
       Modification of the Linux Protocol Stack."
       Author: Glenn Herrin.
       URL: http://www.cs.unh.edu/cnrg/gherrin
       Keywords: network, networking, protocol, IP, UDP, TCP, connection,
       socket, receiving, transmitting, forwarding, routing, packets,
       modules, /proc, sk_buff, FIB, tags.
       Description: Excellent paper devoted to the Linux IP Networking,
       explaining anything from the kernel's to the user space
       configuration tools' code. Very good to get a general overview of
       the kernel networking implementation and understand all steps
       packets follow from the time they are received at the network
       device till they are delivered to applications. The studied kernel
       code is from 2.2.14 version. Provides code for a working packet
       dropper example.
       
     * Title: "Get those boards talking under Linux."
       Author: Alex Ivchenko.
       URL: http://www.edn.com/article/CA46968.html
       Keywords: data-acquisition boards, drivers, modules, interrupts,
       memory allocation.
       Description: Article written for people wishing to make their data
       acquisition boards work on their GNU/Linux machines. Gives a basic
       overview on writing drivers, from the naming of functions to
       interrupt handling.
       Notes: Two-parts article. Part II is at
       URL: http://www.edn.com/article/CA46998.html
       
     * Title: "Linux PCMCIA Programmer's Guide"
       Author: David Hinds.
       URL: http://pcmcia-cs.sourceforge.net/ftp/doc/PCMCIA-PROG.html
       Keywords: PCMCIA.
       Description: "This document describes how to write kernel device
       drivers for the Linux PCMCIA Card Services interface. It also
       describes how to write user-mode utilities for communicating with
       Card Services.

     * Title: "The Linux Kernel NFSD Implementation"
       Author: Neil Brown.
       URL:
       http://www.cse.unsw.edu.au/~neilb/oss/linux-commentary/nfsd.html
       Keywords: knfsd, nfsd, NFS, RPC, lockd, mountd, statd.
       Description: The title says it all.
       Notes: Covers knfsd's version 1.4.7 (patch against 2.2.7 kernel).
       
     * Title: "A Linux vm README"
       Author: Kanoj Sarcar.
       URL: http://kos.enix.org/pub/linux-vmm.html
       Keywords: virtual memory, mm, pgd, vma, page, page flags, page
       cache, swap cache, kswapd.
       Description: Telegraphic, short descriptions and definitions
       relating the Linux virtual memory implementation.
       
     * Title: "(nearly) Complete Linux Loadable Kernel Modules. The
       definitive guide for hackers, virus coders and system
       administrators."
       Author: pragmatic/THC.
       URL: http://packetstormsecurity.org/docs/hack/LKM_HACKING.html
       Keywords: syscalls, intercept, hide, abuse, symbol table.
       Description: Interesting paper on how to abuse the Linux kernel in
       order to intercept and modify syscalls, make
       files/directories/processes invisible, become root, hijack ttys,
       write kernel modules based virus... and solutions for admins to
       avoid all those abuses.
       Notes: For 2.0.x kernels. Gives guidances to port it to 2.2.x
       kernels.
       
     BOOKS: (Not on-line)
   
     * Title: "Linux Device Drivers"
       Author: Alessandro Rubini.
       Publisher: O'Reilly & Associates.
       Date: 1998.
       Pages: 439.
       ISBN: 1-56592-292-1
       
     * Title: "Linux Device Drivers, 2nd Edition"
       Author: Alessandro Rubini and Jonathan Corbet.
       Publisher: O'Reilly & Associates.
       Date: 2001.
       Pages: 586.
       ISBN: 0-59600-008-1
       Notes: Further information in
       http://www.oreilly.com/catalog/linuxdrive2/

     * Title: "Linux Device Drivers, 3rd Edition"
       Authors: Jonathan Corbet, Alessandro Rubini, and Greg Kroah-Hartman
       Publisher: O'Reilly & Associates.
       Date: 2005.
       Pages: 636.
       ISBN: 0-596-00590-3
       Notes: Further information in
       http://www.oreilly.com/catalog/linuxdrive3/
       PDF format, URL: http://lwn.net/Kernel/LDD3/

     * Title: "Linux Kernel Internals"
       Author: Michael Beck.
       Publisher: Addison-Wesley.
       Date: 1997.
       ISBN: 0-201-33143-8 (second edition)
       
     * Title: "The Design of the UNIX Operating System"
       Author: Maurice J. Bach.
       Publisher: Prentice Hall.
       Date: 1986.
       Pages: 471.
       ISBN: 0-13-201757-1
       
     * Title: "The Design and Implementation of the 4.3 BSD UNIX
       Operating System"
       Author: Samuel J. Leffler, Marshall Kirk McKusick, Michael J.
       Karels, John S. Quarterman.
       Publisher: Addison-Wesley.
       Date: 1989 (reprinted with corrections on October, 1990).
       ISBN: 0-201-06196-1
       
     * Title: "The Design and Implementation of the 4.4 BSD UNIX
       Operating System"
       Author: Marshall Kirk McKusick, Keith Bostic, Michael J. Karels,
       John S. Quarterman.
       Publisher: Addison-Wesley.
       Date: 1996.
       ISBN: 0-201-54979-4
       
     * Title: "Programmation Linux 2.0 API systeme et fonctionnement du
       noyau"
       Author: Remy Card, Eric Dumas, Franck Mevel.
       Publisher: Eyrolles.
       Date: 1997.
       Pages: 520.
       ISBN: 2-212-08932-5
       Notes: French.

     * Title: "Unix internals -- the new frontiers"
       Author: Uresh Vahalia.
       Publisher: Prentice Hall.
       Date: 1996.
       Pages: 600.
       ISBN: 0-13-101908-2

     * Title: "Programming for the real world - POSIX.4"
       Author: Bill O. Gallmeister.
       Publisher: O'Reilly & Associates, Inc..
       Date: 1995.
       Pages: ???.
       ISBN: I-56592-074-0
       Notes: Though not being directly about Linux, Linux aims to be
       POSIX. Good reference.

     * Title:  "UNIX  Systems  for  Modern Architectures: Symmetric
       Multiprocessing and Caching for Kernel Programmers"
       Author: Curt Schimmel.
       Publisher: Addison Wesley.
       Date: June, 1994.
       Pages: 432.
       ISBN: 0-201-63338-8

     MISCELLANEOUS:

     * Name: linux/Documentation
       Author: Many.
       URL: Just look inside your kernel sources.
       Keywords: anything, DocBook.
       Description: Documentation that comes with the kernel sources,
       inside the Documentation directory. Some pages from this document
       (including this document itself) have been moved there, and might
       be more up to date than the web version.

     * Name: "Linux Kernel Source Reference"
       Author: Thomas Graichen.
       URL: http://marc.info/?l=linux-kernel&m=96446640102205&w=4
       Keywords: CVS, web, cvsweb, browsing source code.
       Description: Web interface to a CVS server with the kernel
       sources. "Here you can have a look at any file of the Linux kernel
       sources of any version starting from 1.0 up to the (daily updated)
       current version available. Also you can check the differences
       between two versions of a file".

     * Name: "Cross-Referencing Linux"
       URL: http://lxr.linux.no/source/
       Keywords: Browsing source code.
       Description: Another web-based Linux kernel source code browser.
       Lots of cross references to variables and functions. You can see
       where they are defined and where they are used.

     * Name: "Linux Weekly News"
       URL: http://lwn.net
       Keywords: latest kernel news.
       Description: The title says it all. There's a fixed kernel section
       summarizing developers' work, bug fixes, new features and versions
       produced during the week. Published every Thursday.

     * Name: "Kernel Traffic"
       URL: http://kt.earth.li/kernel-traffic/index.html
       Keywords: linux-kernel mailing list, weekly kernel news.
       Description: Weekly newsletter covering the most relevant
       discussions of the linux-kernel mailing list.

     * Name: "CuTTiNG.eDGe.LiNuX"
       URL: http://edge.kernelnotes.org
       Keywords: changelist.
       Description: Site which provides the changelist for every kernel
       release. What's new, what's better, what's changed. Myrdraal reads
       the patches and describes them. Pointers to the patches are there,
       too.

     * Name: "New linux-kernel Mailing List FAQ"
       URL: http://www.tux.org/lkml/
       Keywords: linux-kernel mailing list FAQ.
       Description: linux-kernel is a mailing list for developers to
       communicate. This FAQ builds on the previous linux-kernel mailing
       list FAQ maintained by Frohwalt Egerer, who no longer maintains
       it. Read it to see how to join the mailing list. Dozens of
       interesting questions regarding the list, Linux, developers (who
       is ...?), terms (what is...?) are answered here too. Just read it.

     * Name: "Linux Virtual File System"
       Author: Peter J. Braam.
       URL: http://www.coda.cs.cmu.edu/doc/talks/linuxvfs/
       Keywords: slides, VFS, inode, superblock, dentry, dcache.
       Description: Set of slides, presumably from a presentation on the
       Linux VFS layer. Covers version 2.1.x, with dentries and the
       dcache.

     * Name: "Gary's Encyclopedia - The Linux Kernel"
       Author: Gary (I suppose...).
       URL: http://slencyclopedia.berlios.de/index.html
       Keywords: linux, community, everything!
       Description: Gary's Encyclopedia exists to allow the rapid finding
       of documentation and other information of interest to GNU/Linux
       users. It has about 4000 links to external pages in 150 major
       categories. This link is for kernel-specific links, documents,
       sites...  This list is now hosted by developer.Berlios.de,
       but seems not to have been updated since sometime in 1999.

     * Name: "The home page of Linux-MM"
       Author: The Linux-MM team.
       URL: http://linux-mm.org/
       Keywords: memory management, Linux-MM, mm patches, TODO, docs,
       mailing list.
       Description: Site devoted to Linux Memory Management development.
       Memory related patches, HOWTOs, links, mm developers... Don't miss
       it if you are interested in memory management development!

     * Name: "Kernel Newbies IRC Channel"
       URL: http://www.kernelnewbies.org
       Keywords: IRC, newbies, channel, asking doubts.
       Description: #kernelnewbies on irc.openprojects.net. From the web
       page: "#kernelnewbies is an IRC network dedicated to the 'newbie'
       kernel hacker. The audience mostly consists of people who are
       learning about the kernel, working on kernel projects or
       professional kernel hackers that want to help less seasoned kernel
       people. [...] #kernelnewbies is on the Open Projects IRC Network,
       try irc.openprojects.net or irc.<country>.openprojects.net as your
       server and then /join #kernelnewbies". It also hosts articles,
       documents, FAQs...
       
     * Name: "linux-kernel mailing list archives and search engines"
       URL: http://vger.kernel.org/vger-lists.html
       URL: http://www.uwsg.indiana.edu/hypermail/linux/kernel/index.html
       URL: http://marc.theaimsgroup.com/?l=linux-kernel
       URL: http://groups.google.com/group/mlist.linux.kernel
       URL: http://www.cs.helsinki.fi/linux/linux-kernel/
       URL: http://www.lib.uaa.alaska.edu/linux-kernel/
       Keywords: linux-kernel, archives, search.
       Description: Some of the linux-kernel mailing list archivers. If
       you have a better/another one, please let me know.
     _________________________________________________________________
   
   Document last updated on Sat 2005-NOV-19
                          Kernel Parameters
                          ~~~~~~~~~~~~~~~~~

The following is a consolidated list of the kernel parameters as
implemented by the __setup(), core_param() and module_param() macros
and sorted into English Dictionary order (defined as ignoring all
punctuation and sorting digits before letters in a case insensitive
manner), and with descriptions where known.

The kernel parses parameters from the kernel command line up to "--";
if it doesn't recognize a parameter and it doesn't contain a '.', the
parameter gets passed to init: parameters with '=' go into init's
environment, others are passed as command line arguments to init.
Everything after "--" is passed as an argument to init.

Module parameters can be specified in two ways: via the kernel command
line with a module name prefix, or via modprobe, e.g.:

	(kernel command line) usbcore.blinkenlights=1
	(modprobe command line) modprobe usbcore blinkenlights=1

Parameters for modules which are built into the kernel need to be
specified on the kernel command line.  modprobe looks through the
kernel command line (/proc/cmdline) and collects module parameters
when it loads a module, so the kernel command line can be used for
loadable modules too.

Hyphens (dashes) and underscores are equivalent in parameter names, so
	log_buf_len=1M print-fatal-signals=1
can also be entered as
	log-buf-len=1M print_fatal_signals=1

Double-quotes can be used to protect spaces in values, e.g.:
	param="spaces in here"

This document may not be entirely up to date and comprehensive. The command
"modinfo -p ${modulename}" shows a current list of all parameters of a loadable
module. Loadable modules, after being loaded into the running kernel, also
reveal their parameters in /sys/module/${modulename}/parameters/. Some of these
parameters may be changed at runtime by the command
"echo -n ${value} > /sys/module/${modulename}/parameters/${parm}".

The parameters listed below are only valid if certain kernel build options were
enabled and if respective hardware is present. The text in square brackets at
the beginning of each description states the restrictions within which a
parameter is applicable:

	ACPI	ACPI support is enabled.
	AGP	AGP (Accelerated Graphics Port) is enabled.
	ALSA	ALSA sound support is enabled.
	APIC	APIC support is enabled.
	APM	Advanced Power Management support is enabled.
	ARM	ARM architecture is enabled.
	AVR32	AVR32 architecture is enabled.
	AX25	Appropriate AX.25 support is enabled.
	BLACKFIN Blackfin architecture is enabled.
	CLK	Common clock infrastructure is enabled.
	CMA	Contiguous Memory Area support is enabled.
	DRM	Direct Rendering Management support is enabled.
	DYNAMIC_DEBUG Build in debug messages and enable them at runtime
	EDD	BIOS Enhanced Disk Drive Services (EDD) is enabled
	EFI	EFI Partitioning (GPT) is enabled
	EIDE	EIDE/ATAPI support is enabled.
	EVM	Extended Verification Module
	FB	The frame buffer device is enabled.
	FTRACE	Function tracing enabled.
	GCOV	GCOV profiling is enabled.
	HW	Appropriate hardware is enabled.
	IA-64	IA-64 architecture is enabled.
	IMA     Integrity measurement architecture is enabled.
	IOSCHED	More than one I/O scheduler is enabled.
	IP_PNP	IP DHCP, BOOTP, or RARP is enabled.
	IPV6	IPv6 support is enabled.
	ISAPNP	ISA PnP code is enabled.
	ISDN	Appropriate ISDN support is enabled.
	JOY	Appropriate joystick support is enabled.
	KGDB	Kernel debugger support is enabled.
	KVM	Kernel Virtual Machine support is enabled.
	LIBATA  Libata driver is enabled
	LP	Printer support is enabled.
	LOOP	Loopback device support is enabled.
	M68k	M68k architecture is enabled.
			These options have more detailed description inside of
			Documentation/m68k/kernel-options.txt.
	MDA	MDA console support is enabled.
	MIPS	MIPS architecture is enabled.
	MOUSE	Appropriate mouse support is enabled.
	MSI	Message Signaled Interrupts (PCI).
	MTD	MTD (Memory Technology Device) support is enabled.
	NET	Appropriate network support is enabled.
	NUMA	NUMA support is enabled.
	NFS	Appropriate NFS support is enabled.
	OSS	OSS sound support is enabled.
	PV_OPS	A paravirtualized kernel is enabled.
	PARIDE	The ParIDE (parallel port IDE) subsystem is enabled.
	PARISC	The PA-RISC architecture is enabled.
	PCI	PCI bus support is enabled.
	PCIE	PCI Express support is enabled.
	PCMCIA	The PCMCIA subsystem is enabled.
	PNP	Plug & Play support is enabled.
	PPC	PowerPC architecture is enabled.
	PPT	Parallel port support is enabled.
	PS2	Appropriate PS/2 support is enabled.
	RAM	RAM disk support is enabled.
	S390	S390 architecture is enabled.
	SCSI	Appropriate SCSI support is enabled.
			A lot of drivers have their options described inside
			the Documentation/scsi/ sub-directory.
	SECURITY Different security models are enabled.
	SELINUX SELinux support is enabled.
	APPARMOR AppArmor support is enabled.
	SERIAL	Serial support is enabled.
	SH	SuperH architecture is enabled.
	SMP	The kernel is an SMP kernel.
	SPARC	Sparc architecture is enabled.
	SWSUSP	Software suspend (hibernation) is enabled.
	SUSPEND	System suspend states are enabled.
	TPM	TPM drivers are enabled.
	TS	Appropriate touchscreen support is enabled.
	UMS	USB Mass Storage support is enabled.
	USB	USB support is enabled.
	USBHID	USB Human Interface Device support is enabled.
	V4L	Video For Linux support is enabled.
	VMMIO   Driver for memory mapped virtio devices is enabled.
	VGA	The VGA console has been enabled.
	VT	Virtual terminal support is enabled.
	WDT	Watchdog support is enabled.
	XT	IBM PC/XT MFM hard disk support is enabled.
	X86-32	X86-32, aka i386 architecture is enabled.
	X86-64	X86-64 architecture is enabled.
			More X86-64 boot options can be found in
			Documentation/x86/x86_64/boot-options.txt .
	X86	Either 32-bit or 64-bit x86 (same as X86-32+X86-64)
	XEN	Xen support is enabled

In addition, the following text indicates that the option:

	BUGS=	Relates to possible processor bugs on the said processor.
	KNL	Is a kernel start-up parameter.
	BOOT	Is a boot loader parameter.

Parameters denoted with BOOT are actually interpreted by the boot
loader, and have no meaning to the kernel directly.
Do not modify the syntax of boot loader parameters without extreme
need or coordination with <Documentation/x86/boot.txt>.

There are also arch-specific kernel-parameters not documented here.
See for example <Documentation/x86/x86_64/boot-options.txt>.

Note that ALL kernel parameters listed below are CASE SENSITIVE, and that
a trailing = on the name of any parameter states that that parameter will
be entered as an environment variable, whereas its absence indicates that
it will appear as a kernel argument readable via /proc/cmdline by programs
running once the system is up.

The number of kernel parameters is not limited, but the length of the
complete command line (parameters including spaces etc.) is limited to
a fixed number of characters. This limit depends on the architecture
and is between 256 and 4096 characters. It is defined in the file
./include/asm/setup.h as COMMAND_LINE_SIZE.

Finally, the [KMG] suffix is commonly described after a number of kernel
parameter values. These 'K', 'M', and 'G' letters represent the _binary_
multipliers 'Kilo', 'Mega', and 'Giga', equalling 2^10, 2^20, and 2^30
bytes respectively. Such letter suffixes can also be entirely omitted.


	acpi=		[HW,ACPI,X86]
			Advanced Configuration and Power Interface
			Format: { force | off | strict | noirq | rsdt }
			force -- enable ACPI if default was off
			off -- disable ACPI if default was on
			noirq -- do not use ACPI for IRQ routing
			strict -- Be less tolerant of platforms that are not
				strictly ACPI specification compliant.
			rsdt -- prefer RSDT over (default) XSDT
			copy_dsdt -- copy DSDT to memory

			See also Documentation/power/runtime_pm.txt, pci=noacpi

	acpi_rsdp=	[ACPI,EFI,KEXEC]
			Pass the RSDP address to the kernel, mostly used
			on machines running EFI runtime service to boot the
			second kernel for kdump.

	acpi_apic_instance=	[ACPI, IOAPIC]
			Format: <int>
			2: use 2nd APIC table, if available
			1,0: use 1st APIC table
			default: 0

	acpi_backlight=	[HW,ACPI]
			acpi_backlight=vendor
			acpi_backlight=video
			If set to vendor, prefer vendor specific driver
			(e.g. thinkpad_acpi, sony_acpi, etc.) instead
			of the ACPI video.ko driver.

	acpi.debug_layer=	[HW,ACPI,ACPI_DEBUG]
	acpi.debug_level=	[HW,ACPI,ACPI_DEBUG]
			Format: <int>
			CONFIG_ACPI_DEBUG must be enabled to produce any ACPI
			debug output.  Bits in debug_layer correspond to a
			_COMPONENT in an ACPI source file, e.g.,
			    #define _COMPONENT ACPI_PCI_COMPONENT
			Bits in debug_level correspond to a level in
			ACPI_DEBUG_PRINT statements, e.g.,
			    ACPI_DEBUG_PRINT((ACPI_DB_INFO, ...
			The debug_level mask defaults to "info".  See
			Documentation/acpi/debug.txt for more information about
			debug layers and levels.

			Enable processor driver info messages:
			    acpi.debug_layer=0x20000000
			Enable PCI/PCI interrupt routing info messages:
			    acpi.debug_layer=0x400000
			Enable AML "Debug" output, i.e., stores to the Debug
			object while interpreting AML:
			    acpi.debug_layer=0xffffffff acpi.debug_level=0x2
			Enable all messages related to ACPI hardware:
			    acpi.debug_layer=0x2 acpi.debug_level=0xffffffff

			Some values produce so much output that the system is
			unusable.  The "log_buf_len" parameter may be useful
			if you need to capture more output.

	acpi_force_table_verification	[HW,ACPI]
			Enable table checksum verification during early stage.
			By default, this is disabled due to x86 early mapping
			size limitation.

	acpi_irq_balance [HW,ACPI]
			ACPI will balance active IRQs
			default in APIC mode

	acpi_irq_nobalance [HW,ACPI]
			ACPI will not move active IRQs (default)
			default in PIC mode

	acpi_irq_isa=	[HW,ACPI] If irq_balance, mark listed IRQs used by ISA
			Format: <irq>,<irq>...

	acpi_irq_pci=	[HW,ACPI] If irq_balance, clear listed IRQs for
			use by PCI
			Format: <irq>,<irq>...

	acpi_no_auto_serialize	[HW,ACPI]
			Disable auto-serialization of AML methods
			AML control methods that contain the opcodes to create
			named objects will be marked as "Serialized" by the
			auto-serialization feature.
			This feature is enabled by default.
			This option allows to turn off the feature.

	acpi_no_static_ssdt	[HW,ACPI]
			Disable installation of static SSDTs at early boot time
			By default, SSDTs contained in the RSDT/XSDT will be
			installed automatically and they will appear under
			/sys/firmware/acpi/tables.
			This option turns off this feature.
			Note that specifying this option does not affect
			dynamic table installation which will install SSDT
			tables to /sys/firmware/acpi/tables/dynamic.

	acpica_no_return_repair [HW, ACPI]
			Disable AML predefined validation mechanism
			This mechanism can repair the evaluation result to make
			the return objects more ACPI specification compliant.
			This option is useful for developers to identify the
			root cause of an AML interpreter issue when the issue
			has something to do with the repair mechanism.

	acpi_os_name=	[HW,ACPI] Tell ACPI BIOS the name of the OS
			Format: To spoof as Windows 98: ="Microsoft Windows"

	acpi_osi=	[HW,ACPI] Modify list of supported OS interface strings
			acpi_osi="string1"	# add string1
			acpi_osi="!string2"	# remove string2
			acpi_osi=!*		# remove all strings
			acpi_osi=!		# disable all built-in OS vendor
						  strings
			acpi_osi=		# disable all strings

			'acpi_osi=!' can be used in combination with single or
			multiple 'acpi_osi="string1"' to support specific OS
			vendor string(s).  Note that such command can only
			affect the default state of the OS vendor strings, thus
			it cannot affect the default state of the feature group
			strings and the current state of the OS vendor strings,
			specifying it multiple times through kernel command line
			is meaningless.  This command is useful when one do not
			care about the state of the feature group strings which
			should be controlled by the OSPM.
			Examples:
			  1. 'acpi_osi=! acpi_osi="Windows 2000"' is equivalent
			     to 'acpi_osi="Windows 2000" acpi_osi=!', they all
			     can make '_OSI("Windows 2000")' TRUE.

			'acpi_osi=' cannot be used in combination with other
			'acpi_osi=' command lines, the _OSI method will not
			exist in the ACPI namespace.  NOTE that such command can
			only affect the _OSI support state, thus specifying it
			multiple times through kernel command line is also
			meaningless.
			Examples:
			  1. 'acpi_osi=' can make 'CondRefOf(_OSI, Local1)'
			     FALSE.

			'acpi_osi=!*' can be used in combination with single or
			multiple 'acpi_osi="string1"' to support specific
			string(s).  Note that such command can affect the
			current state of both the OS vendor strings and the
			feature group strings, thus specifying it multiple times
			through kernel command line is meaningful.  But it may
			still not able to affect the final state of a string if
			there are quirks related to this string.  This command
			is useful when one want to control the state of the
			feature group strings to debug BIOS issues related to
			the OSPM features.
			Examples:
			  1. 'acpi_osi="Module Device" acpi_osi=!*' can make
			     '_OSI("Module Device")' FALSE.
			  2. 'acpi_osi=!* acpi_osi="Module Device"' can make
			     '_OSI("Module Device")' TRUE.
			  3. 'acpi_osi=! acpi_osi=!* acpi_osi="Windows 2000"' is
			     equivalent to
			     'acpi_osi=!* acpi_osi=! acpi_osi="Windows 2000"'
			     and
			     'acpi_osi=!* acpi_osi="Windows 2000" acpi_osi=!',
			     they all will make '_OSI("Windows 2000")' TRUE.

	acpi_pm_good	[X86]
			Override the pmtimer bug detection: force the kernel
			to assume that this machine's pmtimer latches its value
			and always returns good values.

	acpi_sci=	[HW,ACPI] ACPI System Control Interrupt trigger mode
			Format: { level | edge | high | low }

	acpi_skip_timer_override [HW,ACPI]
			Recognize and ignore IRQ0/pin2 Interrupt Override.
			For broken nForce2 BIOS resulting in XT-PIC timer.

	acpi_sleep=	[HW,ACPI] Sleep options
			Format: { s3_bios, s3_mode, s3_beep, s4_nohwsig,
				  old_ordering, nonvs, sci_force_enable }
			See Documentation/power/video.txt for information on
			s3_bios and s3_mode.
			s3_beep is for debugging; it makes the PC's speaker beep
			as soon as the kernel's real-mode entry point is called.
			s4_nohwsig prevents ACPI hardware signature from being
			used during resume from hibernation.
			old_ordering causes the ACPI 1.0 ordering of the _PTS
			control method, with respect to putting devices into
			low power states, to be enforced (the ACPI 2.0 ordering
			of _PTS is used by default).
			nonvs prevents the kernel from saving/restoring the
			ACPI NVS memory during suspend/hibernation and resume.
			sci_force_enable causes the kernel to set SCI_EN directly
			on resume from S1/S3 (which is against the ACPI spec,
			but some broken systems don't work without it).

	acpi_use_timer_override [HW,ACPI]
			Use timer override. For some broken Nvidia NF5 boards
			that require a timer override, but don't have HPET

	acpi_enforce_resources=	[ACPI]
			{ strict | lax | no }
			Check for resource conflicts between native drivers
			and ACPI OperationRegions (SystemIO and SystemMemory
			only). IO ports and memory declared in ACPI might be
			used by the ACPI subsystem in arbitrary AML code and
			can interfere with legacy drivers.
			strict (default): access to resources claimed by ACPI
			is denied; legacy drivers trying to access reserved
			resources will fail to bind to device using them.
			lax: access to resources claimed by ACPI is allowed;
			legacy drivers trying to access reserved resources
			will bind successfully but a warning message is logged.
			no: ACPI OperationRegions are not marked as reserved,
			no further checks are performed.

	acpi_no_memhotplug [ACPI] Disable memory hotplug.  Useful for kdump
			   kernels.

	add_efi_memmap	[EFI; X86] Include EFI memory map in
			kernel's map of available physical RAM.

	agp=		[AGP]
			{ off | try_unsupported }
			off: disable AGP support
			try_unsupported: try to drive unsupported chipsets
				(may crash computer or cause data corruption)

	ALSA		[HW,ALSA]
			See Documentation/sound/alsa/alsa-parameters.txt

	alignment=	[KNL,ARM]
			Allow the default userspace alignment fault handler
			behaviour to be specified.  Bit 0 enables warnings,
			bit 1 enables fixups, and bit 2 sends a segfault.

	align_va_addr=	[X86-64]
			Align virtual addresses by clearing slice [14:12] when
			allocating a VMA at process creation time. This option
			gives you up to 3% performance improvement on AMD F15h
			machines (where it is enabled by default) for a
			CPU-intensive style benchmark, and it can vary highly in
			a microbenchmark depending on workload and compiler.

			32: only for 32-bit processes
			64: only for 64-bit processes
			on: enable for both 32- and 64-bit processes
			off: disable for both 32- and 64-bit processes

	alloc_snapshot	[FTRACE]
			Allocate the ftrace snapshot buffer on boot up when the
			main buffer is allocated. This is handy if debugging
			and you need to use tracing_snapshot() on boot up, and
			do not want to use tracing_snapshot_alloc() as it needs
			to be done where GFP_KERNEL allocations are allowed.

	amd_iommu=	[HW,X86-64]
			Pass parameters to the AMD IOMMU driver in the system.
			Possible values are:
			fullflush - enable flushing of IO/TLB entries when
				    they are unmapped. Otherwise they are
				    flushed before they will be reused, which
				    is a lot of faster
			off	  - do not initialize any AMD IOMMU found in
				    the system
			force_isolation - Force device isolation for all
					  devices. The IOMMU driver is not
					  allowed anymore to lift isolation
					  requirements as needed. This option
					  does not override iommu=pt

	amd_iommu_dump=	[HW,X86-64]
			Enable AMD IOMMU driver option to dump the ACPI table
			for AMD IOMMU. With this option enabled, AMD IOMMU
			driver will print ACPI tables for AMD IOMMU during
			IOMMU initialization.

	amijoy.map=	[HW,JOY] Amiga joystick support
			Map of devices attached to JOY0DAT and JOY1DAT
			Format: <a>,<b>
			See also Documentation/input/joystick.txt

	analog.map=	[HW,JOY] Analog joystick and gamepad support
			Specifies type or capabilities of an analog joystick
			connected to one of 16 gameports
			Format: <type1>,<type2>,..<type16>

	apc=		[HW,SPARC]
			Power management functions (SPARCstation-4/5 + deriv.)
			Format: noidle
			Disable APC CPU standby support. SPARCstation-Fox does
			not play well with APC CPU idle - disable it if you have
			APC and your system crashes randomly.

	apic=		[APIC,X86-32] Advanced Programmable Interrupt Controller
			Change the output verbosity whilst booting
			Format: { quiet (default) | verbose | debug }
			Change the amount of debugging information output
			when initialising the APIC and IO-APIC components.

	autoconf=	[IPV6]
			See Documentation/networking/ipv6.txt.

	show_lapic=	[APIC,X86] Advanced Programmable Interrupt Controller
			Limit apic dumping. The parameter defines the maximal
			number of local apics being dumped. Also it is possible
			to set it to "all" by meaning -- no limit here.
			Format: { 1 (default) | 2 | ... | all }.
			The parameter valid if only apic=debug or
			apic=verbose is specified.
			Example: apic=debug show_lapic=all

	apm=		[APM] Advanced Power Management
			See header of arch/x86/kernel/apm_32.c.

	arcrimi=	[HW,NET] ARCnet - "RIM I" (entirely mem-mapped) cards
			Format: <io>,<irq>,<nodeID>

	ataflop=	[HW,M68k]

	atarimouse=	[HW,MOUSE] Atari Mouse

	atkbd.extra=	[HW] Enable extra LEDs and keys on IBM RapidAccess,
			EzKey and similar keyboards

	atkbd.reset=	[HW] Reset keyboard during initialization

	atkbd.set=	[HW] Select keyboard code set
			Format: <int> (2 = AT (default), 3 = PS/2)

	atkbd.scroll=	[HW] Enable scroll wheel on MS Office and similar
			keyboards

	atkbd.softraw=	[HW] Choose between synthetic and real raw mode
			Format: <bool> (0 = real, 1 = synthetic (default))

	atkbd.softrepeat= [HW]
			Use software keyboard repeat

	audit=		[KNL] Enable the audit sub-system
			Format: { "0" | "1" } (0 = disabled, 1 = enabled)
			0 - kernel audit is disabled and can not be enabled
			    until the next reboot
			unset - kernel audit is initialized but disabled and
			    will be fully enabled by the userspace auditd.
			1 - kernel audit is initialized and partially enabled,
			    storing at most audit_backlog_limit messages in
			    RAM until it is fully enabled by the userspace
			    auditd.
			Default: unset

	audit_backlog_limit= [KNL] Set the audit queue size limit.
			Format: <int> (must be >=0)
			Default: 64

	baycom_epp=	[HW,AX25]
			Format: <io>,<mode>

	baycom_par=	[HW,AX25] BayCom Parallel Port AX.25 Modem
			Format: <io>,<mode>
			See header of drivers/net/hamradio/baycom_par.c.

	baycom_ser_fdx=	[HW,AX25]
			BayCom Serial Port AX.25 Modem (Full Duplex Mode)
			Format: <io>,<irq>,<mode>[,<baud>]
			See header of drivers/net/hamradio/baycom_ser_fdx.c.

	baycom_ser_hdx=	[HW,AX25]
			BayCom Serial Port AX.25 Modem (Half Duplex Mode)
			Format: <io>,<irq>,<mode>
			See header of drivers/net/hamradio/baycom_ser_hdx.c.

	blkdevparts=	Manual partition parsing of block device(s) for
			embedded devices based on command line input.
			See Documentation/block/cmdline-partition.txt

	boot_delay=	Milliseconds to delay each printk during boot.
			Values larger than 10 seconds (10000) are changed to
			no delay (0).
			Format: integer

	bootmem_debug	[KNL] Enable bootmem allocator debug messages.

	bttv.card=	[HW,V4L] bttv (bt848 + bt878 based grabber cards)
	bttv.radio=	Most important insmod options are available as
			kernel args too.
	bttv.pll=	See Documentation/video4linux/bttv/Insmod-options
	bttv.tuner=

	bulk_remove=off	[PPC]  This parameter disables the use of the pSeries
			firmware feature for flushing multiple hpte entries
			at a time.

	c101=		[NET] Moxa C101 synchronous serial card

	cachesize=	[BUGS=X86-32] Override level 2 CPU cache size detection.
			Sometimes CPU hardware bugs make them report the cache
			size incorrectly. The kernel will attempt work arounds
			to fix known problems, but for some CPUs it is not
			possible to determine what the correct size should be.
			This option provides an override for these situations.

	ccw_timeout_log [S390]
			See Documentation/s390/CommonIO for details.

	cgroup_disable= [KNL] Disable a particular controller
			Format: {name of the controller(s) to disable}
			The effects of cgroup_disable=foo are:
			- foo isn't auto-mounted if you mount all cgroups in
			  a single hierarchy
			- foo isn't visible as an individually mountable
			  subsystem
			{Currently only "memory" controller deal with this and
			cut the overhead, others just disable the usage. So
			only cgroup_disable=memory is actually worthy}

	checkreqprot	[SELINUX] Set initial checkreqprot flag value.
			Format: { "0" | "1" }
			See security/selinux/Kconfig help text.
			0 -- check protection applied by kernel (includes
				any implied execute protection).
			1 -- check protection requested by application.
			Default value is set via a kernel config option.
			Value can be changed at runtime via
				/selinux/checkreqprot.

	cio_ignore=	[S390]
			See Documentation/s390/CommonIO for details.
	clk_ignore_unused
			[CLK]
			Keep all clocks already enabled by bootloader on,
			even if no driver has claimed them. This is useful
			for debug and development, but should not be
			needed on a platform with proper driver support.
			For more information, see Documentation/clk.txt.

	clock=		[BUGS=X86-32, HW] gettimeofday clocksource override.
			[Deprecated]
			Forces specified clocksource (if available) to be used
			when calculating gettimeofday(). If specified
			clocksource is not available, it defaults to PIT.
			Format: { pit | tsc | cyclone | pmtmr }

	clocksource=	Override the default clocksource
			Format: <string>
			Override the default clocksource and use the clocksource
			with the name specified.
			Some clocksource names to choose from, depending on
			the platform:
			[all] jiffies (this is the base, fallback clocksource)
			[ACPI] acpi_pm
			[ARM] imx_timer1,OSTS,netx_timer,mpu_timer2,
				pxa_timer,timer3,32k_counter,timer0_1
			[AVR32] avr32
			[X86-32] pit,hpet,tsc;
				scx200_hrt on Geode; cyclone on IBM x440
			[MIPS] MIPS
			[PARISC] cr16
			[S390] tod
			[SH] SuperH
			[SPARC64] tick
			[X86-64] hpet,tsc

	clearcpuid=BITNUM [X86]
			Disable CPUID feature X for the kernel. See
			arch/x86/include/asm/cpufeature.h for the valid bit
			numbers. Note the Linux specific bits are not necessarily
			stable over kernel options, but the vendor specific
			ones should be.
			Also note that user programs calling CPUID directly
			or using the feature without checking anything
			will still see it. This just prevents it from
			being used by the kernel or shown in /proc/cpuinfo.
			Also note the kernel might malfunction if you disable
			some critical bits.

	cma=nn[MG]@[start[MG][-end[MG]]]
			[ARM,X86,KNL]
			Sets the size of kernel global memory area for
			contiguous memory allocations and optionally the
			placement constraint by the physical address range of
			memory allocations. For more information, see
			include/linux/dma-contiguous.h

	cmo_free_hint=	[PPC] Format: { yes | no }
			Specify whether pages are marked as being inactive
			when they are freed.  This is used in CMO environments
			to determine OS memory pressure for page stealing by
			a hypervisor.
			Default: yes

	coherent_pool=nn[KMG]	[ARM,KNL]
			Sets the size of memory pool for coherent, atomic dma
			allocations, by default set to 256K.

	code_bytes	[X86] How many bytes of object code to print
			in an oops report.
			Range: 0 - 8192
			Default: 64

	com20020=	[HW,NET] ARCnet - COM20020 chipset
			Format:
			<io>[,<irq>[,<nodeID>[,<backplane>[,<ckp>[,<timeout>]]]]]

	com90io=	[HW,NET] ARCnet - COM90xx chipset (IO-mapped buffers)
			Format: <io>[,<irq>]

	com90xx=	[HW,NET]
			ARCnet - COM90xx chipset (memory-mapped buffers)
			Format: <io>[,<irq>[,<memstart>]]

	condev=		[HW,S390] console device
	conmode=

	console=	[KNL] Output console device and options.

		tty<n>	Use the virtual console device <n>.

		ttyS<n>[,options]
		ttyUSB0[,options]
			Use the specified serial port.  The options are of
			the form "bbbbpnf", where "bbbb" is the baud rate,
			"p" is parity ("n", "o", or "e"), "n" is number of
			bits, and "f" is flow control ("r" for RTS or
			omit it).  Default is "9600n8".

			See Documentation/serial-console.txt for more
			information.  See
			Documentation/networking/netconsole.txt for an
			alternative.

		uart[8250],io,<addr>[,options]
		uart[8250],mmio,<addr>[,options]
			Start an early, polled-mode console on the 8250/16550
			UART at the specified I/O port or MMIO address,
			switching to the matching ttyS device later.  The
			options are the same as for ttyS, above.
		hvc<n>	Use the hypervisor console device <n>. This is for
			both Xen and PowerPC hypervisors.

                If the device connected to the port is not a TTY but a braille
                device, prepend "brl," before the device type, for instance
			console=brl,ttyS0
		For now, only VisioBraille is supported.

	consoleblank=	[KNL] The console blank (screen saver) timeout in
			seconds. Defaults to 10*60 = 10mins. A value of 0
			disables the blank timer.

	coredump_filter=
			[KNL] Change the default value for
			/proc/<pid>/coredump_filter.
			See also Documentation/filesystems/proc.txt.

	cpuidle.off=1	[CPU_IDLE]
			disable the cpuidle sub-system

	cpcihp_generic=	[HW,PCI] Generic port I/O CompactPCI driver
			Format:
			<first_slot>,<last_slot>,<port>,<enum_bit>[,<debug>]

	crashkernel=size[KMG][@offset[KMG]]
			[KNL] Using kexec, Linux can switch to a 'crash kernel'
			upon panic. This parameter reserves the physical
			memory region [offset, offset + size] for that kernel
			image. If '@offset' is omitted, then a suitable offset
			is selected automatically. Check
			Documentation/kdump/kdump.txt for further details.

	crashkernel=range1:size1[,range2:size2,...][@offset]
			[KNL] Same as above, but depends on the memory
			in the running system. The syntax of range is
			start-[end] where start and end are both
			a memory unit (amount[KMG]). See also
			Documentation/kdump/kdump.txt for an example.

	crashkernel=size[KMG],high
			[KNL, x86_64] range could be above 4G. Allow kernel
			to allocate physical memory region from top, so could
			be above 4G if system have more than 4G ram installed.
			Otherwise memory region will be allocated below 4G, if
			available.
			It will be ignored if crashkernel=X is specified.
	crashkernel=size[KMG],low
			[KNL, x86_64] range under 4G. When crashkernel=X,high
			is passed, kernel could allocate physical memory region
			above 4G, that cause second kernel crash on system
			that require some amount of low memory, e.g. swiotlb
			requires at least 64M+32K low memory.  Kernel would
			try to allocate 72M below 4G automatically.
			This one let user to specify own low range under 4G
			for second kernel instead.
			0: to disable low allocation.
			It will be ignored when crashkernel=X,high is not used
			or memory reserved is below 4G.

	cs89x0_dma=	[HW,NET]
			Format: <dma>

	cs89x0_media=	[HW,NET]
			Format: { rj45 | aui | bnc }

	dasd=		[HW,NET]
			See header of drivers/s390/block/dasd_devmap.c.

	db9.dev[2|3]=	[HW,JOY] Multisystem joystick support via parallel port
			(one device per port)
			Format: <port#>,<type>
			See also Documentation/input/joystick-parport.txt

	ddebug_query=   [KNL,DYNAMIC_DEBUG] Enable debug messages at early boot
			time. See Documentation/dynamic-debug-howto.txt for
			details.  Deprecated, see dyndbg.

	debug		[KNL] Enable kernel debugging (events log level).

	debug_locks_verbose=
			[KNL] verbose self-tests
			Format=<0|1>
			Print debugging info while doing the locking API
			self-tests.
			We default to 0 (no extra messages), setting it to
			1 will print _a lot_ more information - normally
			only useful to kernel developers.

	debug_objects	[KNL] Enable object debugging

	no_debug_objects
			[KNL] Disable object debugging

	debug_guardpage_minorder=
			[KNL] When CONFIG_DEBUG_PAGEALLOC is set, this
			parameter allows control of the order of pages that will
			be intentionally kept free (and hence protected) by the
			buddy allocator. Bigger value increase the probability
			of catching random memory corruption, but reduce the
			amount of memory for normal system use. The maximum
			possible value is MAX_ORDER/2.  Setting this parameter
			to 1 or 2 should be enough to identify most random
			memory corruption problems caused by bugs in kernel or
			driver code when a CPU writes to (or reads from) a
			random memory location. Note that there exists a class
			of memory corruptions problems caused by buggy H/W or
			F/W or by drivers badly programing DMA (basically when
			memory is written at bus level and the CPU MMU is
			bypassed) which are not detectable by
			CONFIG_DEBUG_PAGEALLOC, hence this option will not help
			tracking down these problems.

	debugpat	[X86] Enable PAT debugging

	decnet.addr=	[HW,NET]
			Format: <area>[,<node>]
			See also Documentation/networking/decnet.txt.

	default_hugepagesz=
			[same as hugepagesz=] The size of the default
			HugeTLB page size. This is the size represented by
			the legacy /proc/ hugepages APIs, used for SHM, and
			default size when mounting hugetlbfs filesystems.
			Defaults to the default architecture's huge page size
			if not specified.

	dhash_entries=	[KNL]
			Set number of hash buckets for dentry cache.

	disable=	[IPV6]
			See Documentation/networking/ipv6.txt.

	disable_cpu_apicid= [X86,APIC,SMP]
			Format: <int>
			The number of initial APIC ID for the
			corresponding CPU to be disabled at boot,
			mostly used for the kdump 2nd kernel to
			disable BSP to wake up multiple CPUs without
			causing system reset or hang due to sending
			INIT from AP to BSP.

	disable_ddw     [PPC/PSERIES]
			Disable Dynamic DMA Window support. Use this if
			to workaround buggy firmware.

	disable_ipv6=	[IPV6]
			See Documentation/networking/ipv6.txt.

	disable_mtrr_cleanup [X86]
			The kernel tries to adjust MTRR layout from continuous
			to discrete, to make X server driver able to add WB
			entry later. This parameter disables that.

	disable_mtrr_trim [X86, Intel and AMD only]
			By default the kernel will trim any uncacheable
			memory out of your available memory pool based on
			MTRR settings.  This parameter disables that behavior,
			possibly causing your machine to run very slowly.

	disable_timer_pin_1 [X86]
			Disable PIN 1 of APIC timer
			Can be useful to work around chipset bugs.

	dma_debug=off	If the kernel is compiled with DMA_API_DEBUG support,
			this option disables the debugging code at boot.

	dma_debug_entries=<number>
			This option allows to tune the number of preallocated
			entries for DMA-API debugging code. One entry is
			required per DMA-API allocation. Use this if the
			DMA-API debugging code disables itself because the
			architectural default is too low.

	dma_debug_driver=<driver_name>
			With this option the DMA-API debugging driver
			filter feature can be enabled at boot time. Just
			pass the driver to filter for as the parameter.
			The filter can be disabled or changed to another
			driver later using sysfs.

	drm_kms_helper.edid_firmware=[<connector>:]<file>
			Broken monitors, graphic adapters and KVMs may
			send no or incorrect EDID data sets. This parameter
			allows to specify an EDID data set in the
			/lib/firmware directory that is used instead.
			Generic built-in EDID data sets are used, if one of
			edid/1024x768.bin, edid/1280x1024.bin,
			edid/1680x1050.bin, or edid/1920x1080.bin is given
			and no file with the same name exists. Details and
			instructions how to build your own EDID data are
			available in Documentation/EDID/HOWTO.txt. An EDID
			data set will only be used for a particular connector,
			if its name and a colon are prepended to the EDID
			name.

	dscc4.setup=	[NET]

	dyndbg[="val"]		[KNL,DYNAMIC_DEBUG]
	module.dyndbg[="val"]
			Enable debug messages at boot time.  See
			Documentation/dynamic-debug-howto.txt for details.

	early_ioremap_debug [KNL]
			Enable debug messages in early_ioremap support. This
			is useful for tracking down temporary early mappings
			which are not unmapped.

	earlycon=	[KNL] Output early console device and options.

		uart[8250],io,<addr>[,options]
		uart[8250],mmio,<addr>[,options]
		uart[8250],mmio32,<addr>[,options]
			Start an early, polled-mode console on the 8250/16550
			UART at the specified I/O port or MMIO address.
			MMIO inter-register address stride is either 8-bit
			(mmio) or 32-bit (mmio32).
			The options are the same as for ttyS, above.

		pl011,<addr>
			Start an early, polled-mode console on a pl011 serial
			port at the specified address. The pl011 serial port
			must already be setup and configured. Options are not
			yet supported.

		smh	Use ARM semihosting calls for early console.

	earlyprintk=	[X86,SH,BLACKFIN,ARM,M68k]
			earlyprintk=vga
			earlyprintk=efi
			earlyprintk=xen
			earlyprintk=serial[,ttySn[,baudrate]]
			earlyprintk=serial[,0x...[,baudrate]]
			earlyprintk=ttySn[,baudrate]
			earlyprintk=dbgp[debugController#]

			earlyprintk is useful when the kernel crashes before
			the normal console is initialized. It is not enabled by
			default because it has some cosmetic problems.

			Append ",keep" to not disable it when the real console
			takes over.

			Only one of vga, efi, serial, or usb debug port can
			be used at a time.

			Currently only ttyS0 and ttyS1 may be specified by
			name.  Other I/O ports may be explicitly specified
			on some architectures (x86 and arm at least) by
			replacing ttySn with an I/O port address, like this:
				earlyprintk=serial,0x1008,115200
			You can find the port for a given device in
			/proc/tty/driver/serial:
				2: uart:ST16650V2 port:00001008 irq:18 ...

			Interaction with the standard serial driver is not
			very good.

			The VGA and EFI output is eventually overwritten by
			the real console.

			The xen output can only be used by Xen PV guests.

	edac_report=	[HW,EDAC] Control how to report EDAC event
			Format: {"on" | "off" | "force"}
			on: enable EDAC to report H/W event. May be overridden
			by other higher priority error reporting module.
			off: disable H/W event reporting through EDAC.
			force: enforce the use of EDAC to report H/W event.
			default: on.

	ekgdboc=	[X86,KGDB] Allow early kernel console debugging
			ekgdboc=kbd

			This is designed to be used in conjunction with
			the boot argument: earlyprintk=vga

	edd=		[EDD]
			Format: {"off" | "on" | "skip[mbr]"}

	efi=		[EFI]
			Format: { "old_map" }
			old_map [X86-64]: switch to the old ioremap-based EFI
			runtime services mapping. 32-bit still uses this one by
			default.

	efi_no_storage_paranoia [EFI; X86]
			Using this parameter you can use more than 50% of
			your efi variable storage. Use this parameter only if
			you are really sure that your UEFI does sane gc and
			fulfills the spec otherwise your board may brick.

	eisa_irq_edge=	[PARISC,HW]
			See header of drivers/parisc/eisa.c.

	elanfreq=	[X86-32]
			See comment before function elanfreq_setup() in
			arch/x86/kernel/cpu/cpufreq/elanfreq.c.

	elevator=	[IOSCHED]
			Format: {"cfq" | "deadline" | "noop"}
			See Documentation/block/cfq-iosched.txt and
			Documentation/block/deadline-iosched.txt for details.

	elfcorehdr=[size[KMG]@]offset[KMG] [IA64,PPC,SH,X86,S390]
			Specifies physical address of start of kernel core
			image elf header and optionally the size. Generally
			kexec loader will pass this option to capture kernel.
			See Documentation/kdump/kdump.txt for details.

	enable_mtrr_cleanup [X86]
			The kernel tries to adjust MTRR layout from continuous
			to discrete, to make X server driver able to add WB
			entry later. This parameter enables that.

	enable_timer_pin_1 [X86]
			Enable PIN 1 of APIC timer
			Can be useful to work around chipset bugs
			(in particular on some ATI chipsets).
			The kernel tries to set a reasonable default.

	enforcing	[SELINUX] Set initial enforcing status.
			Format: {"0" | "1"}
			See security/selinux/Kconfig help text.
			0 -- permissive (log only, no denials).
			1 -- enforcing (deny and log).
			Default value is 0.
			Value can be changed at runtime via /selinux/enforce.

	erst_disable	[ACPI]
			Disable Error Record Serialization Table (ERST)
			support.

	ether=		[HW,NET] Ethernet cards parameters
			This option is obsoleted by the "netdev=" option, which
			has equivalent usage. See its documentation for details.

	evm=		[EVM]
			Format: { "fix" }
			Permit 'security.evm' to be updated regardless of
			current integrity status.

	failslab=
	fail_page_alloc=
	fail_make_request=[KNL]
			General fault injection mechanism.
			Format: <interval>,<probability>,<space>,<times>
			See also Documentation/fault-injection/.

	floppy=		[HW]
			See Documentation/blockdev/floppy.txt.

	force_pal_cache_flush
			[IA-64] Avoid check_sal_cache_flush which may hang on
			buggy SAL_CACHE_FLUSH implementations. Using this
			parameter will force ia64_sal_cache_flush to call
			ia64_pal_cache_flush instead of SAL_CACHE_FLUSH.

	forcepae [X86-32]
			Forcefully enable Physical Address Extension (PAE).
			Many Pentium M systems disable PAE but may have a
			functionally usable PAE implementation.
			Warning: use of this parameter will taint the kernel
			and may cause unknown problems.

	ftrace=[tracer]
			[FTRACE] will set and start the specified tracer
			as early as possible in order to facilitate early
			boot debugging.

	ftrace_dump_on_oops[=orig_cpu]
			[FTRACE] will dump the trace buffers on oops.
			If no parameter is passed, ftrace will dump
			buffers of all CPUs, but if you pass orig_cpu, it will
			dump only the buffer of the CPU that triggered the
			oops.

	ftrace_filter=[function-list]
			[FTRACE] Limit the functions traced by the function
			tracer at boot up. function-list is a comma separated
			list of functions. This list can be changed at run
			time by the set_ftrace_filter file in the debugfs
			tracing directory.

	ftrace_notrace=[function-list]
			[FTRACE] Do not trace the functions specified in
			function-list. This list can be changed at run time
			by the set_ftrace_notrace file in the debugfs
			tracing directory.

	ftrace_graph_filter=[function-list]
			[FTRACE] Limit the top level callers functions traced
			by the function graph tracer at boot up.
			function-list is a comma separated list of functions
			that can be changed at run time by the
			set_graph_function file in the debugfs tracing directory.

	gamecon.map[2|3]=
			[HW,JOY] Multisystem joystick and NES/SNES/PSX pad
			support via parallel port (up to 5 devices per port)
			Format: <port#>,<pad1>,<pad2>,<pad3>,<pad4>,<pad5>
			See also Documentation/input/joystick-parport.txt

	gamma=		[HW,DRM]

	gart_fix_e820=  [X86_64] disable the fix e820 for K8 GART
			Format: off | on
			default: on

	gcov_persist=	[GCOV] When non-zero (default), profiling data for
			kernel modules is saved and remains accessible via
			debugfs, even when the module is unloaded/reloaded.
			When zero, profiling data is discarded and associated
			debugfs files are removed at module unload time.

	gpt		[EFI] Forces disk with valid GPT signature but
			invalid Protective MBR to be treated as GPT. If the
			primary GPT is corrupted, it enables the backup/alternate
			GPT to be used instead.

	grcan.enable0=	[HW] Configuration of physical interface 0. Determines
			the "Enable 0" bit of the configuration register.
			Format: 0 | 1
			Default: 0
	grcan.enable1=	[HW] Configuration of physical interface 1. Determines
			the "Enable 0" bit of the configuration register.
			Format: 0 | 1
			Default: 0
	grcan.select=	[HW] Select which physical interface to use.
			Format: 0 | 1
			Default: 0
	grcan.txsize=	[HW] Sets the size of the tx buffer.
			Format: <unsigned int> such that (txsize & ~0x1fffc0) == 0.
			Default: 1024
	grcan.rxsize=	[HW] Sets the size of the rx buffer.
			Format: <unsigned int> such that (rxsize & ~0x1fffc0) == 0.
			Default: 1024

	hashdist=	[KNL,NUMA] Large hashes allocated during boot
			are distributed across NUMA nodes.  Defaults on
			for 64-bit NUMA, off otherwise.
			Format: 0 | 1 (for off | on)

	hcl=		[IA-64] SGI's Hardware Graph compatibility layer

	hd=		[EIDE] (E)IDE hard drive subsystem geometry
			Format: <cyl>,<head>,<sect>

	hest_disable	[ACPI]
			Disable Hardware Error Source Table (HEST) support;
			corresponding firmware-first mode error processing
			logic will be disabled.

	highmem=nn[KMG]	[KNL,BOOT] forces the highmem zone to have an exact
			size of <nn>. This works even on boxes that have no
			highmem otherwise. This also works to reduce highmem
			size on bigger boxes.

	highres=	[KNL] Enable/disable high resolution timer mode.
			Valid parameters: "on", "off"
			Default: "on"

	hisax=		[HW,ISDN]
			See Documentation/isdn/README.HiSax.

	hlt		[BUGS=ARM,SH]

	hpet=		[X86-32,HPET] option to control HPET usage
			Format: { enable (default) | disable | force |
				verbose }
			disable: disable HPET and use PIT instead
			force: allow force enabled of undocumented chips (ICH4,
				VIA, nVidia)
			verbose: show contents of HPET registers during setup

	hpet_mmap=	[X86, HPET_MMAP] Allow userspace to mmap HPET
			registers.  Default set by CONFIG_HPET_MMAP_DEFAULT.

	hugepages=	[HW,X86-32,IA-64] HugeTLB pages to allocate at boot.
	hugepagesz=	[HW,IA-64,PPC,X86-64] The size of the HugeTLB pages.
			On x86-64 and powerpc, this option can be specified
			multiple times interleaved with hugepages= to reserve
			huge pages of different sizes. Valid pages sizes on
			x86-64 are 2M (when the CPU supports "pse") and 1G
			(when the CPU supports the "pdpe1gb" cpuinfo flag)
			Note that 1GB pages can only be allocated at boot time
			using hugepages= and not freed afterwards.

	hvc_iucv=	[S390] Number of z/VM IUCV hypervisor console (HVC)
			       terminal devices. Valid values: 0..8
	hvc_iucv_allow=	[S390] Comma-separated list of z/VM user IDs.
			       If specified, z/VM IUCV HVC accepts connections
			       from listed z/VM user IDs only.

	hwthread_map=	[METAG] Comma-separated list of Linux cpu id to
			        hardware thread id mappings.
				Format: <cpu>:<hwthread>

	keep_bootcon	[KNL]
			Do not unregister boot console at start. This is only
			useful for debugging when something happens in the window
			between unregistering the boot console and initializing
			the real console.

	i2c_bus=	[HW] Override the default board specific I2C bus speed
			     or register an additional I2C bus that is not
			     registered from board initialization code.
			     Format:
			     <bus_id>,<clkrate>

	i8042.debug	[HW] Toggle i8042 debug mode
	i8042.direct	[HW] Put keyboard port into non-translated mode
	i8042.dumbkbd	[HW] Pretend that controller can only read data from
			     keyboard and cannot control its state
			     (Don't attempt to blink the leds)
	i8042.noaux	[HW] Don't check for auxiliary (== mouse) port
	i8042.nokbd	[HW] Don't check/create keyboard port
	i8042.noloop	[HW] Disable the AUX Loopback command while probing
			     for the AUX port
	i8042.nomux	[HW] Don't check presence of an active multiplexing
			     controller
	i8042.nopnp	[HW] Don't use ACPIPnP / PnPBIOS to discover KBD/AUX
			     controllers
	i8042.notimeout	[HW] Ignore timeout condition signalled by controller
	i8042.reset	[HW] Reset the controller during init and cleanup
	i8042.unlock	[HW] Unlock (ignore) the keylock

	i810=		[HW,DRM]

	i8k.ignore_dmi	[HW] Continue probing hardware even if DMI data
			indicates that the driver is running on unsupported
			hardware.
	i8k.force	[HW] Activate i8k driver even if SMM BIOS signature
			does not match list of supported models.
	i8k.power_status
			[HW] Report power status in /proc/i8k
			(disabled by default)
	i8k.restricted	[HW] Allow controlling fans only if SYS_ADMIN
			capability is set.

	i915.invert_brightness=
			[DRM] Invert the sense of the variable that is used to
			set the brightness of the panel backlight. Normally a
			brightness value of 0 indicates backlight switched off,
			and the maximum of the brightness value sets the backlight
			to maximum brightness. If this parameter is set to 0
			(default) and the machine requires it, or this parameter
			is set to 1, a brightness value of 0 sets the backlight
			to maximum brightness, and the maximum of the brightness
			value switches the backlight off.
			-1 -- never invert brightness
			 0 -- machine default
			 1 -- force brightness inversion

	icn=		[HW,ISDN]
			Format: <io>[,<membase>[,<icn_id>[,<icn_id2>]]]

	ide-core.nodma=	[HW] (E)IDE subsystem
			Format: =0.0 to prevent dma on hda, =0.1 hdb =1.0 hdc
			.vlb_clock .pci_clock .noflush .nohpa .noprobe .nowerr
			.cdrom .chs .ignore_cable are additional options
			See Documentation/ide/ide.txt.

	ide-pci-generic.all-generic-ide [HW] (E)IDE subsystem
			Claim all unknown PCI IDE storage controllers.

	idle=		[X86]
			Format: idle=poll, idle=halt, idle=nomwait
			Poll forces a polling idle loop that can slightly
			improve the performance of waking up a idle CPU, but
			will use a lot of power and make the system run hot.
			Not recommended.
			idle=halt: Halt is forced to be used for CPU idle.
			In such case C2/C3 won't be used again.
			idle=nomwait: Disable mwait for CPU C-states

	ignore_loglevel	[KNL]
			Ignore loglevel setting - this will print /all/
			kernel messages to the console. Useful for debugging.
			We also add it as printk module parameter, so users
			could change it dynamically, usually by
			/sys/module/printk/parameters/ignore_loglevel.

	ihash_entries=	[KNL]
			Set number of hash buckets for inode cache.

	ima_appraise=	[IMA] appraise integrity measurements
			Format: { "off" | "enforce" | "fix" }
			default: "enforce"

	ima_appraise_tcb [IMA]
			The builtin appraise policy appraises all files
			owned by uid=0.

	ima_hash=	[IMA]
			Format: { md5 | sha1 | rmd160 | sha256 | sha384
				   | sha512 | ... }
			default: "sha1"

			The list of supported hash algorithms is defined
			in crypto/hash_info.h.

	ima_tcb		[IMA]
			Load a policy which meets the needs of the Trusted
			Computing Base.  This means IMA will measure all
			programs exec'd, files mmap'd for exec, and all files
			opened for read by uid=0.

	ima_template=   [IMA]
			Select one of defined IMA measurements template formats.
			Formats: { "ima" | "ima-ng" }
			Default: "ima-ng"

	init=		[KNL]
			Format: <full_path>
			Run specified binary instead of /sbin/init as init
			process.

	initcall_debug	[KNL] Trace initcalls as they are executed.  Useful
			for working out where the kernel is dying during
			startup.

	initcall_blacklist=  [KNL] Do not execute a comma-separated list of
			initcall functions.  Useful for debugging built-in
			modules and initcalls.

	initrd=		[BOOT] Specify the location of the initial ramdisk

	inport.irq=	[HW] Inport (ATI XL and Microsoft) busmouse driver
			Format: <irq>

	int_pln_enable  [x86] Enable power limit notification interrupt

	integrity_audit=[IMA]
			Format: { "0" | "1" }
			0 -- basic integrity auditing messages. (Default)
			1 -- additional integrity auditing messages.

	intel_iommu=	[DMAR] Intel IOMMU driver (DMAR) option
		on
			Enable intel iommu driver.
		off
			Disable intel iommu driver.
		igfx_off [Default Off]
			By default, gfx is mapped as normal device. If a gfx
			device has a dedicated DMAR unit, the DMAR unit is
			bypassed by not enabling DMAR with this option. In
			this case, gfx device will use physical address for
			DMA.
		forcedac [x86_64]
			With this option iommu will not optimize to look
			for io virtual address below 32-bit forcing dual
			address cycle on pci bus for cards supporting greater
			than 32-bit addressing. The default is to look
			for translation below 32-bit and if not available
			then look in the higher range.
		strict [Default Off]
			With this option on every unmap_single operation will
			result in a hardware IOTLB flush operation as opposed
			to batching them for performance.
		sp_off [Default Off]
			By default, super page will be supported if Intel IOMMU
			has the capability. With this option, super page will
			not be supported.

	intel_idle.max_cstate=	[KNL,HW,ACPI,X86]
			0	disables intel_idle and fall back on acpi_idle.
			1 to 6	specify maximum depth of C-state.

	intel_pstate=  [X86]
		       disable
		         Do not enable intel_pstate as the default
		         scaling driver for the supported processors

	intremap=	[X86-64, Intel-IOMMU]
			on	enable Interrupt Remapping (default)
			off	disable Interrupt Remapping
			nosid	disable Source ID checking
			no_x2apic_optout
				BIOS x2APIC opt-out request will be ignored

	iomem=		Disable strict checking of access to MMIO memory
		strict	regions from userspace.
		relaxed

	iommu=		[x86]
		off
		force
		noforce
		biomerge
		panic
		nopanic
		merge
		nomerge
		forcesac
		soft
		pt		[x86, IA-64]


	io7=		[HW] IO7 for Marvel based alpha systems
			See comment before marvel_specify_io7 in
			arch/alpha/kernel/core_marvel.c.

	io_delay=	[X86] I/O delay method
		0x80
			Standard port 0x80 based delay
		0xed
			Alternate port 0xed based delay (needed on some systems)
		udelay
			Simple two microseconds delay
		none
			No delay

	ip=		[IP_PNP]
			See Documentation/filesystems/nfs/nfsroot.txt.

	ip2=		[HW] Set IO/IRQ pairs for up to 4 IntelliPort boards
			See comment before ip2_setup() in
			drivers/char/ip2/ip2base.c.

	irqfixup	[HW]
			When an interrupt is not handled search all handlers
			for it. Intended to get systems with badly broken
			firmware running.

	irqpoll		[HW]
			When an interrupt is not handled search all handlers
			for it. Also check all handlers each timer
			interrupt. Intended to get systems with badly broken
			firmware running.

	isapnp=		[ISAPNP]
			Format: <RDP>,<reset>,<pci_scan>,<verbosity>

	isolcpus=	[KNL,SMP] Isolate CPUs from the general scheduler.
			Format:
			<cpu number>,...,<cpu number>
			or
			<cpu number>-<cpu number>
			(must be a positive range in ascending order)
			or a mixture
			<cpu number>,...,<cpu number>-<cpu number>

			This option can be used to specify one or more CPUs
			to isolate from the general SMP balancing and scheduling
			algorithms. You can move a process onto or off an
			"isolated" CPU via the CPU affinity syscalls or cpuset.
			<cpu number> begins at 0 and the maximum value is
			"number of CPUs in system - 1".

			This option is the preferred way to isolate CPUs. The
			alternative -- manually setting the CPU mask of all
			tasks in the system -- can cause problems and
			suboptimal load balancer performance.

	iucv=		[HW,NET]

	ivrs_ioapic	[HW,X86_64]
			Provide an override to the IOAPIC-ID<->DEVICE-ID
			mapping provided in the IVRS ACPI table. For
			example, to map IOAPIC-ID decimal 10 to
			PCI device 00:14.0 write the parameter as:
				ivrs_ioapic[10]=00:14.0

	ivrs_hpet	[HW,X86_64]
			Provide an override to the HPET-ID<->DEVICE-ID
			mapping provided in the IVRS ACPI table. For
			example, to map HPET-ID decimal 0 to
			PCI device 00:14.0 write the parameter as:
				ivrs_hpet[0]=00:14.0

	js=		[HW,JOY] Analog joystick
			See Documentation/input/joystick.txt.

	keepinitrd	[HW,ARM]

	kernelcore=nn[KMG]	[KNL,X86,IA-64,PPC] This parameter
			specifies the amount of memory usable by the kernel
			for non-movable allocations.  The requested amount is
			spread evenly throughout all nodes in the system. The
			remaining memory in each node is used for Movable
			pages. In the event, a node is too small to have both
			kernelcore and Movable pages, kernelcore pages will
			take priority and other nodes will have a larger number
			of Movable pages.  The Movable zone is used for the
			allocation of pages that may be reclaimed or moved
			by the page migration subsystem.  This means that
			HugeTLB pages may not be allocated from this zone.
			Note that allocations like PTEs-from-HighMem still
			use the HighMem zone if it exists, and the Normal
			zone if it does not.

	kgdbdbgp=	[KGDB,HW] kgdb over EHCI usb debug port.
			Format: <Controller#>[,poll interval]
			The controller # is the number of the ehci usb debug
			port as it is probed via PCI.  The poll interval is
			optional and is the number seconds in between
			each poll cycle to the debug port in case you need
			the functionality for interrupting the kernel with
			gdb or control-c on the dbgp connection.  When
			not using this parameter you use sysrq-g to break into
			the kernel debugger.

	kgdboc=		[KGDB,HW] kgdb over consoles.
			Requires a tty driver that supports console polling,
			or a supported polling keyboard driver (non-usb).
			 Serial only format: <serial_device>[,baud]
			 keyboard only format: kbd
			 keyboard and serial format: kbd,<serial_device>[,baud]
			Optional Kernel mode setting:
			 kms, kbd format: kms,kbd
			 kms, kbd and serial format: kms,kbd,<ser_dev>[,baud]

	kgdbwait	[KGDB] Stop kernel execution and enter the
			kernel debugger at the earliest opportunity.

	kmac=		[MIPS] korina ethernet MAC address.
			Configure the RouterBoard 532 series on-chip
			Ethernet adapter MAC address.

	kmemleak=	[KNL] Boot-time kmemleak enable/disable
			Valid arguments: on, off
			Default: on

	kmemcheck=	[X86] Boot-time kmemcheck enable/disable/one-shot mode
			Valid arguments: 0, 1, 2
			kmemcheck=0 (disabled)
			kmemcheck=1 (enabled)
			kmemcheck=2 (one-shot mode)
			Default: 2 (one-shot mode)

	kstack=N	[X86] Print N words from the kernel stack
			in oops dumps.

	kvm.ignore_msrs=[KVM] Ignore guest accesses to unhandled MSRs.
			Default is 0 (don't ignore, but inject #GP)

	kvm.mmu_audit=	[KVM] This is a R/W parameter which allows audit
			KVM MMU at runtime.
			Default is 0 (off)

	kvm-amd.nested=	[KVM,AMD] Allow nested virtualization in KVM/SVM.
			Default is 1 (enabled)

	kvm-amd.npt=	[KVM,AMD] Disable nested paging (virtualized MMU)
			for all guests.
			Default is 1 (enabled) if in 64-bit or 32-bit PAE mode.

	kvm-intel.ept=	[KVM,Intel] Disable extended page tables
			(virtualized MMU) support on capable Intel chips.
			Default is 1 (enabled)

	kvm-intel.emulate_invalid_guest_state=
			[KVM,Intel] Enable emulation of invalid guest states
			Default is 0 (disabled)

	kvm-intel.flexpriority=
			[KVM,Intel] Disable FlexPriority feature (TPR shadow).
			Default is 1 (enabled)

	kvm-intel.nested=
			[KVM,Intel] Enable VMX nesting (nVMX).
			Default is 0 (disabled)

	kvm-intel.unrestricted_guest=
			[KVM,Intel] Disable unrestricted guest feature
			(virtualized real and unpaged mode) on capable
			Intel chips. Default is 1 (enabled)

	kvm-intel.vpid=	[KVM,Intel] Disable Virtual Processor Identification
			feature (tagged TLBs) on capable Intel chips.
			Default is 1 (enabled)

	l2cr=		[PPC]

	l3cr=		[PPC]

	lapic		[X86-32,APIC] Enable the local APIC even if BIOS
			disabled it.

	lapic=		[x86,APIC] "notscdeadline" Do not use TSC deadline
			value for LAPIC timer one-shot implementation. Default
			back to the programmable timer unit in the LAPIC.

	lapic_timer_c2_ok	[X86,APIC] trust the local apic timer
			in C2 power state.

	libata.dma=	[LIBATA] DMA control
			libata.dma=0	  Disable all PATA and SATA DMA
			libata.dma=1	  PATA and SATA Disk DMA only
			libata.dma=2	  ATAPI (CDROM) DMA only
			libata.dma=4	  Compact Flash DMA only
			Combinations also work, so libata.dma=3 enables DMA
			for disks and CDROMs, but not CFs.

	libata.ignore_hpa=	[LIBATA] Ignore HPA limit
			libata.ignore_hpa=0	  keep BIOS limits (default)
			libata.ignore_hpa=1	  ignore limits, using full disk

	libata.noacpi	[LIBATA] Disables use of ACPI in libata suspend/resume
			when set.
			Format: <int>

	libata.force=	[LIBATA] Force configurations.  The format is comma
			separated list of "[ID:]VAL" where ID is
			PORT[.DEVICE].  PORT and DEVICE are decimal numbers
			matching port, link or device.  Basically, it matches
			the ATA ID string printed on console by libata.  If
			the whole ID part is omitted, the last PORT and DEVICE
			values are used.  If ID hasn't been specified yet, the
			configuration applies to all ports, links and devices.

			If only DEVICE is omitted, the parameter applies to
			the port and all links and devices behind it.  DEVICE
			number of 0 either selects the first device or the
			first fan-out link behind PMP device.  It does not
			select the host link.  DEVICE number of 15 selects the
			host link and device attached to it.

			The VAL specifies the configuration to force.  As long
			as there's no ambiguity shortcut notation is allowed.
			For example, both 1.5 and 1.5G would work for 1.5Gbps.
			The following configurations can be forced.

			* Cable type: 40c, 80c, short40c, unk, ign or sata.
			  Any ID with matching PORT is used.

			* SATA link speed limit: 1.5Gbps or 3.0Gbps.

			* Transfer mode: pio[0-7], mwdma[0-4] and udma[0-7].
			  udma[/][16,25,33,44,66,100,133] notation is also
			  allowed.

			* [no]ncq: Turn on or off NCQ.

			* nohrst, nosrst, norst: suppress hard, soft
                          and both resets.

			* rstonce: only attempt one reset during
			  hot-unplug link recovery

			* dump_id: dump IDENTIFY data.

			* atapi_dmadir: Enable ATAPI DMADIR bridge support

			* disable: Disable this device.

			If there are multiple matching configurations changing
			the same attribute, the last one is used.

	memblock=debug	[KNL] Enable memblock debug messages.

	load_ramdisk=	[RAM] List of ramdisks to load from floppy
			See Documentation/blockdev/ramdisk.txt.

	lockd.nlm_grace_period=P  [NFS] Assign grace period.
			Format: <integer>

	lockd.nlm_tcpport=N	[NFS] Assign TCP port.
			Format: <integer>

	lockd.nlm_timeout=T	[NFS] Assign timeout value.
			Format: <integer>

	lockd.nlm_udpport=M	[NFS] Assign UDP port.
			Format: <integer>

	logibm.irq=	[HW,MOUSE] Logitech Bus Mouse Driver
			Format: <irq>

	loglevel=	All Kernel Messages with a loglevel smaller than the
			console loglevel will be printed to the console. It can
			also be changed with klogd or other programs. The
			loglevels are defined as follows:

			0 (KERN_EMERG)		system is unusable
			1 (KERN_ALERT)		action must be taken immediately
			2 (KERN_CRIT)		critical conditions
			3 (KERN_ERR)		error conditions
			4 (KERN_WARNING)	warning conditions
			5 (KERN_NOTICE)		normal but significant condition
			6 (KERN_INFO)		informational
			7 (KERN_DEBUG)		debug-level messages

	log_buf_len=n[KMG]	Sets the size of the printk ring buffer,
			in bytes.  n must be a power of two.  The default
			size is set in the kernel config file.

	logo.nologo	[FB] Disables display of the built-in Linux logo.
			This may be used to provide more screen space for
			kernel log messages and is useful when debugging
			kernel boot problems.

	lp=0		[LP]	Specify parallel ports to use, e.g,
	lp=port[,port...]	lp=none,parport0 (lp0 not configured, lp1 uses
	lp=reset		first parallel port). 'lp=0' disables the
	lp=auto			printer driver. 'lp=reset' (which can be
				specified in addition to the ports) causes
				attached printers to be reset. Using
				lp=port1,port2,... specifies the parallel ports
				to associate lp devices with, starting with
				lp0. A port specification may be 'none' to skip
				that lp device, or a parport name such as
				'parport0'. Specifying 'lp=auto' instead of a
				port specification list means that device IDs
				from each port should be examined, to see if
				an IEEE 1284-compliant printer is attached; if
				so, the driver will manage that printer.
				See also header of drivers/char/lp.c.

	lpj=n		[KNL]
			Sets loops_per_jiffy to given constant, thus avoiding
			time-consuming boot-time autodetection (up to 250 ms per
			CPU). 0 enables autodetection (default). To determine
			the correct value for your kernel, boot with normal
			autodetection and see what value is printed. Note that
			on SMP systems the preset will be applied to all CPUs,
			which is likely to cause problems if your CPUs need
			significantly divergent settings. An incorrect value
			will cause delays in the kernel to be wrong, leading to
			unpredictable I/O errors and other breakage. Although
			unlikely, in the extreme case this might damage your
			hardware.

	ltpc=		[NET]
			Format: <io>,<irq>,<dma>

	machvec=	[IA-64] Force the use of a particular machine-vector
			(machvec) in a generic kernel.
			Example: machvec=hpzx1_swiotlb

	machtype=	[Loongson] Share the same kernel image file between different
			 yeeloong laptop.
			Example: machtype=lemote-yeeloong-2f-7inch

	max_addr=nn[KMG]	[KNL,BOOT,ia64] All physical memory greater
			than or equal to this physical address is ignored.

	maxcpus=	[SMP] Maximum number of processors that	an SMP kernel
			should make use of.  maxcpus=n : n >= 0 limits the
			kernel to using 'n' processors.  n=0 is a special case,
			it is equivalent to "nosmp", which also disables
			the IO APIC.

	max_loop=	[LOOP] The number of loop block devices that get
	(loop.max_loop)	unconditionally pre-created at init time. The default
			number is configured by BLK_DEV_LOOP_MIN_COUNT. Instead
			of statically allocating a predefined number, loop
			devices can be requested on-demand with the
			/dev/loop-control interface.

	mce		[X86-32] Machine Check Exception

	mce=option	[X86-64] See Documentation/x86/x86_64/boot-options.txt

	md=		[HW] RAID subsystems devices and level
			See Documentation/md.txt.

	mdacon=		[MDA]
			Format: <first>,<last>
			Specifies range of consoles to be captured by the MDA.

	mem=nn[KMG]	[KNL,BOOT] Force usage of a specific amount of memory
			Amount of memory to be used when the kernel is not able
			to see the whole system memory or for test.
			[X86] Work as limiting max address. Use together
			with memmap= to avoid physical address space collisions.
			Without memmap= PCI devices could be placed at addresses
			belonging to unused RAM.

	mem=nopentium	[BUGS=X86-32] Disable usage of 4MB pages for kernel
			memory.

	memchunk=nn[KMG]
			[KNL,SH] Allow user to override the default size for
			per-device physically contiguous DMA buffers.

	memmap=exactmap	[KNL,X86] Enable setting of an exact
			E820 memory map, as specified by the user.
			Such memmap=exactmap lines can be constructed based on
			BIOS output or other requirements. See the memmap=nn@ss
			option description.

	memmap=nn[KMG]@ss[KMG]
			[KNL] Force usage of a specific region of memory.
			Region of memory to be used is from ss to ss+nn.

	memmap=nn[KMG]#ss[KMG]
			[KNL,ACPI] Mark specific memory as ACPI data.
			Region of memory to be marked is from ss to ss+nn.

	memmap=nn[KMG]$ss[KMG]
			[KNL,ACPI] Mark specific memory as reserved.
			Region of memory to be reserved is from ss to ss+nn.
			Example: Exclude memory from 0x18690000-0x1869ffff
			         memmap=64K$0x18690000
			         or
			         memmap=0x10000$0x18690000

	memory_corruption_check=0/1 [X86]
			Some BIOSes seem to corrupt the first 64k of
			memory when doing things like suspend/resume.
			Setting this option will scan the memory
			looking for corruption.  Enabling this will
			both detect corruption and prevent the kernel
			from using the memory being corrupted.
			However, its intended as a diagnostic tool; if
			repeatable BIOS-originated corruption always
			affects the same memory, you can use memmap=
			to prevent the kernel from using that memory.

	memory_corruption_check_size=size [X86]
			By default it checks for corruption in the low
			64k, making this memory unavailable for normal
			use.  Use this parameter to scan for
			corruption in more or less memory.

	memory_corruption_check_period=seconds [X86]
			By default it checks for corruption every 60
			seconds.  Use this parameter to check at some
			other rate.  0 disables periodic checking.

	memtest=	[KNL,X86] Enable memtest
			Format: <integer>
			default : 0 <disable>
			Specifies the number of memtest passes to be
			performed. Each pass selects another test
			pattern from a given set of patterns. Memtest
			fills the memory with this pattern, validates
			memory contents and reserves bad memory
			regions that are detected.

	meye.*=		[HW] Set MotionEye Camera parameters
			See Documentation/video4linux/meye.txt.

	mfgpt_irq=	[IA-32] Specify the IRQ to use for the
			Multi-Function General Purpose Timers on AMD Geode
			platforms.

	mfgptfix	[X86-32] Fix MFGPT timers on AMD Geode platforms when
			the BIOS has incorrectly applied a workaround. TinyBIOS
			version 0.98 is known to be affected, 0.99 fixes the
			problem by letting the user disable the workaround.

	mga=		[HW,DRM]

	min_addr=nn[KMG]	[KNL,BOOT,ia64] All physical memory below this
			physical address is ignored.

	mini2440=	[ARM,HW,KNL]
			Format:[0..2][b][c][t]
			Default: "0tb"
			MINI2440 configuration specification:
			0 - The attached screen is the 3.5" TFT
			1 - The attached screen is the 7" TFT
			2 - The VGA Shield is attached (1024x768)
			Leaving out the screen size parameter will not load
			the TFT driver, and the framebuffer will be left
			unconfigured.
			b - Enable backlight. The TFT backlight pin will be
			linked to the kernel VESA blanking code and a GPIO
			LED. This parameter is not necessary when using the
			VGA shield.
			c - Enable the s3c camera interface.
			t - Reserved for enabling touchscreen support. The
			touchscreen support is not enabled in the mainstream
			kernel as of 2.6.30, a preliminary port can be found
			in the "bleeding edge" mini2440 support kernel at
			http://repo.or.cz/w/linux-2.6/mini2440.git

	mminit_loglevel=
			[KNL] When CONFIG_DEBUG_MEMORY_INIT is set, this
			parameter allows control of the logging verbosity for
			the additional memory initialisation checks. A value
			of 0 disables mminit logging and a level of 4 will
			log everything. Information is printed at KERN_DEBUG
			so loglevel=8 may also need to be specified.

	module.sig_enforce
			[KNL] When CONFIG_MODULE_SIG is set, this means that
			modules without (valid) signatures will fail to load.
			Note that if CONFIG_MODULE_SIG_FORCE is set, that
			is always true, so this option does nothing.

	mousedev.tap_time=
			[MOUSE] Maximum time between finger touching and
			leaving touchpad surface for touch to be considered
			a tap and be reported as a left button click (for
			touchpads working in absolute mode only).
			Format: <msecs>
	mousedev.xres=	[MOUSE] Horizontal screen resolution, used for devices
			reporting absolute coordinates, such as tablets
	mousedev.yres=	[MOUSE] Vertical screen resolution, used for devices
			reporting absolute coordinates, such as tablets

	movablecore=nn[KMG]	[KNL,X86,IA-64,PPC] This parameter
			is similar to kernelcore except it specifies the
			amount of memory used for migratable allocations.
			If both kernelcore and movablecore is specified,
			then kernelcore will be at *least* the specified
			value but may be more. If movablecore on its own
			is specified, the administrator must be careful
			that the amount of memory usable for all allocations
			is not too small.

	movable_node	[KNL,X86] Boot-time switch to enable the effects
			of CONFIG_MOVABLE_NODE=y. See mm/Kconfig for details.

	MTD_Partition=	[MTD]
			Format: <name>,<region-number>,<size>,<offset>

	MTD_Region=	[MTD] Format:
			<name>,<region-number>[,<base>,<size>,<buswidth>,<altbuswidth>]

	mtdparts=	[MTD]
			See drivers/mtd/cmdlinepart.c.

	multitce=off	[PPC]  This parameter disables the use of the pSeries
			firmware feature for updating multiple TCE entries
			at a time.

	onenand.bdry=	[HW,MTD] Flex-OneNAND Boundary Configuration

			Format: [die0_boundary][,die0_lock][,die1_boundary][,die1_lock]

			boundary - index of last SLC block on Flex-OneNAND.
				   The remaining blocks are configured as MLC blocks.
			lock	 - Configure if Flex-OneNAND boundary should be locked.
				   Once locked, the boundary cannot be changed.
				   1 indicates lock status, 0 indicates unlock status.

	mtdset=		[ARM]
			ARM/S3C2412 JIVE boot control

			See arch/arm/mach-s3c2412/mach-jive.c

	mtouchusb.raw_coordinates=
			[HW] Make the MicroTouch USB driver use raw coordinates
			('y', default) or cooked coordinates ('n')

	mtrr_chunk_size=nn[KMG] [X86]
			used for mtrr cleanup. It is largest continuous chunk
			that could hold holes aka. UC entries.

	mtrr_gran_size=nn[KMG] [X86]
			Used for mtrr cleanup. It is granularity of mtrr block.
			Default is 1.
			Large value could prevent small alignment from
			using up MTRRs.

	mtrr_spare_reg_nr=n [X86]
			Format: <integer>
			Range: 0,7 : spare reg number
			Default : 1
			Used for mtrr cleanup. It is spare mtrr entries number.
			Set to 2 or more if your graphical card needs more.

	n2=		[NET] SDL Inc. RISCom/N2 synchronous serial card

	netdev=		[NET] Network devices parameters
			Format: <irq>,<io>,<mem_start>,<mem_end>,<name>
			Note that mem_start is often overloaded to mean
			something different and driver-specific.
			This usage is only documented in each driver source
			file if at all.

	nf_conntrack.acct=
			[NETFILTER] Enable connection tracking flow accounting
			0 to disable accounting
			1 to enable accounting
			Default value is 0.

	nfsaddrs=	[NFS] Deprecated.  Use ip= instead.
			See Documentation/filesystems/nfs/nfsroot.txt.

	nfsroot=	[NFS] nfs root filesystem for disk-less boxes.
			See Documentation/filesystems/nfs/nfsroot.txt.

	nfsrootdebug	[NFS] enable nfsroot debugging messages.
			See Documentation/filesystems/nfs/nfsroot.txt.

	nfs.callback_tcpport=
			[NFS] set the TCP port on which the NFSv4 callback
			channel should listen.

	nfs.cache_getent=
			[NFS] sets the pathname to the program which is used
			to update the NFS client cache entries.

	nfs.cache_getent_timeout=
			[NFS] sets the timeout after which an attempt to
			update a cache entry is deemed to have failed.

	nfs.idmap_cache_timeout=
			[NFS] set the maximum lifetime for idmapper cache
			entries.

	nfs.enable_ino64=
			[NFS] enable 64-bit inode numbers.
			If zero, the NFS client will fake up a 32-bit inode
			number for the readdir() and stat() syscalls instead
			of returning the full 64-bit number.
			The default is to return 64-bit inode numbers.

	nfs.max_session_slots=
			[NFSv4.1] Sets the maximum number of session slots
			the client will attempt to negotiate with the server.
			This limits the number of simultaneous RPC requests
			that the client can send to the NFSv4.1 server.
			Note that there is little point in setting this
			value higher than the max_tcp_slot_table_limit.

	nfs.nfs4_disable_idmapping=
			[NFSv4] When set to the default of '1', this option
			ensures that both the RPC level authentication
			scheme and the NFS level operations agree to use
			numeric uids/gids if the mount is using the
			'sec=sys' security flavour. In effect it is
			disabling idmapping, which can make migration from
			legacy NFSv2/v3 systems to NFSv4 easier.
			Servers that do not support this mode of operation
			will be autodetected by the client, and it will fall
			back to using the idmapper.
			To turn off this behaviour, set the value to '0'.
	nfs.nfs4_unique_id=
			[NFS4] Specify an additional fixed unique ident-
			ification string that NFSv4 clients can insert into
			their nfs_client_id4 string.  This is typically a
			UUID that is generated at system install time.

	nfs.send_implementation_id =
			[NFSv4.1] Send client implementation identification
			information in exchange_id requests.
			If zero, no implementation identification information
			will be sent.
			The default is to send the implementation identification
			information.
	
	nfs.recover_lost_locks =
			[NFSv4] Attempt to recover locks that were lost due
			to a lease timeout on the server. Please note that
			doing this risks data corruption, since there are
			no guarantees that the file will remain unchanged
			after the locks are lost.
			If you want to enable the kernel legacy behaviour of
			attempting to recover these locks, then set this
			parameter to '1'.
			The default parameter value of '0' causes the kernel
			not to attempt recovery of lost locks.

	nfsd.nfs4_disable_idmapping=
			[NFSv4] When set to the default of '1', the NFSv4
			server will return only numeric uids and gids to
			clients using auth_sys, and will accept numeric uids
			and gids from such clients.  This is intended to ease
			migration from NFSv2/v3.

	objlayoutdriver.osd_login_prog=
			[NFS] [OBJLAYOUT] sets the pathname to the program which
			is used to automatically discover and login into new
			osd-targets. Please see:
			Documentation/filesystems/pnfs.txt for more explanations

	nmi_debug=	[KNL,AVR32,SH] Specify one or more actions to take
			when a NMI is triggered.
			Format: [state][,regs][,debounce][,die]

	nmi_watchdog=	[KNL,BUGS=X86] Debugging features for SMP kernels
			Format: [panic,][nopanic,][num]
			Valid num: 0
			0 - turn nmi_watchdog off
			When panic is specified, panic when an NMI watchdog
			timeout occurs (or 'nopanic' to override the opposite
			default).
			This is useful when you use a panic=... timeout and
			need the box quickly up again.

	netpoll.carrier_timeout=
			[NET] Specifies amount of time (in seconds) that
			netpoll should wait for a carrier. By default netpoll
			waits 4 seconds.

	no387		[BUGS=X86-32] Tells the kernel to use the 387 maths
			emulation library even if a 387 maths coprocessor
			is present.

	no_console_suspend
			[HW] Never suspend the console
			Disable suspending of consoles during suspend and
			hibernate operations.  Once disabled, debugging
			messages can reach various consoles while the rest
			of the system is being put to sleep (ie, while
			debugging driver suspend/resume hooks).  This may
			not work reliably with all consoles, but is known
			to work with serial and VGA consoles.
			To facilitate more flexible debugging, we also add
			console_suspend, a printk module parameter to control
			it. Users could use console_suspend (usually
			/sys/module/printk/parameters/console_suspend) to
			turn on/off it dynamically.

	noaliencache	[MM, NUMA, SLAB] Disables the allocation of alien
			caches in the slab allocator.  Saves per-node memory,
			but will impact performance.

	noalign		[KNL,ARM]

	noapic		[SMP,APIC] Tells the kernel to not make use of any
			IOAPICs that may be present in the system.

	nokaslr		[X86]
			Disable kernel and module base offset ASLR (Address
			Space Layout Randomization) if built into the kernel.

	noautogroup	Disable scheduler automatic task group creation.

	nobats		[PPC] Do not use BATs for mapping kernel lowmem
			on "Classic" PPC cores.

	nocache		[ARM]

	noclflush	[BUGS=X86] Don't use the CLFLUSH instruction

	nodelayacct	[KNL] Disable per-task delay accounting

	nodisconnect	[HW,SCSI,M68K] Disables SCSI disconnects.

	nodsp		[SH] Disable hardware DSP at boot time.

	noefi		[X86] Disable EFI runtime services support.

	noexec		[IA-64]

	noexec		[X86]
			On X86-32 available only on PAE configured kernels.
			noexec=on: enable non-executable mappings (default)
			noexec=off: disable non-executable mappings

	nosmap		[X86]
			Disable SMAP (Supervisor Mode Access Prevention)
			even if it is supported by processor.

	nosmep		[X86]
			Disable SMEP (Supervisor Mode Execution Prevention)
			even if it is supported by processor.

	noexec32	[X86-64]
			This affects only 32-bit executables.
			noexec32=on: enable non-executable mappings (default)
				read doesn't imply executable mappings
			noexec32=off: disable non-executable mappings
				read implies executable mappings

	nofpu		[SH] Disable hardware FPU at boot time.

	nofxsr		[BUGS=X86-32] Disables x86 floating point extended
			register save and restore. The kernel will only save
			legacy floating-point registers on task switch.

	noxsave		[BUGS=X86] Disables x86 extended register state save
			and restore using xsave. The kernel will fallback to
			enabling legacy floating-point and sse state.

	eagerfpu=	[X86]
			on	enable eager fpu restore
			off	disable eager fpu restore
			auto	selects the default scheme, which automatically
				enables eagerfpu restore for xsaveopt.

	nohlt		[BUGS=ARM,SH] Tells the kernel that the sleep(SH) or
			wfi(ARM) instruction doesn't work correctly and not to
			use it. This is also useful when using JTAG debugger.

	no_file_caps	Tells the kernel not to honor file capabilities.  The
			only way then for a file to be executed with privilege
			is to be setuid root or executed by root.

	nohalt		[IA-64] Tells the kernel not to use the power saving
			function PAL_HALT_LIGHT when idle. This increases
			power-consumption. On the positive side, it reduces
			interrupt wake-up latency, which may improve performance
			in certain environments such as networked servers or
			real-time systems.

	nohz=		[KNL] Boottime enable/disable dynamic ticks
			Valid arguments: on, off
			Default: on

	nohz_full=	[KNL,BOOT]
			In kernels built with CONFIG_NO_HZ_FULL=y, set
			the specified list of CPUs whose tick will be stopped
			whenever possible. The boot CPU will be forced outside
			the range to maintain the timekeeping.
			The CPUs in this range must also be included in the
			rcu_nocbs= set.

	noiotrap	[SH] Disables trapped I/O port accesses.

	noirqdebug	[X86-32] Disables the code which attempts to detect and
			disable unhandled interrupt sources.

	no_timer_check	[X86,APIC] Disables the code which tests for
			broken timer IRQ sources.

	noisapnp	[ISAPNP] Disables ISA PnP code.

	noinitrd	[RAM] Tells the kernel not to load any configured
			initial RAM disk.

	nointremap	[X86-64, Intel-IOMMU] Do not enable interrupt
			remapping.
			[Deprecated - use intremap=off]

	nointroute	[IA-64]

	nojitter	[IA-64] Disables jitter checking for ITC timers.

	no-kvmclock	[X86,KVM] Disable paravirtualized KVM clock driver

	no-kvmapf	[X86,KVM] Disable paravirtualized asynchronous page
			fault handling.

	no-steal-acc    [X86,KVM] Disable paravirtualized steal time accounting.
			steal time is computed, but won't influence scheduler
			behaviour

	nolapic		[X86-32,APIC] Do not enable or use the local APIC.

	nolapic_timer	[X86-32,APIC] Do not use the local APIC timer.

	noltlbs		[PPC] Do not use large page/tlb entries for kernel
			lowmem mapping on PPC40x.

	nomca		[IA-64] Disable machine check abort handling

	nomce		[X86-32] Machine Check Exception

	nomfgpt		[X86-32] Disable Multi-Function General Purpose
			Timer usage (for AMD Geode machines).

	nonmi_ipi	[X86] Disable using NMI IPIs during panic/reboot to
			shutdown the other cpus.  Instead use the REBOOT_VECTOR
			irq.

	nomodule	Disable module load

	nopat		[X86] Disable PAT (page attribute table extension of
			pagetables) support.

	norandmaps	Don't use address space randomization.  Equivalent to
			echo 0 > /proc/sys/kernel/randomize_va_space

	noreplace-paravirt	[X86,IA-64,PV_OPS] Don't patch paravirt_ops

	noreplace-smp	[X86-32,SMP] Don't replace SMP instructions
			with UP alternatives

	nordrand	[X86] Disable kernel use of the RDRAND and
			RDSEED instructions even if they are supported
			by the processor.  RDRAND and RDSEED are still
			available to user space applications.

	noresume	[SWSUSP] Disables resume and restores original swap
			space.

	no-scroll	[VGA] Disables scrollback.
			This is required for the Braillex ib80-piezo Braille
			reader made by F.H. Papenmeier (Germany).

	nosbagart	[IA-64]

	nosep		[BUGS=X86-32] Disables x86 SYSENTER/SYSEXIT support.

	nosmp		[SMP] Tells an SMP kernel to act as a UP kernel,
			and disable the IO APIC.  legacy for "maxcpus=0".

	nosoftlockup	[KNL] Disable the soft-lockup detector.

	nosync		[HW,M68K] Disables sync negotiation for all devices.

	notsc		[BUGS=X86-32] Disable Time Stamp Counter

	nousb		[USB] Disable the USB subsystem

	nowatchdog	[KNL] Disable the lockup detector (NMI watchdog).

	nowb		[ARM]

	nox2apic	[X86-64,APIC] Do not enable x2APIC mode.

	cpu0_hotplug	[X86] Turn on CPU0 hotplug feature when
			CONFIG_BOOTPARAM_HOTPLUG_CPU0 is off.
			Some features depend on CPU0. Known dependencies are:
			1. Resume from suspend/hibernate depends on CPU0.
			Suspend/hibernate will fail if CPU0 is offline and you
			need to online CPU0 before suspend/hibernate.
			2. PIC interrupts also depend on CPU0. CPU0 can't be
			removed if a PIC interrupt is detected.
			It's said poweroff/reboot may depend on CPU0 on some
			machines although I haven't seen such issues so far
			after CPU0 is offline on a few tested machines.
			If the dependencies are under your control, you can
			turn on cpu0_hotplug.

	nptcg=		[IA-64] Override max number of concurrent global TLB
			purges which is reported from either PAL_VM_SUMMARY or
			SAL PALO.

	nr_cpus=	[SMP] Maximum number of processors that	an SMP kernel
			could support.  nr_cpus=n : n >= 1 limits the kernel to
			supporting 'n' processors. Later in runtime you can not
			use hotplug cpu feature to put more cpu back to online.
			just like you compile the kernel NR_CPUS=n

	nr_uarts=	[SERIAL] maximum number of UARTs to be registered.

	numa_balancing=	[KNL,X86] Enable or disable automatic NUMA balancing.
			Allowed values are enable and disable

	numa_zonelist_order= [KNL, BOOT] Select zonelist order for NUMA.
			one of ['zone', 'node', 'default'] can be specified
			This can be set from sysctl after boot.
			See Documentation/sysctl/vm.txt for details.

	ohci1394_dma=early	[HW] enable debugging via the ohci1394 driver.
			See Documentation/debugging-via-ohci1394.txt for more
			info.

	olpc_ec_timeout= [OLPC] ms delay when issuing EC commands
			Rather than timing out after 20 ms if an EC
			command is not properly ACKed, override the length
			of the timeout.  We have interrupts disabled while
			waiting for the ACK, so if this is set too high
			interrupts *may* be lost!

	omap_mux=	[OMAP] Override bootloader pin multiplexing.
			Format: <mux_mode0.mode_name=value>...
			For example, to override I2C bus2:
			omap_mux=i2c2_scl.i2c2_scl=0x100,i2c2_sda.i2c2_sda=0x100

	oprofile.timer=	[HW]
			Use timer interrupt instead of performance counters

	oprofile.cpu_type=	Force an oprofile cpu type
			This might be useful if you have an older oprofile
			userland or if you want common events.
			Format: { arch_perfmon }
			arch_perfmon: [X86] Force use of architectural
				perfmon on Intel CPUs instead of the
				CPU specific event set.
			timer: [X86] Force use of architectural NMI
				timer mode (see also oprofile.timer
				for generic hr timer mode)
				[s390] Force legacy basic mode sampling
                                (report cpu_type "timer")

	oops=panic	Always panic on oopses. Default is to just kill the
			process, but there is a small probability of
			deadlocking the machine.
			This will also cause panics on machine check exceptions.
			Useful together with panic=30 to trigger a reboot.

	OSS		[HW,OSS]
			See Documentation/sound/oss/oss-parameters.txt

	panic=		[KNL] Kernel behaviour on panic: delay <timeout>
			timeout > 0: seconds before rebooting
			timeout = 0: wait forever
			timeout < 0: reboot immediately
			Format: <timeout>

	crash_kexec_post_notifiers
			Run kdump after running panic-notifiers and dumping
			kmsg. This only for the users who doubt kdump always
			succeeds in any situation.
			Note that this also increases risks of kdump failure,
			because some panic notifiers can make the crashed
			kernel more unstable.

	parkbd.port=	[HW] Parallel port number the keyboard adapter is
			connected to, default is 0.
			Format: <parport#>
	parkbd.mode=	[HW] Parallel port keyboard adapter mode of operation,
			0 for XT, 1 for AT (default is AT).
			Format: <mode>

	parport=	[HW,PPT] Specify parallel ports. 0 disables.
			Format: { 0 | auto | 0xBBB[,IRQ[,DMA]] }
			Use 'auto' to force the driver to use any
			IRQ/DMA settings detected (the default is to
			ignore detected IRQ/DMA settings because of
			possible conflicts). You can specify the base
			address, IRQ, and DMA settings; IRQ and DMA
			should be numbers, or 'auto' (for using detected
			settings on that particular port), or 'nofifo'
			(to avoid using a FIFO even if it is detected).
			Parallel ports are assigned in the order they
			are specified on the command line, starting
			with parport0.

	parport_init_mode=	[HW,PPT]
			Configure VIA parallel port to operate in
			a specific mode. This is necessary on Pegasos
			computer where firmware has no options for setting
			up parallel port mode and sets it to spp.
			Currently this function knows 686a and 8231 chips.
			Format: [spp|ps2|epp|ecp|ecpepp]

	pause_on_oops=
			Halt all CPUs after the first oops has been printed for
			the specified number of seconds.  This is to be used if
			your oopses keep scrolling off the screen.

	pcbit=		[HW,ISDN]

	pcd.		[PARIDE]
			See header of drivers/block/paride/pcd.c.
			See also Documentation/blockdev/paride.txt.

	pci=option[,option...]	[PCI] various PCI subsystem options:
		earlydump	[X86] dump PCI config space before the kernel
			        changes anything
		off		[X86] don't probe for the PCI bus
		bios		[X86-32] force use of PCI BIOS, don't access
				the hardware directly. Use this if your machine
				has a non-standard PCI host bridge.
		nobios		[X86-32] disallow use of PCI BIOS, only direct
				hardware access methods are allowed. Use this
				if you experience crashes upon bootup and you
				suspect they are caused by the BIOS.
		conf1		[X86] Force use of PCI Configuration
				Mechanism 1.
		conf2		[X86] Force use of PCI Configuration
				Mechanism 2.
		noaer		[PCIE] If the PCIEAER kernel config parameter is
				enabled, this kernel boot option can be used to
				disable the use of PCIE advanced error reporting.
		nodomains	[PCI] Disable support for multiple PCI
				root domains (aka PCI segments, in ACPI-speak).
		nommconf	[X86] Disable use of MMCONFIG for PCI
				Configuration
		check_enable_amd_mmconf [X86] check for and enable
				properly configured MMIO access to PCI
				config space on AMD family 10h CPU
		nomsi		[MSI] If the PCI_MSI kernel config parameter is
				enabled, this kernel boot option can be used to
				disable the use of MSI interrupts system-wide.
		noioapicquirk	[APIC] Disable all boot interrupt quirks.
				Safety option to keep boot IRQs enabled. This
				should never be necessary.
		ioapicreroute	[APIC] Enable rerouting of boot IRQs to the
				primary IO-APIC for bridges that cannot disable
				boot IRQs. This fixes a source of spurious IRQs
				when the system masks IRQs.
		noioapicreroute	[APIC] Disable workaround that uses the
				boot IRQ equivalent of an IRQ that connects to
				a chipset where boot IRQs cannot be disabled.
				The opposite of ioapicreroute.
		biosirq		[X86-32] Use PCI BIOS calls to get the interrupt
				routing table. These calls are known to be buggy
				on several machines and they hang the machine
				when used, but on other computers it's the only
				way to get the interrupt routing table. Try
				this option if the kernel is unable to allocate
				IRQs or discover secondary PCI buses on your
				motherboard.
		rom		[X86] Assign address space to expansion ROMs.
				Use with caution as certain devices share
				address decoders between ROMs and other
				resources.
		norom		[X86] Do not assign address space to
				expansion ROMs that do not already have
				BIOS assigned address ranges.
		nobar		[X86] Do not assign address space to the
				BARs that weren't assigned by the BIOS.
		irqmask=0xMMMM	[X86] Set a bit mask of IRQs allowed to be
				assigned automatically to PCI devices. You can
				make the kernel exclude IRQs of your ISA cards
				this way.
		pirqaddr=0xAAAAA	[X86] Specify the physical address
				of the PIRQ table (normally generated
				by the BIOS) if it is outside the
				F0000h-100000h range.
		lastbus=N	[X86] Scan all buses thru bus #N. Can be
				useful if the kernel is unable to find your
				secondary buses and you want to tell it
				explicitly which ones they are.
		assign-busses	[X86] Always assign all PCI bus
				numbers ourselves, overriding
				whatever the firmware may have done.
		usepirqmask	[X86] Honor the possible IRQ mask stored
				in the BIOS $PIR table. This is needed on
				some systems with broken BIOSes, notably
				some HP Pavilion N5400 and Omnibook XE3
				notebooks. This will have no effect if ACPI
				IRQ routing is enabled.
		noacpi		[X86] Do not use ACPI for IRQ routing
				or for PCI scanning.
		use_crs		[X86] Use PCI host bridge window information
				from ACPI.  On BIOSes from 2008 or later, this
				is enabled by default.  If you need to use this,
				please report a bug.
		nocrs		[X86] Ignore PCI host bridge windows from ACPI.
			        If you need to use this, please report a bug.
		routeirq	Do IRQ routing for all PCI devices.
				This is normally done in pci_enable_device(),
				so this option is a temporary workaround
				for broken drivers that don't call it.
		skip_isa_align	[X86] do not align io start addr, so can
				handle more pci cards
		firmware	[ARM] Do not re-enumerate the bus but instead
				just use the configuration from the
				bootloader. This is currently used on
				IXP2000 systems where the bus has to be
				configured a certain way for adjunct CPUs.
		noearly		[X86] Don't do any early type 1 scanning.
				This might help on some broken boards which
				machine check when some devices' config space
				is read. But various workarounds are disabled
				and some IOMMU drivers will not work.
		bfsort		Sort PCI devices into breadth-first order.
				This sorting is done to get a device
				order compatible with older (<= 2.4) kernels.
		nobfsort	Don't sort PCI devices into breadth-first order.
		pcie_bus_tune_off	Disable PCIe MPS (Max Payload Size)
				tuning and use the BIOS-configured MPS defaults.
		pcie_bus_safe	Set every device's MPS to the largest value
				supported by all devices below the root complex.
		pcie_bus_perf	Set device MPS to the largest allowable MPS
				based on its parent bus. Also set MRRS (Max
				Read Request Size) to the largest supported
				value (no larger than the MPS that the device
				or bus can support) for best performance.
		pcie_bus_peer2peer	Set every device's MPS to 128B, which
				every device is guaranteed to support. This
				configuration allows peer-to-peer DMA between
				any pair of devices, possibly at the cost of
				reduced performance.  This also guarantees
				that hot-added devices will work.
		cbiosize=nn[KMG]	The fixed amount of bus space which is
				reserved for the CardBus bridge's IO window.
				The default value is 256 bytes.
		cbmemsize=nn[KMG]	The fixed amount of bus space which is
				reserved for the CardBus bridge's memory
				window. The default value is 64 megabytes.
		resource_alignment=
				Format:
				[<order of align>@][<domain>:]<bus>:<slot>.<func>[; ...]
				Specifies alignment and device to reassign
				aligned memory resources.
				If <order of align> is not specified,
				PAGE_SIZE is used as alignment.
				PCI-PCI bridge can be specified, if resource
				windows need to be expanded.
		ecrc=		Enable/disable PCIe ECRC (transaction layer
				end-to-end CRC checking).
				bios: Use BIOS/firmware settings. This is the
				the default.
				off: Turn ECRC off
				on: Turn ECRC on.
		hpiosize=nn[KMG]	The fixed amount of bus space which is
				reserved for hotplug bridge's IO window.
				Default size is 256 bytes.
		hpmemsize=nn[KMG]	The fixed amount of bus space which is
				reserved for hotplug bridge's memory window.
				Default size is 2 megabytes.
		realloc=	Enable/disable reallocating PCI bridge resources
				if allocations done by BIOS are too small to
				accommodate resources required by all child
				devices.
				off: Turn realloc off
				on: Turn realloc on
		realloc		same as realloc=on
		noari		do not use PCIe ARI.
		pcie_scan_all	Scan all possible PCIe devices.  Otherwise we
				only look for one device below a PCIe downstream
				port.

	pcie_aspm=	[PCIE] Forcibly enable or disable PCIe Active State Power
			Management.
		off	Disable ASPM.
		force	Enable ASPM even on devices that claim not to support it.
			WARNING: Forcing ASPM on may cause system lockups.

	pcie_hp=	[PCIE] PCI Express Hotplug driver options:
		nomsi	Do not use MSI for PCI Express Native Hotplug (this
			makes all PCIe ports use INTx for hotplug services).

	pcie_ports=	[PCIE] PCIe ports handling:
		auto	Ask the BIOS whether or not to use native PCIe services
			associated with PCIe ports (PME, hot-plug, AER).  Use
			them only if that is allowed by the BIOS.
		native	Use native PCIe services associated with PCIe ports
			unconditionally.
		compat	Treat PCIe ports as PCI-to-PCI bridges, disable the PCIe
			ports driver.

	pcie_pme=	[PCIE,PM] Native PCIe PME signaling options:
		nomsi	Do not use MSI for native PCIe PME signaling (this makes
			all PCIe root ports use INTx for all services).

	pcmv=		[HW,PCMCIA] BadgePAD 4

	pd_ignore_unused
			[PM]
			Keep all power-domains already enabled by bootloader on,
			even if no driver has claimed them. This is useful
			for debug and development, but should not be
			needed on a platform with proper driver support.

	pd.		[PARIDE]
			See Documentation/blockdev/paride.txt.

	pdcchassis=	[PARISC,HW] Disable/Enable PDC Chassis Status codes at
			boot time.
			Format: { 0 | 1 }
			See arch/parisc/kernel/pdc_chassis.c

	percpu_alloc=	Select which percpu first chunk allocator to use.
			Currently supported values are "embed" and "page".
			Archs may support subset or none of the	selections.
			See comments in mm/percpu.c for details on each
			allocator.  This parameter is primarily	for debugging
			and performance comparison.

	pf.		[PARIDE]
			See Documentation/blockdev/paride.txt.

	pg.		[PARIDE]
			See Documentation/blockdev/paride.txt.

	pirq=		[SMP,APIC] Manual mp-table setup
			See Documentation/x86/i386/IO-APIC.txt.

	plip=		[PPT,NET] Parallel port network link
			Format: { parport<nr> | timid | 0 }
			See also Documentation/parport.txt.

	pmtmr=		[X86] Manual setup of pmtmr I/O Port.
			Override pmtimer IOPort with a hex value.
			e.g. pmtmr=0x508

	pnp.debug=1	[PNP]
			Enable PNP debug messages (depends on the
			CONFIG_PNP_DEBUG_MESSAGES option).  Change at run-time
			via /sys/module/pnp/parameters/debug.  We always show
			current resource usage; turning this on also shows
			possible settings and some assignment information.

	pnpacpi=	[ACPI]
			{ off }

	pnpbios=	[ISAPNP]
			{ on | off | curr | res | no-curr | no-res }

	pnp_reserve_irq=
			[ISAPNP] Exclude IRQs for the autoconfiguration

	pnp_reserve_dma=
			[ISAPNP] Exclude DMAs for the autoconfiguration

	pnp_reserve_io=	[ISAPNP] Exclude I/O ports for the autoconfiguration
			Ranges are in pairs (I/O port base and size).

	pnp_reserve_mem=
			[ISAPNP] Exclude memory regions for the
			autoconfiguration.
			Ranges are in pairs (memory base and size).

	ports=		[IP_VS_FTP] IPVS ftp helper module
			Default is 21.
			Up to 8 (IP_VS_APP_MAX_PORTS) ports
			may be specified.
			Format: <port>,<port>....

	print-fatal-signals=
			[KNL] debug: print fatal signals

			If enabled, warn about various signal handling
			related application anomalies: too many signals,
			too many POSIX.1 timers, fatal signals causing a
			coredump - etc.

			If you hit the warning due to signal overflow,
			you might want to try "ulimit -i unlimited".

			default: off.

	printk.always_kmsg_dump=
			Trigger kmsg_dump for cases other than kernel oops or
			panics
			Format: <bool>  (1/Y/y=enable, 0/N/n=disable)
			default: disabled

	printk.time=	Show timing data prefixed to each printk message line
			Format: <bool>  (1/Y/y=enable, 0/N/n=disable)

	processor.max_cstate=	[HW,ACPI]
			Limit processor to maximum C-state
			max_cstate=9 overrides any DMI blacklist limit.

	processor.nocst	[HW,ACPI]
			Ignore the _CST method to determine C-states,
			instead using the legacy FADT method

	profile=	[KNL] Enable kernel profiling via /proc/profile
			Format: [schedule,]<number>
			Param: "schedule" - profile schedule points.
			Param: <number> - step/bucket size as a power of 2 for
				statistical time based profiling.
			Param: "sleep" - profile D-state sleeping (millisecs).
				Requires CONFIG_SCHEDSTATS
			Param: "kvm" - profile VM exits.

	prompt_ramdisk=	[RAM] List of RAM disks to prompt for floppy disk
			before loading.
			See Documentation/blockdev/ramdisk.txt.

	psmouse.proto=	[HW,MOUSE] Highest PS2 mouse protocol extension to
			probe for; one of (bare|imps|exps|lifebook|any).
	psmouse.rate=	[HW,MOUSE] Set desired mouse report rate, in reports
			per second.
	psmouse.resetafter=	[HW,MOUSE]
			Try to reset the device after so many bad packets
			(0 = never).
	psmouse.resolution=
			[HW,MOUSE] Set desired mouse resolution, in dpi.
	psmouse.smartscroll=
			[HW,MOUSE] Controls Logitech smartscroll autorepeat.
			0 = disabled, 1 = enabled (default).

	pstore.backend=	Specify the name of the pstore backend to use

	pt.		[PARIDE]
			See Documentation/blockdev/paride.txt.

	pty.legacy_count=
			[KNL] Number of legacy pty's. Overwrites compiled-in
			default number.

	quiet		[KNL] Disable most log messages

	r128=		[HW,DRM]

	raid=		[HW,RAID]
			See Documentation/md.txt.

	ramdisk_blocksize=	[RAM]
			See Documentation/blockdev/ramdisk.txt.

	ramdisk_size=	[RAM] Sizes of RAM disks in kilobytes
			See Documentation/blockdev/ramdisk.txt.

	rcu_nocbs=	[KNL]
			In kernels built with CONFIG_RCU_NOCB_CPU=y, set
			the specified list of CPUs to be no-callback CPUs.
			Invocation of these CPUs' RCU callbacks will
			be offloaded to "rcuox/N" kthreads created for
			that purpose, where "x" is "b" for RCU-bh, "p"
			for RCU-preempt, and "s" for RCU-sched, and "N"
			is the CPU number.  This reduces OS jitter on the
			offloaded CPUs, which can be useful for HPC and
			real-time workloads.  It can also improve energy
			efficiency for asymmetric multiprocessors.

	rcu_nocb_poll	[KNL]
			Rather than requiring that offloaded CPUs
			(specified by rcu_nocbs= above) explicitly
			awaken the corresponding "rcuoN" kthreads,
			make these kthreads poll for callbacks.
			This improves the real-time response for the
			offloaded CPUs by relieving them of the need to
			wake up the corresponding kthread, but degrades
			energy efficiency by requiring that the kthreads
			periodically wake up to do the polling.

	rcutree.blimit=	[KNL]
			Set maximum number of finished RCU callbacks to
			process in one batch.

	rcutree.rcu_fanout_leaf= [KNL]
			Increase the number of CPUs assigned to each
			leaf rcu_node structure.  Useful for very large
			systems.

	rcutree.jiffies_till_first_fqs= [KNL]
			Set delay from grace-period initialization to
			first attempt to force quiescent states.
			Units are jiffies, minimum value is zero,
			and maximum value is HZ.

	rcutree.jiffies_till_next_fqs= [KNL]
			Set delay between subsequent attempts to force
			quiescent states.  Units are jiffies, minimum
			value is one, and maximum value is HZ.

	rcutree.qhimark= [KNL]
			Set threshold of queued RCU callbacks beyond which
			batch limiting is disabled.

	rcutree.qlowmark= [KNL]
			Set threshold of queued RCU callbacks below which
			batch limiting is re-enabled.

	rcutree.rcu_idle_gp_delay= [KNL]
			Set wakeup interval for idle CPUs that have
			RCU callbacks (RCU_FAST_NO_HZ=y).

	rcutree.rcu_idle_lazy_gp_delay= [KNL]
			Set wakeup interval for idle CPUs that have
			only "lazy" RCU callbacks (RCU_FAST_NO_HZ=y).
			Lazy RCU callbacks are those which RCU can
			prove do nothing more than free memory.

	rcutorture.fqs_duration= [KNL]
			Set duration of force_quiescent_state bursts.

	rcutorture.fqs_holdoff= [KNL]
			Set holdoff time within force_quiescent_state bursts.

	rcutorture.fqs_stutter= [KNL]
			Set wait time between force_quiescent_state bursts.

	rcutorture.gp_exp= [KNL]
			Use expedited update-side primitives.

	rcutorture.gp_normal= [KNL]
			Use normal (non-expedited) update-side primitives.
			If both gp_exp and gp_normal are set, do both.
			If neither gp_exp nor gp_normal are set, still
			do both.

	rcutorture.n_barrier_cbs= [KNL]
			Set callbacks/threads for rcu_barrier() testing.

	rcutorture.nfakewriters= [KNL]
			Set number of concurrent RCU writers.  These just
			stress RCU, they don't participate in the actual
			test, hence the "fake".

	rcutorture.nreaders= [KNL]
			Set number of RCU readers.

	rcutorture.object_debug= [KNL]
			Enable debug-object double-call_rcu() testing.

	rcutorture.onoff_holdoff= [KNL]
			Set time (s) after boot for CPU-hotplug testing.

	rcutorture.onoff_interval= [KNL]
			Set time (s) between CPU-hotplug operations, or
			zero to disable CPU-hotplug testing.

	rcutorture.rcutorture_runnable= [BOOT]
			Start rcutorture running at boot time.

	rcutorture.shuffle_interval= [KNL]
			Set task-shuffle interval (s).  Shuffling tasks
			allows some CPUs to go into dyntick-idle mode
			during the rcutorture test.

	rcutorture.shutdown_secs= [KNL]
			Set time (s) after boot system shutdown.  This
			is useful for hands-off automated testing.

	rcutorture.stall_cpu= [KNL]
			Duration of CPU stall (s) to test RCU CPU stall
			warnings, zero to disable.

	rcutorture.stall_cpu_holdoff= [KNL]
			Time to wait (s) after boot before inducing stall.

	rcutorture.stat_interval= [KNL]
			Time (s) between statistics printk()s.

	rcutorture.stutter= [KNL]
			Time (s) to stutter testing, for example, specifying
			five seconds causes the test to run for five seconds,
			wait for five seconds, and so on.  This tests RCU's
			ability to transition abruptly to and from idle.

	rcutorture.test_boost= [KNL]
			Test RCU priority boosting?  0=no, 1=maybe, 2=yes.
			"Maybe" means test if the RCU implementation
			under test support RCU priority boosting.

	rcutorture.test_boost_duration= [KNL]
			Duration (s) of each individual boost test.

	rcutorture.test_boost_interval= [KNL]
			Interval (s) between each boost test.

	rcutorture.test_no_idle_hz= [KNL]
			Test RCU's dyntick-idle handling.  See also the
			rcutorture.shuffle_interval parameter.

	rcutorture.torture_type= [KNL]
			Specify the RCU implementation to test.

	rcutorture.verbose= [KNL]
			Enable additional printk() statements.

	rcupdate.rcu_expedited= [KNL]
			Use expedited grace-period primitives, for
			example, synchronize_rcu_expedited() instead
			of synchronize_rcu().  This reduces latency,
			but can increase CPU utilization, degrade
			real-time latency, and degrade energy efficiency.

	rcupdate.rcu_cpu_stall_suppress= [KNL]
			Suppress RCU CPU stall warning messages.

	rcupdate.rcu_cpu_stall_timeout= [KNL]
			Set timeout for RCU CPU stall warning messages.

	rdinit=		[KNL]
			Format: <full_path>
			Run specified binary instead of /init from the ramdisk,
			used for early userspace startup. See initrd.

	reboot=		[KNL]
			Format (x86 or x86_64):
				[w[arm] | c[old] | h[ard] | s[oft] | g[pio]] \
				[[,]s[mp]#### \
				[[,]b[ios] | a[cpi] | k[bd] | t[riple] | e[fi] | p[ci]] \
				[[,]f[orce]
			Where reboot_mode is one of warm (soft) or cold (hard) or gpio,
			      reboot_type is one of bios, acpi, kbd, triple, efi, or pci,
			      reboot_force is either force or not specified,
			      reboot_cpu is s[mp]#### with #### being the processor
					to be used for rebooting.

	relax_domain_level=
			[KNL, SMP] Set scheduler's default relax_domain_level.
			See Documentation/cgroups/cpusets.txt.

	relative_sleep_states=
			[SUSPEND] Use sleep state labeling where the deepest
			state available other than hibernation is always "mem".
			Format: { "0" | "1" }
			0 -- Traditional sleep state labels.
			1 -- Relative sleep state labels.

	reserve=	[KNL,BUGS] Force the kernel to ignore some iomem area

	reservetop=	[X86-32]
			Format: nn[KMG]
			Reserves a hole at the top of the kernel virtual
			address space.

	reservelow=	[X86]
			Format: nn[K]
			Set the amount of memory to reserve for BIOS at
			the bottom of the address space.

	reset_devices	[KNL] Force drivers to reset the underlying device
			during initialization.

	resume=		[SWSUSP]
			Specify the partition device for software suspend
			Format:
			{/dev/<dev> | PARTUUID=<uuid> | <int>:<int> | <hex>}

	resume_offset=	[SWSUSP]
			Specify the offset from the beginning of the partition
			given by "resume=" at which the swap header is located,
			in <PAGE_SIZE> units (needed only for swap files).
			See  Documentation/power/swsusp-and-swap-files.txt

	resumedelay=	[HIBERNATION] Delay (in seconds) to pause before attempting to
			read the resume files

	resumewait	[HIBERNATION] Wait (indefinitely) for resume device to show up.
			Useful for devices that are detected asynchronously
			(e.g. USB and MMC devices).

	hibernate=	[HIBERNATION]
		noresume	Don't check if there's a hibernation image
				present during boot.
		nocompress	Don't compress/decompress hibernation images.

	retain_initrd	[RAM] Keep initrd memory after extraction

	rhash_entries=	[KNL,NET]
			Set number of hash buckets for route cache

	ro		[KNL] Mount root device read-only on boot

	root=		[KNL] Root filesystem
			See name_to_dev_t comment in init/do_mounts.c.

	rootdelay=	[KNL] Delay (in seconds) to pause before attempting to
			mount the root filesystem

	rootflags=	[KNL] Set root filesystem mount option string

	rootfstype=	[KNL] Set root filesystem type

	rootwait	[KNL] Wait (indefinitely) for root device to show up.
			Useful for devices that are detected asynchronously
			(e.g. USB and MMC devices).

	rproc_mem=nn[KMG][@address]
			[KNL,ARM,CMA] Remoteproc physical memory block.
			Memory area to be used by remote processor image,
			managed by CMA.

	rw		[KNL] Mount root device read-write on boot

	S		[KNL] Run init in single mode

	sa1100ir	[NET]
			See drivers/net/irda/sa1100_ir.c.

	sbni=		[NET] Granch SBNI12 leased line adapter

	sched_debug	[KNL] Enables verbose scheduler debug messages.

	skew_tick=	[KNL] Offset the periodic timer tick per cpu to mitigate
			xtime_lock contention on larger systems, and/or RCU lock
			contention on all systems with CONFIG_MAXSMP set.
			Format: { "0" | "1" }
			0 -- disable. (may be 1 via CONFIG_CMDLINE="skew_tick=1"
			1 -- enable.
			Note: increases power consumption, thus should only be
			enabled if running jitter sensitive (HPC/RT) workloads.

	security=	[SECURITY] Choose a security module to enable at boot.
			If this boot parameter is not specified, only the first
			security module asking for security registration will be
			loaded. An invalid security module name will be treated
			as if no module has been chosen.

	selinux=	[SELINUX] Disable or enable SELinux at boot time.
			Format: { "0" | "1" }
			See security/selinux/Kconfig help text.
			0 -- disable.
			1 -- enable.
			Default value is set via kernel config option.
			If enabled at boot time, /selinux/disable can be used
			later to disable prior to initial policy load.

	apparmor=	[APPARMOR] Disable or enable AppArmor at boot time
			Format: { "0" | "1" }
			See security/apparmor/Kconfig help text
			0 -- disable.
			1 -- enable.
			Default value is set via kernel config option.

	serialnumber	[BUGS=X86-32]

	shapers=	[NET]
			Maximal number of shapers.

	show_msr=	[x86] show boot-time MSR settings
			Format: { <integer> }
			Show boot-time (BIOS-initialized) MSR settings.
			The parameter means the number of CPUs to show,
			for example 1 means boot CPU only.

	simeth=		[IA-64]
	simscsi=

	slram=		[HW,MTD]

	slab_max_order=	[MM, SLAB]
			Determines the maximum allowed order for slabs.
			A high setting may cause OOMs due to memory
			fragmentation.  Defaults to 1 for systems with
			more than 32MB of RAM, 0 otherwise.

	slub_debug[=options[,slabs]]	[MM, SLUB]
			Enabling slub_debug allows one to determine the
			culprit if slab objects become corrupted. Enabling
			slub_debug can create guard zones around objects and
			may poison objects when not in use. Also tracks the
			last alloc / free. For more information see
			Documentation/vm/slub.txt.

	slub_max_order= [MM, SLUB]
			Determines the maximum allowed order for slabs.
			A high setting may cause OOMs due to memory
			fragmentation. For more information see
			Documentation/vm/slub.txt.

	slub_min_objects=	[MM, SLUB]
			The minimum number of objects per slab. SLUB will
			increase the slab order up to slub_max_order to
			generate a sufficiently large slab able to contain
			the number of objects indicated. The higher the number
			of objects the smaller the overhead of tracking slabs
			and the less frequently locks need to be acquired.
			For more information see Documentation/vm/slub.txt.

	slub_min_order=	[MM, SLUB]
			Determines the minimum page order for slabs. Must be
			lower than slub_max_order.
			For more information see Documentation/vm/slub.txt.

	slub_nomerge	[MM, SLUB]
			Disable merging of slabs with similar size. May be
			necessary if there is some reason to distinguish
			allocs to different slabs. Debug options disable
			merging on their own.
			For more information see Documentation/vm/slub.txt.

	smart2=		[HW]
			Format: <io1>[,<io2>[,...,<io8>]]

	smsc-ircc2.nopnp	[HW] Don't use PNP to discover SMC devices
	smsc-ircc2.ircc_cfg=	[HW] Device configuration I/O port
	smsc-ircc2.ircc_sir=	[HW] SIR base I/O port
	smsc-ircc2.ircc_fir=	[HW] FIR base I/O port
	smsc-ircc2.ircc_irq=	[HW] IRQ line
	smsc-ircc2.ircc_dma=	[HW] DMA channel
	smsc-ircc2.ircc_transceiver= [HW] Transceiver type:
				0: Toshiba Satellite 1800 (GP data pin select)
				1: Fast pin select (default)
				2: ATC IRMode

	softlockup_panic=
			[KNL] Should the soft-lockup detector generate panics.
			Format: <integer>

	sonypi.*=	[HW] Sony Programmable I/O Control Device driver
			See Documentation/laptops/sonypi.txt

	spia_io_base=	[HW,MTD]
	spia_fio_base=
	spia_pedr=
	spia_peddr=

	stacktrace	[FTRACE]
			Enabled the stack tracer on boot up.

	stacktrace_filter=[function-list]
			[FTRACE] Limit the functions that the stack tracer
			will trace at boot up. function-list is a comma separated
			list of functions. This list can be changed at run
			time by the stack_trace_filter file in the debugfs
			tracing directory. Note, this enables stack tracing
			and the stacktrace above is not needed.

	sti=		[PARISC,HW]
			Format: <num>
			Set the STI (builtin display/keyboard on the HP-PARISC
			machines) console (graphic card) which should be used
			as the initial boot-console.
			See also comment in drivers/video/console/sticore.c.

	sti_font=	[HW]
			See comment in drivers/video/console/sticore.c.

	stifb=		[HW]
			Format: bpp:<bpp1>[:<bpp2>[:<bpp3>...]]

	sunrpc.min_resvport=
	sunrpc.max_resvport=
			[NFS,SUNRPC]
			SunRPC servers often require that client requests
			originate from a privileged port (i.e. a port in the
			range 0 < portnr < 1024).
			An administrator who wishes to reserve some of these
			ports for other uses may adjust the range that the
			kernel's sunrpc client considers to be privileged
			using these two parameters to set the minimum and
			maximum port values.

	sunrpc.pool_mode=
			[NFS]
			Control how the NFS server code allocates CPUs to
			service thread pools.  Depending on how many NICs
			you have and where their interrupts are bound, this
			option will affect which CPUs will do NFS serving.
			Note: this parameter cannot be changed while the
			NFS server is running.

			auto	    the server chooses an appropriate mode
				    automatically using heuristics
			global	    a single global pool contains all CPUs
			percpu	    one pool for each CPU
			pernode	    one pool for each NUMA node (equivalent
				    to global on non-NUMA machines)

	sunrpc.tcp_slot_table_entries=
	sunrpc.udp_slot_table_entries=
			[NFS,SUNRPC]
			Sets the upper limit on the number of simultaneous
			RPC calls that can be sent from the client to a
			server. Increasing these values may allow you to
			improve throughput, but will also increase the
			amount of memory reserved for use by the client.

	swapaccount=[0|1]
			[KNL] Enable accounting of swap in memory resource
			controller if no parameter or 1 is given or disable
			it if 0 is given (See Documentation/cgroups/memory.txt)

	swiotlb=	[ARM,IA-64,PPC,MIPS,X86]
			Format: { <int> | force }
			<int> -- Number of I/O TLB slabs
			force -- force using of bounce buffers even if they
			         wouldn't be automatically used by the kernel

	switches=	[HW,M68k]

	sysfs.deprecated=0|1 [KNL]
			Enable/disable old style sysfs layout for old udev
			on older distributions. When this option is enabled
			very new udev will not work anymore. When this option
			is disabled (or CONFIG_SYSFS_DEPRECATED not compiled)
			in older udev will not work anymore.
			Default depends on CONFIG_SYSFS_DEPRECATED_V2 set in
			the kernel configuration.

	sysrq_always_enabled
			[KNL]
			Ignore sysrq setting - this boot parameter will
			neutralize any effect of /proc/sys/kernel/sysrq.
			Useful for debugging.

	tdfx=		[HW,DRM]

	test_suspend=	[SUSPEND]
			Specify "mem" (for Suspend-to-RAM) or "standby" (for
			standby suspend) as the system sleep state to briefly
			enter during system startup.  The system is woken from
			this state using a wakeup-capable RTC alarm.

	thash_entries=	[KNL,NET]
			Set number of hash buckets for TCP connection

	thermal.act=	[HW,ACPI]
			-1: disable all active trip points in all thermal zones
			<degrees C>: override all lowest active trip points

	thermal.crt=	[HW,ACPI]
			-1: disable all critical trip points in all thermal zones
			<degrees C>: override all critical trip points

	thermal.nocrt=	[HW,ACPI]
			Set to disable actions on ACPI thermal zone
			critical and hot trip points.

	thermal.off=	[HW,ACPI]
			1: disable ACPI thermal control

	thermal.psv=	[HW,ACPI]
			-1: disable all passive trip points
			<degrees C>: override all passive trip points to this
			value

	thermal.tzp=	[HW,ACPI]
			Specify global default ACPI thermal zone polling rate
			<deci-seconds>: poll all this frequency
			0: no polling (default)

	threadirqs	[KNL]
			Force threading of all interrupt handlers except those
			marked explicitly IRQF_NO_THREAD.

	tmem		[KNL,XEN]
			Enable the Transcendent memory driver if built-in.

	tmem.cleancache=0|1 [KNL, XEN]
			Default is on (1). Disable the usage of the cleancache
			API to send anonymous pages to the hypervisor.

	tmem.frontswap=0|1 [KNL, XEN]
			Default is on (1). Disable the usage of the frontswap
			API to send swap pages to the hypervisor. If disabled
			the selfballooning and selfshrinking are force disabled.

	tmem.selfballooning=0|1 [KNL, XEN]
			Default is on (1). Disable the driving of swap pages
			to the hypervisor.

	tmem.selfshrinking=0|1 [KNL, XEN]
			Default is on (1). Partial swapoff that immediately
			transfers pages from Xen hypervisor back to the
			kernel based on different criteria.

	topology=	[S390]
			Format: {off | on}
			Specify if the kernel should make use of the cpu
			topology information if the hardware supports this.
			The scheduler will make use of this information and
			e.g. base its process migration decisions on it.
			Default is on.

	tp720=		[HW,PS2]

	tpm_suspend_pcr=[HW,TPM]
			Format: integer pcr id
			Specify that at suspend time, the tpm driver
			should extend the specified pcr with zeros,
			as a workaround for some chips which fail to
			flush the last written pcr on TPM_SaveState.
			This will guarantee that all the other pcrs
			are saved.

	trace_buf_size=nn[KMG]
			[FTRACE] will set tracing buffer size.

	trace_event=[event-list]
			[FTRACE] Set and start specified trace events in order
			to facilitate early boot debugging.
			See also Documentation/trace/events.txt

	trace_options=[option-list]
			[FTRACE] Enable or disable tracer options at boot.
			The option-list is a comma delimited list of options
			that can be enabled or disabled just as if you were
			to echo the option name into

			    /sys/kernel/debug/tracing/trace_options

			For example, to enable stacktrace option (to dump the
			stack trace of each event), add to the command line:

			      trace_options=stacktrace

			See also Documentation/trace/ftrace.txt "trace options"
			section.

	traceoff_on_warning
			[FTRACE] enable this option to disable tracing when a
			warning is hit. This turns off "tracing_on". Tracing can
			be enabled again by echoing '1' into the "tracing_on"
			file located in /sys/kernel/debug/tracing/

			This option is useful, as it disables the trace before
			the WARNING dump is called, which prevents the trace to
			be filled with content caused by the warning output.

			This option can also be set at run time via the sysctl
			option:  kernel/traceoff_on_warning

	transparent_hugepage=
			[KNL]
			Format: [always|madvise|never]
			Can be used to control the default behavior of the system
			with respect to transparent hugepages.
			See Documentation/vm/transhuge.txt for more details.

	tsc=		Disable clocksource stability checks for TSC.
			Format: <string>
			[x86] reliable: mark tsc clocksource as reliable, this
			disables clocksource verification at runtime, as well
			as the stability checks done at bootup.	Used to enable
			high-resolution timer mode on older hardware, and in
			virtualized environment.
			[x86] noirqtime: Do not use TSC to do irq accounting.
			Used to run time disable IRQ_TIME_ACCOUNTING on any
			platforms where RDTSC is slow and this accounting
			can add overhead.

	turbografx.map[2|3]=	[HW,JOY]
			TurboGraFX parallel port interface
			Format:
			<port#>,<js1>,<js2>,<js3>,<js4>,<js5>,<js6>,<js7>
			See also Documentation/input/joystick-parport.txt

	udbg-immortal	[PPC] When debugging early kernel crashes that
			happen after console_init() and before a proper 
			console driver takes over, this boot options might
			help "seeing" what's going on.

	uhash_entries=	[KNL,NET]
			Set number of hash buckets for UDP/UDP-Lite connections

	uhci-hcd.ignore_oc=
			[USB] Ignore overcurrent events (default N).
			Some badly-designed motherboards generate lots of
			bogus events, for ports that aren't wired to
			anything.  Set this parameter to avoid log spamming.
			Note that genuine overcurrent events won't be
			reported either.

	unknown_nmi_panic
			[X86] Cause panic on unknown NMI.

	usbcore.authorized_default=
			[USB] Default USB device authorization:
			(default -1 = authorized except for wireless USB,
			0 = not authorized, 1 = authorized)

	usbcore.autosuspend=
			[USB] The autosuspend time delay (in seconds) used
			for newly-detected USB devices (default 2).  This
			is the time required before an idle device will be
			autosuspended.  Devices for which the delay is set
			to a negative value won't be autosuspended at all.

	usbcore.usbfs_snoop=
			[USB] Set to log all usbfs traffic (default 0 = off).

	usbcore.blinkenlights=
			[USB] Set to cycle leds on hubs (default 0 = off).

	usbcore.old_scheme_first=
			[USB] Start with the old device initialization
			scheme (default 0 = off).

	usbcore.usbfs_memory_mb=
			[USB] Memory limit (in MB) for buffers allocated by
			usbfs (default = 16, 0 = max = 2047).

	usbcore.use_both_schemes=
			[USB] Try the other device initialization scheme
			if the first one fails (default 1 = enabled).

	usbcore.initial_descriptor_timeout=
			[USB] Specifies timeout for the initial 64-byte
                        USB_REQ_GET_DESCRIPTOR request in milliseconds
			(default 5000 = 5.0 seconds).

	usbhid.mousepoll=
			[USBHID] The interval which mice are to be polled at.

	usb-storage.delay_use=
			[UMS] The delay in seconds before a new device is
			scanned for Logical Units (default 5).

	usb-storage.quirks=
			[UMS] A list of quirks entries to supplement or
			override the built-in unusual_devs list.  List
			entries are separated by commas.  Each entry has
			the form VID:PID:Flags where VID and PID are Vendor
			and Product ID values (4-digit hex numbers) and
			Flags is a set of characters, each corresponding
			to a common usb-storage quirk flag as follows:
				a = SANE_SENSE (collect more than 18 bytes
					of sense data);
				b = BAD_SENSE (don't collect more than 18
					bytes of sense data);
				c = FIX_CAPACITY (decrease the reported
					device capacity by one sector);
				d = NO_READ_DISC_INFO (don't use
					READ_DISC_INFO command);
				e = NO_READ_CAPACITY_16 (don't use
					READ_CAPACITY_16 command);
				h = CAPACITY_HEURISTICS (decrease the
					reported device capacity by one
					sector if the number is odd);
				i = IGNORE_DEVICE (don't bind to this
					device);
				l = NOT_LOCKABLE (don't try to lock and
					unlock ejectable media);
				m = MAX_SECTORS_64 (don't transfer more
					than 64 sectors = 32 KB at a time);
				n = INITIAL_READ10 (force a retry of the
					initial READ(10) command);
				o = CAPACITY_OK (accept the capacity
					reported by the device);
				p = WRITE_CACHE (the device cache is ON
					by default);
				r = IGNORE_RESIDUE (the device reports
					bogus residue values);
				s = SINGLE_LUN (the device has only one
					Logical Unit);
				w = NO_WP_DETECT (don't test whether the
					medium is write-protected).
			Example: quirks=0419:aaf5:rl,0421:0433:rc

	user_debug=	[KNL,ARM]
			Format: <int>
			See arch/arm/Kconfig.debug help text.
				 1 - undefined instruction events
				 2 - system calls
				 4 - invalid data aborts
				 8 - SIGSEGV faults
				16 - SIGBUS faults
			Example: user_debug=31

	userpte=
			[X86] Flags controlling user PTE allocations.

				nohigh = do not allocate PTE pages in
					HIGHMEM regardless of setting
					of CONFIG_HIGHPTE.

	vdso=		[X86,SH]
			On X86_32, this is an alias for vdso32=.  Otherwise:

			vdso=1: enable VDSO (the default)
			vdso=0: disable VDSO mapping

	vdso32=		[X86] Control the 32-bit vDSO
			vdso32=1: enable 32-bit VDSO
			vdso32=0 or vdso32=2: disable 32-bit VDSO

			See the help text for CONFIG_COMPAT_VDSO for more
			details.  If CONFIG_COMPAT_VDSO is set, the default is
			vdso32=0; otherwise, the default is vdso32=1.

			For compatibility with older kernels, vdso32=2 is an
			alias for vdso32=0.

			Try vdso32=0 if you encounter an error that says:
			dl_main: Assertion `(void *) ph->p_vaddr == _rtld_local._dl_sysinfo_dso' failed!

	vector=		[IA-64,SMP]
			vector=percpu: enable percpu vector domain

	video=		[FB] Frame buffer configuration
			See Documentation/fb/modedb.txt.

	video.brightness_switch_enabled= [0,1]
			If set to 1, on receiving an ACPI notify event
			generated by hotkey, video driver will adjust brightness
			level and then send out the event to user space through
			the allocated input device; If set to 0, video driver
			will only send out the event without touching backlight
			brightness level.
			default: 0

	virtio_mmio.device=
			[VMMIO] Memory mapped virtio (platform) device.

				<size>@<baseaddr>:<irq>[:<id>]
			where:
				<size>     := size (can use standard suffixes
						like K, M and G)
				<baseaddr> := physical base address
				<irq>      := interrupt number (as passed to
						request_irq())
				<id>       := (optional) platform device id
			example:
				virtio_mmio.device=1K@0x100b0000:48:7

			Can be used multiple times for multiple devices.

	vga=		[BOOT,X86-32] Select a particular video mode
			See Documentation/x86/boot.txt and
			Documentation/svga.txt.
			Use vga=ask for menu.
			This is actually a boot loader parameter; the value is
			passed to the kernel using a special protocol.

	vmalloc=nn[KMG]	[KNL,BOOT] Forces the vmalloc area to have an exact
			size of <nn>. This can be used to increase the
			minimum size (128MB on x86). It can also be used to
			decrease the size and leave more room for directly
			mapped kernel RAM.

	vmhalt=		[KNL,S390] Perform z/VM CP command after system halt.
			Format: <command>

	vmpanic=	[KNL,S390] Perform z/VM CP command after kernel panic.
			Format: <command>

	vmpoff=		[KNL,S390] Perform z/VM CP command after power off.
			Format: <command>

	vsyscall=	[X86-64]
			Controls the behavior of vsyscalls (i.e. calls to
			fixed addresses of 0xffffffffff600x00 from legacy
			code).  Most statically-linked binaries and older
			versions of glibc use these calls.  Because these
			functions are at fixed addresses, they make nice
			targets for exploits that can control RIP.

			emulate     [default] Vsyscalls turn into traps and are
			            emulated reasonably safely.

			native      Vsyscalls are native syscall instructions.
			            This is a little bit faster than trapping
			            and makes a few dynamic recompilers work
			            better than they would in emulation mode.
			            It also makes exploits much easier to write.

			none        Vsyscalls don't work at all.  This makes
			            them quite hard to use for exploits but
			            might break your system.

	vt.color=	[VT] Default text color.
			Format: 0xYX, X = foreground, Y = background.
			Default: 0x07 = light gray on black.

	vt.cur_default=	[VT] Default cursor shape.
			Format: 0xCCBBAA, where AA, BB, and CC are the same as
			the parameters of the <Esc>[?A;B;Cc escape sequence;
			see VGA-softcursor.txt. Default: 2 = underline.

	vt.default_blu=	[VT]
			Format: <blue0>,<blue1>,<blue2>,...,<blue15>
			Change the default blue palette of the console.
			This is a 16-member array composed of values
			ranging from 0-255.

	vt.default_grn=	[VT]
			Format: <green0>,<green1>,<green2>,...,<green15>
			Change the default green palette of the console.
			This is a 16-member array composed of values
			ranging from 0-255.

	vt.default_red=	[VT]
			Format: <red0>,<red1>,<red2>,...,<red15>
			Change the default red palette of the console.
			This is a 16-member array composed of values
			ranging from 0-255.

	vt.default_utf8=
			[VT]
			Format=<0|1>
			Set system-wide default UTF-8 mode for all tty's.
			Default is 1, i.e. UTF-8 mode is enabled for all
			newly opened terminals.

	vt.global_cursor_default=
			[VT]
			Format=<-1|0|1>
			Set system-wide default for whether a cursor
			is shown on new VTs. Default is -1,
			i.e. cursors will be created by default unless
			overridden by individual drivers. 0 will hide
			cursors, 1 will display them.

	vt.italic=	[VT] Default color for italic text; 0-15.
			Default: 2 = green.

	vt.underline=	[VT] Default color for underlined text; 0-15.
			Default: 3 = cyan.

	watchdog timers	[HW,WDT] For information on watchdog timers,
			see Documentation/watchdog/watchdog-parameters.txt
			or other driver-specific files in the
			Documentation/watchdog/ directory.

	workqueue.disable_numa
			By default, all work items queued to unbound
			workqueues are affine to the NUMA nodes they're
			issued on, which results in better behavior in
			general.  If NUMA affinity needs to be disabled for
			whatever reason, this option can be used.  Note
			that this also can be controlled per-workqueue for
			workqueues visible under /sys/bus/workqueue/.

	workqueue.power_efficient
			Per-cpu workqueues are generally preferred because
			they show better performance thanks to cache
			locality; unfortunately, per-cpu workqueues tend to
			be more power hungry than unbound workqueues.

			Enabling this makes the per-cpu workqueues which
			were observed to contribute significantly to power
			consumption unbound, leading to measurably lower
			power usage at the cost of small performance
			overhead.

			The default value of this parameter is determined by
			the config option CONFIG_WQ_POWER_EFFICIENT_DEFAULT.

	x2apic_phys	[X86-64,APIC] Use x2apic physical mode instead of
			default x2apic cluster mode on platforms
			supporting x2apic.

	x86_intel_mid_timer= [X86-32,APBT]
			Choose timer option for x86 Intel MID platform.
			Two valid options are apbt timer only and lapic timer
			plus one apbt timer for broadcast timer.
			x86_intel_mid_timer=apbt_only | lapic_and_apbt

	xen_emul_unplug=		[HW,X86,XEN]
			Unplug Xen emulated devices
			Format: [unplug0,][unplug1]
			ide-disks -- unplug primary master IDE devices
			aux-ide-disks -- unplug non-primary-master IDE devices
			nics -- unplug network devices
			all -- unplug all emulated devices (NICs and IDE disks)
			unnecessary -- unplugging emulated devices is
				unnecessary even if the host did not respond to
				the unplug protocol
			never -- do not unplug even if version check succeeds

	xen_nopvspin	[X86,XEN]
			Disables the ticketlock slowpath using Xen PV
			optimizations.

	xirc2ps_cs=	[NET,PCMCIA]
			Format:
			<irq>,<irq_mask>,<io>,<full_duplex>,<do_sound>,<lockup_hack>[,<irq2>[,<irq3>[,<irq4>]]]

______________________________________________________________________

TODO:

	Add more DRM drivers.
REDUCING OS JITTER DUE TO PER-CPU KTHREADS

This document lists per-CPU kthreads in the Linux kernel and presents
options to control their OS jitter.  Note that non-per-CPU kthreads are
not listed here.  To reduce OS jitter from non-per-CPU kthreads, bind
them to a "housekeeping" CPU dedicated to such work.


REFERENCES

o	Documentation/IRQ-affinity.txt:  Binding interrupts to sets of CPUs.

o	Documentation/cgroups:  Using cgroups to bind tasks to sets of CPUs.

o	man taskset:  Using the taskset command to bind tasks to sets
	of CPUs.

o	man sched_setaffinity:  Using the sched_setaffinity() system
	call to bind tasks to sets of CPUs.

o	/sys/devices/system/cpu/cpuN/online:  Control CPU N's hotplug state,
	writing "0" to offline and "1" to online.

o	In order to locate kernel-generated OS jitter on CPU N:

		cd /sys/kernel/debug/tracing
		echo 1 > max_graph_depth # Increase the "1" for more detail
		echo function_graph > current_tracer
		# run workload
		cat per_cpu/cpuN/trace


KTHREADS

Name: ehca_comp/%u
Purpose: Periodically process Infiniband-related work.
To reduce its OS jitter, do any of the following:
1.	Don't use eHCA Infiniband hardware, instead choosing hardware
	that does not require per-CPU kthreads.  This will prevent these
	kthreads from being created in the first place.  (This will
	work for most people, as this hardware, though important, is
	relatively old and is produced in relatively low unit volumes.)
2.	Do all eHCA-Infiniband-related work on other CPUs, including
	interrupts.
3.	Rework the eHCA driver so that its per-CPU kthreads are
	provisioned only on selected CPUs.


Name: irq/%d-%s
Purpose: Handle threaded interrupts.
To reduce its OS jitter, do the following:
1.	Use irq affinity to force the irq threads to execute on
	some other CPU.

Name: kcmtpd_ctr_%d
Purpose: Handle Bluetooth work.
To reduce its OS jitter, do one of the following:
1.	Don't use Bluetooth, in which case these kthreads won't be
	created in the first place.
2.	Use irq affinity to force Bluetooth-related interrupts to
	occur on some other CPU and furthermore initiate all
	Bluetooth activity on some other CPU.

Name: ksoftirqd/%u
Purpose: Execute softirq handlers when threaded or when under heavy load.
To reduce its OS jitter, each softirq vector must be handled
separately as follows:
TIMER_SOFTIRQ:  Do all of the following:
1.	To the extent possible, keep the CPU out of the kernel when it
	is non-idle, for example, by avoiding system calls and by forcing
	both kernel threads and interrupts to execute elsewhere.
2.	Build with CONFIG_HOTPLUG_CPU=y.  After boot completes, force
	the CPU offline, then bring it back online.  This forces
	recurring timers to migrate elsewhere.	If you are concerned
	with multiple CPUs, force them all offline before bringing the
	first one back online.  Once you have onlined the CPUs in question,
	do not offline any other CPUs, because doing so could force the
	timer back onto one of the CPUs in question.
NET_TX_SOFTIRQ and NET_RX_SOFTIRQ:  Do all of the following:
1.	Force networking interrupts onto other CPUs.
2.	Initiate any network I/O on other CPUs.
3.	Once your application has started, prevent CPU-hotplug operations
	from being initiated from tasks that might run on the CPU to
	be de-jittered.  (It is OK to force this CPU offline and then
	bring it back online before you start your application.)
BLOCK_SOFTIRQ:  Do all of the following:
1.	Force block-device interrupts onto some other CPU.
2.	Initiate any block I/O on other CPUs.
3.	Once your application has started, prevent CPU-hotplug operations
	from being initiated from tasks that might run on the CPU to
	be de-jittered.  (It is OK to force this CPU offline and then
	bring it back online before you start your application.)
BLOCK_IOPOLL_SOFTIRQ:  Do all of the following:
1.	Force block-device interrupts onto some other CPU.
2.	Initiate any block I/O and block-I/O polling on other CPUs.
3.	Once your application has started, prevent CPU-hotplug operations
	from being initiated from tasks that might run on the CPU to
	be de-jittered.  (It is OK to force this CPU offline and then
	bring it back online before you start your application.)
TASKLET_SOFTIRQ: Do one or more of the following:
1.	Avoid use of drivers that use tasklets.  (Such drivers will contain
	calls to things like tasklet_schedule().)
2.	Convert all drivers that you must use from tasklets to workqueues.
3.	Force interrupts for drivers using tasklets onto other CPUs,
	and also do I/O involving these drivers on other CPUs.
SCHED_SOFTIRQ: Do all of the following:
1.	Avoid sending scheduler IPIs to the CPU to be de-jittered,
	for example, ensure that at most one runnable kthread is present
	on that CPU.  If a thread that expects to run on the de-jittered
	CPU awakens, the scheduler will send an IPI that can result in
	a subsequent SCHED_SOFTIRQ.
2.	Build with CONFIG_RCU_NOCB_CPU=y, CONFIG_RCU_NOCB_CPU_ALL=y,
	CONFIG_NO_HZ_FULL=y, and, in addition, ensure that the CPU
	to be de-jittered is marked as an adaptive-ticks CPU using the
	"nohz_full=" boot parameter.  This reduces the number of
	scheduler-clock interrupts that the de-jittered CPU receives,
	minimizing its chances of being selected to do the load balancing
	work that runs in SCHED_SOFTIRQ context.
3.	To the extent possible, keep the CPU out of the kernel when it
	is non-idle, for example, by avoiding system calls and by
	forcing both kernel threads and interrupts to execute elsewhere.
	This further reduces the number of scheduler-clock interrupts
	received by the de-jittered CPU.
HRTIMER_SOFTIRQ:  Do all of the following:
1.	To the extent possible, keep the CPU out of the kernel when it
	is non-idle.  For example, avoid system calls and force both
	kernel threads and interrupts to execute elsewhere.
2.	Build with CONFIG_HOTPLUG_CPU=y.  Once boot completes, force the
	CPU offline, then bring it back online.  This forces recurring
	timers to migrate elsewhere.  If you are concerned with multiple
	CPUs, force them all offline before bringing the first one
	back online.  Once you have onlined the CPUs in question, do not
	offline any other CPUs, because doing so could force the timer
	back onto one of the CPUs in question.
RCU_SOFTIRQ:  Do at least one of the following:
1.	Offload callbacks and keep the CPU in either dyntick-idle or
	adaptive-ticks state by doing all of the following:
	a.	Build with CONFIG_RCU_NOCB_CPU=y, CONFIG_RCU_NOCB_CPU_ALL=y,
		CONFIG_NO_HZ_FULL=y, and, in addition ensure that the CPU
		to be de-jittered is marked as an adaptive-ticks CPU using
		the "nohz_full=" boot parameter.  Bind the rcuo kthreads
		to housekeeping CPUs, which can tolerate OS jitter.
	b.	To the extent possible, keep the CPU out of the kernel
		when it is non-idle, for example, by avoiding system
		calls and by forcing both kernel threads and interrupts
		to execute elsewhere.
2.	Enable RCU to do its processing remotely via dyntick-idle by
	doing all of the following:
	a.	Build with CONFIG_NO_HZ=y and CONFIG_RCU_FAST_NO_HZ=y.
	b.	Ensure that the CPU goes idle frequently, allowing other
		CPUs to detect that it has passed through an RCU quiescent
		state.	If the kernel is built with CONFIG_NO_HZ_FULL=y,
		userspace execution also allows other CPUs to detect that
		the CPU in question has passed through a quiescent state.
	c.	To the extent possible, keep the CPU out of the kernel
		when it is non-idle, for example, by avoiding system
		calls and by forcing both kernel threads and interrupts
		to execute elsewhere.

Name: kworker/%u:%d%s (cpu, id, priority)
Purpose: Execute workqueue requests
To reduce its OS jitter, do any of the following:
1.	Run your workload at a real-time priority, which will allow
	preempting the kworker daemons.
2.	A given workqueue can be made visible in the sysfs filesystem
	by passing the WQ_SYSFS to that workqueue's alloc_workqueue().
	Such a workqueue can be confined to a given subset of the
	CPUs using the /sys/devices/virtual/workqueue/*/cpumask sysfs
	files.	The set of WQ_SYSFS workqueues can be displayed using
	"ls sys/devices/virtual/workqueue".  That said, the workqueues
	maintainer would like to caution people against indiscriminately
	sprinkling WQ_SYSFS across all the workqueues.	The reason for
	caution is that it is easy to add WQ_SYSFS, but because sysfs is
	part of the formal user/kernel API, it can be nearly impossible
	to remove it, even if its addition was a mistake.
3.	Do any of the following needed to avoid jitter that your
	application cannot tolerate:
	a.	Build your kernel with CONFIG_SLUB=y rather than
		CONFIG_SLAB=y, thus avoiding the slab allocator's periodic
		use of each CPU's workqueues to run its cache_reap()
		function.
	b.	Avoid using oprofile, thus avoiding OS jitter from
		wq_sync_buffer().
	c.	Limit your CPU frequency so that a CPU-frequency
		governor is not required, possibly enlisting the aid of
		special heatsinks or other cooling technologies.  If done
		correctly, and if you CPU architecture permits, you should
		be able to build your kernel with CONFIG_CPU_FREQ=n to
		avoid the CPU-frequency governor periodically running
		on each CPU, including cs_dbs_timer() and od_dbs_timer().
		WARNING:  Please check your CPU specifications to
		make sure that this is safe on your particular system.
	d.	It is not possible to entirely get rid of OS jitter
		from vmstat_update() on CONFIG_SMP=y systems, but you
		can decrease its frequency by writing a large value
		to /proc/sys/vm/stat_interval.	The default value is
		HZ, for an interval of one second.  Of course, larger
		values will make your virtual-memory statistics update
		more slowly.  Of course, you can also run your workload
		at a real-time priority, thus preempting vmstat_update(),
		but if your workload is CPU-bound, this is a bad idea.
		However, there is an RFC patch from Christoph Lameter
		(based on an earlier one from Gilad Ben-Yossef) that
		reduces or even eliminates vmstat overhead for some
		workloads at https://lkml.org/lkml/2013/9/4/379.
	e.	If running on high-end powerpc servers, build with
		CONFIG_PPC_RTAS_DAEMON=n.  This prevents the RTAS
		daemon from running on each CPU every second or so.
		(This will require editing Kconfig files and will defeat
		this platform's RAS functionality.)  This avoids jitter
		due to the rtas_event_scan() function.
		WARNING:  Please check your CPU specifications to
		make sure that this is safe on your particular system.
	f.	If running on Cell Processor, build your kernel with
		CBE_CPUFREQ_SPU_GOVERNOR=n to avoid OS jitter from
		spu_gov_work().
		WARNING:  Please check your CPU specifications to
		make sure that this is safe on your particular system.
	g.	If running on PowerMAC, build your kernel with
		CONFIG_PMAC_RACKMETER=n to disable the CPU-meter,
		avoiding OS jitter from rackmeter_do_timer().

Name: rcuc/%u
Purpose: Execute RCU callbacks in CONFIG_RCU_BOOST=y kernels.
To reduce its OS jitter, do at least one of the following:
1.	Build the kernel with CONFIG_PREEMPT=n.  This prevents these
	kthreads from being created in the first place, and also obviates
	the need for RCU priority boosting.  This approach is feasible
	for workloads that do not require high degrees of responsiveness.
2.	Build the kernel with CONFIG_RCU_BOOST=n.  This prevents these
	kthreads from being created in the first place.  This approach
	is feasible only if your workload never requires RCU priority
	boosting, for example, if you ensure frequent idle time on all
	CPUs that might execute within the kernel.
3.	Build with CONFIG_RCU_NOCB_CPU=y and CONFIG_RCU_NOCB_CPU_ALL=y,
	which offloads all RCU callbacks to kthreads that can be moved
	off of CPUs susceptible to OS jitter.  This approach prevents the
	rcuc/%u kthreads from having any work to do, so that they are
	never awakened.
4.	Ensure that the CPU never enters the kernel, and, in particular,
	avoid initiating any CPU hotplug operations on this CPU.  This is
	another way of preventing any callbacks from being queued on the
	CPU, again preventing the rcuc/%u kthreads from having any work
	to do.

Name: rcuob/%d, rcuop/%d, and rcuos/%d
Purpose: Offload RCU callbacks from the corresponding CPU.
To reduce its OS jitter, do at least one of the following:
1.	Use affinity, cgroups, or other mechanism to force these kthreads
	to execute on some other CPU.
2.	Build with CONFIG_RCU_NOCB_CPU=n, which will prevent these
	kthreads from being created in the first place.  However, please
	note that this will not eliminate OS jitter, but will instead
	shift it to RCU_SOFTIRQ.

Name: watchdog/%u
Purpose: Detect software lockups on each CPU.
To reduce its OS jitter, do at least one of the following:
1.	Build with CONFIG_LOCKUP_DETECTOR=n, which will prevent these
	kthreads from being created in the first place.
2.	Echo a zero to /proc/sys/kernel/watchdog to disable the
	watchdog timer.
3.	Echo a large number of /proc/sys/kernel/watchdog_thresh in
	order to reduce the frequency of OS jitter due to the watchdog
	timer down to a level that is acceptable for your workload.
GETTING STARTED WITH KMEMCHECK
==============================

Vegard Nossum <vegardno@ifi.uio.no>


Contents
========
0. Introduction
1. Downloading
2. Configuring and compiling
3. How to use
3.1. Booting
3.2. Run-time enable/disable
3.3. Debugging
3.4. Annotating false positives
4. Reporting errors
5. Technical description


0. Introduction
===============

kmemcheck is a debugging feature for the Linux Kernel. More specifically, it
is a dynamic checker that detects and warns about some uses of uninitialized
memory.

Userspace programmers might be familiar with Valgrind's memcheck. The main
difference between memcheck and kmemcheck is that memcheck works for userspace
programs only, and kmemcheck works for the kernel only. The implementations
are of course vastly different. Because of this, kmemcheck is not as accurate
as memcheck, but it turns out to be good enough in practice to discover real
programmer errors that the compiler is not able to find through static
analysis.

Enabling kmemcheck on a kernel will probably slow it down to the extent that
the machine will not be usable for normal workloads such as e.g. an
interactive desktop. kmemcheck will also cause the kernel to use about twice
as much memory as normal. For this reason, kmemcheck is strictly a debugging
feature.


1. Downloading
==============

As of version 2.6.31-rc1, kmemcheck is included in the mainline kernel.


2. Configuring and compiling
============================

kmemcheck only works for the x86 (both 32- and 64-bit) platform. A number of
configuration variables must have specific settings in order for the kmemcheck
menu to even appear in "menuconfig". These are:

  o CONFIG_CC_OPTIMIZE_FOR_SIZE=n

	This option is located under "General setup" / "Optimize for size".

	Without this, gcc will use certain optimizations that usually lead to
	false positive warnings from kmemcheck. An example of this is a 16-bit
	field in a struct, where gcc may load 32 bits, then discard the upper
	16 bits. kmemcheck sees only the 32-bit load, and may trigger a
	warning for the upper 16 bits (if they're uninitialized).

  o CONFIG_SLAB=y or CONFIG_SLUB=y

	This option is located under "General setup" / "Choose SLAB
	allocator".

  o CONFIG_FUNCTION_TRACER=n

	This option is located under "Kernel hacking" / "Tracers" / "Kernel
	Function Tracer"

	When function tracing is compiled in, gcc emits a call to another
	function at the beginning of every function. This means that when the
	page fault handler is called, the ftrace framework will be called
	before kmemcheck has had a chance to handle the fault. If ftrace then
	modifies memory that was tracked by kmemcheck, the result is an
	endless recursive page fault.

  o CONFIG_DEBUG_PAGEALLOC=n

	This option is located under "Kernel hacking" / "Debug page memory
	allocations".

In addition, I highly recommend turning on CONFIG_DEBUG_INFO=y. This is also
located under "Kernel hacking". With this, you will be able to get line number
information from the kmemcheck warnings, which is extremely valuable in
debugging a problem. This option is not mandatory, however, because it slows
down the compilation process and produces a much bigger kernel image.

Now the kmemcheck menu should be visible (under "Kernel hacking" / "Memory
Debugging" / "kmemcheck: trap use of uninitialized memory"). Here follows
a description of the kmemcheck configuration variables:

  o CONFIG_KMEMCHECK

	This must be enabled in order to use kmemcheck at all...

  o CONFIG_KMEMCHECK_[DISABLED | ENABLED | ONESHOT]_BY_DEFAULT

	This option controls the status of kmemcheck at boot-time. "Enabled"
	will enable kmemcheck right from the start, "disabled" will boot the
	kernel as normal (but with the kmemcheck code compiled in, so it can
	be enabled at run-time after the kernel has booted), and "one-shot" is
	a special mode which will turn kmemcheck off automatically after
	detecting the first use of uninitialized memory.

	If you are using kmemcheck to actively debug a problem, then you
	probably want to choose "enabled" here.

	The one-shot mode is mostly useful in automated test setups because it
	can prevent floods of warnings and increase the chances of the machine
	surviving in case something is really wrong. In other cases, the one-
	shot mode could actually be counter-productive because it would turn
	itself off at the very first error -- in the case of a false positive
	too -- and this would come in the way of debugging the specific
	problem you were interested in.

	If you would like to use your kernel as normal, but with a chance to
	enable kmemcheck in case of some problem, it might be a good idea to
	choose "disabled" here. When kmemcheck is disabled, most of the run-
	time overhead is not incurred, and the kernel will be almost as fast
	as normal.

  o CONFIG_KMEMCHECK_QUEUE_SIZE

	Select the maximum number of error reports to store in an internal
	(fixed-size) buffer. Since errors can occur virtually anywhere and in
	any context, we need a temporary storage area which is guaranteed not
	to generate any other page faults when accessed. The queue will be
	emptied as soon as a tasklet may be scheduled. If the queue is full,
	new error reports will be lost.

	The default value of 64 is probably fine. If some code produces more
	than 64 errors within an irqs-off section, then the code is likely to
	produce many, many more, too, and these additional reports seldom give
	any more information (the first report is usually the most valuable
	anyway).

	This number might have to be adjusted if you are not using serial
	console or similar to capture the kernel log. If you are using the
	"dmesg" command to save the log, then getting a lot of kmemcheck
	warnings might overflow the kernel log itself, and the earlier reports
	will get lost in that way instead. Try setting this to 10 or so on
	such a setup.

  o CONFIG_KMEMCHECK_SHADOW_COPY_SHIFT

	Select the number of shadow bytes to save along with each entry of the
	error-report queue. These bytes indicate what parts of an allocation
	are initialized, uninitialized, etc. and will be displayed when an
	error is detected to help the debugging of a particular problem.

	The number entered here is actually the logarithm of the number of
	bytes that will be saved. So if you pick for example 5 here, kmemcheck
	will save 2^5 = 32 bytes.

	The default value should be fine for debugging most problems. It also
	fits nicely within 80 columns.

  o CONFIG_KMEMCHECK_PARTIAL_OK

	This option (when enabled) works around certain GCC optimizations that
	produce 32-bit reads from 16-bit variables where the upper 16 bits are
	thrown away afterwards.

	The default value (enabled) is recommended. This may of course hide
	some real errors, but disabling it would probably produce a lot of
	false positives.

  o CONFIG_KMEMCHECK_BITOPS_OK

	This option silences warnings that would be generated for bit-field
	accesses where not all the bits are initialized at the same time. This
	may also hide some real bugs.

	This option is probably obsolete, or it should be replaced with
	the kmemcheck-/bitfield-annotations for the code in question. The
	default value is therefore fine.

Now compile the kernel as usual.


3. How to use
=============

3.1. Booting
============

First some information about the command-line options. There is only one
option specific to kmemcheck, and this is called "kmemcheck". It can be used
to override the default mode as chosen by the CONFIG_KMEMCHECK_*_BY_DEFAULT
option. Its possible settings are:

  o kmemcheck=0 (disabled)
  o kmemcheck=1 (enabled)
  o kmemcheck=2 (one-shot mode)

If SLUB debugging has been enabled in the kernel, it may take precedence over
kmemcheck in such a way that the slab caches which are under SLUB debugging
will not be tracked by kmemcheck. In order to ensure that this doesn't happen
(even though it shouldn't by default), use SLUB's boot option "slub_debug",
like this: slub_debug=-

In fact, this option may also be used for fine-grained control over SLUB vs.
kmemcheck. For example, if the command line includes "kmemcheck=1
slub_debug=,dentry", then SLUB debugging will be used only for the "dentry"
slab cache, and with kmemcheck tracking all the other caches. This is advanced
usage, however, and is not generally recommended.


3.2. Run-time enable/disable
============================

When the kernel has booted, it is possible to enable or disable kmemcheck at
run-time. WARNING: This feature is still experimental and may cause false
positive warnings to appear. Therefore, try not to use this. If you find that
it doesn't work properly (e.g. you see an unreasonable amount of warnings), I
will be happy to take bug reports.

Use the file /proc/sys/kernel/kmemcheck for this purpose, e.g.:

	$ echo 0 > /proc/sys/kernel/kmemcheck # disables kmemcheck

The numbers are the same as for the kmemcheck= command-line option.


3.3. Debugging
==============

A typical report will look something like this:

WARNING: kmemcheck: Caught 32-bit read from uninitialized memory (ffff88003e4a2024)
80000000000000000000000000000000000000000088ffff0000000000000000
 i i i i u u u u i i i i i i i i u u u u u u u u u u u u u u u u
         ^

Pid: 1856, comm: ntpdate Not tainted 2.6.29-rc5 #264 945P-A
RIP: 0010:[<ffffffff8104ede8>]  [<ffffffff8104ede8>] __dequeue_signal+0xc8/0x190
RSP: 0018:ffff88003cdf7d98  EFLAGS: 00210002
RAX: 0000000000000030 RBX: ffff88003d4ea968 RCX: 0000000000000009
RDX: ffff88003e5d6018 RSI: ffff88003e5d6024 RDI: ffff88003cdf7e84
RBP: ffff88003cdf7db8 R08: ffff88003e5d6000 R09: 0000000000000000
R10: 0000000000000080 R11: 0000000000000000 R12: 000000000000000e
R13: ffff88003cdf7e78 R14: ffff88003d530710 R15: ffff88003d5a98c8
FS:  0000000000000000(0000) GS:ffff880001982000(0063) knlGS:00000
CS:  0010 DS: 002b ES: 002b CR0: 0000000080050033
CR2: ffff88003f806ea0 CR3: 000000003c036000 CR4: 00000000000006a0
DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
DR3: 0000000000000000 DR6: 00000000ffff4ff0 DR7: 0000000000000400
 [<ffffffff8104f04e>] dequeue_signal+0x8e/0x170
 [<ffffffff81050bd8>] get_signal_to_deliver+0x98/0x390
 [<ffffffff8100b87d>] do_notify_resume+0xad/0x7d0
 [<ffffffff8100c7b5>] int_signal+0x12/0x17
 [<ffffffffffffffff>] 0xffffffffffffffff

The single most valuable information in this report is the RIP (or EIP on 32-
bit) value. This will help us pinpoint exactly which instruction that caused
the warning.

If your kernel was compiled with CONFIG_DEBUG_INFO=y, then all we have to do
is give this address to the addr2line program, like this:

	$ addr2line -e vmlinux -i ffffffff8104ede8
	arch/x86/include/asm/string_64.h:12
	include/asm-generic/siginfo.h:287
	kernel/signal.c:380
	kernel/signal.c:410

The "-e vmlinux" tells addr2line which file to look in. IMPORTANT: This must
be the vmlinux of the kernel that produced the warning in the first place! If
not, the line number information will almost certainly be wrong.

The "-i" tells addr2line to also print the line numbers of inlined functions.
In this case, the flag was very important, because otherwise, it would only
have printed the first line, which is just a call to memcpy(), which could be
called from a thousand places in the kernel, and is therefore not very useful.
These inlined functions would not show up in the stack trace above, simply
because the kernel doesn't load the extra debugging information. This
technique can of course be used with ordinary kernel oopses as well.

In this case, it's the caller of memcpy() that is interesting, and it can be
found in include/asm-generic/siginfo.h, line 287:

281 static inline void copy_siginfo(struct siginfo *to, struct siginfo *from)
282 {
283         if (from->si_code < 0)
284                 memcpy(to, from, sizeof(*to));
285         else
286                 /* _sigchld is currently the largest know union member */
287                 memcpy(to, from, __ARCH_SI_PREAMBLE_SIZE + sizeof(from->_sifields._sigchld));
288 }

Since this was a read (kmemcheck usually warns about reads only, though it can
warn about writes to unallocated or freed memory as well), it was probably the
"from" argument which contained some uninitialized bytes. Following the chain
of calls, we move upwards to see where "from" was allocated or initialized,
kernel/signal.c, line 380:

359 static void collect_signal(int sig, struct sigpending *list, siginfo_t *info)
360 {
...
367         list_for_each_entry(q, &list->list, list) {
368                 if (q->info.si_signo == sig) {
369                         if (first)
370                                 goto still_pending;
371                         first = q;
...
377         if (first) {
378 still_pending:
379                 list_del_init(&first->list);
380                 copy_siginfo(info, &first->info);
381                 __sigqueue_free(first);
...
392         }
393 }

Here, it is &first->info that is being passed on to copy_siginfo(). The
variable "first" was found on a list -- passed in as the second argument to
collect_signal(). We  continue our journey through the stack, to figure out
where the item on "list" was allocated or initialized. We move to line 410:

395 static int __dequeue_signal(struct sigpending *pending, sigset_t *mask,
396                         siginfo_t *info)
397 {
...
410                 collect_signal(sig, pending, info);
...
414 }

Now we need to follow the "pending" pointer, since that is being passed on to
collect_signal() as "list". At this point, we've run out of lines from the
"addr2line" output. Not to worry, we just paste the next addresses from the
kmemcheck stack dump, i.e.:

 [<ffffffff8104f04e>] dequeue_signal+0x8e/0x170
 [<ffffffff81050bd8>] get_signal_to_deliver+0x98/0x390
 [<ffffffff8100b87d>] do_notify_resume+0xad/0x7d0
 [<ffffffff8100c7b5>] int_signal+0x12/0x17

	$ addr2line -e vmlinux -i ffffffff8104f04e ffffffff81050bd8 \
		ffffffff8100b87d ffffffff8100c7b5
	kernel/signal.c:446
	kernel/signal.c:1806
	arch/x86/kernel/signal.c:805
	arch/x86/kernel/signal.c:871
	arch/x86/kernel/entry_64.S:694

Remember that since these addresses were found on the stack and not as the
RIP value, they actually point to the _next_ instruction (they are return
addresses). This becomes obvious when we look at the code for line 446:

422 int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
423 {
...
431                 signr = __dequeue_signal(&tsk->signal->shared_pending,
432                                          mask, info);
433                 /*
434                  * itimer signal ?
435                  *
436                  * itimers are process shared and we restart periodic
437                  * itimers in the signal delivery path to prevent DoS
438                  * attacks in the high resolution timer case. This is
439                  * compliant with the old way of self restarting
440                  * itimers, as the SIGALRM is a legacy signal and only
441                  * queued once. Changing the restart behaviour to
442                  * restart the timer in the signal dequeue path is
443                  * reducing the timer noise on heavy loaded !highres
444                  * systems too.
445                  */
446                 if (unlikely(signr == SIGALRM)) {
...
489 }

So instead of looking at 446, we should be looking at 431, which is the line
that executes just before 446. Here we see that what we are looking for is
&tsk->signal->shared_pending.

Our next task is now to figure out which function that puts items on this
"shared_pending" list. A crude, but efficient tool, is git grep:

	$ git grep -n 'shared_pending' kernel/
	...
	kernel/signal.c:828:    pending = group ? &t->signal->shared_pending : &t->pending;
	kernel/signal.c:1339:   pending = group ? &t->signal->shared_pending : &t->pending;
	...

There were more results, but none of them were related to list operations,
and these were the only assignments. We inspect the line numbers more closely
and find that this is indeed where items are being added to the list:

816 static int send_signal(int sig, struct siginfo *info, struct task_struct *t,
817                         int group)
818 {
...
828         pending = group ? &t->signal->shared_pending : &t->pending;
...
851         q = __sigqueue_alloc(t, GFP_ATOMIC, (sig < SIGRTMIN &&
852                                              (is_si_special(info) ||
853                                               info->si_code >= 0)));
854         if (q) {
855                 list_add_tail(&q->list, &pending->list);
...
890 }

and:

1309 int send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)
1310 {
....
1339         pending = group ? &t->signal->shared_pending : &t->pending;
1340         list_add_tail(&q->list, &pending->list);
....
1347 }

In the first case, the list element we are looking for, "q", is being returned
from the function __sigqueue_alloc(), which looks like an allocation function.
Let's take a look at it:

187 static struct sigqueue *__sigqueue_alloc(struct task_struct *t, gfp_t flags,
188                                          int override_rlimit)
189 {
190         struct sigqueue *q = NULL;
191         struct user_struct *user;
192 
193         /*
194          * We won't get problems with the target's UID changing under us
195          * because changing it requires RCU be used, and if t != current, the
196          * caller must be holding the RCU readlock (by way of a spinlock) and
197          * we use RCU protection here
198          */
199         user = get_uid(__task_cred(t)->user);
200         atomic_inc(&user->sigpending);
201         if (override_rlimit ||
202             atomic_read(&user->sigpending) <=
203                         t->signal->rlim[RLIMIT_SIGPENDING].rlim_cur)
204                 q = kmem_cache_alloc(sigqueue_cachep, flags);
205         if (unlikely(q == NULL)) {
206                 atomic_dec(&user->sigpending);
207                 free_uid(user);
208         } else {
209                 INIT_LIST_HEAD(&q->list);
210                 q->flags = 0;
211                 q->user = user;
212         }
213 
214         return q;
215 }

We see that this function initializes q->list, q->flags, and q->user. It seems
that now is the time to look at the definition of "struct sigqueue", e.g.:

14 struct sigqueue {
15         struct list_head list;
16         int flags;
17         siginfo_t info;
18         struct user_struct *user;
19 };

And, you might remember, it was a memcpy() on &first->info that caused the
warning, so this makes perfect sense. It also seems reasonable to assume that
it is the caller of __sigqueue_alloc() that has the responsibility of filling
out (initializing) this member.

But just which fields of the struct were uninitialized? Let's look at
kmemcheck's report again:

WARNING: kmemcheck: Caught 32-bit read from uninitialized memory (ffff88003e4a2024)
80000000000000000000000000000000000000000088ffff0000000000000000
 i i i i u u u u i i i i i i i i u u u u u u u u u u u u u u u u
         ^

These first two lines are the memory dump of the memory object itself, and the
shadow bytemap, respectively. The memory object itself is in this case
&first->info. Just beware that the start of this dump is NOT the start of the
object itself! The position of the caret (^) corresponds with the address of
the read (ffff88003e4a2024).

The shadow bytemap dump legend is as follows:

  i - initialized
  u - uninitialized
  a - unallocated (memory has been allocated by the slab layer, but has not
      yet been handed off to anybody)
  f - freed (memory has been allocated by the slab layer, but has been freed
      by the previous owner)

In order to figure out where (relative to the start of the object) the
uninitialized memory was located, we have to look at the disassembly. For
that, we'll need the RIP address again:

RIP: 0010:[<ffffffff8104ede8>]  [<ffffffff8104ede8>] __dequeue_signal+0xc8/0x190

	$ objdump -d --no-show-raw-insn vmlinux | grep -C 8 ffffffff8104ede8:
	ffffffff8104edc8:       mov    %r8,0x8(%r8)
	ffffffff8104edcc:       test   %r10d,%r10d
	ffffffff8104edcf:       js     ffffffff8104ee88 <__dequeue_signal+0x168>
	ffffffff8104edd5:       mov    %rax,%rdx
	ffffffff8104edd8:       mov    $0xc,%ecx
	ffffffff8104eddd:       mov    %r13,%rdi
	ffffffff8104ede0:       mov    $0x30,%eax
	ffffffff8104ede5:       mov    %rdx,%rsi
	ffffffff8104ede8:       rep movsl %ds:(%rsi),%es:(%rdi)
	ffffffff8104edea:       test   $0x2,%al
	ffffffff8104edec:       je     ffffffff8104edf0 <__dequeue_signal+0xd0>
	ffffffff8104edee:       movsw  %ds:(%rsi),%es:(%rdi)
	ffffffff8104edf0:       test   $0x1,%al
	ffffffff8104edf2:       je     ffffffff8104edf5 <__dequeue_signal+0xd5>
	ffffffff8104edf4:       movsb  %ds:(%rsi),%es:(%rdi)
	ffffffff8104edf5:       mov    %r8,%rdi
	ffffffff8104edf8:       callq  ffffffff8104de60 <__sigqueue_free>

As expected, it's the "rep movsl" instruction from the memcpy() that causes
the warning. We know about REP MOVSL that it uses the register RCX to count
the number of remaining iterations. By taking a look at the register dump
again (from the kmemcheck report), we can figure out how many bytes were left
to copy:

RAX: 0000000000000030 RBX: ffff88003d4ea968 RCX: 0000000000000009

By looking at the disassembly, we also see that %ecx is being loaded with the
value $0xc just before (ffffffff8104edd8), so we are very lucky. Keep in mind
that this is the number of iterations, not bytes. And since this is a "long"
operation, we need to multiply by 4 to get the number of bytes. So this means
that the uninitialized value was encountered at 4 * (0xc - 0x9) = 12 bytes
from the start of the object.

We can now try to figure out which field of the "struct siginfo" that was not
initialized. This is the beginning of the struct:

40 typedef struct siginfo {
41         int si_signo;
42         int si_errno;
43         int si_code;
44                 
45         union {
..
92         } _sifields;
93 } siginfo_t;

On 64-bit, the int is 4 bytes long, so it must the union member that has
not been initialized. We can verify this using gdb:

	$ gdb vmlinux
	...
	(gdb) p &((struct siginfo *) 0)->_sifields
	$1 = (union {...} *) 0x10

Actually, it seems that the union member is located at offset 0x10 -- which
means that gcc has inserted 4 bytes of padding between the members si_code
and _sifields. We can now get a fuller picture of the memory dump:

         _----------------------------=> si_code
        /        _--------------------=> (padding)
       |        /        _------------=> _sifields(._kill._pid)
       |       |        /        _----=> _sifields(._kill._uid)
       |       |       |        / 
-------|-------|-------|-------|
80000000000000000000000000000000000000000088ffff0000000000000000
 i i i i u u u u i i i i i i i i u u u u u u u u u u u u u u u u

This allows us to realize another important fact: si_code contains the value
0x80. Remember that x86 is little endian, so the first 4 bytes "80000000" are
really the number 0x00000080. With a bit of research, we find that this is
actually the constant SI_KERNEL defined in include/asm-generic/siginfo.h:

144 #define SI_KERNEL       0x80            /* sent by the kernel from somewhere     */

This macro is used in exactly one place in the x86 kernel: In send_signal()
in kernel/signal.c:

816 static int send_signal(int sig, struct siginfo *info, struct task_struct *t,
817                         int group)
818 {
...
828         pending = group ? &t->signal->shared_pending : &t->pending;
...
851         q = __sigqueue_alloc(t, GFP_ATOMIC, (sig < SIGRTMIN &&
852                                              (is_si_special(info) ||
853                                               info->si_code >= 0)));
854         if (q) {
855                 list_add_tail(&q->list, &pending->list);
856                 switch ((unsigned long) info) {
...
865                 case (unsigned long) SEND_SIG_PRIV:
866                         q->info.si_signo = sig;
867                         q->info.si_errno = 0;
868                         q->info.si_code = SI_KERNEL;
869                         q->info.si_pid = 0;
870                         q->info.si_uid = 0;
871                         break;
...
890 }

Not only does this match with the .si_code member, it also matches the place
we found earlier when looking for where siginfo_t objects are enqueued on the
"shared_pending" list.

So to sum up: It seems that it is the padding introduced by the compiler
between two struct fields that is uninitialized, and this gets reported when
we do a memcpy() on the struct. This means that we have identified a false
positive warning.

Normally, kmemcheck will not report uninitialized accesses in memcpy() calls
when both the source and destination addresses are tracked. (Instead, we copy
the shadow bytemap as well). In this case, the destination address clearly
was not tracked. We can dig a little deeper into the stack trace from above:

	arch/x86/kernel/signal.c:805
	arch/x86/kernel/signal.c:871
	arch/x86/kernel/entry_64.S:694

And we clearly see that the destination siginfo object is located on the
stack:

782 static void do_signal(struct pt_regs *regs)
783 {
784         struct k_sigaction ka;
785         siginfo_t info;
...
804         signr = get_signal_to_deliver(&info, &ka, regs, NULL);
...
854 }

And this &info is what eventually gets passed to copy_siginfo() as the
destination argument.

Now, even though we didn't find an actual error here, the example is still a
good one, because it shows how one would go about to find out what the report
was all about.


3.4. Annotating false positives
===============================

There are a few different ways to make annotations in the source code that
will keep kmemcheck from checking and reporting certain allocations. Here
they are:

  o __GFP_NOTRACK_FALSE_POSITIVE

	This flag can be passed to kmalloc() or kmem_cache_alloc() (therefore
	also to other functions that end up calling one of these) to indicate
	that the allocation should not be tracked because it would lead to
	a false positive report. This is a "big hammer" way of silencing
	kmemcheck; after all, even if the false positive pertains to 
	particular field in a struct, for example, we will now lose the
	ability to find (real) errors in other parts of the same struct.

	Example:

	    /* No warnings will ever trigger on accessing any part of x */
	    x = kmalloc(sizeof *x, GFP_KERNEL | __GFP_NOTRACK_FALSE_POSITIVE);

  o kmemcheck_bitfield_begin(name)/kmemcheck_bitfield_end(name) and
	kmemcheck_annotate_bitfield(ptr, name)

	The first two of these three macros can be used inside struct
	definitions to signal, respectively, the beginning and end of a
	bitfield. Additionally, this will assign the bitfield a name, which
	is given as an argument to the macros.

	Having used these markers, one can later use
	kmemcheck_annotate_bitfield() at the point of allocation, to indicate
	which parts of the allocation is part of a bitfield.

	Example:

	    struct foo {
		int x;

		kmemcheck_bitfield_begin(flags);
		int flag_a:1;
		int flag_b:1;
		kmemcheck_bitfield_end(flags);

		int y;
	    };

	    struct foo *x = kmalloc(sizeof *x);

	    /* No warnings will trigger on accessing the bitfield of x */
	    kmemcheck_annotate_bitfield(x, flags);

	Note that kmemcheck_annotate_bitfield() can be used even before the
	return value of kmalloc() is checked -- in other words, passing NULL
	as the first argument is legal (and will do nothing).


4. Reporting errors
===================

As we have seen, kmemcheck will produce false positive reports. Therefore, it
is not very wise to blindly post kmemcheck warnings to mailing lists and
maintainers. Instead, I encourage maintainers and developers to find errors
in their own code. If you get a warning, you can try to work around it, try
to figure out if it's a real error or not, or simply ignore it. Most
developers know their own code and will quickly and efficiently determine the
root cause of a kmemcheck report. This is therefore also the most efficient
way to work with kmemcheck.

That said, we (the kmemcheck maintainers) will always be on the lookout for
false positives that we can annotate and silence. So whatever you find,
please drop us a note privately! Kernel configs and steps to reproduce (if
available) are of course a great help too.

Happy hacking!


5. Technical description
========================

kmemcheck works by marking memory pages non-present. This means that whenever
somebody attempts to access the page, a page fault is generated. The page
fault handler notices that the page was in fact only hidden, and so it calls
on the kmemcheck code to make further investigations.

When the investigations are completed, kmemcheck "shows" the page by marking
it present (as it would be under normal circumstances). This way, the
interrupted code can continue as usual.

But after the instruction has been executed, we should hide the page again, so
that we can catch the next access too! Now kmemcheck makes use of a debugging
feature of the processor, namely single-stepping. When the processor has
finished the one instruction that generated the memory access, a debug
exception is raised. From here, we simply hide the page again and continue
execution, this time with the single-stepping feature turned off.

kmemcheck requires some assistance from the memory allocator in order to work.
The memory allocator needs to

  1. Tell kmemcheck about newly allocated pages and pages that are about to
     be freed. This allows kmemcheck to set up and tear down the shadow memory
     for the pages in question. The shadow memory stores the status of each
     byte in the allocation proper, e.g. whether it is initialized or
     uninitialized.

  2. Tell kmemcheck which parts of memory should be marked uninitialized.
     There are actually a few more states, such as "not yet allocated" and
     "recently freed".

If a slab cache is set up using the SLAB_NOTRACK flag, it will never return
memory that can take page faults because of kmemcheck.

If a slab cache is NOT set up using the SLAB_NOTRACK flag, callers can still
request memory with the __GFP_NOTRACK or __GFP_NOTRACK_FALSE_POSITIVE flags.
This does not prevent the page faults from occurring, however, but marks the
object in question as being initialized so that no warnings will ever be
produced for this object.

Currently, the SLAB and SLUB allocators are supported by kmemcheck.
Kernel Memory Leak Detector
===========================

Introduction
------------

Kmemleak provides a way of detecting possible kernel memory leaks in a
way similar to a tracing garbage collector
(http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tracing_garbage_collectors),
with the difference that the orphan objects are not freed but only
reported via /sys/kernel/debug/kmemleak. A similar method is used by the
Valgrind tool (memcheck --leak-check) to detect the memory leaks in
user-space applications.
Kmemleak is supported on x86, arm, powerpc, sparc, sh, microblaze, ppc, mips, s390, metag and tile.

Usage
-----

CONFIG_DEBUG_KMEMLEAK in "Kernel hacking" has to be enabled. A kernel
thread scans the memory every 10 minutes (by default) and prints the
number of new unreferenced objects found. To display the details of all
the possible memory leaks:

  # mount -t debugfs nodev /sys/kernel/debug/
  # cat /sys/kernel/debug/kmemleak

To trigger an intermediate memory scan:

  # echo scan > /sys/kernel/debug/kmemleak

To clear the list of all current possible memory leaks:

  # echo clear > /sys/kernel/debug/kmemleak

New leaks will then come up upon reading /sys/kernel/debug/kmemleak
again.

Note that the orphan objects are listed in the order they were allocated
and one object at the beginning of the list may cause other subsequent
objects to be reported as orphan.

Memory scanning parameters can be modified at run-time by writing to the
/sys/kernel/debug/kmemleak file. The following parameters are supported:

  off		- disable kmemleak (irreversible)
  stack=on	- enable the task stacks scanning (default)
  stack=off	- disable the tasks stacks scanning
  scan=on	- start the automatic memory scanning thread (default)
  scan=off	- stop the automatic memory scanning thread
  scan=<secs>	- set the automatic memory scanning period in seconds
		  (default 600, 0 to stop the automatic scanning)
  scan		- trigger a memory scan
  clear		- clear list of current memory leak suspects, done by
		  marking all current reported unreferenced objects grey,
		  or free all kmemleak objects if kmemleak has been disabled.
  dump=<addr>	- dump information about the object found at <addr>

Kmemleak can also be disabled at boot-time by passing "kmemleak=off" on
the kernel command line.

Memory may be allocated or freed before kmemleak is initialised and
these actions are stored in an early log buffer. The size of this buffer
is configured via the CONFIG_DEBUG_KMEMLEAK_EARLY_LOG_SIZE option.

Basic Algorithm
---------------

The memory allocations via kmalloc, vmalloc, kmem_cache_alloc and
friends are traced and the pointers, together with additional
information like size and stack trace, are stored in a rbtree.
The corresponding freeing function calls are tracked and the pointers
removed from the kmemleak data structures.

An allocated block of memory is considered orphan if no pointer to its
start address or to any location inside the block can be found by
scanning the memory (including saved registers). This means that there
might be no way for the kernel to pass the address of the allocated
block to a freeing function and therefore the block is considered a
memory leak.

The scanning algorithm steps:

  1. mark all objects as white (remaining white objects will later be
     considered orphan)
  2. scan the memory starting with the data section and stacks, checking
     the values against the addresses stored in the rbtree. If
     a pointer to a white object is found, the object is added to the
     gray list
  3. scan the gray objects for matching addresses (some white objects
     can become gray and added at the end of the gray list) until the
     gray set is finished
  4. the remaining white objects are considered orphan and reported via
     /sys/kernel/debug/kmemleak

Some allocated memory blocks have pointers stored in the kernel's
internal data structures and they cannot be detected as orphans. To
avoid this, kmemleak can also store the number of values pointing to an
address inside the block address range that need to be found so that the
block is not considered a leak. One example is __vmalloc().

Testing specific sections with kmemleak
---------------------------------------

Upon initial bootup your /sys/kernel/debug/kmemleak output page may be
quite extensive. This can also be the case if you have very buggy code
when doing development. To work around these situations you can use the
'clear' command to clear all reported unreferenced objects from the
/sys/kernel/debug/kmemleak output. By issuing a 'scan' after a 'clear'
you can find new unreferenced objects; this should help with testing
specific sections of code.

To test a critical section on demand with a clean kmemleak do:

  # echo clear > /sys/kernel/debug/kmemleak
  ... test your kernel or modules ...
  # echo scan > /sys/kernel/debug/kmemleak

Then as usual to get your report with:

  # cat /sys/kernel/debug/kmemleak

Freeing kmemleak internal objects
---------------------------------

To allow access to previosuly found memory leaks after kmemleak has been
disabled by the user or due to an fatal error, internal kmemleak objects
won't be freed when kmemleak is disabled, and those objects may occupy
a large part of physical memory.

In this situation, you may reclaim memory with:

  # echo clear > /sys/kernel/debug/kmemleak

Kmemleak API
------------

See the include/linux/kmemleak.h header for the functions prototype.

kmemleak_init		 - initialize kmemleak
kmemleak_alloc		 - notify of a memory block allocation
kmemleak_alloc_percpu	 - notify of a percpu memory block allocation
kmemleak_free		 - notify of a memory block freeing
kmemleak_free_part	 - notify of a partial memory block freeing
kmemleak_free_percpu	 - notify of a percpu memory block freeing
kmemleak_update_trace	 - update object allocation stack trace
kmemleak_not_leak	 - mark an object as not a leak
kmemleak_ignore		 - do not scan or report an object as leak
kmemleak_scan_area	 - add scan areas inside a memory block
kmemleak_no_scan	 - do not scan a memory block
kmemleak_erase		 - erase an old value in a pointer variable
kmemleak_alloc_recursive - as kmemleak_alloc but checks the recursiveness
kmemleak_free_recursive	 - as kmemleak_free but checks the recursiveness

Dealing with false positives/negatives
--------------------------------------

The false negatives are real memory leaks (orphan objects) but not
reported by kmemleak because values found during the memory scanning
point to such objects. To reduce the number of false negatives, kmemleak
provides the kmemleak_ignore, kmemleak_scan_area, kmemleak_no_scan and
kmemleak_erase functions (see above). The task stacks also increase the
amount of false negatives and their scanning is not enabled by default.

The false positives are objects wrongly reported as being memory leaks
(orphan). For objects known not to be leaks, kmemleak provides the
kmemleak_not_leak function. The kmemleak_ignore could also be used if
the memory block is known not to contain other pointers and it will no
longer be scanned.

Some of the reported leaks are only transient, especially on SMP
systems, because of pointers temporarily stored in CPU registers or
stacks. Kmemleak defines MSECS_MIN_AGE (defaulting to 1000) representing
the minimum age of an object to be reported as a memory leak.

Limitations and Drawbacks
-------------------------

The main drawback is the reduced performance of memory allocation and
freeing. To avoid other penalties, the memory scanning is only performed
when the /sys/kernel/debug/kmemleak file is read. Anyway, this tool is
intended for debugging purposes where the performance might not be the
most important requirement.

To keep the algorithm simple, kmemleak scans for values pointing to any
address inside a block's address range. This may lead to an increased
number of false negatives. However, it is likely that a real memory leak
will eventually become visible.

Another source of false negatives is the data stored in non-pointer
values. In a future version, kmemleak could only scan the pointer
members in the allocated structures. This feature would solve many of
the false negative cases described above.

The tool can report false positives. These are cases where an allocated
block doesn't need to be freed (some cases in the init_call functions),
the pointer is calculated by other methods than the usual container_of
macro or the pointer is stored in a location not scanned by kmemleak.

Page allocations and ioremap are not tracked.
Everything you never wanted to know about kobjects, ksets, and ktypes

Greg Kroah-Hartman <gregkh@linuxfoundation.org>

Based on an original article by Jon Corbet for lwn.net written October 1,
2003 and located at http://lwn.net/Articles/51437/

Last updated December 19, 2007


Part of the difficulty in understanding the driver model - and the kobject
abstraction upon which it is built - is that there is no obvious starting
place. Dealing with kobjects requires understanding a few different types,
all of which make reference to each other. In an attempt to make things
easier, we'll take a multi-pass approach, starting with vague terms and
adding detail as we go. To that end, here are some quick definitions of
some terms we will be working with.

 - A kobject is an object of type struct kobject.  Kobjects have a name
   and a reference count.  A kobject also has a parent pointer (allowing
   objects to be arranged into hierarchies), a specific type, and,
   usually, a representation in the sysfs virtual filesystem.

   Kobjects are generally not interesting on their own; instead, they are
   usually embedded within some other structure which contains the stuff
   the code is really interested in.

   No structure should EVER have more than one kobject embedded within it.
   If it does, the reference counting for the object is sure to be messed
   up and incorrect, and your code will be buggy.  So do not do this.

 - A ktype is the type of object that embeds a kobject.  Every structure
   that embeds a kobject needs a corresponding ktype.  The ktype controls
   what happens to the kobject when it is created and destroyed.

 - A kset is a group of kobjects.  These kobjects can be of the same ktype
   or belong to different ktypes.  The kset is the basic container type for
   collections of kobjects. Ksets contain their own kobjects, but you can
   safely ignore that implementation detail as the kset core code handles
   this kobject automatically.

   When you see a sysfs directory full of other directories, generally each
   of those directories corresponds to a kobject in the same kset.

We'll look at how to create and manipulate all of these types. A bottom-up
approach will be taken, so we'll go back to kobjects.


Embedding kobjects

It is rare for kernel code to create a standalone kobject, with one major
exception explained below.  Instead, kobjects are used to control access to
a larger, domain-specific object.  To this end, kobjects will be found
embedded in other structures.  If you are used to thinking of things in
object-oriented terms, kobjects can be seen as a top-level, abstract class
from which other classes are derived.  A kobject implements a set of
capabilities which are not particularly useful by themselves, but which are
nice to have in other objects.  The C language does not allow for the
direct expression of inheritance, so other techniques - such as structure
embedding - must be used.

(As an aside, for those familiar with the kernel linked list implementation,
this is analogous as to how "list_head" structs are rarely useful on
their own, but are invariably found embedded in the larger objects of
interest.)

So, for example, the UIO code in drivers/uio/uio.c has a structure that
defines the memory region associated with a uio device:

    struct uio_map {
	struct kobject kobj;
	struct uio_mem *mem;
    };

If you have a struct uio_map structure, finding its embedded kobject is
just a matter of using the kobj member.  Code that works with kobjects will
often have the opposite problem, however: given a struct kobject pointer,
what is the pointer to the containing structure?  You must avoid tricks
(such as assuming that the kobject is at the beginning of the structure)
and, instead, use the container_of() macro, found in <linux/kernel.h>:

    container_of(pointer, type, member)

where:

  * "pointer" is the pointer to the embedded kobject,
  * "type" is the type of the containing structure, and
  * "member" is the name of the structure field to which "pointer" points.

The return value from container_of() is a pointer to the corresponding
container type. So, for example, a pointer "kp" to a struct kobject
embedded *within* a struct uio_map could be converted to a pointer to the
*containing* uio_map structure with:

    struct uio_map *u_map = container_of(kp, struct uio_map, kobj);

For convenience, programmers often define a simple macro for "back-casting"
kobject pointers to the containing type.  Exactly this happens in the
earlier drivers/uio/uio.c, as you can see here:

    struct uio_map {
        struct kobject kobj;
        struct uio_mem *mem;
    };

    #define to_map(map) container_of(map, struct uio_map, kobj)

where the macro argument "map" is a pointer to the struct kobject in
question.  That macro is subsequently invoked with:

    struct uio_map *map = to_map(kobj);


Initialization of kobjects

Code which creates a kobject must, of course, initialize that object. Some
of the internal fields are setup with a (mandatory) call to kobject_init():

    void kobject_init(struct kobject *kobj, struct kobj_type *ktype);

The ktype is required for a kobject to be created properly, as every kobject
must have an associated kobj_type.  After calling kobject_init(), to
register the kobject with sysfs, the function kobject_add() must be called:

    int kobject_add(struct kobject *kobj, struct kobject *parent, const char *fmt, ...);

This sets up the parent of the kobject and the name for the kobject
properly.  If the kobject is to be associated with a specific kset,
kobj->kset must be assigned before calling kobject_add().  If a kset is
associated with a kobject, then the parent for the kobject can be set to
NULL in the call to kobject_add() and then the kobject's parent will be the
kset itself.

As the name of the kobject is set when it is added to the kernel, the name
of the kobject should never be manipulated directly.  If you must change
the name of the kobject, call kobject_rename():

    int kobject_rename(struct kobject *kobj, const char *new_name);

kobject_rename does not perform any locking or have a solid notion of
what names are valid so the caller must provide their own sanity checking
and serialization.

There is a function called kobject_set_name() but that is legacy cruft and
is being removed.  If your code needs to call this function, it is
incorrect and needs to be fixed.

To properly access the name of the kobject, use the function
kobject_name():

    const char *kobject_name(const struct kobject * kobj);

There is a helper function to both initialize and add the kobject to the
kernel at the same time, called surprisingly enough kobject_init_and_add():

    int kobject_init_and_add(struct kobject *kobj, struct kobj_type *ktype,
                             struct kobject *parent, const char *fmt, ...);

The arguments are the same as the individual kobject_init() and
kobject_add() functions described above.


Uevents

After a kobject has been registered with the kobject core, you need to
announce to the world that it has been created.  This can be done with a
call to kobject_uevent():

    int kobject_uevent(struct kobject *kobj, enum kobject_action action);

Use the KOBJ_ADD action for when the kobject is first added to the kernel.
This should be done only after any attributes or children of the kobject
have been initialized properly, as userspace will instantly start to look
for them when this call happens.

When the kobject is removed from the kernel (details on how to do that is
below), the uevent for KOBJ_REMOVE will be automatically created by the
kobject core, so the caller does not have to worry about doing that by
hand.


Reference counts

One of the key functions of a kobject is to serve as a reference counter
for the object in which it is embedded. As long as references to the object
exist, the object (and the code which supports it) must continue to exist.
The low-level functions for manipulating a kobject's reference counts are:

    struct kobject *kobject_get(struct kobject *kobj);
    void kobject_put(struct kobject *kobj);

A successful call to kobject_get() will increment the kobject's reference
counter and return the pointer to the kobject.

When a reference is released, the call to kobject_put() will decrement the
reference count and, possibly, free the object. Note that kobject_init()
sets the reference count to one, so the code which sets up the kobject will
need to do a kobject_put() eventually to release that reference.

Because kobjects are dynamic, they must not be declared statically or on
the stack, but instead, always allocated dynamically.  Future versions of
the kernel will contain a run-time check for kobjects that are created
statically and will warn the developer of this improper usage.

If all that you want to use a kobject for is to provide a reference counter
for your structure, please use the struct kref instead; a kobject would be
overkill.  For more information on how to use struct kref, please see the
file Documentation/kref.txt in the Linux kernel source tree.


Creating "simple" kobjects

Sometimes all that a developer wants is a way to create a simple directory
in the sysfs hierarchy, and not have to mess with the whole complication of
ksets, show and store functions, and other details.  This is the one
exception where a single kobject should be created.  To create such an
entry, use the function:

    struct kobject *kobject_create_and_add(char *name, struct kobject *parent);

This function will create a kobject and place it in sysfs in the location
underneath the specified parent kobject.  To create simple attributes
associated with this kobject, use:

    int sysfs_create_file(struct kobject *kobj, struct attribute *attr);
or
    int sysfs_create_group(struct kobject *kobj, struct attribute_group *grp);

Both types of attributes used here, with a kobject that has been created
with the kobject_create_and_add(), can be of type kobj_attribute, so no
special custom attribute is needed to be created.

See the example module, samples/kobject/kobject-example.c for an
implementation of a simple kobject and attributes.



ktypes and release methods

One important thing still missing from the discussion is what happens to a
kobject when its reference count reaches zero. The code which created the
kobject generally does not know when that will happen; if it did, there
would be little point in using a kobject in the first place. Even
predictable object lifecycles become more complicated when sysfs is brought
in as other portions of the kernel can get a reference on any kobject that
is registered in the system.

The end result is that a structure protected by a kobject cannot be freed
before its reference count goes to zero. The reference count is not under
the direct control of the code which created the kobject. So that code must
be notified asynchronously whenever the last reference to one of its
kobjects goes away.

Once you registered your kobject via kobject_add(), you must never use
kfree() to free it directly. The only safe way is to use kobject_put(). It
is good practice to always use kobject_put() after kobject_init() to avoid
errors creeping in.

This notification is done through a kobject's release() method. Usually
such a method has a form like:

    void my_object_release(struct kobject *kobj)
    {
    	    struct my_object *mine = container_of(kobj, struct my_object, kobj);

	    /* Perform any additional cleanup on this object, then... */
	    kfree(mine);
    }

One important point cannot be overstated: every kobject must have a
release() method, and the kobject must persist (in a consistent state)
until that method is called. If these constraints are not met, the code is
flawed.  Note that the kernel will warn you if you forget to provide a
release() method.  Do not try to get rid of this warning by providing an
"empty" release function; you will be mocked mercilessly by the kobject
maintainer if you attempt this.

Note, the name of the kobject is available in the release function, but it
must NOT be changed within this callback.  Otherwise there will be a memory
leak in the kobject core, which makes people unhappy.

Interestingly, the release() method is not stored in the kobject itself;
instead, it is associated with the ktype. So let us introduce struct
kobj_type:

    struct kobj_type {
	    void (*release)(struct kobject *kobj);
	    const struct sysfs_ops *sysfs_ops;
	    struct attribute **default_attrs;
	    const struct kobj_ns_type_operations *(*child_ns_type)(struct kobject *kobj);
	    const void *(*namespace)(struct kobject *kobj);
    };

This structure is used to describe a particular type of kobject (or, more
correctly, of containing object). Every kobject needs to have an associated
kobj_type structure; a pointer to that structure must be specified when you
call kobject_init() or kobject_init_and_add().

The release field in struct kobj_type is, of course, a pointer to the
release() method for this type of kobject. The other two fields (sysfs_ops
and default_attrs) control how objects of this type are represented in
sysfs; they are beyond the scope of this document.

The default_attrs pointer is a list of default attributes that will be
automatically created for any kobject that is registered with this ktype.


ksets

A kset is merely a collection of kobjects that want to be associated with
each other.  There is no restriction that they be of the same ktype, but be
very careful if they are not.

A kset serves these functions:

 - It serves as a bag containing a group of objects. A kset can be used by
   the kernel to track "all block devices" or "all PCI device drivers."

 - A kset is also a subdirectory in sysfs, where the associated kobjects
   with the kset can show up.  Every kset contains a kobject which can be
   set up to be the parent of other kobjects; the top-level directories of
   the sysfs hierarchy are constructed in this way.

 - Ksets can support the "hotplugging" of kobjects and influence how
   uevent events are reported to user space.

In object-oriented terms, "kset" is the top-level container class; ksets
contain their own kobject, but that kobject is managed by the kset code and
should not be manipulated by any other user.

A kset keeps its children in a standard kernel linked list.  Kobjects point
back to their containing kset via their kset field. In almost all cases,
the kobjects belonging to a kset have that kset (or, strictly, its embedded
kobject) in their parent.

As a kset contains a kobject within it, it should always be dynamically
created and never declared statically or on the stack.  To create a new
kset use:
  struct kset *kset_create_and_add(const char *name,
				   struct kset_uevent_ops *u,
				   struct kobject *parent);

When you are finished with the kset, call:
  void kset_unregister(struct kset *kset);
to destroy it.  This removes the kset from sysfs and decrements its reference
count.  When the reference count goes to zero, the kset will be released.
Because other references to the kset may still exist, the release may happen
after kset_unregister() returns.

An example of using a kset can be seen in the
samples/kobject/kset-example.c file in the kernel tree.

If a kset wishes to control the uevent operations of the kobjects
associated with it, it can use the struct kset_uevent_ops to handle it:

struct kset_uevent_ops {
        int (*filter)(struct kset *kset, struct kobject *kobj);
        const char *(*name)(struct kset *kset, struct kobject *kobj);
        int (*uevent)(struct kset *kset, struct kobject *kobj,
                      struct kobj_uevent_env *env);
};


The filter function allows a kset to prevent a uevent from being emitted to
userspace for a specific kobject.  If the function returns 0, the uevent
will not be emitted.

The name function will be called to override the default name of the kset
that the uevent sends to userspace.  By default, the name will be the same
as the kset itself, but this function, if present, can override that name.

The uevent function will be called when the uevent is about to be sent to
userspace to allow more environment variables to be added to the uevent.

One might ask how, exactly, a kobject is added to a kset, given that no
functions which perform that function have been presented.  The answer is
that this task is handled by kobject_add().  When a kobject is passed to
kobject_add(), its kset member should point to the kset to which the
kobject will belong.  kobject_add() will handle the rest.

If the kobject belonging to a kset has no parent kobject set, it will be
added to the kset's directory.  Not all members of a kset do necessarily
live in the kset directory.  If an explicit parent kobject is assigned
before the kobject is added, the kobject is registered with the kset, but
added below the parent kobject.


Kobject removal

After a kobject has been registered with the kobject core successfully, it
must be cleaned up when the code is finished with it.  To do that, call
kobject_put().  By doing this, the kobject core will automatically clean up
all of the memory allocated by this kobject.  If a KOBJ_ADD uevent has been
sent for the object, a corresponding KOBJ_REMOVE uevent will be sent, and
any other sysfs housekeeping will be handled for the caller properly.

If you need to do a two-stage delete of the kobject (say you are not
allowed to sleep when you need to destroy the object), then call
kobject_del() which will unregister the kobject from sysfs.  This makes the
kobject "invisible", but it is not cleaned up, and the reference count of
the object is still the same.  At a later time call kobject_put() to finish
the cleanup of the memory associated with the kobject.

kobject_del() can be used to drop the reference to the parent object, if
circular references are constructed.  It is valid in some cases, that a
parent objects references a child.  Circular references _must_ be broken
with an explicit call to kobject_del(), so that a release functions will be
called, and the objects in the former circle release each other.


Example code to copy from

For a more complete example of using ksets and kobjects properly, see the
example programs samples/kobject/{kobject-example.c,kset-example.c},
which will be built as loadable modules if you select CONFIG_SAMPLE_KOBJECT.
Title	: Kernel Probes (Kprobes)
Authors	: Jim Keniston <jkenisto@us.ibm.com>
	: Prasanna S Panchamukhi <prasanna.panchamukhi@gmail.com>
	: Masami Hiramatsu <mhiramat@redhat.com>

CONTENTS

1. Concepts: Kprobes, Jprobes, Return Probes
2. Architectures Supported
3. Configuring Kprobes
4. API Reference
5. Kprobes Features and Limitations
6. Probe Overhead
7. TODO
8. Kprobes Example
9. Jprobes Example
10. Kretprobes Example
Appendix A: The kprobes debugfs interface
Appendix B: The kprobes sysctl interface

1. Concepts: Kprobes, Jprobes, Return Probes

Kprobes enables you to dynamically break into any kernel routine and
collect debugging and performance information non-disruptively. You
can trap at almost any kernel code address(*), specifying a handler
routine to be invoked when the breakpoint is hit.
(*: some parts of the kernel code can not be trapped, see 1.5 Blacklist)

There are currently three types of probes: kprobes, jprobes, and
kretprobes (also called return probes).  A kprobe can be inserted
on virtually any instruction in the kernel.  A jprobe is inserted at
the entry to a kernel function, and provides convenient access to the
function's arguments.  A return probe fires when a specified function
returns.

In the typical case, Kprobes-based instrumentation is packaged as
a kernel module.  The module's init function installs ("registers")
one or more probes, and the exit function unregisters them.  A
registration function such as register_kprobe() specifies where
the probe is to be inserted and what handler is to be called when
the probe is hit.

There are also register_/unregister_*probes() functions for batch
registration/unregistration of a group of *probes. These functions
can speed up unregistration process when you have to unregister
a lot of probes at once.

The next four subsections explain how the different types of
probes work and how jump optimization works.  They explain certain
things that you'll need to know in order to make the best use of
Kprobes -- e.g., the difference between a pre_handler and
a post_handler, and how to use the maxactive and nmissed fields of
a kretprobe.  But if you're in a hurry to start using Kprobes, you
can skip ahead to section 2.

1.1 How Does a Kprobe Work?

When a kprobe is registered, Kprobes makes a copy of the probed
instruction and replaces the first byte(s) of the probed instruction
with a breakpoint instruction (e.g., int3 on i386 and x86_64).

When a CPU hits the breakpoint instruction, a trap occurs, the CPU's
registers are saved, and control passes to Kprobes via the
notifier_call_chain mechanism.  Kprobes executes the "pre_handler"
associated with the kprobe, passing the handler the addresses of the
kprobe struct and the saved registers.

Next, Kprobes single-steps its copy of the probed instruction.
(It would be simpler to single-step the actual instruction in place,
but then Kprobes would have to temporarily remove the breakpoint
instruction.  This would open a small time window when another CPU
could sail right past the probepoint.)

After the instruction is single-stepped, Kprobes executes the
"post_handler," if any, that is associated with the kprobe.
Execution then continues with the instruction following the probepoint.

1.2 How Does a Jprobe Work?

A jprobe is implemented using a kprobe that is placed on a function's
entry point.  It employs a simple mirroring principle to allow
seamless access to the probed function's arguments.  The jprobe
handler routine should have the same signature (arg list and return
type) as the function being probed, and must always end by calling
the Kprobes function jprobe_return().

Here's how it works.  When the probe is hit, Kprobes makes a copy of
the saved registers and a generous portion of the stack (see below).
Kprobes then points the saved instruction pointer at the jprobe's
handler routine, and returns from the trap.  As a result, control
passes to the handler, which is presented with the same register and
stack contents as the probed function.  When it is done, the handler
calls jprobe_return(), which traps again to restore the original stack
contents and processor state and switch to the probed function.

By convention, the callee owns its arguments, so gcc may produce code
that unexpectedly modifies that portion of the stack.  This is why
Kprobes saves a copy of the stack and restores it after the jprobe
handler has run.  Up to MAX_STACK_SIZE bytes are copied -- e.g.,
64 bytes on i386.

Note that the probed function's args may be passed on the stack
or in registers.  The jprobe will work in either case, so long as the
handler's prototype matches that of the probed function.

1.3 Return Probes

1.3.1 How Does a Return Probe Work?

When you call register_kretprobe(), Kprobes establishes a kprobe at
the entry to the function.  When the probed function is called and this
probe is hit, Kprobes saves a copy of the return address, and replaces
the return address with the address of a "trampoline."  The trampoline
is an arbitrary piece of code -- typically just a nop instruction.
At boot time, Kprobes registers a kprobe at the trampoline.

When the probed function executes its return instruction, control
passes to the trampoline and that probe is hit.  Kprobes' trampoline
handler calls the user-specified return handler associated with the
kretprobe, then sets the saved instruction pointer to the saved return
address, and that's where execution resumes upon return from the trap.

While the probed function is executing, its return address is
stored in an object of type kretprobe_instance.  Before calling
register_kretprobe(), the user sets the maxactive field of the
kretprobe struct to specify how many instances of the specified
function can be probed simultaneously.  register_kretprobe()
pre-allocates the indicated number of kretprobe_instance objects.

For example, if the function is non-recursive and is called with a
spinlock held, maxactive = 1 should be enough.  If the function is
non-recursive and can never relinquish the CPU (e.g., via a semaphore
or preemption), NR_CPUS should be enough.  If maxactive <= 0, it is
set to a default value.  If CONFIG_PREEMPT is enabled, the default
is max(10, 2*NR_CPUS).  Otherwise, the default is NR_CPUS.

It's not a disaster if you set maxactive too low; you'll just miss
some probes.  In the kretprobe struct, the nmissed field is set to
zero when the return probe is registered, and is incremented every
time the probed function is entered but there is no kretprobe_instance
object available for establishing the return probe.

1.3.2 Kretprobe entry-handler

Kretprobes also provides an optional user-specified handler which runs
on function entry. This handler is specified by setting the entry_handler
field of the kretprobe struct. Whenever the kprobe placed by kretprobe at the
function entry is hit, the user-defined entry_handler, if any, is invoked.
If the entry_handler returns 0 (success) then a corresponding return handler
is guaranteed to be called upon function return. If the entry_handler
returns a non-zero error then Kprobes leaves the return address as is, and
the kretprobe has no further effect for that particular function instance.

Multiple entry and return handler invocations are matched using the unique
kretprobe_instance object associated with them. Additionally, a user
may also specify per return-instance private data to be part of each
kretprobe_instance object. This is especially useful when sharing private
data between corresponding user entry and return handlers. The size of each
private data object can be specified at kretprobe registration time by
setting the data_size field of the kretprobe struct. This data can be
accessed through the data field of each kretprobe_instance object.

In case probed function is entered but there is no kretprobe_instance
object available, then in addition to incrementing the nmissed count,
the user entry_handler invocation is also skipped.

1.4 How Does Jump Optimization Work?

If your kernel is built with CONFIG_OPTPROBES=y (currently this flag
is automatically set 'y' on x86/x86-64, non-preemptive kernel) and
the "debug.kprobes_optimization" kernel parameter is set to 1 (see
sysctl(8)), Kprobes tries to reduce probe-hit overhead by using a jump
instruction instead of a breakpoint instruction at each probepoint.

1.4.1 Init a Kprobe

When a probe is registered, before attempting this optimization,
Kprobes inserts an ordinary, breakpoint-based kprobe at the specified
address. So, even if it's not possible to optimize this particular
probepoint, there'll be a probe there.

1.4.2 Safety Check

Before optimizing a probe, Kprobes performs the following safety checks:

- Kprobes verifies that the region that will be replaced by the jump
instruction (the "optimized region") lies entirely within one function.
(A jump instruction is multiple bytes, and so may overlay multiple
instructions.)

- Kprobes analyzes the entire function and verifies that there is no
jump into the optimized region.  Specifically:
  - the function contains no indirect jump;
  - the function contains no instruction that causes an exception (since
  the fixup code triggered by the exception could jump back into the
  optimized region -- Kprobes checks the exception tables to verify this);
  and
  - there is no near jump to the optimized region (other than to the first
  byte).

- For each instruction in the optimized region, Kprobes verifies that
the instruction can be executed out of line.

1.4.3 Preparing Detour Buffer

Next, Kprobes prepares a "detour" buffer, which contains the following
instruction sequence:
- code to push the CPU's registers (emulating a breakpoint trap)
- a call to the trampoline code which calls user's probe handlers.
- code to restore registers
- the instructions from the optimized region
- a jump back to the original execution path.

1.4.4 Pre-optimization

After preparing the detour buffer, Kprobes verifies that none of the
following situations exist:
- The probe has either a break_handler (i.e., it's a jprobe) or a
post_handler.
- Other instructions in the optimized region are probed.
- The probe is disabled.
In any of the above cases, Kprobes won't start optimizing the probe.
Since these are temporary situations, Kprobes tries to start
optimizing it again if the situation is changed.

If the kprobe can be optimized, Kprobes enqueues the kprobe to an
optimizing list, and kicks the kprobe-optimizer workqueue to optimize
it.  If the to-be-optimized probepoint is hit before being optimized,
Kprobes returns control to the original instruction path by setting
the CPU's instruction pointer to the copied code in the detour buffer
-- thus at least avoiding the single-step.

1.4.5 Optimization

The Kprobe-optimizer doesn't insert the jump instruction immediately;
rather, it calls synchronize_sched() for safety first, because it's
possible for a CPU to be interrupted in the middle of executing the
optimized region(*).  As you know, synchronize_sched() can ensure
that all interruptions that were active when synchronize_sched()
was called are done, but only if CONFIG_PREEMPT=n.  So, this version
of kprobe optimization supports only kernels with CONFIG_PREEMPT=n.(**)

After that, the Kprobe-optimizer calls stop_machine() to replace
the optimized region with a jump instruction to the detour buffer,
using text_poke_smp().

1.4.6 Unoptimization

When an optimized kprobe is unregistered, disabled, or blocked by
another kprobe, it will be unoptimized.  If this happens before
the optimization is complete, the kprobe is just dequeued from the
optimized list.  If the optimization has been done, the jump is
replaced with the original code (except for an int3 breakpoint in
the first byte) by using text_poke_smp().

(*)Please imagine that the 2nd instruction is interrupted and then
the optimizer replaces the 2nd instruction with the jump *address*
while the interrupt handler is running. When the interrupt
returns to original address, there is no valid instruction,
and it causes an unexpected result.

(**)This optimization-safety checking may be replaced with the
stop-machine method that ksplice uses for supporting a CONFIG_PREEMPT=y
kernel.

NOTE for geeks:
The jump optimization changes the kprobe's pre_handler behavior.
Without optimization, the pre_handler can change the kernel's execution
path by changing regs->ip and returning 1.  However, when the probe
is optimized, that modification is ignored.  Thus, if you want to
tweak the kernel's execution path, you need to suppress optimization,
using one of the following techniques:
- Specify an empty function for the kprobe's post_handler or break_handler.
 or
- Execute 'sysctl -w debug.kprobes_optimization=n'

1.5 Blacklist

Kprobes can probe most of the kernel except itself. This means
that there are some functions where kprobes cannot probe. Probing
(trapping) such functions can cause a recursive trap (e.g. double
fault) or the nested probe handler may never be called.
Kprobes manages such functions as a blacklist.
If you want to add a function into the blacklist, you just need
to (1) include linux/kprobes.h and (2) use NOKPROBE_SYMBOL() macro
to specify a blacklisted function.
Kprobes checks the given probe address against the blacklist and
rejects registering it, if the given address is in the blacklist.

2. Architectures Supported

Kprobes, jprobes, and return probes are implemented on the following
architectures:

- i386 (Supports jump optimization)
- x86_64 (AMD-64, EM64T) (Supports jump optimization)
- ppc64
- ia64 (Does not support probes on instruction slot1.)
- sparc64 (Return probes not yet implemented.)
- arm
- ppc
- mips

3. Configuring Kprobes

When configuring the kernel using make menuconfig/xconfig/oldconfig,
ensure that CONFIG_KPROBES is set to "y".  Under "Instrumentation
Support", look for "Kprobes".

So that you can load and unload Kprobes-based instrumentation modules,
make sure "Loadable module support" (CONFIG_MODULES) and "Module
unloading" (CONFIG_MODULE_UNLOAD) are set to "y".

Also make sure that CONFIG_KALLSYMS and perhaps even CONFIG_KALLSYMS_ALL
are set to "y", since kallsyms_lookup_name() is used by the in-kernel
kprobe address resolution code.

If you need to insert a probe in the middle of a function, you may find
it useful to "Compile the kernel with debug info" (CONFIG_DEBUG_INFO),
so you can use "objdump -d -l vmlinux" to see the source-to-object
code mapping.

4. API Reference

The Kprobes API includes a "register" function and an "unregister"
function for each type of probe. The API also includes "register_*probes"
and "unregister_*probes" functions for (un)registering arrays of probes.
Here are terse, mini-man-page specifications for these functions and
the associated probe handlers that you'll write. See the files in the
samples/kprobes/ sub-directory for examples.

4.1 register_kprobe

#include <linux/kprobes.h>
int register_kprobe(struct kprobe *kp);

Sets a breakpoint at the address kp->addr.  When the breakpoint is
hit, Kprobes calls kp->pre_handler.  After the probed instruction
is single-stepped, Kprobe calls kp->post_handler.  If a fault
occurs during execution of kp->pre_handler or kp->post_handler,
or during single-stepping of the probed instruction, Kprobes calls
kp->fault_handler.  Any or all handlers can be NULL. If kp->flags
is set KPROBE_FLAG_DISABLED, that kp will be registered but disabled,
so, its handlers aren't hit until calling enable_kprobe(kp).

NOTE:
1. With the introduction of the "symbol_name" field to struct kprobe,
the probepoint address resolution will now be taken care of by the kernel.
The following will now work:

	kp.symbol_name = "symbol_name";

(64-bit powerpc intricacies such as function descriptors are handled
transparently)

2. Use the "offset" field of struct kprobe if the offset into the symbol
to install a probepoint is known. This field is used to calculate the
probepoint.

3. Specify either the kprobe "symbol_name" OR the "addr". If both are
specified, kprobe registration will fail with -EINVAL.

4. With CISC architectures (such as i386 and x86_64), the kprobes code
does not validate if the kprobe.addr is at an instruction boundary.
Use "offset" with caution.

register_kprobe() returns 0 on success, or a negative errno otherwise.

User's pre-handler (kp->pre_handler):
#include <linux/kprobes.h>
#include <linux/ptrace.h>
int pre_handler(struct kprobe *p, struct pt_regs *regs);

Called with p pointing to the kprobe associated with the breakpoint,
and regs pointing to the struct containing the registers saved when
the breakpoint was hit.  Return 0 here unless you're a Kprobes geek.

User's post-handler (kp->post_handler):
#include <linux/kprobes.h>
#include <linux/ptrace.h>
void post_handler(struct kprobe *p, struct pt_regs *regs,
	unsigned long flags);

p and regs are as described for the pre_handler.  flags always seems
to be zero.

User's fault-handler (kp->fault_handler):
#include <linux/kprobes.h>
#include <linux/ptrace.h>
int fault_handler(struct kprobe *p, struct pt_regs *regs, int trapnr);

p and regs are as described for the pre_handler.  trapnr is the
architecture-specific trap number associated with the fault (e.g.,
on i386, 13 for a general protection fault or 14 for a page fault).
Returns 1 if it successfully handled the exception.

4.2 register_jprobe

#include <linux/kprobes.h>
int register_jprobe(struct jprobe *jp)

Sets a breakpoint at the address jp->kp.addr, which must be the address
of the first instruction of a function.  When the breakpoint is hit,
Kprobes runs the handler whose address is jp->entry.

The handler should have the same arg list and return type as the probed
function; and just before it returns, it must call jprobe_return().
(The handler never actually returns, since jprobe_return() returns
control to Kprobes.)  If the probed function is declared asmlinkage
or anything else that affects how args are passed, the handler's
declaration must match.

register_jprobe() returns 0 on success, or a negative errno otherwise.

4.3 register_kretprobe

#include <linux/kprobes.h>
int register_kretprobe(struct kretprobe *rp);

Establishes a return probe for the function whose address is
rp->kp.addr.  When that function returns, Kprobes calls rp->handler.
You must set rp->maxactive appropriately before you call
register_kretprobe(); see "How Does a Return Probe Work?" for details.

register_kretprobe() returns 0 on success, or a negative errno
otherwise.

User's return-probe handler (rp->handler):
#include <linux/kprobes.h>
#include <linux/ptrace.h>
int kretprobe_handler(struct kretprobe_instance *ri, struct pt_regs *regs);

regs is as described for kprobe.pre_handler.  ri points to the
kretprobe_instance object, of which the following fields may be
of interest:
- ret_addr: the return address
- rp: points to the corresponding kretprobe object
- task: points to the corresponding task struct
- data: points to per return-instance private data; see "Kretprobe
	entry-handler" for details.

The regs_return_value(regs) macro provides a simple abstraction to
extract the return value from the appropriate register as defined by
the architecture's ABI.

The handler's return value is currently ignored.

4.4 unregister_*probe

#include <linux/kprobes.h>
void unregister_kprobe(struct kprobe *kp);
void unregister_jprobe(struct jprobe *jp);
void unregister_kretprobe(struct kretprobe *rp);

Removes the specified probe.  The unregister function can be called
at any time after the probe has been registered.

NOTE:
If the functions find an incorrect probe (ex. an unregistered probe),
they clear the addr field of the probe.

4.5 register_*probes

#include <linux/kprobes.h>
int register_kprobes(struct kprobe **kps, int num);
int register_kretprobes(struct kretprobe **rps, int num);
int register_jprobes(struct jprobe **jps, int num);

Registers each of the num probes in the specified array.  If any
error occurs during registration, all probes in the array, up to
the bad probe, are safely unregistered before the register_*probes
function returns.
- kps/rps/jps: an array of pointers to *probe data structures
- num: the number of the array entries.

NOTE:
You have to allocate(or define) an array of pointers and set all
of the array entries before using these functions.

4.6 unregister_*probes

#include <linux/kprobes.h>
void unregister_kprobes(struct kprobe **kps, int num);
void unregister_kretprobes(struct kretprobe **rps, int num);
void unregister_jprobes(struct jprobe **jps, int num);

Removes each of the num probes in the specified array at once.

NOTE:
If the functions find some incorrect probes (ex. unregistered
probes) in the specified array, they clear the addr field of those
incorrect probes. However, other probes in the array are
unregistered correctly.

4.7 disable_*probe

#include <linux/kprobes.h>
int disable_kprobe(struct kprobe *kp);
int disable_kretprobe(struct kretprobe *rp);
int disable_jprobe(struct jprobe *jp);

Temporarily disables the specified *probe. You can enable it again by using
enable_*probe(). You must specify the probe which has been registered.

4.8 enable_*probe

#include <linux/kprobes.h>
int enable_kprobe(struct kprobe *kp);
int enable_kretprobe(struct kretprobe *rp);
int enable_jprobe(struct jprobe *jp);

Enables *probe which has been disabled by disable_*probe(). You must specify
the probe which has been registered.

5. Kprobes Features and Limitations

Kprobes allows multiple probes at the same address.  Currently,
however, there cannot be multiple jprobes on the same function at
the same time.  Also, a probepoint for which there is a jprobe or
a post_handler cannot be optimized.  So if you install a jprobe,
or a kprobe with a post_handler, at an optimized probepoint, the
probepoint will be unoptimized automatically.

In general, you can install a probe anywhere in the kernel.
In particular, you can probe interrupt handlers.  Known exceptions
are discussed in this section.

The register_*probe functions will return -EINVAL if you attempt
to install a probe in the code that implements Kprobes (mostly
kernel/kprobes.c and arch/*/kernel/kprobes.c, but also functions such
as do_page_fault and notifier_call_chain).

If you install a probe in an inline-able function, Kprobes makes
no attempt to chase down all inline instances of the function and
install probes there.  gcc may inline a function without being asked,
so keep this in mind if you're not seeing the probe hits you expect.

A probe handler can modify the environment of the probed function
-- e.g., by modifying kernel data structures, or by modifying the
contents of the pt_regs struct (which are restored to the registers
upon return from the breakpoint).  So Kprobes can be used, for example,
to install a bug fix or to inject faults for testing.  Kprobes, of
course, has no way to distinguish the deliberately injected faults
from the accidental ones.  Don't drink and probe.

Kprobes makes no attempt to prevent probe handlers from stepping on
each other -- e.g., probing printk() and then calling printk() from a
probe handler.  If a probe handler hits a probe, that second probe's
handlers won't be run in that instance, and the kprobe.nmissed member
of the second probe will be incremented.

As of Linux v2.6.15-rc1, multiple handlers (or multiple instances of
the same handler) may run concurrently on different CPUs.

Kprobes does not use mutexes or allocate memory except during
registration and unregistration.

Probe handlers are run with preemption disabled.  Depending on the
architecture and optimization state, handlers may also run with
interrupts disabled (e.g., kretprobe handlers and optimized kprobe
handlers run without interrupt disabled on x86/x86-64).  In any case,
your handler should not yield the CPU (e.g., by attempting to acquire
a semaphore).

Since a return probe is implemented by replacing the return
address with the trampoline's address, stack backtraces and calls
to __builtin_return_address() will typically yield the trampoline's
address instead of the real return address for kretprobed functions.
(As far as we can tell, __builtin_return_address() is used only
for instrumentation and error reporting.)

If the number of times a function is called does not match the number
of times it returns, registering a return probe on that function may
produce undesirable results. In such a case, a line:
kretprobe BUG!: Processing kretprobe d000000000041aa8 @ c00000000004f48c
gets printed. With this information, one will be able to correlate the
exact instance of the kretprobe that caused the problem. We have the
do_exit() case covered. do_execve() and do_fork() are not an issue.
We're unaware of other specific cases where this could be a problem.

If, upon entry to or exit from a function, the CPU is running on
a stack other than that of the current task, registering a return
probe on that function may produce undesirable results.  For this
reason, Kprobes doesn't support return probes (or kprobes or jprobes)
on the x86_64 version of __switch_to(); the registration functions
return -EINVAL.

On x86/x86-64, since the Jump Optimization of Kprobes modifies
instructions widely, there are some limitations to optimization. To
explain it, we introduce some terminology. Imagine a 3-instruction
sequence consisting of a two 2-byte instructions and one 3-byte
instruction.

        IA
         |
[-2][-1][0][1][2][3][4][5][6][7]
        [ins1][ins2][  ins3 ]
	[<-     DCR       ->]
	   [<- JTPR ->]

ins1: 1st Instruction
ins2: 2nd Instruction
ins3: 3rd Instruction
IA:  Insertion Address
JTPR: Jump Target Prohibition Region
DCR: Detoured Code Region

The instructions in DCR are copied to the out-of-line buffer
of the kprobe, because the bytes in DCR are replaced by
a 5-byte jump instruction. So there are several limitations.

a) The instructions in DCR must be relocatable.
b) The instructions in DCR must not include a call instruction.
c) JTPR must not be targeted by any jump or call instruction.
d) DCR must not straddle the border between functions.

Anyway, these limitations are checked by the in-kernel instruction
decoder, so you don't need to worry about that.

6. Probe Overhead

On a typical CPU in use in 2005, a kprobe hit takes 0.5 to 1.0
microseconds to process.  Specifically, a benchmark that hits the same
probepoint repeatedly, firing a simple handler each time, reports 1-2
million hits per second, depending on the architecture.  A jprobe or
return-probe hit typically takes 50-75% longer than a kprobe hit.
When you have a return probe set on a function, adding a kprobe at
the entry to that function adds essentially no overhead.

Here are sample overhead figures (in usec) for different architectures.
k = kprobe; j = jprobe; r = return probe; kr = kprobe + return probe
on same function; jr = jprobe + return probe on same function

i386: Intel Pentium M, 1495 MHz, 2957.31 bogomips
k = 0.57 usec; j = 1.00; r = 0.92; kr = 0.99; jr = 1.40

x86_64: AMD Opteron 246, 1994 MHz, 3971.48 bogomips
k = 0.49 usec; j = 0.76; r = 0.80; kr = 0.82; jr = 1.07

ppc64: POWER5 (gr), 1656 MHz (SMT disabled, 1 virtual CPU per physical CPU)
k = 0.77 usec; j = 1.31; r = 1.26; kr = 1.45; jr = 1.99

6.1 Optimized Probe Overhead

Typically, an optimized kprobe hit takes 0.07 to 0.1 microseconds to
process. Here are sample overhead figures (in usec) for x86 architectures.
k = unoptimized kprobe, b = boosted (single-step skipped), o = optimized kprobe,
r = unoptimized kretprobe, rb = boosted kretprobe, ro = optimized kretprobe.

i386: Intel(R) Xeon(R) E5410, 2.33GHz, 4656.90 bogomips
k = 0.80 usec; b = 0.33; o = 0.05; r = 1.10; rb = 0.61; ro = 0.33

x86-64: Intel(R) Xeon(R) E5410, 2.33GHz, 4656.90 bogomips
k = 0.99 usec; b = 0.43; o = 0.06; r = 1.24; rb = 0.68; ro = 0.30

7. TODO

a. SystemTap (http://sourceware.org/systemtap): Provides a simplified
programming interface for probe-based instrumentation.  Try it out.
b. Kernel return probes for sparc64.
c. Support for other architectures.
d. User-space probes.
e. Watchpoint probes (which fire on data references).

8. Kprobes Example

See samples/kprobes/kprobe_example.c

9. Jprobes Example

See samples/kprobes/jprobe_example.c

10. Kretprobes Example

See samples/kprobes/kretprobe_example.c

For additional information on Kprobes, refer to the following URLs:
http://www-106.ibm.com/developerworks/library/l-kprobes.html?ca=dgr-lnxw42Kprobe
http://www.redhat.com/magazine/005mar05/features/kprobes/
http://www-users.cs.umn.edu/~boutcher/kprobes/
http://www.linuxsymposium.org/2006/linuxsymposium_procv2.pdf (pages 101-115)


Appendix A: The kprobes debugfs interface

With recent kernels (> 2.6.20) the list of registered kprobes is visible
under the /sys/kernel/debug/kprobes/ directory (assuming debugfs is mounted at //sys/kernel/debug).

/sys/kernel/debug/kprobes/list: Lists all registered probes on the system

c015d71a  k  vfs_read+0x0
c011a316  j  do_fork+0x0
c03dedc5  r  tcp_v4_rcv+0x0

The first column provides the kernel address where the probe is inserted.
The second column identifies the type of probe (k - kprobe, r - kretprobe
and j - jprobe), while the third column specifies the symbol+offset of
the probe. If the probed function belongs to a module, the module name
is also specified. Following columns show probe status. If the probe is on
a virtual address that is no longer valid (module init sections, module
virtual addresses that correspond to modules that've been unloaded),
such probes are marked with [GONE]. If the probe is temporarily disabled,
such probes are marked with [DISABLED]. If the probe is optimized, it is
marked with [OPTIMIZED].

/sys/kernel/debug/kprobes/enabled: Turn kprobes ON/OFF forcibly.

Provides a knob to globally and forcibly turn registered kprobes ON or OFF.
By default, all kprobes are enabled. By echoing "0" to this file, all
registered probes will be disarmed, till such time a "1" is echoed to this
file. Note that this knob just disarms and arms all kprobes and doesn't
change each probe's disabling state. This means that disabled kprobes (marked
[DISABLED]) will be not enabled if you turn ON all kprobes by this knob.


Appendix B: The kprobes sysctl interface

/proc/sys/debug/kprobes-optimization: Turn kprobes optimization ON/OFF.

When CONFIG_OPTPROBES=y, this sysctl interface appears and it provides
a knob to globally and forcibly turn jump optimization (see section
1.4) ON or OFF. By default, jump optimization is allowed (ON).
If you echo "0" to this file or set "debug.kprobes_optimization" to
0 via sysctl, all optimized probes will be unoptimized, and any new
probes registered after that will not be optimized.  Note that this
knob *changes* the optimized state. This means that optimized probes
(marked [OPTIMIZED]) will be unoptimized ([OPTIMIZED] tag will be
removed). If the knob is turned on, they will be optimized again.


krefs allow you to add reference counters to your objects.  If you
have objects that are used in multiple places and passed around, and
you don't have refcounts, your code is almost certainly broken.  If
you want refcounts, krefs are the way to go.

To use a kref, add one to your data structures like:

struct my_data
{
	.
	.
	struct kref refcount;
	.
	.
};

The kref can occur anywhere within the data structure.

You must initialize the kref after you allocate it.  To do this, call
kref_init as so:

     struct my_data *data;

     data = kmalloc(sizeof(*data), GFP_KERNEL);
     if (!data)
            return -ENOMEM;
     kref_init(&data->refcount);

This sets the refcount in the kref to 1.

Once you have an initialized kref, you must follow the following
rules:

1) If you make a non-temporary copy of a pointer, especially if
   it can be passed to another thread of execution, you must
   increment the refcount with kref_get() before passing it off:
       kref_get(&data->refcount);
   If you already have a valid pointer to a kref-ed structure (the
   refcount cannot go to zero) you may do this without a lock.

2) When you are done with a pointer, you must call kref_put():
       kref_put(&data->refcount, data_release);
   If this is the last reference to the pointer, the release
   routine will be called.  If the code never tries to get
   a valid pointer to a kref-ed structure without already
   holding a valid pointer, it is safe to do this without
   a lock.

3) If the code attempts to gain a reference to a kref-ed structure
   without already holding a valid pointer, it must serialize access
   where a kref_put() cannot occur during the kref_get(), and the
   structure must remain valid during the kref_get().

For example, if you allocate some data and then pass it to another
thread to process:

void data_release(struct kref *ref)
{
	struct my_data *data = container_of(ref, struct my_data, refcount);
	kfree(data);
}

void more_data_handling(void *cb_data)
{
	struct my_data *data = cb_data;
	.
	. do stuff with data here
	.
	kref_put(&data->refcount, data_release);
}

int my_data_handler(void)
{
	int rv = 0;
	struct my_data *data;
	struct task_struct *task;
	data = kmalloc(sizeof(*data), GFP_KERNEL);
	if (!data)
		return -ENOMEM;
	kref_init(&data->refcount);

	kref_get(&data->refcount);
	task = kthread_run(more_data_handling, data, "more_data_handling");
	if (task == ERR_PTR(-ENOMEM)) {
		rv = -ENOMEM;
		goto out;
	}

	.
	. do stuff with data here
	.
 out:
	kref_put(&data->refcount, data_release);
	return rv;
}

This way, it doesn't matter what order the two threads handle the
data, the kref_put() handles knowing when the data is not referenced
any more and releasing it.  The kref_get() does not require a lock,
since we already have a valid pointer that we own a refcount for.  The
put needs no lock because nothing tries to get the data without
already holding a pointer.

Note that the "before" in rule 1 is very important.  You should never
do something like:

	task = kthread_run(more_data_handling, data, "more_data_handling");
	if (task == ERR_PTR(-ENOMEM)) {
		rv = -ENOMEM;
		goto out;
	} else
		/* BAD BAD BAD - get is after the handoff */
		kref_get(&data->refcount);

Don't assume you know what you are doing and use the above construct.
First of all, you may not know what you are doing.  Second, you may
know what you are doing (there are some situations where locking is
involved where the above may be legal) but someone else who doesn't
know what they are doing may change the code or copy the code.  It's
bad style.  Don't do it.

There are some situations where you can optimize the gets and puts.
For instance, if you are done with an object and enqueuing it for
something else or passing it off to something else, there is no reason
to do a get then a put:

	/* Silly extra get and put */
	kref_get(&obj->ref);
	enqueue(obj);
	kref_put(&obj->ref, obj_cleanup);

Just do the enqueue.  A comment about this is always welcome:

	enqueue(obj);
	/* We are done with obj, so we pass our refcount off
	   to the queue.  DON'T TOUCH obj AFTER HERE! */

The last rule (rule 3) is the nastiest one to handle.  Say, for
instance, you have a list of items that are each kref-ed, and you wish
to get the first one.  You can't just pull the first item off the list
and kref_get() it.  That violates rule 3 because you are not already
holding a valid pointer.  You must add a mutex (or some other lock).
For instance:

static DEFINE_MUTEX(mutex);
static LIST_HEAD(q);
struct my_data
{
	struct kref      refcount;
	struct list_head link;
};

static struct my_data *get_entry()
{
	struct my_data *entry = NULL;
	mutex_lock(&mutex);
	if (!list_empty(&q)) {
		entry = container_of(q.next, struct my_data, link);
		kref_get(&entry->refcount);
	}
	mutex_unlock(&mutex);
	return entry;
}

static void release_entry(struct kref *ref)
{
	struct my_data *entry = container_of(ref, struct my_data, refcount);

	list_del(&entry->link);
	kfree(entry);
}

static void put_entry(struct my_data *entry)
{
	mutex_lock(&mutex);
	kref_put(&entry->refcount, release_entry);
	mutex_unlock(&mutex);
}

The kref_put() return value is useful if you do not want to hold the
lock during the whole release operation.  Say you didn't want to call
kfree() with the lock held in the example above (since it is kind of
pointless to do so).  You could use kref_put() as follows:

static void release_entry(struct kref *ref)
{
	/* All work is done after the return from kref_put(). */
}

static void put_entry(struct my_data *entry)
{
	mutex_lock(&mutex);
	if (kref_put(&entry->refcount, release_entry)) {
		list_del(&entry->link);
		mutex_unlock(&mutex);
		kfree(entry);
	} else
		mutex_unlock(&mutex);
}

This is really more useful if you have to call other routines as part
of the free operations that could take a long time or might claim the
same lock.  Note that doing everything in the release routine is still
preferred as it is a little neater.


Corey Minyard <minyard@acm.org>

A lot of this was lifted from Greg Kroah-Hartman's 2004 OLS paper and
presentation on krefs, which can be found at:
  http://www.kroah.com/linux/talks/ols_2004_kref_paper/Reprint-Kroah-Hartman-OLS2004.pdf
and:
  http://www.kroah.com/linux/talks/ols_2004_kref_talk/


The above example could also be optimized using kref_get_unless_zero() in
the following way:

static struct my_data *get_entry()
{
	struct my_data *entry = NULL;
	mutex_lock(&mutex);
	if (!list_empty(&q)) {
		entry = container_of(q.next, struct my_data, link);
		if (!kref_get_unless_zero(&entry->refcount))
			entry = NULL;
	}
	mutex_unlock(&mutex);
	return entry;
}

static void release_entry(struct kref *ref)
{
	struct my_data *entry = container_of(ref, struct my_data, refcount);

	mutex_lock(&mutex);
	list_del(&entry->link);
	mutex_unlock(&mutex);
	kfree(entry);
}

static void put_entry(struct my_data *entry)
{
	kref_put(&entry->refcount, release_entry);
}

Which is useful to remove the mutex lock around kref_put() in put_entry(), but
it's important that kref_get_unless_zero is enclosed in the same critical
section that finds the entry in the lookup table,
otherwise kref_get_unless_zero may reference already freed memory.
Note that it is illegal to use kref_get_unless_zero without checking its
return value. If you are sure (by already having a valid pointer) that
kref_get_unless_zero() will return true, then use kref_get() instead.

The function kref_get_unless_zero also makes it possible to use rcu
locking for lookups in the above example:

struct my_data
{
	struct rcu_head rhead;
	.
	struct kref refcount;
	.
	.
};

static struct my_data *get_entry_rcu()
{
	struct my_data *entry = NULL;
	rcu_read_lock();
	if (!list_empty(&q)) {
		entry = container_of(q.next, struct my_data, link);
		if (!kref_get_unless_zero(&entry->refcount))
			entry = NULL;
	}
	rcu_read_unlock();
	return entry;
}

static void release_entry_rcu(struct kref *ref)
{
	struct my_data *entry = container_of(ref, struct my_data, refcount);

	mutex_lock(&mutex);
	list_del_rcu(&entry->link);
	mutex_unlock(&mutex);
	kfree_rcu(entry, rhead);
}

static void put_entry(struct my_data *entry)
{
	kref_put(&entry->refcount, release_entry_rcu);
}

But note that the struct kref member needs to remain in valid memory for a
rcu grace period after release_entry_rcu was called. That can be accomplished
by using kfree_rcu(entry, rhead) as done above, or by calling synchronize_rcu()
before using kfree, but note that synchronize_rcu() may sleep for a
substantial amount of time.


Thomas Hellstrom <thellstrom@vmware.com>

            LDM - Logical Disk Manager (Dynamic Disks)
            ------------------------------------------

Originally Written by FlatCap - Richard Russon <ldm@flatcap.org>.
Last Updated by Anton Altaparmakov on 30 March 2007 for Windows Vista.

Overview
--------

Windows 2000, XP, and Vista use a new partitioning scheme.  It is a complete
replacement for the MSDOS style partitions.  It stores its information in a
1MiB journalled database at the end of the physical disk.  The size of
partitions is limited only by disk space.  The maximum number of partitions is
nearly 2000.

Any partitions created under the LDM are called "Dynamic Disks".  There are no
longer any primary or extended partitions.  Normal MSDOS style partitions are
now known as Basic Disks.

If you wish to use Spanned, Striped, Mirrored or RAID 5 Volumes, you must use
Dynamic Disks.  The journalling allows Windows to make changes to these
partitions and filesystems without the need to reboot.

Once the LDM driver has divided up the disk, you can use the MD driver to
assemble any multi-partition volumes, e.g.  Stripes, RAID5.

To prevent legacy applications from repartitioning the disk, the LDM creates a
dummy MSDOS partition containing one disk-sized partition.  This is what is
supported with the Linux LDM driver.

A newer approach that has been implemented with Vista is to put LDM on top of a
GPT label disk.  This is not supported by the Linux LDM driver yet.


Example
-------

Below we have a 50MiB disk, divided into seven partitions.
N.B.  The missing 1MiB at the end of the disk is where the LDM database is
      stored.

  Device | Offset Bytes  Sectors  MiB | Size   Bytes  Sectors  MiB
  -------+----------------------------+---------------------------
  hda    |            0        0    0 |     52428800   102400   50
  hda1   |     51380224   100352   49 |      1048576     2048    1
  hda2   |        16384       32    0 |      6979584    13632    6
  hda3   |      6995968    13664    6 |     10485760    20480   10
  hda4   |     17481728    34144   16 |      4194304     8192    4
  hda5   |     21676032    42336   20 |      5242880    10240    5
  hda6   |     26918912    52576   25 |     10485760    20480   10
  hda7   |     37404672    73056   35 |     13959168    27264   13

The LDM Database may not store the partitions in the order that they appear on
disk, but the driver will sort them.

When Linux boots, you will see something like:

  hda: 102400 sectors w/32KiB Cache, CHS=50/64/32
  hda: [LDM] hda1 hda2 hda3 hda4 hda5 hda6 hda7


Compiling LDM Support
---------------------

To enable LDM, choose the following two options: 

  "Advanced partition selection" CONFIG_PARTITION_ADVANCED
  "Windows Logical Disk Manager (Dynamic Disk) support" CONFIG_LDM_PARTITION

If you believe the driver isn't working as it should, you can enable the extra
debugging code.  This will produce a LOT of output.  The option is:

  "Windows LDM extra logging" CONFIG_LDM_DEBUG

N.B. The partition code cannot be compiled as a module.

As with all the partition code, if the driver doesn't see signs of its type of
partition, it will pass control to another driver, so there is no harm in
enabling it.

If you have Dynamic Disks but don't enable the driver, then all you will see
is a dummy MSDOS partition filling the whole disk.  You won't be able to mount
any of the volumes on the disk.


Booting
-------

If you enable LDM support, then lilo is capable of booting from any of the
discovered partitions.  However, grub does not understand the LDM partitioning
and cannot boot from a Dynamic Disk.


More Documentation
------------------

There is an Overview of the LDM together with complete Technical Documentation.
It is available for download.

  http://www.linux-ntfs.org/

If you have any LDM questions that aren't answered in the documentation, email
me.

Cheers,
    FlatCap - Richard Russon
    ldm@flatcap.org

	     Semantics and Behavior of Local Atomic Operations

			    Mathieu Desnoyers


	This document explains the purpose of the local atomic operations, how
to implement them for any given architecture and shows how they can be used
properly. It also stresses on the precautions that must be taken when reading
those local variables across CPUs when the order of memory writes matters.



* Purpose of local atomic operations

Local atomic operations are meant to provide fast and highly reentrant per CPU
counters. They minimize the performance cost of standard atomic operations by
removing the LOCK prefix and memory barriers normally required to synchronize
across CPUs.

Having fast per CPU atomic counters is interesting in many cases : it does not
require disabling interrupts to protect from interrupt handlers and it permits
coherent counters in NMI handlers. It is especially useful for tracing purposes
and for various performance monitoring counters.

Local atomic operations only guarantee variable modification atomicity wrt the
CPU which owns the data. Therefore, care must taken to make sure that only one
CPU writes to the local_t data. This is done by using per cpu data and making
sure that we modify it from within a preemption safe context. It is however
permitted to read local_t data from any CPU : it will then appear to be written
out of order wrt other memory writes by the owner CPU.


* Implementation for a given architecture

It can be done by slightly modifying the standard atomic operations : only
their UP variant must be kept. It typically means removing LOCK prefix (on
i386 and x86_64) and any SMP synchronization barrier. If the architecture does
not have a different behavior between SMP and UP, including asm-generic/local.h
in your architecture's local.h is sufficient.

The local_t type is defined as an opaque signed long by embedding an
atomic_long_t inside a structure. This is made so a cast from this type to a
long fails. The definition looks like :

typedef struct { atomic_long_t a; } local_t;


* Rules to follow when using local atomic operations

- Variables touched by local ops must be per cpu variables.
- _Only_ the CPU owner of these variables must write to them.
- This CPU can use local ops from any context (process, irq, softirq, nmi, ...)
  to update its local_t variables.
- Preemption (or interrupts) must be disabled when using local ops in
  process context to   make sure the process won't be migrated to a
  different CPU between getting the per-cpu variable and doing the
  actual local op.
- When using local ops in interrupt context, no special care must be
  taken on a mainline kernel, since they will run on the local CPU with
  preemption already disabled. I suggest, however, to explicitly
  disable preemption anyway to make sure it will still work correctly on
  -rt kernels.
- Reading the local cpu variable will provide the current copy of the
  variable.
- Reads of these variables can be done from any CPU, because updates to
  "long", aligned, variables are always atomic. Since no memory
  synchronization is done by the writer CPU, an outdated copy of the
  variable can be read when reading some _other_ cpu's variables.


* How to use local atomic operations

#include <linux/percpu.h>
#include <asm/local.h>

static DEFINE_PER_CPU(local_t, counters) = LOCAL_INIT(0);


* Counting

Counting is done on all the bits of a signed long.

In preemptible context, use get_cpu_var() and put_cpu_var() around local atomic
operations : it makes sure that preemption is disabled around write access to
the per cpu variable. For instance :

	local_inc(&get_cpu_var(counters));
	put_cpu_var(counters);

If you are already in a preemption-safe context, you can directly use
__get_cpu_var() instead.

	local_inc(&__get_cpu_var(counters));



* Reading the counters

Those local counters can be read from foreign CPUs to sum the count. Note that
the data seen by local_read across CPUs must be considered to be out of order
relatively to other memory writes happening on the CPU that owns the data.

	long sum = 0;
	for_each_online_cpu(cpu)
		sum += local_read(&per_cpu(counters, cpu));

If you want to use a remote local_read to synchronize access to a resource
between CPUs, explicit smp_wmb() and smp_rmb() memory barriers must be used
respectively on the writer and the reader CPUs. It would be the case if you use
the local_t variable as a counter of bytes written in a buffer : there should
be a smp_wmb() between the buffer write and the counter increment and also a
smp_rmb() between the counter read and the buffer read.


Here is a sample module which implements a basic per cpu counter using local.h.

--- BEGIN ---
/* test-local.c
 *
 * Sample module for local.h usage.
 */


#include <asm/local.h>
#include <linux/module.h>
#include <linux/timer.h>

static DEFINE_PER_CPU(local_t, counters) = LOCAL_INIT(0);

static struct timer_list test_timer;

/* IPI called on each CPU. */
static void test_each(void *info)
{
	/* Increment the counter from a non preemptible context */
	printk("Increment on cpu %d\n", smp_processor_id());
	local_inc(&__get_cpu_var(counters));

	/* This is what incrementing the variable would look like within a
	 * preemptible context (it disables preemption) :
	 *
	 * local_inc(&get_cpu_var(counters));
	 * put_cpu_var(counters);
	 */
}

static void do_test_timer(unsigned long data)
{
	int cpu;

	/* Increment the counters */
	on_each_cpu(test_each, NULL, 1);
	/* Read all the counters */
	printk("Counters read from CPU %d\n", smp_processor_id());
	for_each_online_cpu(cpu) {
		printk("Read : CPU %d, count %ld\n", cpu,
			local_read(&per_cpu(counters, cpu)));
	}
	del_timer(&test_timer);
	test_timer.expires = jiffies + 1000;
	add_timer(&test_timer);
}

static int __init test_init(void)
{
	/* initialize the timer that will increment the counter */
	init_timer(&test_timer);
	test_timer.function = do_test_timer;
	test_timer.expires = jiffies + 1;
	add_timer(&test_timer);

	return 0;
}

static void __exit test_exit(void)
{
	del_timer_sync(&test_timer);
}

module_init(test_init);
module_exit(test_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Mathieu Desnoyers");
MODULE_DESCRIPTION("Local Atomic Ops");
--- END ---
Runtime locking correctness validator
=====================================

started by Ingo Molnar <mingo@redhat.com>
additions by Arjan van de Ven <arjan@linux.intel.com>

Lock-class
----------

The basic object the validator operates upon is a 'class' of locks.

A class of locks is a group of locks that are logically the same with
respect to locking rules, even if the locks may have multiple (possibly
tens of thousands of) instantiations. For example a lock in the inode
struct is one class, while each inode has its own instantiation of that
lock class.

The validator tracks the 'state' of lock-classes, and it tracks
dependencies between different lock-classes. The validator maintains a
rolling proof that the state and the dependencies are correct.

Unlike an lock instantiation, the lock-class itself never goes away: when
a lock-class is used for the first time after bootup it gets registered,
and all subsequent uses of that lock-class will be attached to this
lock-class.

State
-----

The validator tracks lock-class usage history into 4n + 1 separate state bits:

- 'ever held in STATE context'
- 'ever held as readlock in STATE context'
- 'ever held with STATE enabled'
- 'ever held as readlock with STATE enabled'

Where STATE can be either one of (kernel/lockdep_states.h)
 - hardirq
 - softirq
 - reclaim_fs

- 'ever used'                                       [ == !unused        ]

When locking rules are violated, these state bits are presented in the
locking error messages, inside curlies. A contrived example:

   modprobe/2287 is trying to acquire lock:
    (&sio_locks[i].lock){-.-...}, at: [<c02867fd>] mutex_lock+0x21/0x24

   but task is already holding lock:
    (&sio_locks[i].lock){-.-...}, at: [<c02867fd>] mutex_lock+0x21/0x24


The bit position indicates STATE, STATE-read, for each of the states listed
above, and the character displayed in each indicates:

   '.'  acquired while irqs disabled and not in irq context
   '-'  acquired in irq context
   '+'  acquired with irqs enabled
   '?'  acquired in irq context with irqs enabled.

Unused mutexes cannot be part of the cause of an error.


Single-lock state rules:
------------------------

A softirq-unsafe lock-class is automatically hardirq-unsafe as well. The
following states are exclusive, and only one of them is allowed to be
set for any lock-class:

 <hardirq-safe> and <hardirq-unsafe>
 <softirq-safe> and <softirq-unsafe>

The validator detects and reports lock usage that violate these
single-lock state rules.

Multi-lock dependency rules:
----------------------------

The same lock-class must not be acquired twice, because this could lead
to lock recursion deadlocks.

Furthermore, two locks may not be taken in different order:

 <L1> -> <L2>
 <L2> -> <L1>

because this could lead to lock inversion deadlocks. (The validator
finds such dependencies in arbitrary complexity, i.e. there can be any
other locking sequence between the acquire-lock operations, the
validator will still track all dependencies between locks.)

Furthermore, the following usage based lock dependencies are not allowed
between any two lock-classes:

   <hardirq-safe>   ->  <hardirq-unsafe>
   <softirq-safe>   ->  <softirq-unsafe>

The first rule comes from the fact the a hardirq-safe lock could be
taken by a hardirq context, interrupting a hardirq-unsafe lock - and
thus could result in a lock inversion deadlock. Likewise, a softirq-safe
lock could be taken by an softirq context, interrupting a softirq-unsafe
lock.

The above rules are enforced for any locking sequence that occurs in the
kernel: when acquiring a new lock, the validator checks whether there is
any rule violation between the new lock and any of the held locks.

When a lock-class changes its state, the following aspects of the above
dependency rules are enforced:

- if a new hardirq-safe lock is discovered, we check whether it
  took any hardirq-unsafe lock in the past.

- if a new softirq-safe lock is discovered, we check whether it took
  any softirq-unsafe lock in the past.

- if a new hardirq-unsafe lock is discovered, we check whether any
  hardirq-safe lock took it in the past.

- if a new softirq-unsafe lock is discovered, we check whether any
  softirq-safe lock took it in the past.

(Again, we do these checks too on the basis that an interrupt context
could interrupt _any_ of the irq-unsafe or hardirq-unsafe locks, which
could lead to a lock inversion deadlock - even if that lock scenario did
not trigger in practice yet.)

Exception: Nested data dependencies leading to nested locking
-------------------------------------------------------------

There are a few cases where the Linux kernel acquires more than one
instance of the same lock-class. Such cases typically happen when there
is some sort of hierarchy within objects of the same type. In these
cases there is an inherent "natural" ordering between the two objects
(defined by the properties of the hierarchy), and the kernel grabs the
locks in this fixed order on each of the objects.

An example of such an object hierarchy that results in "nested locking"
is that of a "whole disk" block-dev object and a "partition" block-dev
object; the partition is "part of" the whole device and as long as one
always takes the whole disk lock as a higher lock than the partition
lock, the lock ordering is fully correct. The validator does not
automatically detect this natural ordering, as the locking rule behind
the ordering is not static.

In order to teach the validator about this correct usage model, new
versions of the various locking primitives were added that allow you to
specify a "nesting level". An example call, for the block device mutex,
looks like this:

enum bdev_bd_mutex_lock_class
{
       BD_MUTEX_NORMAL,
       BD_MUTEX_WHOLE,
       BD_MUTEX_PARTITION
};

 mutex_lock_nested(&bdev->bd_contains->bd_mutex, BD_MUTEX_PARTITION);

In this case the locking is done on a bdev object that is known to be a
partition.

The validator treats a lock that is taken in such a nested fashion as a
separate (sub)class for the purposes of validation.

Note: When changing code to use the _nested() primitives, be careful and
check really thoroughly that the hierarchy is correctly mapped; otherwise
you can get false positives or false negatives.

Proof of 100% correctness:
--------------------------

The validator achieves perfect, mathematical 'closure' (proof of locking
correctness) in the sense that for every simple, standalone single-task
locking sequence that occurred at least once during the lifetime of the
kernel, the validator proves it with a 100% certainty that no
combination and timing of these locking sequences can cause any class of
lock related deadlock. [*]

I.e. complex multi-CPU and multi-task locking scenarios do not have to
occur in practice to prove a deadlock: only the simple 'component'
locking chains have to occur at least once (anytime, in any
task/context) for the validator to be able to prove correctness. (For
example, complex deadlocks that would normally need more than 3 CPUs and
a very unlikely constellation of tasks, irq-contexts and timings to
occur, can be detected on a plain, lightly loaded single-CPU system as
well!)

This radically decreases the complexity of locking related QA of the
kernel: what has to be done during QA is to trigger as many "simple"
single-task locking dependencies in the kernel as possible, at least
once, to prove locking correctness - instead of having to trigger every
possible combination of locking interaction between CPUs, combined with
every possible hardirq and softirq nesting scenario (which is impossible
to do in practice).

[*] assuming that the validator itself is 100% correct, and no other
    part of the system corrupts the state of the validator in any way.
    We also assume that all NMI/SMM paths [which could interrupt
    even hardirq-disabled codepaths] are correct and do not interfere
    with the validator. We also assume that the 64-bit 'chain hash'
    value is unique for every lock-chain in the system. Also, lock
    recursion must not be higher than 20.

Performance:
------------

The above rules require _massive_ amounts of runtime checking. If we did
that for every lock taken and for every irqs-enable event, it would
render the system practically unusably slow. The complexity of checking
is O(N^2), so even with just a few hundred lock-classes we'd have to do
tens of thousands of checks for every event.

This problem is solved by checking any given 'locking scenario' (unique
sequence of locks taken after each other) only once. A simple stack of
held locks is maintained, and a lightweight 64-bit hash value is
calculated, which hash is unique for every lock chain. The hash value,
when the chain is validated for the first time, is then put into a hash
table, which hash-table can be checked in a lockfree manner. If the
locking chain occurs again later on, the hash table tells us that we
dont have to validate the chain again.

Troubleshooting:
----------------

The validator tracks a maximum of MAX_LOCKDEP_KEYS number of lock classes.
Exceeding this number will trigger the following lockdep warning:

	(DEBUG_LOCKS_WARN_ON(id >= MAX_LOCKDEP_KEYS))

By default, MAX_LOCKDEP_KEYS is currently set to 8191, and typical
desktop systems have less than 1,000 lock classes, so this warning
normally results from lock-class leakage or failure to properly
initialize locks.  These two problems are illustrated below:

1.	Repeated module loading and unloading while running the validator
	will result in lock-class leakage.  The issue here is that each
	load of the module will create a new set of lock classes for
	that module's locks, but module unloading does not remove old
	classes (see below discussion of reuse of lock classes for why).
	Therefore, if that module is loaded and unloaded repeatedly,
	the number of lock classes will eventually reach the maximum.

2.	Using structures such as arrays that have large numbers of
	locks that are not explicitly initialized.  For example,
	a hash table with 8192 buckets where each bucket has its own
	spinlock_t will consume 8192 lock classes -unless- each spinlock
	is explicitly initialized at runtime, for example, using the
	run-time spin_lock_init() as opposed to compile-time initializers
	such as __SPIN_LOCK_UNLOCKED().  Failure to properly initialize
	the per-bucket spinlocks would guarantee lock-class overflow.
	In contrast, a loop that called spin_lock_init() on each lock
	would place all 8192 locks into a single lock class.

	The moral of this story is that you should always explicitly
	initialize your locks.

One might argue that the validator should be modified to allow
lock classes to be reused.  However, if you are tempted to make this
argument, first review the code and think through the changes that would
be required, keeping in mind that the lock classes to be removed are
likely to be linked into the lock-dependency graph.  This turns out to
be harder to do than to say.

Of course, if you do run out of lock classes, the next thing to do is
to find the offending lock classes.  First, the following command gives
you the number of lock classes currently in use along with the maximum:

	grep "lock-classes" /proc/lockdep_stats

This command produces the following output on a modest system:

	 lock-classes:                          748 [max: 8191]

If the number allocated (748 above) increases continually over time,
then there is likely a leak.  The following command can be used to
identify the leaking lock classes:

	grep "BD" /proc/lockdep

Run the command and save the output, then compare against the output from
a later run of this command to identify the leakers.  This same output
can also help you find situations where runtime lock initialization has
been omitted.

LOCK STATISTICS

- WHAT

As the name suggests, it provides statistics on locks.

- WHY

Because things like lock contention can severely impact performance.

- HOW

Lockdep already has hooks in the lock functions and maps lock instances to
lock classes. We build on that (see Documentation/lockdep-design.txt).
The graph below shows the relation between the lock functions and the various
hooks therein.

        __acquire
            |
           lock _____
            |        \
            |    __contended
            |         |
            |       <wait>
            | _______/
            |/
            |
       __acquired
            |
            .
          <hold>
            .
            |
       __release
            |
         unlock

lock, unlock	- the regular lock functions
__*		- the hooks
<> 		- states

With these hooks we provide the following statistics:

 con-bounces       - number of lock contention that involved x-cpu data
 contentions       - number of lock acquisitions that had to wait
 wait time min     - shortest (non-0) time we ever had to wait for a lock
           max     - longest time we ever had to wait for a lock
	   total   - total time we spend waiting on this lock
	   avg     - average time spent waiting on this lock
 acq-bounces       - number of lock acquisitions that involved x-cpu data
 acquisitions      - number of times we took the lock
 hold time min     - shortest (non-0) time we ever held the lock
	   max     - longest time we ever held the lock
	   total   - total time this lock was held
	   avg     - average time this lock was held

These numbers are gathered per lock class, per read/write state (when
applicable).

It also tracks 4 contention points per class. A contention point is a call site
that had to wait on lock acquisition.

 - CONFIGURATION

Lock statistics are enabled via CONFIG_LOCK_STAT.

 - USAGE

Enable collection of statistics:

# echo 1 >/proc/sys/kernel/lock_stat

Disable collection of statistics:

# echo 0 >/proc/sys/kernel/lock_stat

Look at the current lock statistics:

( line numbers not part of actual output, done for clarity in the explanation
  below )

# less /proc/lock_stat

01 lock_stat version 0.4
02-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
03                              class name    con-bounces    contentions   waittime-min   waittime-max waittime-total   waittime-avg    acq-bounces   acquisitions   holdtime-min   holdtime-max holdtime-total   holdtime-avg
04-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
05
06                         &mm->mmap_sem-W:            46             84           0.26         939.10       16371.53         194.90          47291        2922365           0.16     2220301.69 17464026916.32        5975.99
07                         &mm->mmap_sem-R:            37            100           1.31      299502.61      325629.52        3256.30         212344       34316685           0.10        7744.91    95016910.20           2.77
08                         ---------------
09                           &mm->mmap_sem              1          [<ffffffff811502a7>] khugepaged_scan_mm_slot+0x57/0x280
19                           &mm->mmap_sem             96          [<ffffffff815351c4>] __do_page_fault+0x1d4/0x510
11                           &mm->mmap_sem             34          [<ffffffff81113d77>] vm_mmap_pgoff+0x87/0xd0
12                           &mm->mmap_sem             17          [<ffffffff81127e71>] vm_munmap+0x41/0x80
13                         ---------------
14                           &mm->mmap_sem              1          [<ffffffff81046fda>] dup_mmap+0x2a/0x3f0
15                           &mm->mmap_sem             60          [<ffffffff81129e29>] SyS_mprotect+0xe9/0x250
16                           &mm->mmap_sem             41          [<ffffffff815351c4>] __do_page_fault+0x1d4/0x510
17                           &mm->mmap_sem             68          [<ffffffff81113d77>] vm_mmap_pgoff+0x87/0xd0
18
19.............................................................................................................................................................................................................................
20
21                         unix_table_lock:           110            112           0.21          49.24         163.91           1.46          21094          66312           0.12         624.42       31589.81           0.48
22                         ---------------
23                         unix_table_lock             45          [<ffffffff8150ad8e>] unix_create1+0x16e/0x1b0
24                         unix_table_lock             47          [<ffffffff8150b111>] unix_release_sock+0x31/0x250
25                         unix_table_lock             15          [<ffffffff8150ca37>] unix_find_other+0x117/0x230
26                         unix_table_lock              5          [<ffffffff8150a09f>] unix_autobind+0x11f/0x1b0
27                         ---------------
28                         unix_table_lock             39          [<ffffffff8150b111>] unix_release_sock+0x31/0x250
29                         unix_table_lock             49          [<ffffffff8150ad8e>] unix_create1+0x16e/0x1b0
30                         unix_table_lock             20          [<ffffffff8150ca37>] unix_find_other+0x117/0x230
31                         unix_table_lock              4          [<ffffffff8150a09f>] unix_autobind+0x11f/0x1b0


This excerpt shows the first two lock class statistics. Line 01 shows the
output version - each time the format changes this will be updated. Line 02-04
show the header with column descriptions. Lines 05-18 and 20-31 show the actual
statistics. These statistics come in two parts; the actual stats separated by a
short separator (line 08, 13) from the contention points.

The first lock (05-18) is a read/write lock, and shows two lines above the
short separator. The contention points don't match the column descriptors,
they have two: contentions and [<IP>] symbol. The second set of contention
points are the points we're contending with.

The integer part of the time values is in us.

Dealing with nested locks, subclasses may appear:

32...........................................................................................................................................................................................................................
33
34                               &rq->lock:       13128          13128           0.43         190.53      103881.26           7.91          97454        3453404           0.00         401.11    13224683.11           3.82
35                               ---------
36                               &rq->lock          645          [<ffffffff8103bfc4>] task_rq_lock+0x43/0x75
37                               &rq->lock          297          [<ffffffff8104ba65>] try_to_wake_up+0x127/0x25a
38                               &rq->lock          360          [<ffffffff8103c4c5>] select_task_rq_fair+0x1f0/0x74a
39                               &rq->lock          428          [<ffffffff81045f98>] scheduler_tick+0x46/0x1fb
40                               ---------
41                               &rq->lock           77          [<ffffffff8103bfc4>] task_rq_lock+0x43/0x75
42                               &rq->lock          174          [<ffffffff8104ba65>] try_to_wake_up+0x127/0x25a
43                               &rq->lock         4715          [<ffffffff8103ed4b>] double_rq_lock+0x42/0x54
44                               &rq->lock          893          [<ffffffff81340524>] schedule+0x157/0x7b8
45
46...........................................................................................................................................................................................................................
47
48                             &rq->lock/1:        1526          11488           0.33         388.73      136294.31          11.86          21461          38404           0.00          37.93      109388.53           2.84
49                             -----------
50                             &rq->lock/1        11526          [<ffffffff8103ed58>] double_rq_lock+0x4f/0x54
51                             -----------
52                             &rq->lock/1         5645          [<ffffffff8103ed4b>] double_rq_lock+0x42/0x54
53                             &rq->lock/1         1224          [<ffffffff81340524>] schedule+0x157/0x7b8
54                             &rq->lock/1         4336          [<ffffffff8103ed58>] double_rq_lock+0x4f/0x54
55                             &rq->lock/1          181          [<ffffffff8104ba65>] try_to_wake_up+0x127/0x25a

Line 48 shows statistics for the second subclass (/1) of &rq->lock class
(subclass starts from 0), since in this case, as line 50 suggests,
double_rq_lock actually acquires a nested lock of two spinlocks.

View the top contending locks:

# grep : /proc/lock_stat | head
			clockevents_lock:       2926159        2947636           0.15       46882.81  1784540466.34         605.41        3381345        3879161           0.00        2260.97    53178395.68          13.71
		     tick_broadcast_lock:        346460         346717           0.18        2257.43    39364622.71         113.54        3642919        4242696           0.00        2263.79    49173646.60          11.59
		  &mapping->i_mmap_mutex:        203896         203899           3.36      645530.05 31767507988.39      155800.21        3361776        8893984           0.17        2254.15    14110121.02           1.59
			       &rq->lock:        135014         136909           0.18         606.09      842160.68           6.15        1540728       10436146           0.00         728.72    17606683.41           1.69
	       &(&zone->lru_lock)->rlock:         93000          94934           0.16          59.18      188253.78           1.98        1199912        3809894           0.15         391.40     3559518.81           0.93
			 tasklist_lock-W:         40667          41130           0.23        1189.42      428980.51          10.43         270278         510106           0.16         653.51     3939674.91           7.72
			 tasklist_lock-R:         21298          21305           0.20        1310.05      215511.12          10.12         186204         241258           0.14        1162.33     1179779.23           4.89
			      rcu_node_1:         47656          49022           0.16         635.41      193616.41           3.95         844888        1865423           0.00         764.26     1656226.96           0.89
       &(&dentry->d_lockref.lock)->rlock:         39791          40179           0.15        1302.08       88851.96           2.21        2790851       12527025           0.10        1910.75     3379714.27           0.27
			      rcu_node_0:         29203          30064           0.16         786.55     1555573.00          51.74          88963         244254           0.00         398.87      428872.51           1.76

Clear the statistics:

# echo 0 > /proc/lock_stat
===============================================================
Softlockup detector and hardlockup detector (aka nmi_watchdog)
===============================================================

The Linux kernel can act as a watchdog to detect both soft and hard
lockups.

A 'softlockup' is defined as a bug that causes the kernel to loop in
kernel mode for more than 20 seconds (see "Implementation" below for
details), without giving other tasks a chance to run. The current
stack trace is displayed upon detection and, by default, the system
will stay locked up. Alternatively, the kernel can be configured to
panic; a sysctl, "kernel.softlockup_panic", a kernel parameter,
"softlockup_panic" (see "Documentation/kernel-parameters.txt" for
details), and a compile option, "BOOTPARAM_HARDLOCKUP_PANIC", are
provided for this.

A 'hardlockup' is defined as a bug that causes the CPU to loop in
kernel mode for more than 10 seconds (see "Implementation" below for
details), without letting other interrupts have a chance to run.
Similarly to the softlockup case, the current stack trace is displayed
upon detection and the system will stay locked up unless the default
behavior is changed, which can be done through a compile time knob,
"BOOTPARAM_HARDLOCKUP_PANIC", and a kernel parameter, "nmi_watchdog"
(see "Documentation/kernel-parameters.txt" for details).

The panic option can be used in combination with panic_timeout (this
timeout is set through the confusingly named "kernel.panic" sysctl),
to cause the system to reboot automatically after a specified amount
of time.

=== Implementation ===

The soft and hard lockup detectors are built on top of the hrtimer and
perf subsystems, respectively. A direct consequence of this is that,
in principle, they should work in any architecture where these
subsystems are present.

A periodic hrtimer runs to generate interrupts and kick the watchdog
task. An NMI perf event is generated every "watchdog_thresh"
(compile-time initialized to 10 and configurable through sysctl of the
same name) seconds to check for hardlockups. If any CPU in the system
does not receive any hrtimer interrupt during that time the
'hardlockup detector' (the handler for the NMI perf event) will
generate a kernel warning or call panic, depending on the
configuration.

The watchdog task is a high priority kernel thread that updates a
timestamp every time it is scheduled. If that timestamp is not updated
for 2*watchdog_thresh seconds (the softlockup threshold) the
'softlockup detector' (coded inside the hrtimer callback function)
will dump useful debug information to the system log, after which it
will call panic if it was instructed to do so or resume execution of
other kernel code.

The period of the hrtimer is 2*watchdog_thresh/5, which means it has
two or three chances to generate an interrupt before the hardlockup
detector kicks in.

As explained above, a kernel knob is provided that allows
administrators to configure the period of the hrtimer and the perf
event. The right value for a particular environment is a trade-off
between fast response to lockups and detection overhead.
This is the full-colour version of the currently unofficial Linux logo
("currently unofficial" just means that there has been no paperwork and
that I have not really announced it yet).  It was created by Larry Ewing,
and is freely usable as long as you acknowledge Larry as the original
artist. 

Note that there are black-and-white versions of this available that
scale down to smaller sizes and are better for letterheads or whatever
you want to use it for: for the full range of logos take a look at
Larry's web-page:

	http://www.isc.tamu.edu/~lewing/linux/

This file is a registry of magic numbers which are in use.  When you
add a magic number to a structure, you should also add it to this
file, since it is best if the magic numbers used by various structures
are unique.

It is a *very* good idea to protect kernel data structures with magic
numbers.  This allows you to check at run time whether (a) a structure
has been clobbered, or (b) you've passed the wrong structure to a
routine.  This last is especially useful --- particularly when you are
passing pointers to structures via a void * pointer.  The tty code,
for example, does this frequently to pass driver-specific and line
discipline-specific structures back and forth.

The way to use magic numbers is to declare then at the beginning of
the structure, like so:

struct tty_ldisc {
	int	magic;
	...
};

Please follow this discipline when you are adding future enhancements
to the kernel!  It has saved me countless hours of debugging,
especially in the screwy cases where an array has been overrun and
structures following the array have been overwritten.  Using this
discipline, these cases get detected quickly and safely.

					Theodore Ts'o
					31 Mar 94

The magic table is current to Linux 2.1.55.

					Michael Chastain
					<mailto:mec@shout.net>
					22 Sep 1997

Now it should be up to date with Linux 2.1.112. Because
we are in feature freeze time it is very unlikely that
something will change before 2.2.x. The entries are
sorted by number field.

					Krzysztof G. Baranowski
					<mailto: kgb@knm.org.pl>
					29 Jul 1998

Updated the magic table to Linux 2.5.45. Right over the feature freeze,
but it is possible that some new magic numbers will sneak into the
kernel before 2.6.x yet.

					Petr Baudis
					<pasky@ucw.cz>
					03 Nov 2002

Updated the magic table to Linux 2.5.74.

					Fabian Frederick
					<ffrederick@users.sourceforge.net>
					09 Jul 2003


Magic Name            Number      Structure            File
===========================================================================
PG_MAGIC              'P'         pg_{read,write}_hdr include/linux/pg.h
CMAGIC                0x0111      user              include/linux/a.out.h
MKISS_DRIVER_MAGIC    0x04bf      mkiss_channel     drivers/net/mkiss.h
HDLC_MAGIC            0x239e      n_hdlc            drivers/char/n_hdlc.c
APM_BIOS_MAGIC        0x4101      apm_user          arch/x86/kernel/apm_32.c
CYCLADES_MAGIC        0x4359      cyclades_port     include/linux/cyclades.h
DB_MAGIC              0x4442      fc_info           drivers/net/iph5526_novram.c
DL_MAGIC              0x444d      fc_info           drivers/net/iph5526_novram.c
FASYNC_MAGIC          0x4601      fasync_struct     include/linux/fs.h
FF_MAGIC              0x4646      fc_info           drivers/net/iph5526_novram.c
ISICOM_MAGIC          0x4d54      isi_port          include/linux/isicom.h
PTY_MAGIC             0x5001                        drivers/char/pty.c
PPP_MAGIC             0x5002      ppp               include/linux/if_pppvar.h
SERIAL_MAGIC          0x5301      async_struct      include/linux/serial.h
SSTATE_MAGIC          0x5302      serial_state      include/linux/serial.h
SLIP_MAGIC            0x5302      slip              drivers/net/slip.h
STRIP_MAGIC           0x5303      strip             drivers/net/strip.c
X25_ASY_MAGIC         0x5303      x25_asy           drivers/net/x25_asy.h
SIXPACK_MAGIC         0x5304      sixpack           drivers/net/hamradio/6pack.h
AX25_MAGIC            0x5316      ax_disp           drivers/net/mkiss.h
TTY_MAGIC             0x5401      tty_struct        include/linux/tty.h
MGSL_MAGIC            0x5401      mgsl_info         drivers/char/synclink.c
TTY_DRIVER_MAGIC      0x5402      tty_driver        include/linux/tty_driver.h
MGSLPC_MAGIC          0x5402      mgslpc_info       drivers/char/pcmcia/synclink_cs.c
TTY_LDISC_MAGIC       0x5403      tty_ldisc         include/linux/tty_ldisc.h
USB_SERIAL_MAGIC      0x6702      usb_serial        drivers/usb/serial/usb-serial.h
FULL_DUPLEX_MAGIC     0x6969                        drivers/net/ethernet/dec/tulip/de2104x.c
USB_BLUETOOTH_MAGIC   0x6d02      usb_bluetooth     drivers/usb/class/bluetty.c
RFCOMM_TTY_MAGIC      0x6d02                        net/bluetooth/rfcomm/tty.c
USB_SERIAL_PORT_MAGIC 0x7301      usb_serial_port   drivers/usb/serial/usb-serial.h
CG_MAGIC              0x00090255  ufs_cylinder_group include/linux/ufs_fs.h
RPORT_MAGIC           0x00525001  r_port            drivers/char/rocket_int.h
LSEMAGIC              0x05091998  lse               drivers/fc4/fc.c
GDTIOCTL_MAGIC        0x06030f07  gdth_iowr_str     drivers/scsi/gdth_ioctl.h
RIEBL_MAGIC           0x09051990                    drivers/net/atarilance.c
NBD_REQUEST_MAGIC     0x12560953  nbd_request       include/linux/nbd.h
RED_MAGIC2            0x170fc2a5  (any)             mm/slab.c
BAYCOM_MAGIC          0x19730510  baycom_state      drivers/net/baycom_epp.c
ISDN_X25IFACE_MAGIC   0x1e75a2b9  isdn_x25iface_proto_data
                                                    drivers/isdn/isdn_x25iface.h
ECP_MAGIC             0x21504345  cdkecpsig         include/linux/cdk.h
LSOMAGIC              0x27091997  lso               drivers/fc4/fc.c
LSMAGIC               0x2a3b4d2a  ls                drivers/fc4/fc.c
WANPIPE_MAGIC         0x414C4453  sdla_{dump,exec}  include/linux/wanpipe.h
CS_CARD_MAGIC         0x43525553  cs_card           sound/oss/cs46xx.c
LABELCL_MAGIC         0x4857434c  labelcl_info_s    include/asm/ia64/sn/labelcl.h
ISDN_ASYNC_MAGIC      0x49344C01  modem_info        include/linux/isdn.h
CTC_ASYNC_MAGIC       0x49344C01  ctc_tty_info      drivers/s390/net/ctctty.c
ISDN_NET_MAGIC        0x49344C02  isdn_net_local_s  drivers/isdn/i4l/isdn_net_lib.h
SAVEKMSG_MAGIC2       0x4B4D5347  savekmsg          arch/*/amiga/config.c
CS_STATE_MAGIC        0x4c4f4749  cs_state          sound/oss/cs46xx.c
SLAB_C_MAGIC          0x4f17a36d  kmem_cache        mm/slab.c
COW_MAGIC             0x4f4f4f4d  cow_header_v1     arch/um/drivers/ubd_user.c
I810_CARD_MAGIC       0x5072696E  i810_card         sound/oss/i810_audio.c
TRIDENT_CARD_MAGIC    0x5072696E  trident_card      sound/oss/trident.c
ROUTER_MAGIC          0x524d4157  wan_device        [in wanrouter.h pre 3.9]
SCC_MAGIC             0x52696368  gs_port           drivers/char/scc.h
SAVEKMSG_MAGIC1       0x53415645  savekmsg          arch/*/amiga/config.c
GDA_MAGIC             0x58464552  gda               arch/mips/include/asm/sn/gda.h
RED_MAGIC1            0x5a2cf071  (any)             mm/slab.c
EEPROM_MAGIC_VALUE    0x5ab478d2  lanai_dev         drivers/atm/lanai.c
HDLCDRV_MAGIC         0x5ac6e778  hdlcdrv_state     include/linux/hdlcdrv.h
PCXX_MAGIC            0x5c6df104  channel           drivers/char/pcxx.h
KV_MAGIC              0x5f4b565f  kernel_vars_s     arch/mips/include/asm/sn/klkernvars.h
I810_STATE_MAGIC      0x63657373  i810_state        sound/oss/i810_audio.c
TRIDENT_STATE_MAGIC   0x63657373  trient_state      sound/oss/trident.c
M3_CARD_MAGIC         0x646e6f50  m3_card           sound/oss/maestro3.c
FW_HEADER_MAGIC       0x65726F66  fw_header         drivers/atm/fore200e.h
SLOT_MAGIC            0x67267321  slot              drivers/hotplug/cpqphp.h
SLOT_MAGIC            0x67267322  slot              drivers/hotplug/acpiphp.h
LO_MAGIC              0x68797548  nbd_device        include/linux/nbd.h
OPROFILE_MAGIC        0x6f70726f  super_block       drivers/oprofile/oprofilefs.h
M3_STATE_MAGIC        0x734d724d  m3_state          sound/oss/maestro3.c
VMALLOC_MAGIC         0x87654320  snd_alloc_track   sound/core/memory.c
KMALLOC_MAGIC         0x87654321  snd_alloc_track   sound/core/memory.c
PWC_MAGIC             0x89DC10AB  pwc_device        drivers/usb/media/pwc.h
NBD_REPLY_MAGIC       0x96744668  nbd_reply         include/linux/nbd.h
ENI155_MAGIC          0xa54b872d  midway_eprom	    drivers/atm/eni.h
SCI_MAGIC             0xbabeface  gs_port           drivers/char/sh-sci.h
CODA_MAGIC            0xC0DAC0DA  coda_file_info    fs/coda/coda_fs_i.h
DPMEM_MAGIC           0xc0ffee11  gdt_pci_sram      drivers/scsi/gdth.h
YAM_MAGIC             0xF10A7654  yam_port          drivers/net/hamradio/yam.c
CCB_MAGIC             0xf2691ad2  ccb               drivers/scsi/ncr53c8xx.c
QUEUE_MAGIC_FREE      0xf7e1c9a3  queue_entry       drivers/scsi/arm/queue.c
QUEUE_MAGIC_USED      0xf7e1cc33  queue_entry       drivers/scsi/arm/queue.c
HTB_CMAGIC            0xFEFAFEF1  htb_class         net/sched/sch_htb.c
NMI_MAGIC             0x48414d4d455201 nmi_s        arch/mips/include/asm/sn/nmi.h

Note that there are also defined special per-driver magic numbers in sound
memory management. See include/sound/sndmagic.h for complete list of them. Many
OSS sound drivers have their magic numbers constructed from the soundcard PCI
ID - these are not listed here as well.

IrDA subsystem also uses large number of own magic numbers, see
include/net/irda/irda.h for a complete list of them.

HFS is another larger user of magic numbers - you can find them in
fs/hfs/hfs.h.
Tools that manage md devices can be found at
   http://www.kernel.org/pub/linux/utils/raid/ 


Boot time assembly of RAID arrays
---------------------------------

You can boot with your md device with the following kernel command
lines:

for old raid arrays without persistent superblocks:
  md=<md device no.>,<raid level>,<chunk size factor>,<fault level>,dev0,dev1,...,devn

for raid arrays with persistent superblocks
  md=<md device no.>,dev0,dev1,...,devn
or, to assemble a partitionable array:
  md=d<md device no.>,dev0,dev1,...,devn
  
md device no. = the number of the md device ... 
              0 means md0, 
	      1 md1,
	      2 md2,
	      3 md3,
	      4 md4

raid level = -1 linear mode
              0 striped mode
	      other modes are only supported with persistent super blocks

chunk size factor = (raid-0 and raid-1 only)
              Set  the chunk size as 4k << n.
	      
fault level = totally ignored
			    
dev0-devn: e.g. /dev/hda1,/dev/hdc1,/dev/sda1,/dev/sdb1
			    
A possible loadlin line (Harald Hoyer <HarryH@Royal.Net>)  looks like this:

e:\loadlin\loadlin e:\zimage root=/dev/md0 md=0,0,4,0,/dev/hdb2,/dev/hdc3 ro


Boot time autodetection of RAID arrays
--------------------------------------

When md is compiled into the kernel (not as module), partitions of
type 0xfd are scanned and automatically assembled into RAID arrays.
This autodetection may be suppressed with the kernel parameter
"raid=noautodetect".  As of kernel 2.6.9, only drives with a type 0
superblock can be autodetected and run at boot time.

The kernel parameter "raid=partitionable" (or "raid=part") means
that all auto-detected arrays are assembled as partitionable.

Boot time assembly of degraded/dirty arrays
-------------------------------------------

If a raid5 or raid6 array is both dirty and degraded, it could have
undetectable data corruption.  This is because the fact that it is
'dirty' means that the parity cannot be trusted, and the fact that it
is degraded means that some datablocks are missing and cannot reliably
be reconstructed (due to no parity).

For this reason, md will normally refuse to start such an array.  This
requires the sysadmin to take action to explicitly start the array
despite possible corruption.  This is normally done with
   mdadm --assemble --force ....

This option is not really available if the array has the root
filesystem on it.  In order to support this booting from such an
array, md supports a module parameter "start_dirty_degraded" which,
when set to 1, bypassed the checks and will allows dirty degraded
arrays to be started.

So, to boot with a root filesystem of a dirty degraded raid[56], use

   md-mod.start_dirty_degraded=1


Superblock formats
------------------

The md driver can support a variety of different superblock formats.
Currently, it supports superblock formats "0.90.0" and the "md-1" format
introduced in the 2.5 development series.

The kernel will autodetect which format superblock is being used.

Superblock format '0' is treated differently to others for legacy
reasons - it is the original superblock format.


General Rules - apply for all superblock formats
------------------------------------------------

An array is 'created' by writing appropriate superblocks to all
devices.

It is 'assembled' by associating each of these devices with an
particular md virtual device.  Once it is completely assembled, it can
be accessed.

An array should be created by a user-space tool.  This will write
superblocks to all devices.  It will usually mark the array as
'unclean', or with some devices missing so that the kernel md driver
can create appropriate redundancy (copying in raid1, parity
calculation in raid4/5).

When an array is assembled, it is first initialized with the
SET_ARRAY_INFO ioctl.  This contains, in particular, a major and minor
version number.  The major version number selects which superblock
format is to be used.  The minor number might be used to tune handling
of the format, such as suggesting where on each device to look for the
superblock.

Then each device is added using the ADD_NEW_DISK ioctl.  This
provides, in particular, a major and minor number identifying the
device to add.

The array is started with the RUN_ARRAY ioctl.

Once started, new devices can be added.  They should have an
appropriate superblock written to them, and then be passed in with
ADD_NEW_DISK.

Devices that have failed or are not yet active can be detached from an
array using HOT_REMOVE_DISK.


Specific Rules that apply to format-0 super block arrays, and
       arrays with no superblock (non-persistent).
-------------------------------------------------------------

An array can be 'created' by describing the array (level, chunksize
etc) in a SET_ARRAY_INFO ioctl.  This must have major_version==0 and
raid_disks != 0.

Then uninitialized devices can be added with ADD_NEW_DISK.  The
structure passed to ADD_NEW_DISK must specify the state of the device
and its role in the array.

Once started with RUN_ARRAY, uninitialized spares can be added with
HOT_ADD_DISK.



MD devices in sysfs
-------------------
md devices appear in sysfs (/sys) as regular block devices,
e.g.
   /sys/block/md0

Each 'md' device will contain a subdirectory called 'md' which
contains further md-specific information about the device.

All md devices contain:
  level
     a text file indicating the 'raid level'. e.g. raid0, raid1,
     raid5, linear, multipath, faulty.
     If no raid level has been set yet (array is still being
     assembled), the value will reflect whatever has been written
     to it, which may be a name like the above, or may be a number
     such as '0', '5', etc.

  raid_disks
     a text file with a simple number indicating the number of devices
     in a fully functional array.  If this is not yet known, the file
     will be empty.  If an array is being resized this will contain
     the new number of devices.
     Some raid levels allow this value to be set while the array is
     active.  This will reconfigure the array.   Otherwise it can only
     be set while assembling an array.
     A change to this attribute will not be permitted if it would
     reduce the size of the array.  To reduce the number of drives
     in an e.g. raid5, the array size must first be reduced by
     setting the 'array_size' attribute.

  chunk_size
     This is the size in bytes for 'chunks' and is only relevant to
     raid levels that involve striping (0,4,5,6,10). The address space
     of the array is conceptually divided into chunks and consecutive
     chunks are striped onto neighbouring devices.
     The size should be at least PAGE_SIZE (4k) and should be a power
     of 2.  This can only be set while assembling an array

  layout
     The "layout" for the array for the particular level.  This is
     simply a number that is interpretted differently by different
     levels.  It can be written while assembling an array.

  array_size
     This can be used to artificially constrain the available space in
     the array to be less than is actually available on the combined
     devices.  Writing a number (in Kilobytes) which is less than
     the available size will set the size.  Any reconfiguration of the
     array (e.g. adding devices) will not cause the size to change.
     Writing the word 'default' will cause the effective size of the
     array to be whatever size is actually available based on
     'level', 'chunk_size' and 'component_size'.

     This can be used to reduce the size of the array before reducing
     the number of devices in a raid4/5/6, or to support external
     metadata formats which mandate such clipping.

  reshape_position
     This is either "none" or a sector number within the devices of
     the array where "reshape" is up to.  If this is set, the three
     attributes mentioned above (raid_disks, chunk_size, layout) can
     potentially have 2 values, an old and a new value.  If these
     values differ, reading the attribute returns
        new (old)
     and writing will effect the 'new' value, leaving the 'old'
     unchanged.

  component_size
     For arrays with data redundancy (i.e. not raid0, linear, faulty,
     multipath), all components must be the same size - or at least
     there must a size that they all provide space for.  This is a key
     part or the geometry of the array.  It is measured in sectors
     and can be read from here.  Writing to this value may resize
     the array if the personality supports it (raid1, raid5, raid6),
     and if the component drives are large enough.

  metadata_version
     This indicates the format that is being used to record metadata
     about the array.  It can be 0.90 (traditional format), 1.0, 1.1,
     1.2 (newer format in varying locations) or "none" indicating that
     the kernel isn't managing metadata at all.
     Alternately it can be "external:" followed by a string which
     is set by user-space.  This indicates that metadata is managed
     by a user-space program.  Any device failure or other event that
     requires a metadata update will cause array activity to be
     suspended until the event is acknowledged.

  resync_start
     The point at which resync should start.  If no resync is needed,
     this will be a very large number (or 'none' since 2.6.30-rc1).  At
     array creation it will default to 0, though starting the array as
     'clean' will set it much larger.

   new_dev
     This file can be written but not read.  The value written should
     be a block device number as major:minor.  e.g. 8:0
     This will cause that device to be attached to the array, if it is
     available.  It will then appear at md/dev-XXX (depending on the
     name of the device) and further configuration is then possible.

   safe_mode_delay
     When an md array has seen no write requests for a certain period
     of time, it will be marked as 'clean'.  When another write
     request arrives, the array is marked as 'dirty' before the write
     commences.  This is known as 'safe_mode'.
     The 'certain period' is controlled by this file which stores the
     period as a number of seconds.  The default is 200msec (0.200).
     Writing a value of 0 disables safemode.

   array_state
     This file contains a single word which describes the current
     state of the array.  In many cases, the state can be set by
     writing the word for the desired state, however some states
     cannot be explicitly set, and some transitions are not allowed.

     Select/poll works on this file.  All changes except between
     	active_idle and active (which can be frequent and are not
	very interesting) are notified.  active->active_idle is
	reported if the metadata is externally managed.

     clear
         No devices, no size, no level
         Writing is equivalent to STOP_ARRAY ioctl
     inactive
         May have some settings, but array is not active
            all IO results in error
         When written, doesn't tear down array, but just stops it
     suspended (not supported yet)
         All IO requests will block. The array can be reconfigured.
         Writing this, if accepted, will block until array is quiessent
     readonly
         no resync can happen.  no superblocks get written.
         write requests fail
     read-auto
         like readonly, but behaves like 'clean' on a write request.

     clean - no pending writes, but otherwise active.
         When written to inactive array, starts without resync
         If a write request arrives then
           if metadata is known, mark 'dirty' and switch to 'active'.
           if not known, block and switch to write-pending
         If written to an active array that has pending writes, then fails.
     active
         fully active: IO and resync can be happening.
         When written to inactive array, starts with resync

     write-pending
         clean, but writes are blocked waiting for 'active' to be written.

     active-idle
         like active, but no writes have been seen for a while (safe_mode_delay).

  bitmap/location
     This indicates where the write-intent bitmap for the array is
     stored.
     It can be one of "none", "file" or "[+-]N".
     "file" may later be extended to "file:/file/name"
     "[+-]N" means that many sectors from the start of the metadata.
       This is replicated on all devices.  For arrays with externally
       managed metadata, the offset is from the beginning of the
       device.
  bitmap/chunksize
     The size, in bytes, of the chunk which will be represented by a
     single bit.  For RAID456, it is a portion of an individual
     device. For RAID10, it is a portion of the array.  For RAID1, it
     is both (they come to the same thing).
  bitmap/time_base
     The time, in seconds, between looking for bits in the bitmap to
     be cleared. In the current implementation, a bit will be cleared
     between 2 and 3 times "time_base" after all the covered blocks
     are known to be in-sync.
  bitmap/backlog
     When write-mostly devices are active in a RAID1, write requests
     to those devices proceed in the background - the filesystem (or
     other user of the device) does not have to wait for them.
     'backlog' sets a limit on the number of concurrent background
     writes.  If there are more than this, new writes will by
     synchronous.
  bitmap/metadata
     This can be either 'internal' or 'external'.
     'internal' is the default and means the metadata for the bitmap
     is stored in the first 256 bytes of the allocated space and is
     managed by the md module.
     'external' means that bitmap metadata is managed externally to
     the kernel (i.e. by some userspace program)
  bitmap/can_clear
     This is either 'true' or 'false'.  If 'true', then bits in the
     bitmap will be cleared when the corresponding blocks are thought
     to be in-sync.  If 'false', bits will never be cleared.
     This is automatically set to 'false' if a write happens on a
     degraded array, or if the array becomes degraded during a write.
     When metadata is managed externally, it should be set to true
     once the array becomes non-degraded, and this fact has been
     recorded in the metadata.
     
     
     

As component devices are added to an md array, they appear in the 'md'
directory as new directories named
      dev-XXX
where XXX is a name that the kernel knows for the device, e.g. hdb1.
Each directory contains:

      block
        a symlink to the block device in /sys/block, e.g.
	     /sys/block/md0/md/dev-hdb1/block -> ../../../../block/hdb/hdb1

      super
        A file containing an image of the superblock read from, or
        written to, that device.

      state
	A file recording the current state of the device in the array
	which can be a comma separated list of
	      faulty   - device has been kicked from active use due to
			 a detected fault, or it has unacknowledged bad
			 blocks
	      in_sync  - device is a fully in-sync member of the array
	      writemostly - device will only be subject to read
			 requests if there are no other options.
			 This applies only to raid1 arrays.
	      blocked  - device has failed, and the failure hasn't been
			 acknowledged yet by the metadata handler.
			 Writes that would write to this device if
			 it were not faulty are blocked.
	      spare    - device is working, but not a full member.
			 This includes spares that are in the process
			 of being recovered to
	      write_error - device has ever seen a write error.
	      want_replacement - device is (mostly) working but probably
			 should be replaced, either due to errors or
			 due to user request.
	      replacement - device is a replacement for another active
			 device with same raid_disk.


	This list may grow in future.
	This can be written to.
	Writing "faulty"  simulates a failure on the device.
	Writing "remove" removes the device from the array.
	Writing "writemostly" sets the writemostly flag.
	Writing "-writemostly" clears the writemostly flag.
	Writing "blocked" sets the "blocked" flag.
	Writing "-blocked" clears the "blocked" flags and allows writes
		to complete and possibly simulates an error.
	Writing "in_sync" sets the in_sync flag.
	Writing "write_error" sets writeerrorseen flag.
	Writing "-write_error" clears writeerrorseen flag.
	Writing "want_replacement" is allowed at any time except to a
		replacement device or a spare.  It sets the flag.
	Writing "-want_replacement" is allowed at any time.  It clears
		the flag.
	Writing "replacement" or "-replacement" is only allowed before
		starting the array.  It sets or clears the flag.


	This file responds to select/poll. Any change to 'faulty'
	or 'blocked' causes an event.

      errors
	An approximate count of read errors that have been detected on
	this device but have not caused the device to be evicted from
	the array (either because they were corrected or because they
	happened while the array was read-only).  When using version-1
	metadata, this value persists across restarts of the array.

	This value can be written while assembling an array thus
	providing an ongoing count for arrays with metadata managed by
	userspace.

      slot
        This gives the role that the device has in the array.  It will
	either be 'none' if the device is not active in the array
        (i.e. is a spare or has failed) or an integer less than the
	'raid_disks' number for the array indicating which position
	it currently fills.  This can only be set while assembling an
	array.  A device for which this is set is assumed to be working.

      offset
        This gives the location in the device (in sectors from the
        start) where data from the array will be stored.  Any part of
        the device before this offset is not touched, unless it is
        used for storing metadata (Formats 1.1 and 1.2).

      size
        The amount of the device, after the offset, that can be used
        for storage of data.  This will normally be the same as the
	component_size.  This can be written while assembling an
        array.  If a value less than the current component_size is
        written, it will be rejected.

      recovery_start
        When the device is not 'in_sync', this records the number of
	sectors from the start of the device which are known to be
	correct.  This is normally zero, but during a recovery
	operation it will steadily increase, and if the recovery is
	interrupted, restoring this value can cause recovery to
	avoid repeating the earlier blocks.  With v1.x metadata, this
	value is saved and restored automatically.

	This can be set whenever the device is not an active member of
	the array, either before the array is activated, or before
	the 'slot' is set.

	Setting this to 'none' is equivalent to setting 'in_sync'.
	Setting to any other value also clears the 'in_sync' flag.
	
      bad_blocks
	This gives the list of all known bad blocks in the form of
	start address and length (in sectors respectively). If output
	is too big to fit in a page, it will be truncated. Writing
	"sector length" to this file adds new acknowledged (i.e.
	recorded to disk safely) bad blocks.

      unacknowledged_bad_blocks
	This gives the list of known-but-not-yet-saved-to-disk bad
	blocks in the same form of 'bad_blocks'. If output is too big
	to fit in a page, it will be truncated. Writing to this file
	adds bad blocks without acknowledging them. This is largely
	for testing.



An active md device will also contain an entry for each active device
in the array.  These are named

    rdNN

where 'NN' is the position in the array, starting from 0.
So for a 3 drive array there will be rd0, rd1, rd2.
These are symbolic links to the appropriate 'dev-XXX' entry.
Thus, for example,
       cat /sys/block/md*/md/rd*/state
will show 'in_sync' on every line.



Active md devices for levels that support data redundancy (1,4,5,6,10)
also have

   sync_action
     a text file that can be used to monitor and control the rebuild
     process.  It contains one word which can be one of:
       resync        - redundancy is being recalculated after unclean
                       shutdown or creation
       recover       - a hot spare is being built to replace a
                       failed/missing device
       idle          - nothing is happening
       check         - A full check of redundancy was requested and is
                       happening.  This reads all blocks and checks
                       them. A repair may also happen for some raid
                       levels.
       repair        - A full check and repair is happening.  This is
                       similar to 'resync', but was requested by the
                       user, and the write-intent bitmap is NOT used to
		       optimise the process.

      This file is writable, and each of the strings that could be
      read are meaningful for writing.

       'idle' will stop an active resync/recovery etc.  There is no
           guarantee that another resync/recovery may not be automatically
	   started again, though some event will be needed to trigger
           this.
	'resync' or 'recovery' can be used to restart the
           corresponding operation if it was stopped with 'idle'.
	'check' and 'repair' will start the appropriate process
           providing the current state is 'idle'.

      This file responds to select/poll.  Any important change in the value
      triggers a poll event.  Sometimes the value will briefly be
      "recover" if a recovery seems to be needed, but cannot be
      achieved. In that case, the transition to "recover" isn't
      notified, but the transition away is.

   degraded
      This contains a count of the number of devices by which the
      arrays is degraded.  So an optimal array will show '0'.  A
      single failed/missing drive will show '1', etc.
      This file responds to select/poll, any increase or decrease
      in the count of missing devices will trigger an event.

   mismatch_count
      When performing 'check' and 'repair', and possibly when
      performing 'resync', md will count the number of errors that are
      found.  The count in 'mismatch_cnt' is the number of sectors
      that were re-written, or (for 'check') would have been
      re-written.  As most raid levels work in units of pages rather
      than sectors, this may be larger than the number of actual errors
      by a factor of the number of sectors in a page.

   bitmap_set_bits
      If the array has a write-intent bitmap, then writing to this
      attribute can set bits in the bitmap, indicating that a resync
      would need to check the corresponding blocks. Either individual
      numbers or start-end pairs can be written.  Multiple numbers
      can be separated by a space.
      Note that the numbers are 'bit' numbers, not 'block' numbers.
      They should be scaled by the bitmap_chunksize.

   sync_speed_min
   sync_speed_max
     This are similar to /proc/sys/dev/raid/speed_limit_{min,max}
     however they only apply to the particular array.
     If no value has been written to these, of if the word 'system'
     is written, then the system-wide value is used.  If a value,
     in kibibytes-per-second is written, then it is used.
     When the files are read, they show the currently active value
     followed by "(local)" or "(system)" depending on whether it is
     a locally set or system-wide value.

   sync_completed
     This shows the number of sectors that have been completed of
     whatever the current sync_action is, followed by the number of
     sectors in total that could need to be processed.  The two
     numbers are separated by a '/'  thus effectively showing one
     value, a fraction of the process that is complete.
     A 'select' on this attribute will return when resync completes,
     when it reaches the current sync_max (below) and possibly at
     other times.

   sync_speed
     This shows the current actual speed, in K/sec, of the current
     sync_action.  It is averaged over the last 30 seconds.

   suspend_lo
   suspend_hi
     The two values, given as numbers of sectors, indicate a range
     within the array where IO will be blocked.  This is currently
     only supported for raid4/5/6.

   sync_min
   sync_max
     The two values, given as numbers of sectors, indicate a range
     within the array where 'check'/'repair' will operate. Must be
     a multiple of chunk_size. When it reaches "sync_max" it will
     pause, rather than complete.
     You can use 'select' or 'poll' on "sync_completed" to wait for
     that number to reach sync_max.  Then you can either increase
     "sync_max", or can write 'idle' to "sync_action".

     The value of 'max' for "sync_max" effectively disables the limit.
     When a resync is active, the value can only ever be increased,
     never decreased.
     The value of '0' is the minimum for "sync_min".



Each active md device may also have attributes specific to the
personality module that manages it.
These are specific to the implementation of the module and could
change substantially if the implementation changes.

These currently include

  stripe_cache_size  (currently raid5 only)
      number of entries in the stripe cache.  This is writable, but
      there are upper and lower limits (32768, 16).  Default is 128.
  strip_cache_active (currently raid5 only)
      number of active entries in the stripe cache
  preread_bypass_threshold (currently raid5 only)
      number of times a stripe requiring preread will be bypassed by
      a stripe that does not require preread.  For fairness defaults
      to 1.  Setting this to 0 disables bypass accounting and
      requires preread stripes to wait until all full-width stripe-
      writes are complete.  Valid values are 0 to stripe_cache_size.
Linux kernel media framework
============================

This document describes the Linux kernel media framework, its data structures,
functions and their usage.


Introduction
------------

The media controller API is documented in DocBook format in
Documentation/DocBook/media/v4l/media-controller.xml. This document will focus
on the kernel-side implementation of the media framework.


Abstract media device model
---------------------------

Discovering a device internal topology, and configuring it at runtime, is one
of the goals of the media framework. To achieve this, hardware devices are
modelled as an oriented graph of building blocks called entities connected
through pads.

An entity is a basic media hardware building block. It can correspond to
a large variety of logical blocks such as physical hardware devices
(CMOS sensor for instance), logical hardware devices (a building block
in a System-on-Chip image processing pipeline), DMA channels or physical
connectors.

A pad is a connection endpoint through which an entity can interact with
other entities. Data (not restricted to video) produced by an entity
flows from the entity's output to one or more entity inputs. Pads should
not be confused with physical pins at chip boundaries.

A link is a point-to-point oriented connection between two pads, either
on the same entity or on different entities. Data flows from a source
pad to a sink pad.


Media device
------------

A media device is represented by a struct media_device instance, defined in
include/media/media-device.h. Allocation of the structure is handled by the
media device driver, usually by embedding the media_device instance in a
larger driver-specific structure.

Drivers register media device instances by calling

	media_device_register(struct media_device *mdev);

The caller is responsible for initializing the media_device structure before
registration. The following fields must be set:

 - dev must point to the parent device (usually a pci_dev, usb_interface or
   platform_device instance).

 - model must be filled with the device model name as a NUL-terminated UTF-8
   string. The device/model revision must not be stored in this field.

The following fields are optional:

 - serial is a unique serial number stored as a NUL-terminated ASCII string.
   The field is big enough to store a GUID in text form. If the hardware
   doesn't provide a unique serial number this field must be left empty.

 - bus_info represents the location of the device in the system as a
   NUL-terminated ASCII string. For PCI/PCIe devices bus_info must be set to
   "PCI:" (or "PCIe:") followed by the value of pci_name(). For USB devices,
   the usb_make_path() function must be used. This field is used by
   applications to distinguish between otherwise identical devices that don't
   provide a serial number.

 - hw_revision is the hardware device revision in a driver-specific format.
   When possible the revision should be formatted with the KERNEL_VERSION
   macro.

 - driver_version is formatted with the KERNEL_VERSION macro. The version
   minor must be incremented when new features are added to the userspace API
   without breaking binary compatibility. The version major must be
   incremented when binary compatibility is broken.

Upon successful registration a character device named media[0-9]+ is created.
The device major and minor numbers are dynamic. The model name is exported as
a sysfs attribute.

Drivers unregister media device instances by calling

	media_device_unregister(struct media_device *mdev);

Unregistering a media device that hasn't been registered is *NOT* safe.


Entities, pads and links
------------------------

- Entities

Entities are represented by a struct media_entity instance, defined in
include/media/media-entity.h. The structure is usually embedded into a
higher-level structure, such as a v4l2_subdev or video_device instance,
although drivers can allocate entities directly.

Drivers initialize entities by calling

	media_entity_init(struct media_entity *entity, u16 num_pads,
			  struct media_pad *pads, u16 extra_links);

The media_entity name, type, flags, revision and group_id fields can be
initialized before or after calling media_entity_init. Entities embedded in
higher-level standard structures can have some of those fields set by the
higher-level framework.

As the number of pads is known in advance, the pads array is not allocated
dynamically but is managed by the entity driver. Most drivers will embed the
pads array in a driver-specific structure, avoiding dynamic allocation.

Drivers must set the direction of every pad in the pads array before calling
media_entity_init. The function will initialize the other pads fields.

Unlike the number of pads, the total number of links isn't always known in
advance by the entity driver. As an initial estimate, media_entity_init
pre-allocates a number of links equal to the number of pads plus an optional
number of extra links. The links array will be reallocated if it grows beyond
the initial estimate.

Drivers register entities with a media device by calling

	media_device_register_entity(struct media_device *mdev,
				     struct media_entity *entity);

Entities are identified by a unique positive integer ID. Drivers can provide an
ID by filling the media_entity id field prior to registration, or request the
media controller framework to assign an ID automatically. Drivers that provide
IDs manually must ensure that all IDs are unique. IDs are not guaranteed to be
contiguous even when they are all assigned automatically by the framework.

Drivers unregister entities by calling

	media_device_unregister_entity(struct media_entity *entity);

Unregistering an entity will not change the IDs of the other entities, and the
ID will never be reused for a newly registered entity.

When a media device is unregistered, all its entities are unregistered
automatically. No manual entities unregistration is then required.

Drivers free resources associated with an entity by calling

	media_entity_cleanup(struct media_entity *entity);

This function must be called during the cleanup phase after unregistering the
entity. Note that the media_entity instance itself must be freed explicitly by
the driver if required.

Entities have flags that describe the entity capabilities and state.

	MEDIA_ENT_FL_DEFAULT indicates the default entity for a given type.
	This can be used to report the default audio and video devices or the
	default camera sensor.

Logical entity groups can be defined by setting the group ID of all member
entities to the same non-zero value. An entity group serves no purpose in the
kernel, but is reported to userspace during entities enumeration. The group_id
field belongs to the media device driver and must not by touched by entity
drivers.

Media device drivers should define groups if several entities are logically
bound together. Example usages include reporting

	- ALSA, VBI and video nodes that carry the same media stream
	- lens and flash controllers associated with a sensor

- Pads

Pads are represented by a struct media_pad instance, defined in
include/media/media-entity.h. Each entity stores its pads in a pads array
managed by the entity driver. Drivers usually embed the array in a
driver-specific structure.

Pads are identified by their entity and their 0-based index in the pads array.
Both information are stored in the media_pad structure, making the media_pad
pointer the canonical way to store and pass link references.

Pads have flags that describe the pad capabilities and state.

	MEDIA_PAD_FL_SINK indicates that the pad supports sinking data.
	MEDIA_PAD_FL_SOURCE indicates that the pad supports sourcing data.

One and only one of MEDIA_PAD_FL_SINK and MEDIA_PAD_FL_SOURCE must be set for
each pad.

- Links

Links are represented by a struct media_link instance, defined in
include/media/media-entity.h. Each entity stores all links originating at or
targeting any of its pads in a links array. A given link is thus stored
twice, once in the source entity and once in the target entity. The array is
pre-allocated and grows dynamically as needed.

Drivers create links by calling

	media_entity_create_link(struct media_entity *source, u16 source_pad,
				 struct media_entity *sink,   u16 sink_pad,
				 u32 flags);

An entry in the link array of each entity is allocated and stores pointers
to source and sink pads.

Links have flags that describe the link capabilities and state.

	MEDIA_LNK_FL_ENABLED indicates that the link is enabled and can be used
	to transfer media data. When two or more links target a sink pad, only
	one of them can be enabled at a time.
	MEDIA_LNK_FL_IMMUTABLE indicates that the link enabled state can't be
	modified at runtime. If MEDIA_LNK_FL_IMMUTABLE is set, then
	MEDIA_LNK_FL_ENABLED must also be set since an immutable link is always
	enabled.


Graph traversal
---------------

The media framework provides APIs to iterate over entities in a graph.

To iterate over all entities belonging to a media device, drivers can use the
media_device_for_each_entity macro, defined in include/media/media-device.h.

	struct media_entity *entity;

	media_device_for_each_entity(entity, mdev) {
		/* entity will point to each entity in turn */
		...
	}

Drivers might also need to iterate over all entities in a graph that can be
reached only through enabled links starting at a given entity. The media
framework provides a depth-first graph traversal API for that purpose.

Note that graphs with cycles (whether directed or undirected) are *NOT*
supported by the graph traversal API. To prevent infinite loops, the graph
traversal code limits the maximum depth to MEDIA_ENTITY_ENUM_MAX_DEPTH,
currently defined as 16.

Drivers initiate a graph traversal by calling

	media_entity_graph_walk_start(struct media_entity_graph *graph,
				      struct media_entity *entity);

The graph structure, provided by the caller, is initialized to start graph
traversal at the given entity.

Drivers can then retrieve the next entity by calling

	media_entity_graph_walk_next(struct media_entity_graph *graph);

When the graph traversal is complete the function will return NULL.

Graph traversal can be interrupted at any moment. No cleanup function call is
required and the graph structure can be freed normally.

Helper functions can be used to find a link between two given pads, or a pad
connected to another pad through an enabled link

	media_entity_find_link(struct media_pad *source,
			       struct media_pad *sink);

	media_entity_remote_pad(struct media_pad *pad);

Refer to the kerneldoc documentation for more information.


Use count and power handling
----------------------------

Due to the wide differences between drivers regarding power management needs,
the media controller does not implement power management. However, the
media_entity structure includes a use_count field that media drivers can use to
track the number of users of every entity for power management needs.

The use_count field is owned by media drivers and must not be touched by entity
drivers. Access to the field must be protected by the media device graph_mutex
lock.


Links setup
-----------

Link properties can be modified at runtime by calling

	media_entity_setup_link(struct media_link *link, u32 flags);

The flags argument contains the requested new link flags.

The only configurable property is the ENABLED link flag to enable/disable a
link. Links marked with the IMMUTABLE link flag can not be enabled or disabled.

When a link is enabled or disabled, the media framework calls the
link_setup operation for the two entities at the source and sink of the link,
in that order. If the second link_setup call fails, another link_setup call is
made on the first entity to restore the original link flags.

Media device drivers can be notified of link setup operations by setting the
media_device::link_notify pointer to a callback function. If provided, the
notification callback will be called before enabling and after disabling
links.

Entity drivers must implement the link_setup operation if any of their links
is non-immutable. The operation must either configure the hardware or store
the configuration information to be applied later.

Link configuration must not have any side effect on other links. If an enabled
link at a sink pad prevents another link at the same pad from being enabled,
the link_setup operation must return -EBUSY and can't implicitly disable the
first enabled link.


Pipelines and media streams
---------------------------

When starting streaming, drivers must notify all entities in the pipeline to
prevent link states from being modified during streaming by calling

	media_entity_pipeline_start(struct media_entity *entity,
				    struct media_pipeline *pipe);

The function will mark all entities connected to the given entity through
enabled links, either directly or indirectly, as streaming.

The media_pipeline instance pointed to by the pipe argument will be stored in
every entity in the pipeline. Drivers should embed the media_pipeline structure
in higher-level pipeline structures and can then access the pipeline through
the media_entity pipe field.

Calls to media_entity_pipeline_start() can be nested. The pipeline pointer must
be identical for all nested calls to the function.

media_entity_pipeline_start() may return an error. In that case, it will
clean up any of the changes it did by itself.

When stopping the stream, drivers must notify the entities with

	media_entity_pipeline_stop(struct media_entity *entity);

If multiple calls to media_entity_pipeline_start() have been made the same
number of media_entity_pipeline_stop() calls are required to stop streaming. The
media_entity pipe field is reset to NULL on the last nested stop call.

Link configuration will fail with -EBUSY by default if either end of the link is
a streaming entity. Links that can be modified while streaming must be marked
with the MEDIA_LNK_FL_DYNAMIC flag.

If other operations need to be disallowed on streaming entities (such as
changing entities configuration parameters) drivers can explicitly check the
media_entity stream_count field to find out if an entity is streaming. This
operation must be done with the media_device graph_mutex held.


Link validation
---------------

Link validation is performed by media_entity_pipeline_start() for any
entity which has sink pads in the pipeline. The
media_entity::link_validate() callback is used for that purpose. In
link_validate() callback, entity driver should check that the properties of
the source pad of the connected entity and its own sink pad match. It is up
to the type of the entity (and in the end, the properties of the hardware)
what matching actually means.

Subsystems should facilitate link validation by providing subsystem specific
helper functions to provide easy access for commonly needed information, and
in the end provide a way to use driver-specific callbacks.
			 ============================
			 LINUX KERNEL MEMORY BARRIERS
			 ============================

By: David Howells <dhowells@redhat.com>
    Paul E. McKenney <paulmck@linux.vnet.ibm.com>

Contents:

 (*) Abstract memory access model.

     - Device operations.
     - Guarantees.

 (*) What are memory barriers?

     - Varieties of memory barrier.
     - What may not be assumed about memory barriers?
     - Data dependency barriers.
     - Control dependencies.
     - SMP barrier pairing.
     - Examples of memory barrier sequences.
     - Read memory barriers vs load speculation.
     - Transitivity

 (*) Explicit kernel barriers.

     - Compiler barrier.
     - CPU memory barriers.
     - MMIO write barrier.

 (*) Implicit kernel memory barriers.

     - Locking functions.
     - Interrupt disabling functions.
     - Sleep and wake-up functions.
     - Miscellaneous functions.

 (*) Inter-CPU locking barrier effects.

     - Locks vs memory accesses.
     - Locks vs I/O accesses.

 (*) Where are memory barriers needed?

     - Interprocessor interaction.
     - Atomic operations.
     - Accessing devices.
     - Interrupts.

 (*) Kernel I/O barrier effects.

 (*) Assumed minimum execution ordering model.

 (*) The effects of the cpu cache.

     - Cache coherency.
     - Cache coherency vs DMA.
     - Cache coherency vs MMIO.

 (*) The things CPUs get up to.

     - And then there's the Alpha.

 (*) Example uses.

     - Circular buffers.

 (*) References.


============================
ABSTRACT MEMORY ACCESS MODEL
============================

Consider the following abstract model of the system:

		            :                :
		            :                :
		            :                :
		+-------+   :   +--------+   :   +-------+
		|       |   :   |        |   :   |       |
		|       |   :   |        |   :   |       |
		| CPU 1 |<----->| Memory |<----->| CPU 2 |
		|       |   :   |        |   :   |       |
		|       |   :   |        |   :   |       |
		+-------+   :   +--------+   :   +-------+
		    ^       :       ^        :       ^
		    |       :       |        :       |
		    |       :       |        :       |
		    |       :       v        :       |
		    |       :   +--------+   :       |
		    |       :   |        |   :       |
		    |       :   |        |   :       |
		    +---------->| Device |<----------+
		            :   |        |   :
		            :   |        |   :
		            :   +--------+   :
		            :                :

Each CPU executes a program that generates memory access operations.  In the
abstract CPU, memory operation ordering is very relaxed, and a CPU may actually
perform the memory operations in any order it likes, provided program causality
appears to be maintained.  Similarly, the compiler may also arrange the
instructions it emits in any order it likes, provided it doesn't affect the
apparent operation of the program.

So in the above diagram, the effects of the memory operations performed by a
CPU are perceived by the rest of the system as the operations cross the
interface between the CPU and rest of the system (the dotted lines).


For example, consider the following sequence of events:

	CPU 1		CPU 2
	===============	===============
	{ A == 1; B == 2 }
	A = 3;		x = B;
	B = 4;		y = A;

The set of accesses as seen by the memory system in the middle can be arranged
in 24 different combinations:

	STORE A=3,	STORE B=4,	x=LOAD A->3,	y=LOAD B->4
	STORE A=3,	STORE B=4,	y=LOAD B->4,	x=LOAD A->3
	STORE A=3,	x=LOAD A->3,	STORE B=4,	y=LOAD B->4
	STORE A=3,	x=LOAD A->3,	y=LOAD B->2,	STORE B=4
	STORE A=3,	y=LOAD B->2,	STORE B=4,	x=LOAD A->3
	STORE A=3,	y=LOAD B->2,	x=LOAD A->3,	STORE B=4
	STORE B=4,	STORE A=3,	x=LOAD A->3,	y=LOAD B->4
	STORE B=4, ...
	...

and can thus result in four different combinations of values:

	x == 1, y == 2
	x == 1, y == 4
	x == 3, y == 2
	x == 3, y == 4


Furthermore, the stores committed by a CPU to the memory system may not be
perceived by the loads made by another CPU in the same order as the stores were
committed.


As a further example, consider this sequence of events:

	CPU 1		CPU 2
	===============	===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;		Q = P;
	P = &B		D = *Q;

There is an obvious data dependency here, as the value loaded into D depends on
the address retrieved from P by CPU 2.  At the end of the sequence, any of the
following results are possible:

	(Q == &A) and (D == 1)
	(Q == &B) and (D == 2)
	(Q == &B) and (D == 4)

Note that CPU 2 will never try and load C into D because the CPU will load P
into Q before issuing the load of *Q.


DEVICE OPERATIONS
-----------------

Some devices present their control interfaces as collections of memory
locations, but the order in which the control registers are accessed is very
important.  For instance, imagine an ethernet card with a set of internal
registers that are accessed through an address port register (A) and a data
port register (D).  To read internal register 5, the following code might then
be used:

	*A = 5;
	x = *D;

but this might show up as either of the following two sequences:

	STORE *A = 5, x = LOAD *D
	x = LOAD *D, STORE *A = 5

the second of which will almost certainly result in a malfunction, since it set
the address _after_ attempting to read the register.


GUARANTEES
----------

There are some minimal guarantees that may be expected of a CPU:

 (*) On any given CPU, dependent memory accesses will be issued in order, with
     respect to itself.  This means that for:

	ACCESS_ONCE(Q) = P; smp_read_barrier_depends(); D = ACCESS_ONCE(*Q);

     the CPU will issue the following memory operations:

	Q = LOAD P, D = LOAD *Q

     and always in that order.  On most systems, smp_read_barrier_depends()
     does nothing, but it is required for DEC Alpha.  The ACCESS_ONCE()
     is required to prevent compiler mischief.  Please note that you
     should normally use something like rcu_dereference() instead of
     open-coding smp_read_barrier_depends().

 (*) Overlapping loads and stores within a particular CPU will appear to be
     ordered within that CPU.  This means that for:

	a = ACCESS_ONCE(*X); ACCESS_ONCE(*X) = b;

     the CPU will only issue the following sequence of memory operations:

	a = LOAD *X, STORE *X = b

     And for:

	ACCESS_ONCE(*X) = c; d = ACCESS_ONCE(*X);

     the CPU will only issue:

	STORE *X = c, d = LOAD *X

     (Loads and stores overlap if they are targeted at overlapping pieces of
     memory).

And there are a number of things that _must_ or _must_not_ be assumed:

 (*) It _must_not_ be assumed that the compiler will do what you want with
     memory references that are not protected by ACCESS_ONCE().  Without
     ACCESS_ONCE(), the compiler is within its rights to do all sorts
     of "creative" transformations, which are covered in the Compiler
     Barrier section.

 (*) It _must_not_ be assumed that independent loads and stores will be issued
     in the order given.  This means that for:

	X = *A; Y = *B; *D = Z;

     we may get any of the following sequences:

	X = LOAD *A,  Y = LOAD *B,  STORE *D = Z
	X = LOAD *A,  STORE *D = Z, Y = LOAD *B
	Y = LOAD *B,  X = LOAD *A,  STORE *D = Z
	Y = LOAD *B,  STORE *D = Z, X = LOAD *A
	STORE *D = Z, X = LOAD *A,  Y = LOAD *B
	STORE *D = Z, Y = LOAD *B,  X = LOAD *A

 (*) It _must_ be assumed that overlapping memory accesses may be merged or
     discarded.  This means that for:

	X = *A; Y = *(A + 4);

     we may get any one of the following sequences:

	X = LOAD *A; Y = LOAD *(A + 4);
	Y = LOAD *(A + 4); X = LOAD *A;
	{X, Y} = LOAD {*A, *(A + 4) };

     And for:

	*A = X; *(A + 4) = Y;

     we may get any of:

	STORE *A = X; STORE *(A + 4) = Y;
	STORE *(A + 4) = Y; STORE *A = X;
	STORE {*A, *(A + 4) } = {X, Y};


=========================
WHAT ARE MEMORY BARRIERS?
=========================

As can be seen above, independent memory operations are effectively performed
in random order, but this can be a problem for CPU-CPU interaction and for I/O.
What is required is some way of intervening to instruct the compiler and the
CPU to restrict the order.

Memory barriers are such interventions.  They impose a perceived partial
ordering over the memory operations on either side of the barrier.

Such enforcement is important because the CPUs and other devices in a system
can use a variety of tricks to improve performance, including reordering,
deferral and combination of memory operations; speculative loads; speculative
branch prediction and various types of caching.  Memory barriers are used to
override or suppress these tricks, allowing the code to sanely control the
interaction of multiple CPUs and/or devices.


VARIETIES OF MEMORY BARRIER
---------------------------

Memory barriers come in four basic varieties:

 (1) Write (or store) memory barriers.

     A write memory barrier gives a guarantee that all the STORE operations
     specified before the barrier will appear to happen before all the STORE
     operations specified after the barrier with respect to the other
     components of the system.

     A write barrier is a partial ordering on stores only; it is not required
     to have any effect on loads.

     A CPU can be viewed as committing a sequence of store operations to the
     memory system as time progresses.  All stores before a write barrier will
     occur in the sequence _before_ all the stores after the write barrier.

     [!] Note that write barriers should normally be paired with read or data
     dependency barriers; see the "SMP barrier pairing" subsection.


 (2) Data dependency barriers.

     A data dependency barrier is a weaker form of read barrier.  In the case
     where two loads are performed such that the second depends on the result
     of the first (eg: the first load retrieves the address to which the second
     load will be directed), a data dependency barrier would be required to
     make sure that the target of the second load is updated before the address
     obtained by the first load is accessed.

     A data dependency barrier is a partial ordering on interdependent loads
     only; it is not required to have any effect on stores, independent loads
     or overlapping loads.

     As mentioned in (1), the other CPUs in the system can be viewed as
     committing sequences of stores to the memory system that the CPU being
     considered can then perceive.  A data dependency barrier issued by the CPU
     under consideration guarantees that for any load preceding it, if that
     load touches one of a sequence of stores from another CPU, then by the
     time the barrier completes, the effects of all the stores prior to that
     touched by the load will be perceptible to any loads issued after the data
     dependency barrier.

     See the "Examples of memory barrier sequences" subsection for diagrams
     showing the ordering constraints.

     [!] Note that the first load really has to have a _data_ dependency and
     not a control dependency.  If the address for the second load is dependent
     on the first load, but the dependency is through a conditional rather than
     actually loading the address itself, then it's a _control_ dependency and
     a full read barrier or better is required.  See the "Control dependencies"
     subsection for more information.

     [!] Note that data dependency barriers should normally be paired with
     write barriers; see the "SMP barrier pairing" subsection.


 (3) Read (or load) memory barriers.

     A read barrier is a data dependency barrier plus a guarantee that all the
     LOAD operations specified before the barrier will appear to happen before
     all the LOAD operations specified after the barrier with respect to the
     other components of the system.

     A read barrier is a partial ordering on loads only; it is not required to
     have any effect on stores.

     Read memory barriers imply data dependency barriers, and so can substitute
     for them.

     [!] Note that read barriers should normally be paired with write barriers;
     see the "SMP barrier pairing" subsection.


 (4) General memory barriers.

     A general memory barrier gives a guarantee that all the LOAD and STORE
     operations specified before the barrier will appear to happen before all
     the LOAD and STORE operations specified after the barrier with respect to
     the other components of the system.

     A general memory barrier is a partial ordering over both loads and stores.

     General memory barriers imply both read and write memory barriers, and so
     can substitute for either.


And a couple of implicit varieties:

 (5) ACQUIRE operations.

     This acts as a one-way permeable barrier.  It guarantees that all memory
     operations after the ACQUIRE operation will appear to happen after the
     ACQUIRE operation with respect to the other components of the system.
     ACQUIRE operations include LOCK operations and smp_load_acquire()
     operations.

     Memory operations that occur before an ACQUIRE operation may appear to
     happen after it completes.

     An ACQUIRE operation should almost always be paired with a RELEASE
     operation.


 (6) RELEASE operations.

     This also acts as a one-way permeable barrier.  It guarantees that all
     memory operations before the RELEASE operation will appear to happen
     before the RELEASE operation with respect to the other components of the
     system. RELEASE operations include UNLOCK operations and
     smp_store_release() operations.

     Memory operations that occur after a RELEASE operation may appear to
     happen before it completes.

     The use of ACQUIRE and RELEASE operations generally precludes the need
     for other sorts of memory barrier (but note the exceptions mentioned in
     the subsection "MMIO write barrier").  In addition, a RELEASE+ACQUIRE
     pair is -not- guaranteed to act as a full memory barrier.  However, after
     an ACQUIRE on a given variable, all memory accesses preceding any prior
     RELEASE on that same variable are guaranteed to be visible.  In other
     words, within a given variable's critical section, all accesses of all
     previous critical sections for that variable are guaranteed to have
     completed.

     This means that ACQUIRE acts as a minimal "acquire" operation and
     RELEASE acts as a minimal "release" operation.


Memory barriers are only required where there's a possibility of interaction
between two CPUs or between a CPU and a device.  If it can be guaranteed that
there won't be any such interaction in any particular piece of code, then
memory barriers are unnecessary in that piece of code.


Note that these are the _minimum_ guarantees.  Different architectures may give
more substantial guarantees, but they may _not_ be relied upon outside of arch
specific code.


WHAT MAY NOT BE ASSUMED ABOUT MEMORY BARRIERS?
----------------------------------------------

There are certain things that the Linux kernel memory barriers do not guarantee:

 (*) There is no guarantee that any of the memory accesses specified before a
     memory barrier will be _complete_ by the completion of a memory barrier
     instruction; the barrier can be considered to draw a line in that CPU's
     access queue that accesses of the appropriate type may not cross.

 (*) There is no guarantee that issuing a memory barrier on one CPU will have
     any direct effect on another CPU or any other hardware in the system.  The
     indirect effect will be the order in which the second CPU sees the effects
     of the first CPU's accesses occur, but see the next point:

 (*) There is no guarantee that a CPU will see the correct order of effects
     from a second CPU's accesses, even _if_ the second CPU uses a memory
     barrier, unless the first CPU _also_ uses a matching memory barrier (see
     the subsection on "SMP Barrier Pairing").

 (*) There is no guarantee that some intervening piece of off-the-CPU
     hardware[*] will not reorder the memory accesses.  CPU cache coherency
     mechanisms should propagate the indirect effects of a memory barrier
     between CPUs, but might not do so in order.

	[*] For information on bus mastering DMA and coherency please read:

	    Documentation/PCI/pci.txt
	    Documentation/DMA-API-HOWTO.txt
	    Documentation/DMA-API.txt


DATA DEPENDENCY BARRIERS
------------------------

The usage requirements of data dependency barriers are a little subtle, and
it's not always obvious that they're needed.  To illustrate, consider the
following sequence of events:

	CPU 1		      CPU 2
	===============	      ===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;
	<write barrier>
	ACCESS_ONCE(P) = &B
			      Q = ACCESS_ONCE(P);
			      D = *Q;

There's a clear data dependency here, and it would seem that by the end of the
sequence, Q must be either &A or &B, and that:

	(Q == &A) implies (D == 1)
	(Q == &B) implies (D == 4)

But!  CPU 2's perception of P may be updated _before_ its perception of B, thus
leading to the following situation:

	(Q == &B) and (D == 2) ????

Whilst this may seem like a failure of coherency or causality maintenance, it
isn't, and this behaviour can be observed on certain real CPUs (such as the DEC
Alpha).

To deal with this, a data dependency barrier or better must be inserted
between the address load and the data load:

	CPU 1		      CPU 2
	===============	      ===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;
	<write barrier>
	ACCESS_ONCE(P) = &B
			      Q = ACCESS_ONCE(P);
			      <data dependency barrier>
			      D = *Q;

This enforces the occurrence of one of the two implications, and prevents the
third possibility from arising.

[!] Note that this extremely counterintuitive situation arises most easily on
machines with split caches, so that, for example, one cache bank processes
even-numbered cache lines and the other bank processes odd-numbered cache
lines.  The pointer P might be stored in an odd-numbered cache line, and the
variable B might be stored in an even-numbered cache line.  Then, if the
even-numbered bank of the reading CPU's cache is extremely busy while the
odd-numbered bank is idle, one can see the new value of the pointer P (&B),
but the old value of the variable B (2).


Another example of where data dependency barriers might be required is where a
number is read from memory and then used to calculate the index for an array
access:

	CPU 1		      CPU 2
	===============	      ===============
	{ M[0] == 1, M[1] == 2, M[3] = 3, P == 0, Q == 3 }
	M[1] = 4;
	<write barrier>
	ACCESS_ONCE(P) = 1
			      Q = ACCESS_ONCE(P);
			      <data dependency barrier>
			      D = M[Q];


The data dependency barrier is very important to the RCU system,
for example.  See rcu_assign_pointer() and rcu_dereference() in
include/linux/rcupdate.h.  This permits the current target of an RCU'd
pointer to be replaced with a new modified target, without the replacement
target appearing to be incompletely initialised.

See also the subsection on "Cache Coherency" for a more thorough example.


CONTROL DEPENDENCIES
--------------------

A control dependency requires a full read memory barrier, not simply a data
dependency barrier to make it work correctly.  Consider the following bit of
code:

	q = ACCESS_ONCE(a);
	if (q) {
		<data dependency barrier>  /* BUG: No data dependency!!! */
		p = ACCESS_ONCE(b);
	}

This will not have the desired effect because there is no actual data
dependency, but rather a control dependency that the CPU may short-circuit
by attempting to predict the outcome in advance, so that other CPUs see
the load from b as having happened before the load from a.  In such a
case what's actually required is:

	q = ACCESS_ONCE(a);
	if (q) {
		<read barrier>
		p = ACCESS_ONCE(b);
	}

However, stores are not speculated.  This means that ordering -is- provided
in the following example:

	q = ACCESS_ONCE(a);
	if (ACCESS_ONCE(q)) {
		ACCESS_ONCE(b) = p;
	}

Please note that ACCESS_ONCE() is not optional!  Without the ACCESS_ONCE(),
the compiler is within its rights to transform this example:

	q = a;
	if (q) {
		b = p;  /* BUG: Compiler can reorder!!! */
		do_something();
	} else {
		b = p;  /* BUG: Compiler can reorder!!! */
		do_something_else();
	}

into this, which of course defeats the ordering:

	b = p;
	q = a;
	if (q)
		do_something();
	else
		do_something_else();

Worse yet, if the compiler is able to prove (say) that the value of
variable 'a' is always non-zero, it would be well within its rights
to optimize the original example by eliminating the "if" statement
as follows:

	q = a;
	b = p;  /* BUG: Compiler can reorder!!! */
	do_something();

The solution is again ACCESS_ONCE() and barrier(), which preserves the
ordering between the load from variable 'a' and the store to variable 'b':

	q = ACCESS_ONCE(a);
	if (q) {
		barrier();
		ACCESS_ONCE(b) = p;
		do_something();
	} else {
		barrier();
		ACCESS_ONCE(b) = p;
		do_something_else();
	}

The initial ACCESS_ONCE() is required to prevent the compiler from
proving the value of 'a', and the pair of barrier() invocations are
required to prevent the compiler from pulling the two identical stores
to 'b' out from the legs of the "if" statement.

It is important to note that control dependencies absolutely require a
a conditional.  For example, the following "optimized" version of
the above example breaks ordering, which is why the barrier() invocations
are absolutely required if you have identical stores in both legs of
the "if" statement:

	q = ACCESS_ONCE(a);
	ACCESS_ONCE(b) = p;  /* BUG: No ordering vs. load from a!!! */
	if (q) {
		/* ACCESS_ONCE(b) = p; -- moved up, BUG!!! */
		do_something();
	} else {
		/* ACCESS_ONCE(b) = p; -- moved up, BUG!!! */
		do_something_else();
	}

It is of course legal for the prior load to be part of the conditional,
for example, as follows:

	if (ACCESS_ONCE(a) > 0) {
		barrier();
		ACCESS_ONCE(b) = q / 2;
		do_something();
	} else {
		barrier();
		ACCESS_ONCE(b) = q / 3;
		do_something_else();
	}

This will again ensure that the load from variable 'a' is ordered before the
stores to variable 'b'.

In addition, you need to be careful what you do with the local variable 'q',
otherwise the compiler might be able to guess the value and again remove
the needed conditional.  For example:

	q = ACCESS_ONCE(a);
	if (q % MAX) {
		barrier();
		ACCESS_ONCE(b) = p;
		do_something();
	} else {
		barrier();
		ACCESS_ONCE(b) = p;
		do_something_else();
	}

If MAX is defined to be 1, then the compiler knows that (q % MAX) is
equal to zero, in which case the compiler is within its rights to
transform the above code into the following:

	q = ACCESS_ONCE(a);
	ACCESS_ONCE(b) = p;
	do_something_else();

This transformation loses the ordering between the load from variable 'a'
and the store to variable 'b'.  If you are relying on this ordering, you
should do something like the following:

	q = ACCESS_ONCE(a);
	BUILD_BUG_ON(MAX <= 1); /* Order load from a with store to b. */
	if (q % MAX) {
		ACCESS_ONCE(b) = p;
		do_something();
	} else {
		ACCESS_ONCE(b) = p;
		do_something_else();
	}

Finally, control dependencies do -not- provide transitivity.  This is
demonstrated by two related examples:

	CPU 0                     CPU 1
	=====================     =====================
	r1 = ACCESS_ONCE(x);      r2 = ACCESS_ONCE(y);
	if (r1 >= 0)              if (r2 >= 0)
	  ACCESS_ONCE(y) = 1;       ACCESS_ONCE(x) = 1;

	assert(!(r1 == 1 && r2 == 1));

The above two-CPU example will never trigger the assert().  However,
if control dependencies guaranteed transitivity (which they do not),
then adding the following two CPUs would guarantee a related assertion:

	CPU 2                     CPU 3
	=====================     =====================
	ACCESS_ONCE(x) = 2;       ACCESS_ONCE(y) = 2;

	assert(!(r1 == 2 && r2 == 2 && x == 1 && y == 1)); /* FAILS!!! */

But because control dependencies do -not- provide transitivity, the
above assertion can fail after the combined four-CPU example completes.
If you need the four-CPU example to provide ordering, you will need
smp_mb() between the loads and stores in the CPU 0 and CPU 1 code fragments.

In summary:

  (*) Control dependencies can order prior loads against later stores.
      However, they do -not- guarantee any other sort of ordering:
      Not prior loads against later loads, nor prior stores against
      later anything.  If you need these other forms of ordering,
      use smb_rmb(), smp_wmb(), or, in the case of prior stores and
      later loads, smp_mb().

  (*) If both legs of the "if" statement begin with identical stores
      to the same variable, a barrier() statement is required at the
      beginning of each leg of the "if" statement.

  (*) Control dependencies require at least one run-time conditional
      between the prior load and the subsequent store, and this
      conditional must involve the prior load.  If the compiler
      is able to optimize the conditional away, it will have also
      optimized away the ordering.  Careful use of ACCESS_ONCE() can
      help to preserve the needed conditional.

  (*) Control dependencies require that the compiler avoid reordering the
      dependency into nonexistence.  Careful use of ACCESS_ONCE() or
      barrier() can help to preserve your control dependency.  Please
      see the Compiler Barrier section for more information.

  (*) Control dependencies do -not- provide transitivity.  If you
      need transitivity, use smp_mb().


SMP BARRIER PAIRING
-------------------

When dealing with CPU-CPU interactions, certain types of memory barrier should
always be paired.  A lack of appropriate pairing is almost certainly an error.

A write barrier should always be paired with a data dependency barrier or read
barrier, though a general barrier would also be viable.  Similarly a read
barrier or a data dependency barrier should always be paired with at least an
write barrier, though, again, a general barrier is viable:

	CPU 1		      CPU 2
	===============	      ===============
	ACCESS_ONCE(a) = 1;
	<write barrier>
	ACCESS_ONCE(b) = 2;   x = ACCESS_ONCE(b);
			      <read barrier>
			      y = ACCESS_ONCE(a);

Or:

	CPU 1		      CPU 2
	===============	      ===============================
	a = 1;
	<write barrier>
	ACCESS_ONCE(b) = &a;  x = ACCESS_ONCE(b);
			      <data dependency barrier>
			      y = *x;

Basically, the read barrier always has to be there, even though it can be of
the "weaker" type.

[!] Note that the stores before the write barrier would normally be expected to
match the loads after the read barrier or the data dependency barrier, and vice
versa:

	CPU 1                               CPU 2
	===================                 ===================
	ACCESS_ONCE(a) = 1;  }----   --->{  v = ACCESS_ONCE(c);
	ACCESS_ONCE(b) = 2;  }    \ /    {  w = ACCESS_ONCE(d);
	<write barrier>            \        <read barrier>
	ACCESS_ONCE(c) = 3;  }    / \    {  x = ACCESS_ONCE(a);
	ACCESS_ONCE(d) = 4;  }----   --->{  y = ACCESS_ONCE(b);


EXAMPLES OF MEMORY BARRIER SEQUENCES
------------------------------------

Firstly, write barriers act as partial orderings on store operations.
Consider the following sequence of events:

	CPU 1
	=======================
	STORE A = 1
	STORE B = 2
	STORE C = 3
	<write barrier>
	STORE D = 4
	STORE E = 5

This sequence of events is committed to the memory coherence system in an order
that the rest of the system might perceive as the unordered set of { STORE A,
STORE B, STORE C } all occurring before the unordered set of { STORE D, STORE E
}:

	+-------+       :      :
	|       |       +------+
	|       |------>| C=3  |     }     /\
	|       |  :    +------+     }-----  \  -----> Events perceptible to
	|       |  :    | A=1  |     }        \/       the rest of the system
	|       |  :    +------+     }
	| CPU 1 |  :    | B=2  |     }
	|       |       +------+     }
	|       |   wwwwwwwwwwwwwwww }   <--- At this point the write barrier
	|       |       +------+     }        requires all stores prior to the
	|       |  :    | E=5  |     }        barrier to be committed before
	|       |  :    +------+     }        further stores may take place
	|       |------>| D=4  |     }
	|       |       +------+
	+-------+       :      :
	                   |
	                   | Sequence in which stores are committed to the
	                   | memory system by CPU 1
	                   V


Secondly, data dependency barriers act as partial orderings on data-dependent
loads.  Consider the following sequence of events:

	CPU 1			CPU 2
	=======================	=======================
		{ B = 7; X = 9; Y = 8; C = &Y }
	STORE A = 1
	STORE B = 2
	<write barrier>
	STORE C = &B		LOAD X
	STORE D = 4		LOAD C (gets &B)
				LOAD *C (reads B)

Without intervention, CPU 2 may perceive the events on CPU 1 in some
effectively random order, despite the write barrier issued by CPU 1:

	+-------+       :      :                :       :
	|       |       +------+                +-------+  | Sequence of update
	|       |------>| B=2  |-----       --->| Y->8  |  | of perception on
	|       |  :    +------+     \          +-------+  | CPU 2
	| CPU 1 |  :    | A=1  |      \     --->| C->&Y |  V
	|       |       +------+       |        +-------+
	|       |   wwwwwwwwwwwwwwww   |        :       :
	|       |       +------+       |        :       :
	|       |  :    | C=&B |---    |        :       :       +-------+
	|       |  :    +------+   \   |        +-------+       |       |
	|       |------>| D=4  |    ----------->| C->&B |------>|       |
	|       |       +------+       |        +-------+       |       |
	+-------+       :      :       |        :       :       |       |
	                               |        :       :       |       |
	                               |        :       :       | CPU 2 |
	                               |        +-------+       |       |
	    Apparently incorrect --->  |        | B->7  |------>|       |
	    perception of B (!)        |        +-------+       |       |
	                               |        :       :       |       |
	                               |        +-------+       |       |
	    The load of X holds --->    \       | X->9  |------>|       |
	    up the maintenance           \      +-------+       |       |
	    of coherence of B             ----->| B->2  |       +-------+
	                                        +-------+
	                                        :       :


In the above example, CPU 2 perceives that B is 7, despite the load of *C
(which would be B) coming after the LOAD of C.

If, however, a data dependency barrier were to be placed between the load of C
and the load of *C (ie: B) on CPU 2:

	CPU 1			CPU 2
	=======================	=======================
		{ B = 7; X = 9; Y = 8; C = &Y }
	STORE A = 1
	STORE B = 2
	<write barrier>
	STORE C = &B		LOAD X
	STORE D = 4		LOAD C (gets &B)
				<data dependency barrier>
				LOAD *C (reads B)

then the following will occur:

	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| B=2  |-----       --->| Y->8  |
	|       |  :    +------+     \          +-------+
	| CPU 1 |  :    | A=1  |      \     --->| C->&Y |
	|       |       +------+       |        +-------+
	|       |   wwwwwwwwwwwwwwww   |        :       :
	|       |       +------+       |        :       :
	|       |  :    | C=&B |---    |        :       :       +-------+
	|       |  :    +------+   \   |        +-------+       |       |
	|       |------>| D=4  |    ----------->| C->&B |------>|       |
	|       |       +------+       |        +-------+       |       |
	+-------+       :      :       |        :       :       |       |
	                               |        :       :       |       |
	                               |        :       :       | CPU 2 |
	                               |        +-------+       |       |
	                               |        | X->9  |------>|       |
	                               |        +-------+       |       |
	  Makes sure all effects --->   \   ddddddddddddddddd   |       |
	  prior to the store of C        \      +-------+       |       |
	  are perceptible to              ----->| B->2  |------>|       |
	  subsequent loads                      +-------+       |       |
	                                        :       :       +-------+


And thirdly, a read barrier acts as a partial order on loads.  Consider the
following sequence of events:

	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				LOAD A

Without intervention, CPU 2 may then choose to perceive the events on CPU 1 in
some effectively random order, despite the write barrier issued by CPU 1:

	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       | A->0  |------>|       |
	                                |       +-------+       |       |
	                                |       :       :       +-------+
	                                 \      :       :
	                                  \     +-------+
	                                   ---->| A->1  |
	                                        +-------+
	                                        :       :


If, however, a read barrier were to be placed between the load of B and the
load of A on CPU 2:

	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				<read barrier>
				LOAD A

then the partial ordering imposed by CPU 1 will be perceived correctly by CPU
2:

	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                |       :       :       |       |
	  At this point the read ---->   \  rrrrrrrrrrrrrrrrr   |       |
	  barrier causes all effects      \     +-------+       |       |
	  prior to the storage of B        ---->| A->1  |------>|       |
	  to be perceptible to CPU 2            +-------+       |       |
	                                        :       :       +-------+


To illustrate this more completely, consider what could happen if the code
contained a load of A either side of the read barrier:

	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				LOAD A [first load of A]
				<read barrier>
				LOAD A [second load of A]

Even though the two loads of A both occur after the load of B, they may both
come up with different values:

	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                |       :       :       |       |
	                                |       +-------+       |       |
	                                |       | A->0  |------>| 1st   |
	                                |       +-------+       |       |
	  At this point the read ---->   \  rrrrrrrrrrrrrrrrr   |       |
	  barrier causes all effects      \     +-------+       |       |
	  prior to the storage of B        ---->| A->1  |------>| 2nd   |
	  to be perceptible to CPU 2            +-------+       |       |
	                                        :       :       +-------+


But it may be that the update to A from CPU 1 becomes perceptible to CPU 2
before the read barrier completes anyway:

	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                 \      :       :       |       |
	                                  \     +-------+       |       |
	                                   ---->| A->1  |------>| 1st   |
	                                        +-------+       |       |
	                                    rrrrrrrrrrrrrrrrr   |       |
	                                        +-------+       |       |
	                                        | A->1  |------>| 2nd   |
	                                        +-------+       |       |
	                                        :       :       +-------+


The guarantee is that the second load will always come up with A == 1 if the
load of B came up with B == 2.  No such guarantee exists for the first load of
A; that may come up with either A == 0 or A == 1.


READ MEMORY BARRIERS VS LOAD SPECULATION
----------------------------------------

Many CPUs speculate with loads: that is they see that they will need to load an
item from memory, and they find a time where they're not using the bus for any
other loads, and so do the load in advance - even though they haven't actually
got to that point in the instruction execution flow yet.  This permits the
actual load instruction to potentially complete immediately because the CPU
already has the value to hand.

It may turn out that the CPU didn't actually need the value - perhaps because a
branch circumvented the load - in which case it can discard the value or just
cache it for later use.

Consider:

	CPU 1			CPU 2
	=======================	=======================
				LOAD B
				DIVIDE		} Divide instructions generally
				DIVIDE		} take a long time to perform
				LOAD A

Which might appear as this:

	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	Once the divisions are complete -->     :       :   ~-->|       |
	the CPU can then perform the            :       :       |       |
	LOAD with immediate effect              :       :       +-------+


Placing a read barrier or a data dependency barrier just before the second
load:

	CPU 1			CPU 2
	=======================	=======================
				LOAD B
				DIVIDE
				DIVIDE
				<read barrier>
				LOAD A

will force any value speculatively obtained to be reconsidered to an extent
dependent on the type of barrier used.  If there was no change made to the
speculated memory location, then the speculated value will just be used:

	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	                                        :       :   ~   |       |
	                                    rrrrrrrrrrrrrrrr~   |       |
	                                        :       :   ~   |       |
	                                        :       :   ~-->|       |
	                                        :       :       |       |
	                                        :       :       +-------+


but if there was an update or an invalidation from another CPU pending, then
the speculation will be cancelled and the value reloaded:

	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	                                        :       :   ~   |       |
	                                    rrrrrrrrrrrrrrrrr   |       |
	                                        +-------+       |       |
	The speculation is discarded --->   --->| A->1  |------>|       |
	and an updated value is                 +-------+       |       |
	retrieved                               :       :       +-------+


TRANSITIVITY
------------

Transitivity is a deeply intuitive notion about ordering that is not
always provided by real computer systems.  The following example
demonstrates transitivity (also called "cumulativity"):

	CPU 1			CPU 2			CPU 3
	=======================	=======================	=======================
		{ X = 0, Y = 0 }
	STORE X=1		LOAD X			STORE Y=1
				<general barrier>	<general barrier>
				LOAD Y			LOAD X

Suppose that CPU 2's load from X returns 1 and its load from Y returns 0.
This indicates that CPU 2's load from X in some sense follows CPU 1's
store to X and that CPU 2's load from Y in some sense preceded CPU 3's
store to Y.  The question is then "Can CPU 3's load from X return 0?"

Because CPU 2's load from X in some sense came after CPU 1's store, it
is natural to expect that CPU 3's load from X must therefore return 1.
This expectation is an example of transitivity: if a load executing on
CPU A follows a load from the same variable executing on CPU B, then
CPU A's load must either return the same value that CPU B's load did,
or must return some later value.

In the Linux kernel, use of general memory barriers guarantees
transitivity.  Therefore, in the above example, if CPU 2's load from X
returns 1 and its load from Y returns 0, then CPU 3's load from X must
also return 1.

However, transitivity is -not- guaranteed for read or write barriers.
For example, suppose that CPU 2's general barrier in the above example
is changed to a read barrier as shown below:

	CPU 1			CPU 2			CPU 3
	=======================	=======================	=======================
		{ X = 0, Y = 0 }
	STORE X=1		LOAD X			STORE Y=1
				<read barrier>		<general barrier>
				LOAD Y			LOAD X

This substitution destroys transitivity: in this example, it is perfectly
legal for CPU 2's load from X to return 1, its load from Y to return 0,
and CPU 3's load from X to return 0.

The key point is that although CPU 2's read barrier orders its pair
of loads, it does not guarantee to order CPU 1's store.  Therefore, if
this example runs on a system where CPUs 1 and 2 share a store buffer
or a level of cache, CPU 2 might have early access to CPU 1's writes.
General barriers are therefore required to ensure that all CPUs agree
on the combined order of CPU 1's and CPU 2's accesses.

To reiterate, if your code requires transitivity, use general barriers
throughout.


========================
EXPLICIT KERNEL BARRIERS
========================

The Linux kernel has a variety of different barriers that act at different
levels:

  (*) Compiler barrier.

  (*) CPU memory barriers.

  (*) MMIO write barrier.


COMPILER BARRIER
----------------

The Linux kernel has an explicit compiler barrier function that prevents the
compiler from moving the memory accesses either side of it to the other side:

	barrier();

This is a general barrier -- there are no read-read or write-write variants
of barrier().  However, ACCESS_ONCE() can be thought of as a weak form
for barrier() that affects only the specific accesses flagged by the
ACCESS_ONCE().

The barrier() function has the following effects:

 (*) Prevents the compiler from reordering accesses following the
     barrier() to precede any accesses preceding the barrier().
     One example use for this property is to ease communication between
     interrupt-handler code and the code that was interrupted.

 (*) Within a loop, forces the compiler to load the variables used
     in that loop's conditional on each pass through that loop.

The ACCESS_ONCE() function can prevent any number of optimizations that,
while perfectly safe in single-threaded code, can be fatal in concurrent
code.  Here are some examples of these sorts of optimizations:

 (*) The compiler is within its rights to reorder loads and stores
     to the same variable, and in some cases, the CPU is within its
     rights to reorder loads to the same variable.  This means that
     the following code:

	a[0] = x;
	a[1] = x;

     Might result in an older value of x stored in a[1] than in a[0].
     Prevent both the compiler and the CPU from doing this as follows:

	a[0] = ACCESS_ONCE(x);
	a[1] = ACCESS_ONCE(x);

     In short, ACCESS_ONCE() provides cache coherence for accesses from
     multiple CPUs to a single variable.

 (*) The compiler is within its rights to merge successive loads from
     the same variable.  Such merging can cause the compiler to "optimize"
     the following code:

	while (tmp = a)
		do_something_with(tmp);

     into the following code, which, although in some sense legitimate
     for single-threaded code, is almost certainly not what the developer
     intended:

	if (tmp = a)
		for (;;)
			do_something_with(tmp);

     Use ACCESS_ONCE() to prevent the compiler from doing this to you:

	while (tmp = ACCESS_ONCE(a))
		do_something_with(tmp);

 (*) The compiler is within its rights to reload a variable, for example,
     in cases where high register pressure prevents the compiler from
     keeping all data of interest in registers.  The compiler might
     therefore optimize the variable 'tmp' out of our previous example:

	while (tmp = a)
		do_something_with(tmp);

     This could result in the following code, which is perfectly safe in
     single-threaded code, but can be fatal in concurrent code:

	while (a)
		do_something_with(a);

     For example, the optimized version of this code could result in
     passing a zero to do_something_with() in the case where the variable
     a was modified by some other CPU between the "while" statement and
     the call to do_something_with().

     Again, use ACCESS_ONCE() to prevent the compiler from doing this:

	while (tmp = ACCESS_ONCE(a))
		do_something_with(tmp);

     Note that if the compiler runs short of registers, it might save
     tmp onto the stack.  The overhead of this saving and later restoring
     is why compilers reload variables.  Doing so is perfectly safe for
     single-threaded code, so you need to tell the compiler about cases
     where it is not safe.

 (*) The compiler is within its rights to omit a load entirely if it knows
     what the value will be.  For example, if the compiler can prove that
     the value of variable 'a' is always zero, it can optimize this code:

	while (tmp = a)
		do_something_with(tmp);

     Into this:

	do { } while (0);

     This transformation is a win for single-threaded code because it gets
     rid of a load and a branch.  The problem is that the compiler will
     carry out its proof assuming that the current CPU is the only one
     updating variable 'a'.  If variable 'a' is shared, then the compiler's
     proof will be erroneous.  Use ACCESS_ONCE() to tell the compiler
     that it doesn't know as much as it thinks it does:

	while (tmp = ACCESS_ONCE(a))
		do_something_with(tmp);

     But please note that the compiler is also closely watching what you
     do with the value after the ACCESS_ONCE().  For example, suppose you
     do the following and MAX is a preprocessor macro with the value 1:

	while ((tmp = ACCESS_ONCE(a)) % MAX)
		do_something_with(tmp);

     Then the compiler knows that the result of the "%" operator applied
     to MAX will always be zero, again allowing the compiler to optimize
     the code into near-nonexistence.  (It will still load from the
     variable 'a'.)

 (*) Similarly, the compiler is within its rights to omit a store entirely
     if it knows that the variable already has the value being stored.
     Again, the compiler assumes that the current CPU is the only one
     storing into the variable, which can cause the compiler to do the
     wrong thing for shared variables.  For example, suppose you have
     the following:

	a = 0;
	/* Code that does not store to variable a. */
	a = 0;

     The compiler sees that the value of variable 'a' is already zero, so
     it might well omit the second store.  This would come as a fatal
     surprise if some other CPU might have stored to variable 'a' in the
     meantime.

     Use ACCESS_ONCE() to prevent the compiler from making this sort of
     wrong guess:

	ACCESS_ONCE(a) = 0;
	/* Code that does not store to variable a. */
	ACCESS_ONCE(a) = 0;

 (*) The compiler is within its rights to reorder memory accesses unless
     you tell it not to.  For example, consider the following interaction
     between process-level code and an interrupt handler:

	void process_level(void)
	{
		msg = get_message();
		flag = true;
	}

	void interrupt_handler(void)
	{
		if (flag)
			process_message(msg);
	}

     There is nothing to prevent the compiler from transforming
     process_level() to the following, in fact, this might well be a
     win for single-threaded code:

	void process_level(void)
	{
		flag = true;
		msg = get_message();
	}

     If the interrupt occurs between these two statement, then
     interrupt_handler() might be passed a garbled msg.  Use ACCESS_ONCE()
     to prevent this as follows:

	void process_level(void)
	{
		ACCESS_ONCE(msg) = get_message();
		ACCESS_ONCE(flag) = true;
	}

	void interrupt_handler(void)
	{
		if (ACCESS_ONCE(flag))
			process_message(ACCESS_ONCE(msg));
	}

     Note that the ACCESS_ONCE() wrappers in interrupt_handler()
     are needed if this interrupt handler can itself be interrupted
     by something that also accesses 'flag' and 'msg', for example,
     a nested interrupt or an NMI.  Otherwise, ACCESS_ONCE() is not
     needed in interrupt_handler() other than for documentation purposes.
     (Note also that nested interrupts do not typically occur in modern
     Linux kernels, in fact, if an interrupt handler returns with
     interrupts enabled, you will get a WARN_ONCE() splat.)

     You should assume that the compiler can move ACCESS_ONCE() past
     code not containing ACCESS_ONCE(), barrier(), or similar primitives.

     This effect could also be achieved using barrier(), but ACCESS_ONCE()
     is more selective:  With ACCESS_ONCE(), the compiler need only forget
     the contents of the indicated memory locations, while with barrier()
     the compiler must discard the value of all memory locations that
     it has currented cached in any machine registers.  Of course,
     the compiler must also respect the order in which the ACCESS_ONCE()s
     occur, though the CPU of course need not do so.

 (*) The compiler is within its rights to invent stores to a variable,
     as in the following example:

	if (a)
		b = a;
	else
		b = 42;

     The compiler might save a branch by optimizing this as follows:

	b = 42;
	if (a)
		b = a;

     In single-threaded code, this is not only safe, but also saves
     a branch.  Unfortunately, in concurrent code, this optimization
     could cause some other CPU to see a spurious value of 42 -- even
     if variable 'a' was never zero -- when loading variable 'b'.
     Use ACCESS_ONCE() to prevent this as follows:

	if (a)
		ACCESS_ONCE(b) = a;
	else
		ACCESS_ONCE(b) = 42;

     The compiler can also invent loads.  These are usually less
     damaging, but they can result in cache-line bouncing and thus in
     poor performance and scalability.  Use ACCESS_ONCE() to prevent
     invented loads.

 (*) For aligned memory locations whose size allows them to be accessed
     with a single memory-reference instruction, prevents "load tearing"
     and "store tearing," in which a single large access is replaced by
     multiple smaller accesses.  For example, given an architecture having
     16-bit store instructions with 7-bit immediate fields, the compiler
     might be tempted to use two 16-bit store-immediate instructions to
     implement the following 32-bit store:

	p = 0x00010002;

     Please note that GCC really does use this sort of optimization,
     which is not surprising given that it would likely take more
     than two instructions to build the constant and then store it.
     This optimization can therefore be a win in single-threaded code.
     In fact, a recent bug (since fixed) caused GCC to incorrectly use
     this optimization in a volatile store.  In the absence of such bugs,
     use of ACCESS_ONCE() prevents store tearing in the following example:

	ACCESS_ONCE(p) = 0x00010002;

     Use of packed structures can also result in load and store tearing,
     as in this example:

	struct __attribute__((__packed__)) foo {
		short a;
		int b;
		short c;
	};
	struct foo foo1, foo2;
	...

	foo2.a = foo1.a;
	foo2.b = foo1.b;
	foo2.c = foo1.c;

     Because there are no ACCESS_ONCE() wrappers and no volatile markings,
     the compiler would be well within its rights to implement these three
     assignment statements as a pair of 32-bit loads followed by a pair
     of 32-bit stores.  This would result in load tearing on 'foo1.b'
     and store tearing on 'foo2.b'.  ACCESS_ONCE() again prevents tearing
     in this example:

	foo2.a = foo1.a;
	ACCESS_ONCE(foo2.b) = ACCESS_ONCE(foo1.b);
	foo2.c = foo1.c;

All that aside, it is never necessary to use ACCESS_ONCE() on a variable
that has been marked volatile.  For example, because 'jiffies' is marked
volatile, it is never necessary to say ACCESS_ONCE(jiffies).  The reason
for this is that ACCESS_ONCE() is implemented as a volatile cast, which
has no effect when its argument is already marked volatile.

Please note that these compiler barriers have no direct effect on the CPU,
which may then reorder things however it wishes.


CPU MEMORY BARRIERS
-------------------

The Linux kernel has eight basic CPU memory barriers:

	TYPE		MANDATORY		SMP CONDITIONAL
	===============	=======================	===========================
	GENERAL		mb()			smp_mb()
	WRITE		wmb()			smp_wmb()
	READ		rmb()			smp_rmb()
	DATA DEPENDENCY	read_barrier_depends()	smp_read_barrier_depends()


All memory barriers except the data dependency barriers imply a compiler
barrier. Data dependencies do not impose any additional compiler ordering.

Aside: In the case of data dependencies, the compiler would be expected to
issue the loads in the correct order (eg. `a[b]` would have to load the value
of b before loading a[b]), however there is no guarantee in the C specification
that the compiler may not speculate the value of b (eg. is equal to 1) and load
a before b (eg. tmp = a[1]; if (b != 1) tmp = a[b]; ). There is also the
problem of a compiler reloading b after having loaded a[b], thus having a newer
copy of b than a[b]. A consensus has not yet been reached about these problems,
however the ACCESS_ONCE macro is a good place to start looking.

SMP memory barriers are reduced to compiler barriers on uniprocessor compiled
systems because it is assumed that a CPU will appear to be self-consistent,
and will order overlapping accesses correctly with respect to itself.

[!] Note that SMP memory barriers _must_ be used to control the ordering of
references to shared memory on SMP systems, though the use of locking instead
is sufficient.

Mandatory barriers should not be used to control SMP effects, since mandatory
barriers unnecessarily impose overhead on UP systems. They may, however, be
used to control MMIO effects on accesses through relaxed memory I/O windows.
These are required even on non-SMP systems as they affect the order in which
memory operations appear to a device by prohibiting both the compiler and the
CPU from reordering them.


There are some more advanced barrier functions:

 (*) set_mb(var, value)

     This assigns the value to the variable and then inserts a full memory
     barrier after it, depending on the function.  It isn't guaranteed to
     insert anything more than a compiler barrier in a UP compilation.


 (*) smp_mb__before_atomic();
 (*) smp_mb__after_atomic();

     These are for use with atomic (such as add, subtract, increment and
     decrement) functions that don't return a value, especially when used for
     reference counting.  These functions do not imply memory barriers.

     These are also used for atomic bitop functions that do not return a
     value (such as set_bit and clear_bit).

     As an example, consider a piece of code that marks an object as being dead
     and then decrements the object's reference count:

	obj->dead = 1;
	smp_mb__before_atomic();
	atomic_dec(&obj->ref_count);

     This makes sure that the death mark on the object is perceived to be set
     *before* the reference counter is decremented.

     See Documentation/atomic_ops.txt for more information.  See the "Atomic
     operations" subsection for information on where to use these.


MMIO WRITE BARRIER
------------------

The Linux kernel also has a special barrier for use with memory-mapped I/O
writes:

	mmiowb();

This is a variation on the mandatory write barrier that causes writes to weakly
ordered I/O regions to be partially ordered.  Its effects may go beyond the
CPU->Hardware interface and actually affect the hardware at some level.

See the subsection "Locks vs I/O accesses" for more information.


===============================
IMPLICIT KERNEL MEMORY BARRIERS
===============================

Some of the other functions in the linux kernel imply memory barriers, amongst
which are locking and scheduling functions.

This specification is a _minimum_ guarantee; any particular architecture may
provide more substantial guarantees, but these may not be relied upon outside
of arch specific code.


ACQUIRING FUNCTIONS
-------------------

The Linux kernel has a number of locking constructs:

 (*) spin locks
 (*) R/W spin locks
 (*) mutexes
 (*) semaphores
 (*) R/W semaphores
 (*) RCU

In all cases there are variants on "ACQUIRE" operations and "RELEASE" operations
for each construct.  These operations all imply certain barriers:

 (1) ACQUIRE operation implication:

     Memory operations issued after the ACQUIRE will be completed after the
     ACQUIRE operation has completed.

     Memory operations issued before the ACQUIRE may be completed after
     the ACQUIRE operation has completed.  An smp_mb__before_spinlock(),
     combined with a following ACQUIRE, orders prior loads against
     subsequent loads and stores and also orders prior stores against
     subsequent stores.  Note that this is weaker than smp_mb()!  The
     smp_mb__before_spinlock() primitive is free on many architectures.

 (2) RELEASE operation implication:

     Memory operations issued before the RELEASE will be completed before the
     RELEASE operation has completed.

     Memory operations issued after the RELEASE may be completed before the
     RELEASE operation has completed.

 (3) ACQUIRE vs ACQUIRE implication:

     All ACQUIRE operations issued before another ACQUIRE operation will be
     completed before that ACQUIRE operation.

 (4) ACQUIRE vs RELEASE implication:

     All ACQUIRE operations issued before a RELEASE operation will be
     completed before the RELEASE operation.

 (5) Failed conditional ACQUIRE implication:

     Certain locking variants of the ACQUIRE operation may fail, either due to
     being unable to get the lock immediately, or due to receiving an unblocked
     signal whilst asleep waiting for the lock to become available.  Failed
     locks do not imply any sort of barrier.

[!] Note: one of the consequences of lock ACQUIREs and RELEASEs being only
one-way barriers is that the effects of instructions outside of a critical
section may seep into the inside of the critical section.

An ACQUIRE followed by a RELEASE may not be assumed to be full memory barrier
because it is possible for an access preceding the ACQUIRE to happen after the
ACQUIRE, and an access following the RELEASE to happen before the RELEASE, and
the two accesses can themselves then cross:

	*A = a;
	ACQUIRE M
	RELEASE M
	*B = b;

may occur as:

	ACQUIRE M, STORE *B, STORE *A, RELEASE M

When the ACQUIRE and RELEASE are a lock acquisition and release,
respectively, this same reordering can occur if the lock's ACQUIRE and
RELEASE are to the same lock variable, but only from the perspective of
another CPU not holding that lock.  In short, a ACQUIRE followed by an
RELEASE may -not- be assumed to be a full memory barrier.

Similarly, the reverse case of a RELEASE followed by an ACQUIRE does not
imply a full memory barrier.  If it is necessary for a RELEASE-ACQUIRE
pair to produce a full barrier, the ACQUIRE can be followed by an
smp_mb__after_unlock_lock() invocation.  This will produce a full barrier
if either (a) the RELEASE and the ACQUIRE are executed by the same
CPU or task, or (b) the RELEASE and ACQUIRE act on the same variable.
The smp_mb__after_unlock_lock() primitive is free on many architectures.
Without smp_mb__after_unlock_lock(), the CPU's execution of the critical
sections corresponding to the RELEASE and the ACQUIRE can cross, so that:

	*A = a;
	RELEASE M
	ACQUIRE N
	*B = b;

could occur as:

	ACQUIRE N, STORE *B, STORE *A, RELEASE M

It might appear that this reordering could introduce a deadlock.
However, this cannot happen because if such a deadlock threatened,
the RELEASE would simply complete, thereby avoiding the deadlock.

	Why does this work?

	One key point is that we are only talking about the CPU doing
	the reordering, not the compiler.  If the compiler (or, for
	that matter, the developer) switched the operations, deadlock
	-could- occur.

	But suppose the CPU reordered the operations.  In this case,
	the unlock precedes the lock in the assembly code.  The CPU
	simply elected to try executing the later lock operation first.
	If there is a deadlock, this lock operation will simply spin (or
	try to sleep, but more on that later).	The CPU will eventually
	execute the unlock operation (which preceded the lock operation
	in the assembly code), which will unravel the potential deadlock,
	allowing the lock operation to succeed.

	But what if the lock is a sleeplock?  In that case, the code will
	try to enter the scheduler, where it will eventually encounter
	a memory barrier, which will force the earlier unlock operation
	to complete, again unraveling the deadlock.  There might be
	a sleep-unlock race, but the locking primitive needs to resolve
	such races properly in any case.

With smp_mb__after_unlock_lock(), the two critical sections cannot overlap.
For example, with the following code, the store to *A will always be
seen by other CPUs before the store to *B:

	*A = a;
	RELEASE M
	ACQUIRE N
	smp_mb__after_unlock_lock();
	*B = b;

The operations will always occur in one of the following orders:

	STORE *A, RELEASE, ACQUIRE, smp_mb__after_unlock_lock(), STORE *B
	STORE *A, ACQUIRE, RELEASE, smp_mb__after_unlock_lock(), STORE *B
	ACQUIRE, STORE *A, RELEASE, smp_mb__after_unlock_lock(), STORE *B

If the RELEASE and ACQUIRE were instead both operating on the same lock
variable, only the first of these alternatives can occur.  In addition,
the more strongly ordered systems may rule out some of the above orders.
But in any case, as noted earlier, the smp_mb__after_unlock_lock()
ensures that the store to *A will always be seen as happening before
the store to *B.

Locks and semaphores may not provide any guarantee of ordering on UP compiled
systems, and so cannot be counted on in such a situation to actually achieve
anything at all - especially with respect to I/O accesses - unless combined
with interrupt disabling operations.

See also the section on "Inter-CPU locking barrier effects".


As an example, consider the following:

	*A = a;
	*B = b;
	ACQUIRE
	*C = c;
	*D = d;
	RELEASE
	*E = e;
	*F = f;

The following sequence of events is acceptable:

	ACQUIRE, {*F,*A}, *E, {*C,*D}, *B, RELEASE

	[+] Note that {*F,*A} indicates a combined access.

But none of the following are:

	{*F,*A}, *B,	ACQUIRE, *C, *D,	RELEASE, *E
	*A, *B, *C,	ACQUIRE, *D,		RELEASE, *E, *F
	*A, *B,		ACQUIRE, *C,		RELEASE, *D, *E, *F
	*B,		ACQUIRE, *C, *D,	RELEASE, {*F,*A}, *E



INTERRUPT DISABLING FUNCTIONS
-----------------------------

Functions that disable interrupts (ACQUIRE equivalent) and enable interrupts
(RELEASE equivalent) will act as compiler barriers only.  So if memory or I/O
barriers are required in such a situation, they must be provided from some
other means.


SLEEP AND WAKE-UP FUNCTIONS
---------------------------

Sleeping and waking on an event flagged in global data can be viewed as an
interaction between two pieces of data: the task state of the task waiting for
the event and the global data used to indicate the event.  To make sure that
these appear to happen in the right order, the primitives to begin the process
of going to sleep, and the primitives to initiate a wake up imply certain
barriers.

Firstly, the sleeper normally follows something like this sequence of events:

	for (;;) {
		set_current_state(TASK_UNINTERRUPTIBLE);
		if (event_indicated)
			break;
		schedule();
	}

A general memory barrier is interpolated automatically by set_current_state()
after it has altered the task state:

	CPU 1
	===============================
	set_current_state();
	  set_mb();
	    STORE current->state
	    <general barrier>
	LOAD event_indicated

set_current_state() may be wrapped by:

	prepare_to_wait();
	prepare_to_wait_exclusive();

which therefore also imply a general memory barrier after setting the state.
The whole sequence above is available in various canned forms, all of which
interpolate the memory barrier in the right place:

	wait_event();
	wait_event_interruptible();
	wait_event_interruptible_exclusive();
	wait_event_interruptible_timeout();
	wait_event_killable();
	wait_event_timeout();
	wait_on_bit();
	wait_on_bit_lock();


Secondly, code that performs a wake up normally follows something like this:

	event_indicated = 1;
	wake_up(&event_wait_queue);

or:

	event_indicated = 1;
	wake_up_process(event_daemon);

A write memory barrier is implied by wake_up() and co. if and only if they wake
something up.  The barrier occurs before the task state is cleared, and so sits
between the STORE to indicate the event and the STORE to set TASK_RUNNING:

	CPU 1				CPU 2
	===============================	===============================
	set_current_state();		STORE event_indicated
	  set_mb();			wake_up();
	    STORE current->state	  <write barrier>
	    <general barrier>		  STORE current->state
	LOAD event_indicated

The available waker functions include:

	complete();
	wake_up();
	wake_up_all();
	wake_up_bit();
	wake_up_interruptible();
	wake_up_interruptible_all();
	wake_up_interruptible_nr();
	wake_up_interruptible_poll();
	wake_up_interruptible_sync();
	wake_up_interruptible_sync_poll();
	wake_up_locked();
	wake_up_locked_poll();
	wake_up_nr();
	wake_up_poll();
	wake_up_process();


[!] Note that the memory barriers implied by the sleeper and the waker do _not_
order multiple stores before the wake-up with respect to loads of those stored
values after the sleeper has called set_current_state().  For instance, if the
sleeper does:

	set_current_state(TASK_INTERRUPTIBLE);
	if (event_indicated)
		break;
	__set_current_state(TASK_RUNNING);
	do_something(my_data);

and the waker does:

	my_data = value;
	event_indicated = 1;
	wake_up(&event_wait_queue);

there's no guarantee that the change to event_indicated will be perceived by
the sleeper as coming after the change to my_data.  In such a circumstance, the
code on both sides must interpolate its own memory barriers between the
separate data accesses.  Thus the above sleeper ought to do:

	set_current_state(TASK_INTERRUPTIBLE);
	if (event_indicated) {
		smp_rmb();
		do_something(my_data);
	}

and the waker should do:

	my_data = value;
	smp_wmb();
	event_indicated = 1;
	wake_up(&event_wait_queue);


MISCELLANEOUS FUNCTIONS
-----------------------

Other functions that imply barriers:

 (*) schedule() and similar imply full memory barriers.


===================================
INTER-CPU ACQUIRING BARRIER EFFECTS
===================================

On SMP systems locking primitives give a more substantial form of barrier: one
that does affect memory access ordering on other CPUs, within the context of
conflict on any particular lock.


ACQUIRES VS MEMORY ACCESSES
---------------------------

Consider the following: the system has a pair of spinlocks (M) and (Q), and
three CPUs; then should the following sequence of events occur:

	CPU 1				CPU 2
	===============================	===============================
	ACCESS_ONCE(*A) = a;		ACCESS_ONCE(*E) = e;
	ACQUIRE M			ACQUIRE Q
	ACCESS_ONCE(*B) = b;		ACCESS_ONCE(*F) = f;
	ACCESS_ONCE(*C) = c;		ACCESS_ONCE(*G) = g;
	RELEASE M			RELEASE Q
	ACCESS_ONCE(*D) = d;		ACCESS_ONCE(*H) = h;

Then there is no guarantee as to what order CPU 3 will see the accesses to *A
through *H occur in, other than the constraints imposed by the separate locks
on the separate CPUs. It might, for example, see:

	*E, ACQUIRE M, ACQUIRE Q, *G, *C, *F, *A, *B, RELEASE Q, *D, *H, RELEASE M

But it won't see any of:

	*B, *C or *D preceding ACQUIRE M
	*A, *B or *C following RELEASE M
	*F, *G or *H preceding ACQUIRE Q
	*E, *F or *G following RELEASE Q


However, if the following occurs:

	CPU 1				CPU 2
	===============================	===============================
	ACCESS_ONCE(*A) = a;
	ACQUIRE M		     [1]
	ACCESS_ONCE(*B) = b;
	ACCESS_ONCE(*C) = c;
	RELEASE M	     [1]
	ACCESS_ONCE(*D) = d;		ACCESS_ONCE(*E) = e;
					ACQUIRE M		     [2]
					smp_mb__after_unlock_lock();
					ACCESS_ONCE(*F) = f;
					ACCESS_ONCE(*G) = g;
					RELEASE M	     [2]
					ACCESS_ONCE(*H) = h;

CPU 3 might see:

	*E, ACQUIRE M [1], *C, *B, *A, RELEASE M [1],
		ACQUIRE M [2], *H, *F, *G, RELEASE M [2], *D

But assuming CPU 1 gets the lock first, CPU 3 won't see any of:

	*B, *C, *D, *F, *G or *H preceding ACQUIRE M [1]
	*A, *B or *C following RELEASE M [1]
	*F, *G or *H preceding ACQUIRE M [2]
	*A, *B, *C, *E, *F or *G following RELEASE M [2]

Note that the smp_mb__after_unlock_lock() is critically important
here: Without it CPU 3 might see some of the above orderings.
Without smp_mb__after_unlock_lock(), the accesses are not guaranteed
to be seen in order unless CPU 3 holds lock M.


ACQUIRES VS I/O ACCESSES
------------------------

Under certain circumstances (especially involving NUMA), I/O accesses within
two spinlocked sections on two different CPUs may be seen as interleaved by the
PCI bridge, because the PCI bridge does not necessarily participate in the
cache-coherence protocol, and is therefore incapable of issuing the required
read memory barriers.

For example:

	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	writel(1, DATA);
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					writel(5, DATA);
					spin_unlock(Q);

may be seen by the PCI bridge as follows:

	STORE *ADDR = 0, STORE *ADDR = 4, STORE *DATA = 1, STORE *DATA = 5

which would probably cause the hardware to malfunction.


What is necessary here is to intervene with an mmiowb() before dropping the
spinlock, for example:

	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	writel(1, DATA);
	mmiowb();
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					writel(5, DATA);
					mmiowb();
					spin_unlock(Q);

this will ensure that the two stores issued on CPU 1 appear at the PCI bridge
before either of the stores issued on CPU 2.


Furthermore, following a store by a load from the same device obviates the need
for the mmiowb(), because the load forces the store to complete before the load
is performed:

	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	a = readl(DATA);
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					b = readl(DATA);
					spin_unlock(Q);


See Documentation/DocBook/deviceiobook.tmpl for more information.


=================================
WHERE ARE MEMORY BARRIERS NEEDED?
=================================

Under normal operation, memory operation reordering is generally not going to
be a problem as a single-threaded linear piece of code will still appear to
work correctly, even if it's in an SMP kernel.  There are, however, four
circumstances in which reordering definitely _could_ be a problem:

 (*) Interprocessor interaction.

 (*) Atomic operations.

 (*) Accessing devices.

 (*) Interrupts.


INTERPROCESSOR INTERACTION
--------------------------

When there's a system with more than one processor, more than one CPU in the
system may be working on the same data set at the same time.  This can cause
synchronisation problems, and the usual way of dealing with them is to use
locks.  Locks, however, are quite expensive, and so it may be preferable to
operate without the use of a lock if at all possible.  In such a case
operations that affect both CPUs may have to be carefully ordered to prevent
a malfunction.

Consider, for example, the R/W semaphore slow path.  Here a waiting process is
queued on the semaphore, by virtue of it having a piece of its stack linked to
the semaphore's list of waiting processes:

	struct rw_semaphore {
		...
		spinlock_t lock;
		struct list_head waiters;
	};

	struct rwsem_waiter {
		struct list_head list;
		struct task_struct *task;
	};

To wake up a particular waiter, the up_read() or up_write() functions have to:

 (1) read the next pointer from this waiter's record to know as to where the
     next waiter record is;

 (2) read the pointer to the waiter's task structure;

 (3) clear the task pointer to tell the waiter it has been given the semaphore;

 (4) call wake_up_process() on the task; and

 (5) release the reference held on the waiter's task struct.

In other words, it has to perform this sequence of events:

	LOAD waiter->list.next;
	LOAD waiter->task;
	STORE waiter->task;
	CALL wakeup
	RELEASE task

and if any of these steps occur out of order, then the whole thing may
malfunction.

Once it has queued itself and dropped the semaphore lock, the waiter does not
get the lock again; it instead just waits for its task pointer to be cleared
before proceeding.  Since the record is on the waiter's stack, this means that
if the task pointer is cleared _before_ the next pointer in the list is read,
another CPU might start processing the waiter and might clobber the waiter's
stack before the up*() function has a chance to read the next pointer.

Consider then what might happen to the above sequence of events:

	CPU 1				CPU 2
	===============================	===============================
					down_xxx()
					Queue waiter
					Sleep
	up_yyy()
	LOAD waiter->task;
	STORE waiter->task;
					Woken up by other event
	<preempt>
					Resume processing
					down_xxx() returns
					call foo()
					foo() clobbers *waiter
	</preempt>
	LOAD waiter->list.next;
	--- OOPS ---

This could be dealt with using the semaphore lock, but then the down_xxx()
function has to needlessly get the spinlock again after being woken up.

The way to deal with this is to insert a general SMP memory barrier:

	LOAD waiter->list.next;
	LOAD waiter->task;
	smp_mb();
	STORE waiter->task;
	CALL wakeup
	RELEASE task

In this case, the barrier makes a guarantee that all memory accesses before the
barrier will appear to happen before all the memory accesses after the barrier
with respect to the other CPUs on the system.  It does _not_ guarantee that all
the memory accesses before the barrier will be complete by the time the barrier
instruction itself is complete.

On a UP system - where this wouldn't be a problem - the smp_mb() is just a
compiler barrier, thus making sure the compiler emits the instructions in the
right order without actually intervening in the CPU.  Since there's only one
CPU, that CPU's dependency ordering logic will take care of everything else.


ATOMIC OPERATIONS
-----------------

Whilst they are technically interprocessor interaction considerations, atomic
operations are noted specially as some of them imply full memory barriers and
some don't, but they're very heavily relied on as a group throughout the
kernel.

Any atomic operation that modifies some state in memory and returns information
about the state (old or new) implies an SMP-conditional general memory barrier
(smp_mb()) on each side of the actual operation (with the exception of
explicit lock operations, described later).  These include:

	xchg();
	cmpxchg();
	atomic_xchg();			atomic_long_xchg();
	atomic_cmpxchg();		atomic_long_cmpxchg();
	atomic_inc_return();		atomic_long_inc_return();
	atomic_dec_return();		atomic_long_dec_return();
	atomic_add_return();		atomic_long_add_return();
	atomic_sub_return();		atomic_long_sub_return();
	atomic_inc_and_test();		atomic_long_inc_and_test();
	atomic_dec_and_test();		atomic_long_dec_and_test();
	atomic_sub_and_test();		atomic_long_sub_and_test();
	atomic_add_negative();		atomic_long_add_negative();
	test_and_set_bit();
	test_and_clear_bit();
	test_and_change_bit();

	/* when succeeds (returns 1) */
	atomic_add_unless();		atomic_long_add_unless();

These are used for such things as implementing ACQUIRE-class and RELEASE-class
operations and adjusting reference counters towards object destruction, and as
such the implicit memory barrier effects are necessary.


The following operations are potential problems as they do _not_ imply memory
barriers, but might be used for implementing such things as RELEASE-class
operations:

	atomic_set();
	set_bit();
	clear_bit();
	change_bit();

With these the appropriate explicit memory barrier should be used if necessary
(smp_mb__before_atomic() for instance).


The following also do _not_ imply memory barriers, and so may require explicit
memory barriers under some circumstances (smp_mb__before_atomic() for
instance):

	atomic_add();
	atomic_sub();
	atomic_inc();
	atomic_dec();

If they're used for statistics generation, then they probably don't need memory
barriers, unless there's a coupling between statistical data.

If they're used for reference counting on an object to control its lifetime,
they probably don't need memory barriers because either the reference count
will be adjusted inside a locked section, or the caller will already hold
sufficient references to make the lock, and thus a memory barrier unnecessary.

If they're used for constructing a lock of some description, then they probably
do need memory barriers as a lock primitive generally has to do things in a
specific order.

Basically, each usage case has to be carefully considered as to whether memory
barriers are needed or not.

The following operations are special locking primitives:

	test_and_set_bit_lock();
	clear_bit_unlock();
	__clear_bit_unlock();

These implement ACQUIRE-class and RELEASE-class operations. These should be used in
preference to other operations when implementing locking primitives, because
their implementations can be optimised on many architectures.

[!] Note that special memory barrier primitives are available for these
situations because on some CPUs the atomic instructions used imply full memory
barriers, and so barrier instructions are superfluous in conjunction with them,
and in such cases the special barrier primitives will be no-ops.

See Documentation/atomic_ops.txt for more information.


ACCESSING DEVICES
-----------------

Many devices can be memory mapped, and so appear to the CPU as if they're just
a set of memory locations.  To control such a device, the driver usually has to
make the right memory accesses in exactly the right order.

However, having a clever CPU or a clever compiler creates a potential problem
in that the carefully sequenced accesses in the driver code won't reach the
device in the requisite order if the CPU or the compiler thinks it is more
efficient to reorder, combine or merge accesses - something that would cause
the device to malfunction.

Inside of the Linux kernel, I/O should be done through the appropriate accessor
routines - such as inb() or writel() - which know how to make such accesses
appropriately sequential.  Whilst this, for the most part, renders the explicit
use of memory barriers unnecessary, there are a couple of situations where they
might be needed:

 (1) On some systems, I/O stores are not strongly ordered across all CPUs, and
     so for _all_ general drivers locks should be used and mmiowb() must be
     issued prior to unlocking the critical section.

 (2) If the accessor functions are used to refer to an I/O memory window with
     relaxed memory access properties, then _mandatory_ memory barriers are
     required to enforce ordering.

See Documentation/DocBook/deviceiobook.tmpl for more information.


INTERRUPTS
----------

A driver may be interrupted by its own interrupt service routine, and thus the
two parts of the driver may interfere with each other's attempts to control or
access the device.

This may be alleviated - at least in part - by disabling local interrupts (a
form of locking), such that the critical operations are all contained within
the interrupt-disabled section in the driver.  Whilst the driver's interrupt
routine is executing, the driver's core may not run on the same CPU, and its
interrupt is not permitted to happen again until the current interrupt has been
handled, thus the interrupt handler does not need to lock against that.

However, consider a driver that was talking to an ethernet card that sports an
address register and a data register.  If that driver's core talks to the card
under interrupt-disablement and then the driver's interrupt handler is invoked:

	LOCAL IRQ DISABLE
	writew(ADDR, 3);
	writew(DATA, y);
	LOCAL IRQ ENABLE
	<interrupt>
	writew(ADDR, 4);
	q = readw(DATA);
	</interrupt>

The store to the data register might happen after the second store to the
address register if ordering rules are sufficiently relaxed:

	STORE *ADDR = 3, STORE *ADDR = 4, STORE *DATA = y, q = LOAD *DATA


If ordering rules are relaxed, it must be assumed that accesses done inside an
interrupt disabled section may leak outside of it and may interleave with
accesses performed in an interrupt - and vice versa - unless implicit or
explicit barriers are used.

Normally this won't be a problem because the I/O accesses done inside such
sections will include synchronous load operations on strictly ordered I/O
registers that form implicit I/O barriers. If this isn't sufficient then an
mmiowb() may need to be used explicitly.


A similar situation may occur between an interrupt routine and two routines
running on separate CPUs that communicate with each other. If such a case is
likely, then interrupt-disabling locks should be used to guarantee ordering.


==========================
KERNEL I/O BARRIER EFFECTS
==========================

When accessing I/O memory, drivers should use the appropriate accessor
functions:

 (*) inX(), outX():

     These are intended to talk to I/O space rather than memory space, but
     that's primarily a CPU-specific concept. The i386 and x86_64 processors do
     indeed have special I/O space access cycles and instructions, but many
     CPUs don't have such a concept.

     The PCI bus, amongst others, defines an I/O space concept which - on such
     CPUs as i386 and x86_64 - readily maps to the CPU's concept of I/O
     space.  However, it may also be mapped as a virtual I/O space in the CPU's
     memory map, particularly on those CPUs that don't support alternate I/O
     spaces.

     Accesses to this space may be fully synchronous (as on i386), but
     intermediary bridges (such as the PCI host bridge) may not fully honour
     that.

     They are guaranteed to be fully ordered with respect to each other.

     They are not guaranteed to be fully ordered with respect to other types of
     memory and I/O operation.

 (*) readX(), writeX():

     Whether these are guaranteed to be fully ordered and uncombined with
     respect to each other on the issuing CPU depends on the characteristics
     defined for the memory window through which they're accessing. On later
     i386 architecture machines, for example, this is controlled by way of the
     MTRR registers.

     Ordinarily, these will be guaranteed to be fully ordered and uncombined,
     provided they're not accessing a prefetchable device.

     However, intermediary hardware (such as a PCI bridge) may indulge in
     deferral if it so wishes; to flush a store, a load from the same location
     is preferred[*], but a load from the same device or from configuration
     space should suffice for PCI.

     [*] NOTE! attempting to load from the same location as was written to may
	 cause a malfunction - consider the 16550 Rx/Tx serial registers for
	 example.

     Used with prefetchable I/O memory, an mmiowb() barrier may be required to
     force stores to be ordered.

     Please refer to the PCI specification for more information on interactions
     between PCI transactions.

 (*) readX_relaxed()

     These are similar to readX(), but are not guaranteed to be ordered in any
     way. Be aware that there is no I/O read barrier available.

 (*) ioreadX(), iowriteX()

     These will perform appropriately for the type of access they're actually
     doing, be it inX()/outX() or readX()/writeX().


========================================
ASSUMED MINIMUM EXECUTION ORDERING MODEL
========================================

It has to be assumed that the conceptual CPU is weakly-ordered but that it will
maintain the appearance of program causality with respect to itself.  Some CPUs
(such as i386 or x86_64) are more constrained than others (such as powerpc or
frv), and so the most relaxed case (namely DEC Alpha) must be assumed outside
of arch-specific code.

This means that it must be considered that the CPU will execute its instruction
stream in any order it feels like - or even in parallel - provided that if an
instruction in the stream depends on an earlier instruction, then that
earlier instruction must be sufficiently complete[*] before the later
instruction may proceed; in other words: provided that the appearance of
causality is maintained.

 [*] Some instructions have more than one effect - such as changing the
     condition codes, changing registers or changing memory - and different
     instructions may depend on different effects.

A CPU may also discard any instruction sequence that winds up having no
ultimate effect.  For example, if two adjacent instructions both load an
immediate value into the same register, the first may be discarded.


Similarly, it has to be assumed that compiler might reorder the instruction
stream in any way it sees fit, again provided the appearance of causality is
maintained.


============================
THE EFFECTS OF THE CPU CACHE
============================

The way cached memory operations are perceived across the system is affected to
a certain extent by the caches that lie between CPUs and memory, and by the
memory coherence system that maintains the consistency of state in the system.

As far as the way a CPU interacts with another part of the system through the
caches goes, the memory system has to include the CPU's caches, and memory
barriers for the most part act at the interface between the CPU and its cache
(memory barriers logically act on the dotted line in the following diagram):

	    <--- CPU --->         :       <----------- Memory ----------->
	                          :
	+--------+    +--------+  :   +--------+    +-----------+
	|        |    |        |  :   |        |    |           |    +--------+
	|  CPU   |    | Memory |  :   | CPU    |    |           |    |        |
	|  Core  |--->| Access |----->| Cache  |<-->|           |    |        |
	|        |    | Queue  |  :   |        |    |           |--->| Memory |
	|        |    |        |  :   |        |    |           |    |        |
	+--------+    +--------+  :   +--------+    |           |    |        |
	                          :                 | Cache     |    +--------+
	                          :                 | Coherency |
	                          :                 | Mechanism |    +--------+
	+--------+    +--------+  :   +--------+    |           |    |	      |
	|        |    |        |  :   |        |    |           |    |        |
	|  CPU   |    | Memory |  :   | CPU    |    |           |--->| Device |
	|  Core  |--->| Access |----->| Cache  |<-->|           |    |        |
	|        |    | Queue  |  :   |        |    |           |    |        |
	|        |    |        |  :   |        |    |           |    +--------+
	+--------+    +--------+  :   +--------+    +-----------+
	                          :
	                          :

Although any particular load or store may not actually appear outside of the
CPU that issued it since it may have been satisfied within the CPU's own cache,
it will still appear as if the full memory access had taken place as far as the
other CPUs are concerned since the cache coherency mechanisms will migrate the
cacheline over to the accessing CPU and propagate the effects upon conflict.

The CPU core may execute instructions in any order it deems fit, provided the
expected program causality appears to be maintained.  Some of the instructions
generate load and store operations which then go into the queue of memory
accesses to be performed.  The core may place these in the queue in any order
it wishes, and continue execution until it is forced to wait for an instruction
to complete.

What memory barriers are concerned with is controlling the order in which
accesses cross from the CPU side of things to the memory side of things, and
the order in which the effects are perceived to happen by the other observers
in the system.

[!] Memory barriers are _not_ needed within a given CPU, as CPUs always see
their own loads and stores as if they had happened in program order.

[!] MMIO or other device accesses may bypass the cache system.  This depends on
the properties of the memory window through which devices are accessed and/or
the use of any special device communication instructions the CPU may have.


CACHE COHERENCY
---------------

Life isn't quite as simple as it may appear above, however: for while the
caches are expected to be coherent, there's no guarantee that that coherency
will be ordered.  This means that whilst changes made on one CPU will
eventually become visible on all CPUs, there's no guarantee that they will
become apparent in the same order on those other CPUs.


Consider dealing with a system that has a pair of CPUs (1 & 2), each of which
has a pair of parallel data caches (CPU 1 has A/B, and CPU 2 has C/D):

	            :
	            :                          +--------+
	            :      +---------+         |        |
	+--------+  : +--->| Cache A |<------->|        |
	|        |  : |    +---------+         |        |
	|  CPU 1 |<---+                        |        |
	|        |  : |    +---------+         |        |
	+--------+  : +--->| Cache B |<------->|        |
	            :      +---------+         |        |
	            :                          | Memory |
	            :      +---------+         | System |
	+--------+  : +--->| Cache C |<------->|        |
	|        |  : |    +---------+         |        |
	|  CPU 2 |<---+                        |        |
	|        |  : |    +---------+         |        |
	+--------+  : +--->| Cache D |<------->|        |
	            :      +---------+         |        |
	            :                          +--------+
	            :

Imagine the system has the following properties:

 (*) an odd-numbered cache line may be in cache A, cache C or it may still be
     resident in memory;

 (*) an even-numbered cache line may be in cache B, cache D or it may still be
     resident in memory;

 (*) whilst the CPU core is interrogating one cache, the other cache may be
     making use of the bus to access the rest of the system - perhaps to
     displace a dirty cacheline or to do a speculative load;

 (*) each cache has a queue of operations that need to be applied to that cache
     to maintain coherency with the rest of the system;

 (*) the coherency queue is not flushed by normal loads to lines already
     present in the cache, even though the contents of the queue may
     potentially affect those loads.

Imagine, then, that two writes are made on the first CPU, with a write barrier
between them to guarantee that they will appear to reach that CPU's caches in
the requisite order:

	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();			Make sure change to v is visible before
					 change to p
	<A:modify v=2>			v is now in cache A exclusively
	p = &v;
	<B:modify p=&v>			p is now in cache B exclusively

The write memory barrier forces the other CPUs in the system to perceive that
the local CPU's caches have apparently been updated in the correct order.  But
now imagine that the second CPU wants to read those values:

	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
	...
			q = p;
			x = *q;

The above pair of reads may then fail to happen in the expected order, as the
cacheline holding p may get updated in one of the second CPU's caches whilst
the update to the cacheline holding v is delayed in the other of the second
CPU's caches by some other cache event:

	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();
	<A:modify v=2>	<C:busy>
			<C:queue v=2>
	p = &v;		q = p;
			<D:request p>
	<B:modify p=&v>	<D:commit p=&v>
			<D:read p>
			x = *q;
			<C:read *q>	Reads from v before v updated in cache
			<C:unbusy>
			<C:commit v=2>

Basically, whilst both cachelines will be updated on CPU 2 eventually, there's
no guarantee that, without intervention, the order of update will be the same
as that committed on CPU 1.


To intervene, we need to interpolate a data dependency barrier or a read
barrier between the loads.  This will force the cache to commit its coherency
queue before processing any further requests:

	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();
	<A:modify v=2>	<C:busy>
			<C:queue v=2>
	p = &v;		q = p;
			<D:request p>
	<B:modify p=&v>	<D:commit p=&v>
			<D:read p>
			smp_read_barrier_depends()
			<C:unbusy>
			<C:commit v=2>
			x = *q;
			<C:read *q>	Reads from v after v updated in cache


This sort of problem can be encountered on DEC Alpha processors as they have a
split cache that improves performance by making better use of the data bus.
Whilst most CPUs do imply a data dependency barrier on the read when a memory
access depends on a read, not all do, so it may not be relied on.

Other CPUs may also have split caches, but must coordinate between the various
cachelets for normal memory accesses.  The semantics of the Alpha removes the
need for coordination in the absence of memory barriers.


CACHE COHERENCY VS DMA
----------------------

Not all systems maintain cache coherency with respect to devices doing DMA.  In
such cases, a device attempting DMA may obtain stale data from RAM because
dirty cache lines may be resident in the caches of various CPUs, and may not
have been written back to RAM yet.  To deal with this, the appropriate part of
the kernel must flush the overlapping bits of cache on each CPU (and maybe
invalidate them as well).

In addition, the data DMA'd to RAM by a device may be overwritten by dirty
cache lines being written back to RAM from a CPU's cache after the device has
installed its own data, or cache lines present in the CPU's cache may simply
obscure the fact that RAM has been updated, until at such time as the cacheline
is discarded from the CPU's cache and reloaded.  To deal with this, the
appropriate part of the kernel must invalidate the overlapping bits of the
cache on each CPU.

See Documentation/cachetlb.txt for more information on cache management.


CACHE COHERENCY VS MMIO
-----------------------

Memory mapped I/O usually takes place through memory locations that are part of
a window in the CPU's memory space that has different properties assigned than
the usual RAM directed window.

Amongst these properties is usually the fact that such accesses bypass the
caching entirely and go directly to the device buses.  This means MMIO accesses
may, in effect, overtake accesses to cached memory that were emitted earlier.
A memory barrier isn't sufficient in such a case, but rather the cache must be
flushed between the cached memory write and the MMIO access if the two are in
any way dependent.


=========================
THE THINGS CPUS GET UP TO
=========================

A programmer might take it for granted that the CPU will perform memory
operations in exactly the order specified, so that if the CPU is, for example,
given the following piece of code to execute:

	a = ACCESS_ONCE(*A);
	ACCESS_ONCE(*B) = b;
	c = ACCESS_ONCE(*C);
	d = ACCESS_ONCE(*D);
	ACCESS_ONCE(*E) = e;

they would then expect that the CPU will complete the memory operation for each
instruction before moving on to the next one, leading to a definite sequence of
operations as seen by external observers in the system:

	LOAD *A, STORE *B, LOAD *C, LOAD *D, STORE *E.


Reality is, of course, much messier.  With many CPUs and compilers, the above
assumption doesn't hold because:

 (*) loads are more likely to need to be completed immediately to permit
     execution progress, whereas stores can often be deferred without a
     problem;

 (*) loads may be done speculatively, and the result discarded should it prove
     to have been unnecessary;

 (*) loads may be done speculatively, leading to the result having been fetched
     at the wrong time in the expected sequence of events;

 (*) the order of the memory accesses may be rearranged to promote better use
     of the CPU buses and caches;

 (*) loads and stores may be combined to improve performance when talking to
     memory or I/O hardware that can do batched accesses of adjacent locations,
     thus cutting down on transaction setup costs (memory and PCI devices may
     both be able to do this); and

 (*) the CPU's data cache may affect the ordering, and whilst cache-coherency
     mechanisms may alleviate this - once the store has actually hit the cache
     - there's no guarantee that the coherency management will be propagated in
     order to other CPUs.

So what another CPU, say, might actually observe from the above piece of code
is:

	LOAD *A, ..., LOAD {*C,*D}, STORE *E, STORE *B

	(Where "LOAD {*C,*D}" is a combined load)


However, it is guaranteed that a CPU will be self-consistent: it will see its
_own_ accesses appear to be correctly ordered, without the need for a memory
barrier.  For instance with the following code:

	U = ACCESS_ONCE(*A);
	ACCESS_ONCE(*A) = V;
	ACCESS_ONCE(*A) = W;
	X = ACCESS_ONCE(*A);
	ACCESS_ONCE(*A) = Y;
	Z = ACCESS_ONCE(*A);

and assuming no intervention by an external influence, it can be assumed that
the final result will appear to be:

	U == the original value of *A
	X == W
	Z == Y
	*A == Y

The code above may cause the CPU to generate the full sequence of memory
accesses:

	U=LOAD *A, STORE *A=V, STORE *A=W, X=LOAD *A, STORE *A=Y, Z=LOAD *A

in that order, but, without intervention, the sequence may have almost any
combination of elements combined or discarded, provided the program's view of
the world remains consistent.  Note that ACCESS_ONCE() is -not- optional
in the above example, as there are architectures where a given CPU might
reorder successive loads to the same location.  On such architectures,
ACCESS_ONCE() does whatever is necessary to prevent this, for example, on
Itanium the volatile casts used by ACCESS_ONCE() cause GCC to emit the
special ld.acq and st.rel instructions that prevent such reordering.

The compiler may also combine, discard or defer elements of the sequence before
the CPU even sees them.

For instance:

	*A = V;
	*A = W;

may be reduced to:

	*A = W;

since, without either a write barrier or an ACCESS_ONCE(), it can be
assumed that the effect of the storage of V to *A is lost.  Similarly:

	*A = Y;
	Z = *A;

may, without a memory barrier or an ACCESS_ONCE(), be reduced to:

	*A = Y;
	Z = Y;

and the LOAD operation never appear outside of the CPU.


AND THEN THERE'S THE ALPHA
--------------------------

The DEC Alpha CPU is one of the most relaxed CPUs there is.  Not only that,
some versions of the Alpha CPU have a split data cache, permitting them to have
two semantically-related cache lines updated at separate times.  This is where
the data dependency barrier really becomes necessary as this synchronises both
caches with the memory coherence system, thus making it seem like pointer
changes vs new data occur in the right order.

The Alpha defines the Linux kernel's memory barrier model.

See the subsection on "Cache Coherency" above.


============
EXAMPLE USES
============

CIRCULAR BUFFERS
----------------

Memory barriers can be used to implement circular buffering without the need
of a lock to serialise the producer with the consumer.  See:

	Documentation/circular-buffers.txt

for details.


==========
REFERENCES
==========

Alpha AXP Architecture Reference Manual, Second Edition (Sites & Witek,
Digital Press)
	Chapter 5.2: Physical Address Space Characteristics
	Chapter 5.4: Caches and Write Buffers
	Chapter 5.5: Data Sharing
	Chapter 5.6: Read/Write Ordering

AMD64 Architecture Programmer's Manual Volume 2: System Programming
	Chapter 7.1: Memory-Access Ordering
	Chapter 7.4: Buffering and Combining Memory Writes

IA-32 Intel Architecture Software Developer's Manual, Volume 3:
System Programming Guide
	Chapter 7.1: Locked Atomic Operations
	Chapter 7.2: Memory Ordering
	Chapter 7.4: Serializing Instructions

The SPARC Architecture Manual, Version 9
	Chapter 8: Memory Models
	Appendix D: Formal Specification of the Memory Models
	Appendix J: Programming with the Memory Models

UltraSPARC Programmer Reference Manual
	Chapter 5: Memory Accesses and Cacheability
	Chapter 15: Sparc-V9 Memory Models

UltraSPARC III Cu User's Manual
	Chapter 9: Memory Models

UltraSPARC IIIi Processor User's Manual
	Chapter 8: Memory Models

UltraSPARC Architecture 2005
	Chapter 9: Memory
	Appendix D: Formal Specifications of the Memory Models

UltraSPARC T1 Supplement to the UltraSPARC Architecture 2005
	Chapter 8: Memory Models
	Appendix F: Caches and Cache Coherency

Solaris Internals, Core Kernel Architecture, p63-68:
	Chapter 3.3: Hardware Considerations for Locks and
			Synchronization

Unix Systems for Modern Architectures, Symmetric Multiprocessing and Caching
for Kernel Programmers:
	Chapter 13: Other Memory Models

Intel Itanium Architecture Software Developer's Manual: Volume 1:
	Section 2.6: Speculation
	Section 4.4: Memory Access
==============
Memory Hotplug
==============

Created:					Jul 28 2007
Add description of notifier of memory hotplug	Oct 11 2007

This document is about memory hotplug including how-to-use and current status.
Because Memory Hotplug is still under development, contents of this text will
be changed often.

1. Introduction
  1.1 purpose of memory hotplug
  1.2. Phases of memory hotplug
  1.3. Unit of Memory online/offline operation
2. Kernel Configuration
3. sysfs files for memory hotplug
4. Physical memory hot-add phase
  4.1 Hardware(Firmware) Support
  4.2 Notify memory hot-add event by hand
5. Logical Memory hot-add phase
  5.1. State of memory
  5.2. How to online memory
6. Logical memory remove
  6.1 Memory offline and ZONE_MOVABLE
  6.2. How to offline memory
7. Physical memory remove
8. Memory hotplug event notifier
9. Future Work List

Note(1): x86_64's has special implementation for memory hotplug.
         This text does not describe it.
Note(2): This text assumes that sysfs is mounted at /sys.


---------------
1. Introduction
---------------

1.1 purpose of memory hotplug
------------
Memory Hotplug allows users to increase/decrease the amount of memory.
Generally, there are two purposes.

(A) For changing the amount of memory.
    This is to allow a feature like capacity on demand.
(B) For installing/removing DIMMs or NUMA-nodes physically.
    This is to exchange DIMMs/NUMA-nodes, reduce power consumption, etc.

(A) is required by highly virtualized environments and (B) is required by
hardware which supports memory power management.

Linux memory hotplug is designed for both purpose.


1.2. Phases of memory hotplug
---------------
There are 2 phases in Memory Hotplug.
  1) Physical Memory Hotplug phase
  2) Logical Memory Hotplug phase.

The First phase is to communicate hardware/firmware and make/erase
environment for hotplugged memory. Basically, this phase is necessary
for the purpose (B), but this is good phase for communication between
highly virtualized environments too.

When memory is hotplugged, the kernel recognizes new memory, makes new memory
management tables, and makes sysfs files for new memory's operation.

If firmware supports notification of connection of new memory to OS,
this phase is triggered automatically. ACPI can notify this event. If not,
"probe" operation by system administration is used instead.
(see Section 4.).

Logical Memory Hotplug phase is to change memory state into
available/unavailable for users. Amount of memory from user's view is
changed by this phase. The kernel makes all memory in it as free pages
when a memory range is available.

In this document, this phase is described as online/offline.

Logical Memory Hotplug phase is triggered by write of sysfs file by system
administrator. For the hot-add case, it must be executed after Physical Hotplug
phase by hand.
(However, if you writes udev's hotplug scripts for memory hotplug, these
 phases can be execute in seamless way.)


1.3. Unit of Memory online/offline operation
------------
Memory hotplug uses SPARSEMEM memory model which allows memory to be divided
into chunks of the same size. These chunks are called "sections". The size of
a memory section is architecture dependent. For example, power uses 16MiB, ia64
uses 1GiB.

Memory sections are combined into chunks referred to as "memory blocks". The
size of a memory block is architecture dependent and represents the logical
unit upon which memory online/offline operations are to be performed. The
default size of a memory block is the same as memory section size unless an
architecture specifies otherwise. (see Section 3.)

To determine the size (in bytes) of a memory block please read this file:

/sys/devices/system/memory/block_size_bytes


-----------------------
2. Kernel Configuration
-----------------------
To use memory hotplug feature, kernel must be compiled with following
config options.

- For all memory hotplug
    Memory model -> Sparse Memory  (CONFIG_SPARSEMEM)
    Allow for memory hot-add       (CONFIG_MEMORY_HOTPLUG)

- To enable memory removal, the followings are also necessary
    Allow for memory hot remove    (CONFIG_MEMORY_HOTREMOVE)
    Page Migration                 (CONFIG_MIGRATION)

- For ACPI memory hotplug, the followings are also necessary
    Memory hotplug (under ACPI Support menu) (CONFIG_ACPI_HOTPLUG_MEMORY)
    This option can be kernel module.

- As a related configuration, if your box has a feature of NUMA-node hotplug
  via ACPI, then this option is necessary too.
    ACPI0004,PNP0A05 and PNP0A06 Container Driver (under ACPI Support menu)
    (CONFIG_ACPI_CONTAINER).
    This option can be kernel module too.


--------------------------------
3 sysfs files for memory hotplug
--------------------------------
All memory blocks have their device information in sysfs.  Each memory block
is described under /sys/devices/system/memory as

/sys/devices/system/memory/memoryXXX
(XXX is the memory block id.)

For the memory block covered by the sysfs directory.  It is expected that all
memory sections in this range are present and no memory holes exist in the
range. Currently there is no way to determine if there is a memory hole, but
the existence of one should not affect the hotplug capabilities of the memory
block.

For example, assume 1GiB memory block size. A device for a memory starting at
0x100000000 is /sys/device/system/memory/memory4
(0x100000000 / 1Gib = 4)
This device covers address range [0x100000000 ... 0x140000000)

Under each memory block, you can see 4 files:

/sys/devices/system/memory/memoryXXX/phys_index
/sys/devices/system/memory/memoryXXX/phys_device
/sys/devices/system/memory/memoryXXX/state
/sys/devices/system/memory/memoryXXX/removable

'phys_index'      : read-only and contains memory block id, same as XXX.
'state'           : read-write
                    at read:  contains online/offline state of memory.
                    at write: user can specify "online_kernel",
                    "online_movable", "online", "offline" command
                    which will be performed on all sections in the block.
'phys_device'     : read-only: designed to show the name of physical memory
                    device.  This is not well implemented now.
'removable'       : read-only: contains an integer value indicating
                    whether the memory block is removable or not
                    removable.  A value of 1 indicates that the memory
                    block is removable and a value of 0 indicates that
                    it is not removable. A memory block is removable only if
                    every section in the block is removable.

NOTE:
  These directories/files appear after physical memory hotplug phase.

If CONFIG_NUMA is enabled the memoryXXX/ directories can also be accessed
via symbolic links located in the /sys/devices/system/node/node* directories.

For example:
/sys/devices/system/node/node0/memory9 -> ../../memory/memory9

A backlink will also be created:
/sys/devices/system/memory/memory9/node0 -> ../../node/node0


--------------------------------
4. Physical memory hot-add phase
--------------------------------

4.1 Hardware(Firmware) Support
------------
On x86_64/ia64 platform, memory hotplug by ACPI is supported.

In general, the firmware (ACPI) which supports memory hotplug defines
memory class object of _HID "PNP0C80". When a notify is asserted to PNP0C80,
Linux's ACPI handler does hot-add memory to the system and calls a hotplug udev
script. This will be done automatically.

But scripts for memory hotplug are not contained in generic udev package(now).
You may have to write it by yourself or online/offline memory by hand.
Please see "How to online memory", "How to offline memory" in this text.

If firmware supports NUMA-node hotplug, and defines an object _HID "ACPI0004",
"PNP0A05", or "PNP0A06", notification is asserted to it, and ACPI handler
calls hotplug code for all of objects which are defined in it.
If memory device is found, memory hotplug code will be called.


4.2 Notify memory hot-add event by hand
------------
On powerpc, the firmware does not notify a memory hotplug event to the kernel.
Therefore, "probe" interface is supported to notify the event to the kernel.
This interface depends on CONFIG_ARCH_MEMORY_PROBE.

CONFIG_ARCH_MEMORY_PROBE is supported on powerpc only. On x86, this config
option is disabled by default since ACPI notifies a memory hotplug event to
the kernel, which performs its hotplug operation as the result. Please
enable this option if you need the "probe" interface for testing purposes
on x86.

Probe interface is located at
/sys/devices/system/memory/probe

You can tell the physical address of new memory to the kernel by

% echo start_address_of_new_memory > /sys/devices/system/memory/probe

Then, [start_address_of_new_memory, start_address_of_new_memory +
memory_block_size] memory range is hot-added. In this case, hotplug script is
not called (in current implementation). You'll have to online memory by
yourself.  Please see "How to online memory" in this text.


------------------------------
5. Logical Memory hot-add phase
------------------------------

5.1. State of memory
------------
To see (online/offline) state of a memory block, read 'state' file.

% cat /sys/device/system/memory/memoryXXX/state


If the memory block is online, you'll read "online".
If the memory block is offline, you'll read "offline".


5.2. How to online memory
------------
Even if the memory is hot-added, it is not at ready-to-use state.
For using newly added memory, you have to "online" the memory block.

For onlining, you have to write "online" to the memory block's state file as:

% echo online > /sys/devices/system/memory/memoryXXX/state

This onlining will not change the ZONE type of the target memory block,
If the memory block is in ZONE_NORMAL, you can change it to ZONE_MOVABLE:

% echo online_movable > /sys/devices/system/memory/memoryXXX/state
(NOTE: current limit: this memory block must be adjacent to ZONE_MOVABLE)

And if the memory block is in ZONE_MOVABLE, you can change it to ZONE_NORMAL:

% echo online_kernel > /sys/devices/system/memory/memoryXXX/state
(NOTE: current limit: this memory block must be adjacent to ZONE_NORMAL)

After this, memory block XXX's state will be 'online' and the amount of
available memory will be increased.

Currently, newly added memory is added as ZONE_NORMAL (for powerpc, ZONE_DMA).
This may be changed in future.



------------------------
6. Logical memory remove
------------------------

6.1 Memory offline and ZONE_MOVABLE
------------
Memory offlining is more complicated than memory online. Because memory offline
has to make the whole memory block be unused, memory offline can fail if
the memory block includes memory which cannot be freed.

In general, memory offline can use 2 techniques.

(1) reclaim and free all memory in the memory block.
(2) migrate all pages in the memory block.

In the current implementation, Linux's memory offline uses method (2), freeing
all  pages in the memory block by page migration. But not all pages are
migratable. Under current Linux, migratable pages are anonymous pages and
page caches. For offlining a memory block by migration, the kernel has to
guarantee that the memory block contains only migratable pages.

Now, a boot option for making a memory block which consists of migratable pages
is supported. By specifying "kernelcore=" or "movablecore=" boot option, you can
create ZONE_MOVABLE...a zone which is just used for movable pages.
(See also Documentation/kernel-parameters.txt)

Assume the system has "TOTAL" amount of memory at boot time, this boot option
creates ZONE_MOVABLE as following.

1) When kernelcore=YYYY boot option is used,
  Size of memory not for movable pages (not for offline) is YYYY.
  Size of memory for movable pages (for offline) is TOTAL-YYYY.

2) When movablecore=ZZZZ boot option is used,
  Size of memory not for movable pages (not for offline) is TOTAL - ZZZZ.
  Size of memory for movable pages (for offline) is ZZZZ.


Note: Unfortunately, there is no information to show which memory block belongs
to ZONE_MOVABLE. This is TBD.


6.2. How to offline memory
------------
You can offline a memory block by using the same sysfs interface that was used
in memory onlining.

% echo offline > /sys/devices/system/memory/memoryXXX/state

If offline succeeds, the state of the memory block is changed to be "offline".
If it fails, some error core (like -EBUSY) will be returned by the kernel.
Even if a memory block does not belong to ZONE_MOVABLE, you can try to offline
it.  If it doesn't contain 'unmovable' memory, you'll get success.

A memory block under ZONE_MOVABLE is considered to be able to be offlined
easily.  But under some busy state, it may return -EBUSY. Even if a memory
block cannot be offlined due to -EBUSY, you can retry offlining it and may be
able to offline it (or not). (For example, a page is referred to by some kernel
internal call and released soon.)

Consideration:
Memory hotplug's design direction is to make the possibility of memory offlining
higher and to guarantee unplugging memory under any situation. But it needs
more work. Returning -EBUSY under some situation may be good because the user
can decide to retry more or not by himself. Currently, memory offlining code
does some amount of retry with 120 seconds timeout.

-------------------------
7. Physical memory remove
-------------------------
Need more implementation yet....
 - Notification completion of remove works by OS to firmware.
 - Guard from remove if not yet.

--------------------------------
8. Memory hotplug event notifier
--------------------------------
Memory hotplug has event notifier. There are 6 types of notification.

MEMORY_GOING_ONLINE
  Generated before new memory becomes available in order to be able to
  prepare subsystems to handle memory. The page allocator is still unable
  to allocate from the new memory.

MEMORY_CANCEL_ONLINE
  Generated if MEMORY_GOING_ONLINE fails.

MEMORY_ONLINE
  Generated when memory has successfully brought online. The callback may
  allocate pages from the new memory.

MEMORY_GOING_OFFLINE
  Generated to begin the process of offlining memory. Allocations are no
  longer possible from the memory but some of the memory to be offlined
  is still in use. The callback can be used to free memory known to a
  subsystem from the indicated memory block.

MEMORY_CANCEL_OFFLINE
  Generated if MEMORY_GOING_OFFLINE fails. Memory is available again from
  the memory block that we attempted to offline.

MEMORY_OFFLINE
  Generated after offlining memory is complete.

A callback routine can be registered by
  hotplug_memory_notifier(callback_func, priority)

The second argument of callback function (action) is event types of above.
The third argument is passed by pointer of struct memory_notify.

struct memory_notify {
       unsigned long start_pfn;
       unsigned long nr_pages;
       int status_change_nid_normal;
       int status_change_nid_high;
       int status_change_nid;
}

start_pfn is start_pfn of online/offline memory.
nr_pages is # of pages of online/offline memory.
status_change_nid_normal is set node id when N_NORMAL_MEMORY of nodemask
is (will be) set/clear, if this is -1, then nodemask status is not changed.
status_change_nid_high is set node id when N_HIGH_MEMORY of nodemask
is (will be) set/clear, if this is -1, then nodemask status is not changed.
status_change_nid is set node id when N_MEMORY of nodemask is (will be)
set/clear. It means a new(memoryless) node gets new memory by online and a
node loses all memory. If this is -1, then nodemask status is not changed.
If status_changed_nid* >= 0, callback should create/discard structures for the
node if necessary.

--------------
9. Future Work
--------------
  - allowing memory hot-add to ZONE_MOVABLE. maybe we need some switch like
    sysctl or new control file.
  - showing memory block and physical device relationship.
  - showing memory block is under ZONE_MOVABLE or not
  - test and make it better memory offlining.
  - support HugeTLB page migration and offlining.
  - memmap removing at memory offline.
  - physical remove memory.

			==============================
			KERNEL MODULE SIGNING FACILITY
			==============================

CONTENTS

 - Overview.
 - Configuring module signing.
 - Generating signing keys.
 - Public keys in the kernel.
 - Manually signing modules.
 - Signed modules and stripping.
 - Loading signed modules.
 - Non-valid signatures and unsigned modules.
 - Administering/protecting the private key.


========
OVERVIEW
========

The kernel module signing facility cryptographically signs modules during
installation and then checks the signature upon loading the module.  This
allows increased kernel security by disallowing the loading of unsigned modules
or modules signed with an invalid key.  Module signing increases security by
making it harder to load a malicious module into the kernel.  The module
signature checking is done by the kernel so that it is not necessary to have
trusted userspace bits.

This facility uses X.509 ITU-T standard certificates to encode the public keys
involved.  The signatures are not themselves encoded in any industrial standard
type.  The facility currently only supports the RSA public key encryption
standard (though it is pluggable and permits others to be used).  The possible
hash algorithms that can be used are SHA-1, SHA-224, SHA-256, SHA-384, and
SHA-512 (the algorithm is selected by data in the signature).


==========================
CONFIGURING MODULE SIGNING
==========================

The module signing facility is enabled by going to the "Enable Loadable Module
Support" section of the kernel configuration and turning on

	CONFIG_MODULE_SIG	"Module signature verification"

This has a number of options available:

 (1) "Require modules to be validly signed" (CONFIG_MODULE_SIG_FORCE)

     This specifies how the kernel should deal with a module that has a
     signature for which the key is not known or a module that is unsigned.

     If this is off (ie. "permissive"), then modules for which the key is not
     available and modules that are unsigned are permitted, but the kernel will
     be marked as being tainted, and the concerned modules will be marked as
     tainted, shown with the character 'E'.

     If this is on (ie. "restrictive"), only modules that have a valid
     signature that can be verified by a public key in the kernel's possession
     will be loaded.  All other modules will generate an error.

     Irrespective of the setting here, if the module has a signature block that
     cannot be parsed, it will be rejected out of hand.


 (2) "Automatically sign all modules" (CONFIG_MODULE_SIG_ALL)

     If this is on then modules will be automatically signed during the
     modules_install phase of a build.  If this is off, then the modules must
     be signed manually using:

	scripts/sign-file


 (3) "Which hash algorithm should modules be signed with?"

     This presents a choice of which hash algorithm the installation phase will
     sign the modules with:

	CONFIG_MODULE_SIG_SHA1		"Sign modules with SHA-1"
	CONFIG_MODULE_SIG_SHA224	"Sign modules with SHA-224"
	CONFIG_MODULE_SIG_SHA256	"Sign modules with SHA-256"
	CONFIG_MODULE_SIG_SHA384	"Sign modules with SHA-384"
	CONFIG_MODULE_SIG_SHA512	"Sign modules with SHA-512"

     The algorithm selected here will also be built into the kernel (rather
     than being a module) so that modules signed with that algorithm can have
     their signatures checked without causing a dependency loop.


=======================
GENERATING SIGNING KEYS
=======================

Cryptographic keypairs are required to generate and check signatures.  A
private key is used to generate a signature and the corresponding public key is
used to check it.  The private key is only needed during the build, after which
it can be deleted or stored securely.  The public key gets built into the
kernel so that it can be used to check the signatures as the modules are
loaded.

Under normal conditions, the kernel build will automatically generate a new
keypair using openssl if one does not exist in the files:

	signing_key.priv
	signing_key.x509

during the building of vmlinux (the public part of the key needs to be built
into vmlinux) using parameters in the:

	x509.genkey

file (which is also generated if it does not already exist).

It is strongly recommended that you provide your own x509.genkey file.

Most notably, in the x509.genkey file, the req_distinguished_name section
should be altered from the default:

	[ req_distinguished_name ]
	O = Magrathea
	CN = Glacier signing key
	emailAddress = slartibartfast@magrathea.h2g2

The generated RSA key size can also be set with:

	[ req ]
	default_bits = 4096


It is also possible to manually generate the key private/public files using the
x509.genkey key generation configuration file in the root node of the Linux
kernel sources tree and the openssl command.  The following is an example to
generate the public/private key files:

	openssl req -new -nodes -utf8 -sha256 -days 36500 -batch -x509 \
	   -config x509.genkey -outform DER -out signing_key.x509 \
	   -keyout signing_key.priv


=========================
PUBLIC KEYS IN THE KERNEL
=========================

The kernel contains a ring of public keys that can be viewed by root.  They're
in a keyring called ".system_keyring" that can be seen by:

	[root@deneb ~]# cat /proc/keys
	...
	223c7853 I------     1 perm 1f030000     0     0 keyring   .system_keyring: 1
	302d2d52 I------     1 perm 1f010000     0     0 asymmetri Fedora kernel signing key: d69a84e6bce3d216b979e9505b3e3ef9a7118079: X509.RSA a7118079 []
	...

Beyond the public key generated specifically for module signing, any file
placed in the kernel source root directory or the kernel build root directory
whose name is suffixed with ".x509" will be assumed to be an X.509 public key
and will be added to the keyring.

Further, the architecture code may take public keys from a hardware store and
add those in also (e.g. from the UEFI key database).

Finally, it is possible to add additional public keys by doing:

	keyctl padd asymmetric "" [.system_keyring-ID] <[key-file]

e.g.:

	keyctl padd asymmetric "" 0x223c7853 <my_public_key.x509

Note, however, that the kernel will only permit keys to be added to
.system_keyring _if_ the new key's X.509 wrapper is validly signed by a key
that is already resident in the .system_keyring at the time the key was added.


=========================
MANUALLY SIGNING MODULES
=========================

To manually sign a module, use the scripts/sign-file tool available in
the Linux kernel source tree.  The script requires 4 arguments:

	1.  The hash algorithm (e.g., sha256)
	2.  The private key filename
	3.  The public key filename
	4.  The kernel module to be signed

The following is an example to sign a kernel module:

	scripts/sign-file sha512 kernel-signkey.priv \
		kernel-signkey.x509 module.ko

The hash algorithm used does not have to match the one configured, but if it
doesn't, you should make sure that hash algorithm is either built into the
kernel or can be loaded without requiring itself.


============================
SIGNED MODULES AND STRIPPING
============================

A signed module has a digital signature simply appended at the end.  The string
"~Module signature appended~." at the end of the module's file confirms that a
signature is present but it does not confirm that the signature is valid!

Signed modules are BRITTLE as the signature is outside of the defined ELF
container.  Thus they MAY NOT be stripped once the signature is computed and
attached.  Note the entire module is the signed payload, including any and all
debug information present at the time of signing.


======================
LOADING SIGNED MODULES
======================

Modules are loaded with insmod, modprobe, init_module() or finit_module(),
exactly as for unsigned modules as no processing is done in userspace.  The
signature checking is all done within the kernel.


=========================================
NON-VALID SIGNATURES AND UNSIGNED MODULES
=========================================

If CONFIG_MODULE_SIG_FORCE is enabled or enforcemodulesig=1 is supplied on
the kernel command line, the kernel will only load validly signed modules
for which it has a public key.   Otherwise, it will also load modules that are
unsigned.   Any module for which the kernel has a key, but which proves to have
a signature mismatch will not be permitted to load.

Any module that has an unparseable signature will be rejected.


=========================================
ADMINISTERING/PROTECTING THE PRIVATE KEY
=========================================

Since the private key is used to sign modules, viruses and malware could use
the private key to sign modules and compromise the operating system.  The
private key must be either destroyed or moved to a secure location and not kept
in the root node of the kernel source tree.
               Mono(tm) Binary Kernel Support for Linux
               -----------------------------------------

To configure Linux to automatically execute Mono-based .NET binaries
(in the form of .exe files) without the need to use the mono CLR
wrapper, you can use the BINFMT_MISC kernel support.

This will allow you to execute Mono-based .NET binaries just like any
other program after you have done the following:

1) You MUST FIRST install the Mono CLR support, either by downloading
   a binary package, a source tarball or by installing from CVS. Binary
   packages for several distributions can be found at:

	http://go-mono.com/download.html

   Instructions for compiling Mono can be found at:

	http://www.go-mono.com/compiling.html

   Once the Mono CLR support has been installed, just check that
   /usr/bin/mono (which could be located elsewhere, for example
   /usr/local/bin/mono) is working.

2) You have to compile BINFMT_MISC either as a module or into
   the kernel (CONFIG_BINFMT_MISC) and set it up properly.
   If you choose to compile it as a module, you will have
   to insert it manually with modprobe/insmod, as kmod
   cannot be easily supported with binfmt_misc. 
   Read the file 'binfmt_misc.txt' in this directory to know
   more about the configuration process.

3) Add the following entries to /etc/rc.local or similar script
   to be run at system startup:

# Insert BINFMT_MISC module into the kernel
if [ ! -e /proc/sys/fs/binfmt_misc/register ]; then
        /sbin/modprobe binfmt_misc
	# Some distributions, like Fedora Core, perform
	# the following command automatically when the
	# binfmt_misc module is loaded into the kernel
	# or during normal boot up (systemd-based systems).
	# Thus, it is possible that the following line
	# is not needed at all.
	mount -t binfmt_misc none /proc/sys/fs/binfmt_misc
fi

# Register support for .NET CLR binaries
if [ -e /proc/sys/fs/binfmt_misc/register ]; then
	# Replace /usr/bin/mono with the correct pathname to
	# the Mono CLR runtime (usually /usr/local/bin/mono
	# when compiling from sources or CVS).
        echo ':CLR:M::MZ::/usr/bin/mono:' > /proc/sys/fs/binfmt_misc/register
else
        echo "No binfmt_misc support"
        exit 1
fi

4) Check that .exe binaries can be ran without the need of a
   wrapper script, simply by launching the .exe file directly
   from a command prompt, for example:

	/usr/bin/xsd.exe

   NOTE: If this fails with a permission denied error, check
         that the .exe file has execute permissions.
Generic Mutex Subsystem

started by Ingo Molnar <mingo@redhat.com>
updated by Davidlohr Bueso <davidlohr@hp.com>

What are mutexes?
-----------------

In the Linux kernel, mutexes refer to a particular locking primitive
that enforces serialization on shared memory systems, and not only to
the generic term referring to 'mutual exclusion' found in academia
or similar theoretical text books. Mutexes are sleeping locks which
behave similarly to binary semaphores, and were introduced in 2006[1]
as an alternative to these. This new data structure provided a number
of advantages, including simpler interfaces, and at that time smaller
code (see Disadvantages).

[1] http://lwn.net/Articles/164802/

Implementation
--------------

Mutexes are represented by 'struct mutex', defined in include/linux/mutex.h
and implemented in kernel/locking/mutex.c. These locks use a three
state atomic counter (->count) to represent the different possible
transitions that can occur during the lifetime of a lock:

	  1: unlocked
	  0: locked, no waiters
   negative: locked, with potential waiters

In its most basic form it also includes a wait-queue and a spinlock
that serializes access to it. CONFIG_SMP systems can also include
a pointer to the lock task owner (->owner) as well as a spinner MCS
lock (->osq), both described below in (ii).

When acquiring a mutex, there are three possible paths that can be
taken, depending on the state of the lock:

(i) fastpath: tries to atomically acquire the lock by decrementing the
    counter. If it was already taken by another task it goes to the next
    possible path. This logic is architecture specific. On x86-64, the
    locking fastpath is 2 instructions:

    0000000000000e10 <mutex_lock>:
    e21:   f0 ff 0b                lock decl (%rbx)
    e24:   79 08                   jns    e2e <mutex_lock+0x1e>

   the unlocking fastpath is equally tight:

    0000000000000bc0 <mutex_unlock>:
    bc8:   f0 ff 07                lock incl (%rdi)
    bcb:   7f 0a                   jg     bd7 <mutex_unlock+0x17>


(ii) midpath: aka optimistic spinning, tries to spin for acquisition
     while the lock owner is running and there are no other tasks ready
     to run that have higher priority (need_resched). The rationale is
     that if the lock owner is running, it is likely to release the lock
     soon. The mutex spinners are queued up using MCS lock so that only
     one spinner can compete for the mutex.

     The MCS lock (proposed by Mellor-Crummey and Scott) is a simple spinlock
     with the desirable properties of being fair and with each cpu trying
     to acquire the lock spinning on a local variable. It avoids expensive
     cacheline bouncing that common test-and-set spinlock implementations
     incur. An MCS-like lock is specially tailored for optimistic spinning
     for sleeping lock implementation. An important feature of the customized
     MCS lock is that it has the extra property that spinners are able to exit
     the MCS spinlock queue when they need to reschedule. This further helps
     avoid situations where MCS spinners that need to reschedule would continue
     waiting to spin on mutex owner, only to go directly to slowpath upon
     obtaining the MCS lock.


(iii) slowpath: last resort, if the lock is still unable to be acquired,
      the task is added to the wait-queue and sleeps until woken up by the
      unlock path. Under normal circumstances it blocks as TASK_UNINTERRUPTIBLE.

While formally kernel mutexes are sleepable locks, it is path (ii) that
makes them more practically a hybrid type. By simply not interrupting a
task and busy-waiting for a few cycles instead of immediately sleeping,
the performance of this lock has been seen to significantly improve a
number of workloads. Note that this technique is also used for rw-semaphores.

Semantics
---------

The mutex subsystem checks and enforces the following rules:

    - Only one task can hold the mutex at a time.
    - Only the owner can unlock the mutex.
    - Multiple unlocks are not permitted.
    - Recursive locking/unlocking is not permitted.
    - A mutex must only be initialized via the API (see below).
    - A task may not exit with a mutex held.
    - Memory areas where held locks reside must not be freed.
    - Held mutexes must not be reinitialized.
    - Mutexes may not be used in hardware or software interrupt
      contexts such as tasklets and timers.

These semantics are fully enforced when CONFIG DEBUG_MUTEXES is enabled.
In addition, the mutex debugging code also implements a number of other
features that make lock debugging easier and faster:

    - Uses symbolic names of mutexes, whenever they are printed
      in debug output.
    - Point-of-acquire tracking, symbolic lookup of function names,
      list of all locks held in the system, printout of them.
    - Owner tracking.
    - Detects self-recursing locks and prints out all relevant info.
    - Detects multi-task circular deadlocks and prints out all affected
      locks and tasks (and only those tasks).


Interfaces
----------
Statically define the mutex:
   DEFINE_MUTEX(name);

Dynamically initialize the mutex:
   mutex_init(mutex);

Acquire the mutex, uninterruptible:
   void mutex_lock(struct mutex *lock);
   void mutex_lock_nested(struct mutex *lock, unsigned int subclass);
   int  mutex_trylock(struct mutex *lock);

Acquire the mutex, interruptible:
   int mutex_lock_interruptible_nested(struct mutex *lock,
				       unsigned int subclass);
   int mutex_lock_interruptible(struct mutex *lock);

Acquire the mutex, interruptible, if dec to 0:
   int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);

Unlock the mutex:
   void mutex_unlock(struct mutex *lock);

Test if the mutex is taken:
   int mutex_is_locked(struct mutex *lock);

Disadvantages
-------------

Unlike its original design and purpose, 'struct mutex' is larger than
most locks in the kernel. E.g: on x86-64 it is 40 bytes, almost twice
as large as 'struct semaphore' (24 bytes) and 8 bytes shy of the
'struct rw_semaphore' variant. Larger structure sizes mean more CPU
cache and memory footprint.

When to use mutexes
-------------------

Unless the strict semantics of mutexes are unsuitable and/or the critical
region prevents the lock from being shared, always prefer them to any other
locking primitive.
			 =============================
			 NO-MMU MEMORY MAPPING SUPPORT
			 =============================

The kernel has limited support for memory mapping under no-MMU conditions, such
as are used in uClinux environments. From the userspace point of view, memory
mapping is made use of in conjunction with the mmap() system call, the shmat()
call and the execve() system call. From the kernel's point of view, execve()
mapping is actually performed by the binfmt drivers, which call back into the
mmap() routines to do the actual work.

Memory mapping behaviour also involves the way fork(), vfork(), clone() and
ptrace() work. Under uClinux there is no fork(), and clone() must be supplied
the CLONE_VM flag.

The behaviour is similar between the MMU and no-MMU cases, but not identical;
and it's also much more restricted in the latter case:

 (*) Anonymous mapping, MAP_PRIVATE

	In the MMU case: VM regions backed by arbitrary pages; copy-on-write
	across fork.

	In the no-MMU case: VM regions backed by arbitrary contiguous runs of
	pages.

 (*) Anonymous mapping, MAP_SHARED

	These behave very much like private mappings, except that they're
	shared across fork() or clone() without CLONE_VM in the MMU case. Since
	the no-MMU case doesn't support these, behaviour is identical to
	MAP_PRIVATE there.

 (*) File, MAP_PRIVATE, PROT_READ / PROT_EXEC, !PROT_WRITE

	In the MMU case: VM regions backed by pages read from file; changes to
	the underlying file are reflected in the mapping; copied across fork.

	In the no-MMU case:

         - If one exists, the kernel will re-use an existing mapping to the
           same segment of the same file if that has compatible permissions,
           even if this was created by another process.

         - If possible, the file mapping will be directly on the backing device
           if the backing device has the BDI_CAP_MAP_DIRECT capability and
           appropriate mapping protection capabilities. Ramfs, romfs, cramfs
           and mtd might all permit this.

	 - If the backing device device can't or won't permit direct sharing,
           but does have the BDI_CAP_MAP_COPY capability, then a copy of the
           appropriate bit of the file will be read into a contiguous bit of
           memory and any extraneous space beyond the EOF will be cleared

	 - Writes to the file do not affect the mapping; writes to the mapping
	   are visible in other processes (no MMU protection), but should not
	   happen.

 (*) File, MAP_PRIVATE, PROT_READ / PROT_EXEC, PROT_WRITE

	In the MMU case: like the non-PROT_WRITE case, except that the pages in
	question get copied before the write actually happens. From that point
	on writes to the file underneath that page no longer get reflected into
	the mapping's backing pages. The page is then backed by swap instead.

	In the no-MMU case: works much like the non-PROT_WRITE case, except
	that a copy is always taken and never shared.

 (*) Regular file / blockdev, MAP_SHARED, PROT_READ / PROT_EXEC / PROT_WRITE

	In the MMU case: VM regions backed by pages read from file; changes to
	pages written back to file; writes to file reflected into pages backing
	mapping; shared across fork.

	In the no-MMU case: not supported.

 (*) Memory backed regular file, MAP_SHARED, PROT_READ / PROT_EXEC / PROT_WRITE

	In the MMU case: As for ordinary regular files.

	In the no-MMU case: The filesystem providing the memory-backed file
	(such as ramfs or tmpfs) may choose to honour an open, truncate, mmap
	sequence by providing a contiguous sequence of pages to map. In that
	case, a shared-writable memory mapping will be possible. It will work
	as for the MMU case. If the filesystem does not provide any such
	support, then the mapping request will be denied.

 (*) Memory backed blockdev, MAP_SHARED, PROT_READ / PROT_EXEC / PROT_WRITE

	In the MMU case: As for ordinary regular files.

	In the no-MMU case: As for memory backed regular files, but the
	blockdev must be able to provide a contiguous run of pages without
	truncate being called. The ramdisk driver could do this if it allocated
	all its memory as a contiguous array upfront.

 (*) Memory backed chardev, MAP_SHARED, PROT_READ / PROT_EXEC / PROT_WRITE

	In the MMU case: As for ordinary regular files.

	In the no-MMU case: The character device driver may choose to honour
	the mmap() by providing direct access to the underlying device if it
	provides memory or quasi-memory that can be accessed directly. Examples
	of such are frame buffers and flash devices. If the driver does not
	provide any such support, then the mapping request will be denied.


============================
FURTHER NOTES ON NO-MMU MMAP
============================

 (*) A request for a private mapping of a file may return a buffer that is not
     page-aligned.  This is because XIP may take place, and the data may not be
     paged aligned in the backing store.

 (*) A request for an anonymous mapping will always be page aligned.  If
     possible the size of the request should be a power of two otherwise some
     of the space may be wasted as the kernel must allocate a power-of-2
     granule but will only discard the excess if appropriately configured as
     this has an effect on fragmentation.

 (*) The memory allocated by a request for an anonymous mapping will normally
     be cleared by the kernel before being returned in accordance with the
     Linux man pages (ver 2.22 or later).

     In the MMU case this can be achieved with reasonable performance as
     regions are backed by virtual pages, with the contents only being mapped
     to cleared physical pages when a write happens on that specific page
     (prior to which, the pages are effectively mapped to the global zero page
     from which reads can take place).  This spreads out the time it takes to
     initialize the contents of a page - depending on the write-usage of the
     mapping.

     In the no-MMU case, however, anonymous mappings are backed by physical
     pages, and the entire map is cleared at allocation time.  This can cause
     significant delays during a userspace malloc() as the C library does an
     anonymous mapping and the kernel then does a memset for the entire map.

     However, for memory that isn't required to be precleared - such as that
     returned by malloc() - mmap() can take a MAP_UNINITIALIZED flag to
     indicate to the kernel that it shouldn't bother clearing the memory before
     returning it.  Note that CONFIG_MMAP_ALLOW_UNINITIALIZED must be enabled
     to permit this, otherwise the flag will be ignored.

     uClibc uses this to speed up malloc(), and the ELF-FDPIC binfmt uses this
     to allocate the brk and stack region.

 (*) A list of all the private copy and anonymous mappings on the system is
     visible through /proc/maps in no-MMU mode.

 (*) A list of all the mappings in use by a process is visible through
     /proc/<pid>/maps in no-MMU mode.

 (*) Supplying MAP_FIXED or a requesting a particular mapping address will
     result in an error.

 (*) Files mapped privately usually have to have a read method provided by the
     driver or filesystem so that the contents can be read into the memory
     allocated if mmap() chooses not to map the backing device directly. An
     error will result if they don't. This is most likely to be encountered
     with character device files, pipes, fifos and sockets.


==========================
INTERPROCESS SHARED MEMORY
==========================

Both SYSV IPC SHM shared memory and POSIX shared memory is supported in NOMMU
mode.  The former through the usual mechanism, the latter through files created
on ramfs or tmpfs mounts.


=======
FUTEXES
=======

Futexes are supported in NOMMU mode if the arch supports them.  An error will
be given if an address passed to the futex system call lies outside the
mappings made by a process or if the mapping in which the address lies does not
support futexes (such as an I/O chardev mapping).


=============
NO-MMU MREMAP
=============

The mremap() function is partially supported.  It may change the size of a
mapping, and may move it[*] if MREMAP_MAYMOVE is specified and if the new size
of the mapping exceeds the size of the slab object currently occupied by the
memory to which the mapping refers, or if a smaller slab object could be used.

MREMAP_FIXED is not supported, though it is ignored if there's no change of
address and the object does not need to be moved.

Shared mappings may not be moved.  Shareable mappings may not be moved either,
even if they are not currently shared.

The mremap() function must be given an exact match for base address and size of
a previously mapped object.  It may not be used to create holes in existing
mappings, move parts of existing mappings or resize parts of mappings.  It must
act on a complete mapping.

[*] Not currently supported.


============================================
PROVIDING SHAREABLE CHARACTER DEVICE SUPPORT
============================================

To provide shareable character device support, a driver must provide a
file->f_op->get_unmapped_area() operation. The mmap() routines will call this
to get a proposed address for the mapping. This may return an error if it
doesn't wish to honour the mapping because it's too long, at a weird offset,
under some unsupported combination of flags or whatever.

The driver should also provide backing device information with capabilities set
to indicate the permitted types of mapping on such devices. The default is
assumed to be readable and writable, not executable, and only shareable
directly (can't be copied).

The file->f_op->mmap() operation will be called to actually inaugurate the
mapping. It can be rejected at that point. Returning the ENOSYS error will
cause the mapping to be copied instead if BDI_CAP_MAP_COPY is specified.

The vm_ops->close() routine will be invoked when the last mapping on a chardev
is removed. An existing mapping will be shared, partially or not, if possible
without notifying the driver.

It is permitted also for the file->f_op->get_unmapped_area() operation to
return -ENOSYS. This will be taken to mean that this operation just doesn't
want to handle it, despite the fact it's got an operation. For instance, it
might try directing the call to a secondary driver which turns out not to
implement it. Such is the case for the framebuffer driver which attempts to
direct the call to the device-specific driver. Under such circumstances, the
mapping request will be rejected if BDI_CAP_MAP_COPY is not specified, and a
copy mapped otherwise.

IMPORTANT NOTE:

	Some types of device may present a different appearance to anyone
	looking at them in certain modes. Flash chips can be like this; for
	instance if they're in programming or erase mode, you might see the
	status reflected in the mapping, instead of the data.

	In such a case, care must be taken lest userspace see a shared or a
	private mapping showing such information when the driver is busy
	controlling the device. Remember especially: private executable
	mappings may still be mapped directly off the device under some
	circumstances!


==============================================
PROVIDING SHAREABLE MEMORY-BACKED FILE SUPPORT
==============================================

Provision of shared mappings on memory backed files is similar to the provision
of support for shared mapped character devices. The main difference is that the
filesystem providing the service will probably allocate a contiguous collection
of pages and permit mappings to be made on that.

It is recommended that a truncate operation applied to such a file that
increases the file size, if that file is empty, be taken as a request to gather
enough pages to honour a mapping. This is required to support POSIX shared
memory.

Memory backed devices are indicated by the mapping's backing device info having
the memory_backed flag set.


========================================
PROVIDING SHAREABLE BLOCK DEVICE SUPPORT
========================================

Provision of shared mappings on block device files is exactly the same as for
character devices. If there isn't a real device underneath, then the driver
should allocate sufficient contiguous memory to honour any supported mapping.


=================================
ADJUSTING PAGE TRIMMING BEHAVIOUR
=================================

NOMMU mmap automatically rounds up to the nearest power-of-2 number of pages
when performing an allocation.  This can have adverse effects on memory
fragmentation, and as such, is left configurable.  The default behaviour is to
aggressively trim allocations and discard any excess pages back in to the page
allocator.  In order to retain finer-grained control over fragmentation, this
behaviour can either be disabled completely, or bumped up to a higher page
watermark where trimming begins.

Page trimming behaviour is configurable via the sysctl `vm.nr_trim_pages'.

Numa policy hit/miss statistics

/sys/devices/system/node/node*/numastat

All units are pages. Hugepages have separate counters.

numa_hit	A process wanted to allocate memory from this node,
		and succeeded.

numa_miss	A process wanted to allocate memory from another node,
		but ended up with memory from this node.

numa_foreign	A process wanted to allocate on this node,
		but ended up with memory from another one.

local_node	A process ran on this node and got memory from it.

other_node	A process ran on this node and got memory from another node.

interleave_hit 	Interleaving wanted to allocate from this node
		and succeeded.

For easier reading you can use the numastat utility from the numactl package
(http://oss.sgi.com/projects/libnuma/). Note that it only works
well right now on machines with a small number of CPUs.

NOTE: ksymoops is useless on 2.6.  Please use the Oops in its original format
(from dmesg, etc).  Ignore any references in this or other docs to "decoding
the Oops" or "running it through ksymoops".  If you post an Oops from 2.6 that
has been run through ksymoops, people will just tell you to repost it.

Quick Summary
-------------

Find the Oops and send it to the maintainer of the kernel area that seems to be
involved with the problem.  Don't worry too much about getting the wrong person.
If you are unsure send it to the person responsible for the code relevant to
what you were doing.  If it occurs repeatably try and describe how to recreate
it.  That's worth even more than the oops.

If you are totally stumped as to whom to send the report, send it to 
linux-kernel@vger.kernel.org. Thanks for your help in making Linux as
stable as humanly possible.

Where is the Oops?
----------------------

Normally the Oops text is read from the kernel buffers by klogd and
handed to syslogd which writes it to a syslog file, typically
/var/log/messages (depends on /etc/syslog.conf).  Sometimes klogd dies,
in which case you can run dmesg > file to read the data from the kernel
buffers and save it.  Or you can cat /proc/kmsg > file, however you
have to break in to stop the transfer, kmsg is a "never ending file".
If the machine has crashed so badly that you cannot enter commands or
the disk is not available then you have three options :-

(1) Hand copy the text from the screen and type it in after the machine
    has restarted.  Messy but it is the only option if you have not
    planned for a crash. Alternatively, you can take a picture of
    the screen with a digital camera - not nice, but better than
    nothing.  If the messages scroll off the top of the console, you
    may find that booting with a higher resolution (eg, vga=791)
    will allow you to read more of the text. (Caveat: This needs vesafb,
    so won't help for 'early' oopses)

(2) Boot with a serial console (see Documentation/serial-console.txt),
    run a null modem to a second machine and capture the output there
    using your favourite communication program.  Minicom works well.

(3) Use Kdump (see Documentation/kdump/kdump.txt),
    extract the kernel ring buffer from old memory with using dmesg
    gdbmacro in Documentation/kdump/gdbmacros.txt.


Full Information
----------------

NOTE: the message from Linus below applies to 2.4 kernel.  I have preserved it
for historical reasons, and because some of the information in it still
applies.  Especially, please ignore any references to ksymoops. 

From: Linus Torvalds <torvalds@osdl.org>

How to track down an Oops.. [originally a mail to linux-kernel]

The main trick is having 5 years of experience with those pesky oops 
messages ;-)

Actually, there are things you can do that make this easier. I have two 
separate approaches:

	gdb /usr/src/linux/vmlinux
	gdb> disassemble <offending_function>

That's the easy way to find the problem, at least if the bug-report is 
well made (like this one was - run through ksymoops to get the 
information of which function and the offset in the function that it 
happened in).

Oh, it helps if the report happens on a kernel that is compiled with the 
same compiler and similar setups.

The other thing to do is disassemble the "Code:" part of the bug report: 
ksymoops will do this too with the correct tools, but if you don't have
the tools you can just do a silly program:

	char str[] = "\xXX\xXX\xXX...";
	main(){}

and compile it with gcc -g and then do "disassemble str" (where the "XX" 
stuff are the values reported by the Oops - you can just cut-and-paste 
and do a replace of spaces to "\x" - that's what I do, as I'm too lazy 
to write a program to automate this all).

Alternatively, you can use the shell script in scripts/decodecode.
Its usage is:  decodecode < oops.txt

The hex bytes that follow "Code:" may (in some architectures) have a series
of bytes that precede the current instruction pointer as well as bytes at and
following the current instruction pointer.  In some cases, one instruction
byte or word is surrounded by <> or (), as in "<86>" or "(f00d)".  These
<> or () markings indicate the current instruction pointer.  Example from
i386, split into multiple lines for readability:

Code: f9 0f 8d f9 00 00 00 8d 42 0c e8 dd 26 11 c7 a1 60 ea 2b f9 8b 50 08 a1
64 ea 2b f9 8d 34 82 8b 1e 85 db 74 6d 8b 15 60 ea 2b f9 <8b> 43 04 39 42 54
7e 04 40 89 42 54 8b 43 04 3b 05 00 f6 52 c0

Finally, if you want to see where the code comes from, you can do

	cd /usr/src/linux
	make fs/buffer.s 	# or whatever file the bug happened in

and then you get a better idea of what happens than with the gdb 
disassembly.

Now, the trick is just then to combine all the data you have: the C 
sources (and general knowledge of what it _should_ do), the assembly 
listing and the code disassembly (and additionally the register dump you 
also get from the "oops" message - that can be useful to see _what_ the 
corrupted pointers were, and when you have the assembler listing you can 
also match the other registers to whatever C expressions they were used 
for).

Essentially, you just look at what doesn't match (in this case it was the 
"Code" disassembly that didn't match with what the compiler generated). 
Then you need to find out _why_ they don't match. Often it's simple - you 
see that the code uses a NULL pointer and then you look at the code and 
wonder how the NULL pointer got there, and if it's a valid thing to do 
you just check against it..

Now, if somebody gets the idea that this is time-consuming and requires 
some small amount of concentration, you're right. Which is why I will 
mostly just ignore any panic reports that don't have the symbol table 
info etc looked up: it simply gets too hard to look it up (I have some 
programs to search for specific patterns in the kernel code segment, and 
sometimes I have been able to look up those kinds of panics too, but 
that really requires pretty good knowledge of the kernel just to be able 
to pick out the right sequences etc..)

_Sometimes_ it happens that I just see the disassembled code sequence 
from the panic, and I know immediately where it's coming from. That's when 
I get worried that I've been doing this for too long ;-)

		Linus


---------------------------------------------------------------------------
Notes on Oops tracing with klogd:

In order to help Linus and the other kernel developers there has been
substantial support incorporated into klogd for processing protection
faults.  In order to have full support for address resolution at least
version 1.3-pl3 of the sysklogd package should be used.

When a protection fault occurs the klogd daemon automatically
translates important addresses in the kernel log messages to their
symbolic equivalents.  This translated kernel message is then
forwarded through whatever reporting mechanism klogd is using.  The
protection fault message can be simply cut out of the message files
and forwarded to the kernel developers.

Two types of address resolution are performed by klogd.  The first is
static translation and the second is dynamic translation.  Static
translation uses the System.map file in much the same manner that
ksymoops does.  In order to do static translation the klogd daemon
must be able to find a system map file at daemon initialization time.
See the klogd man page for information on how klogd searches for map
files.

Dynamic address translation is important when kernel loadable modules
are being used.  Since memory for kernel modules is allocated from the
kernel's dynamic memory pools there are no fixed locations for either
the start of the module or for functions and symbols in the module.

The kernel supports system calls which allow a program to determine
which modules are loaded and their location in memory.  Using these
system calls the klogd daemon builds a symbol table which can be used
to debug a protection fault which occurs in a loadable kernel module.

At the very minimum klogd will provide the name of the module which
generated the protection fault.  There may be additional symbolic
information available if the developer of the loadable module chose to
export symbol information from the module.

Since the kernel module environment can be dynamic there must be a
mechanism for notifying the klogd daemon when a change in module
environment occurs.  There are command line options available which
allow klogd to signal the currently executing daemon that symbol
information should be refreshed.  See the klogd manual page for more
information.

A patch is included with the sysklogd distribution which modifies the
modules-2.0.0 package to automatically signal klogd whenever a module
is loaded or unloaded.  Applying this patch provides essentially
seamless support for debugging protection faults which occur with
kernel loadable modules.

The following is an example of a protection fault in a loadable module
processed by klogd:
---------------------------------------------------------------------------
Aug 29 09:51:01 blizard kernel: Unable to handle kernel paging request at virtual address f15e97cc
Aug 29 09:51:01 blizard kernel: current->tss.cr3 = 0062d000, %cr3 = 0062d000
Aug 29 09:51:01 blizard kernel: *pde = 00000000
Aug 29 09:51:01 blizard kernel: Oops: 0002
Aug 29 09:51:01 blizard kernel: CPU:    0
Aug 29 09:51:01 blizard kernel: EIP:    0010:[oops:_oops+16/3868]
Aug 29 09:51:01 blizard kernel: EFLAGS: 00010212
Aug 29 09:51:01 blizard kernel: eax: 315e97cc   ebx: 003a6f80   ecx: 001be77b   edx: 00237c0c
Aug 29 09:51:01 blizard kernel: esi: 00000000   edi: bffffdb3   ebp: 00589f90   esp: 00589f8c
Aug 29 09:51:01 blizard kernel: ds: 0018   es: 0018   fs: 002b   gs: 002b   ss: 0018
Aug 29 09:51:01 blizard kernel: Process oops_test (pid: 3374, process nr: 21, stackpage=00589000)
Aug 29 09:51:01 blizard kernel: Stack: 315e97cc 00589f98 0100b0b4 bffffed4 0012e38e 00240c64 003a6f80 00000001 
Aug 29 09:51:01 blizard kernel:        00000000 00237810 bfffff00 0010a7fa 00000003 00000001 00000000 bfffff00 
Aug 29 09:51:01 blizard kernel:        bffffdb3 bffffed4 ffffffda 0000002b 0007002b 0000002b 0000002b 00000036 
Aug 29 09:51:01 blizard kernel: Call Trace: [oops:_oops_ioctl+48/80] [_sys_ioctl+254/272] [_system_call+82/128] 
Aug 29 09:51:01 blizard kernel: Code: c7 00 05 00 00 00 eb 08 90 90 90 90 90 90 90 90 89 ec 5d c3 
---------------------------------------------------------------------------

Dr. G.W. Wettstein           Oncology Research Div. Computing Facility
Roger Maris Cancer Center    INTERNET: greg@wind.rmcc.com
820 4th St. N.
Fargo, ND  58122
Phone: 701-234-7556


---------------------------------------------------------------------------
Tainted kernels:

Some oops reports contain the string 'Tainted: ' after the program
counter. This indicates that the kernel has been tainted by some
mechanism.  The string is followed by a series of position-sensitive
characters, each representing a particular tainted value.

  1: 'G' if all modules loaded have a GPL or compatible license, 'P' if
     any proprietary module has been loaded.  Modules without a
     MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by
     insmod as GPL compatible are assumed to be proprietary.

  2: 'F' if any module was force loaded by "insmod -f", ' ' if all
     modules were loaded normally.

  3: 'S' if the oops occurred on an SMP kernel running on hardware that
     hasn't been certified as safe to run multiprocessor.
     Currently this occurs only on various Athlons that are not
     SMP capable.

  4: 'R' if a module was force unloaded by "rmmod -f", ' ' if all
     modules were unloaded normally.

  5: 'M' if any processor has reported a Machine Check Exception,
     ' ' if no Machine Check Exceptions have occurred.

  6: 'B' if a page-release function has found a bad page reference or
     some unexpected page flags.

  7: 'U' if a user or user application specifically requested that the
     Tainted flag be set, ' ' otherwise.

  8: 'D' if the kernel has died recently, i.e. there was an OOPS or BUG.

  9: 'A' if the ACPI table has been overridden.

 10: 'W' if a warning has previously been issued by the kernel.
     (Though some warnings may set more specific taint flags.)

 11: 'C' if a staging driver has been loaded.

 12: 'I' if the kernel is working around a severe bug in the platform
     firmware (BIOS or similar).

 13: 'O' if an externally-built ("out-of-tree") module has been loaded.

 14: 'E' if an unsigned module has been loaded in a kernel supporting
     module signature.

The primary reason for the 'Tainted: ' string is to tell kernel
debuggers if this is a clean kernel or if anything unusual has
occurred.  Tainting is permanent: even if an offending module is
unloaded, the tainted value remains to indicate that the kernel is not
trustworthy.
The padata parallel execution mechanism
Last updated for 2.6.36

Padata is a mechanism by which the kernel can farm work out to be done in
parallel on multiple CPUs while retaining the ordering of tasks.  It was
developed for use with the IPsec code, which needs to be able to perform
encryption and decryption on large numbers of packets without reordering
those packets.  The crypto developers made a point of writing padata in a
sufficiently general fashion that it could be put to other uses as well.

The first step in using padata is to set up a padata_instance structure for
overall control of how tasks are to be run:

    #include <linux/padata.h>

    struct padata_instance *padata_alloc(struct workqueue_struct *wq,
					 const struct cpumask *pcpumask,
					 const struct cpumask *cbcpumask);

The pcpumask describes which processors will be used to execute work
submitted to this instance in parallel. The cbcpumask defines which
processors are allowed to be used as the serialization callback processor.
The workqueue wq is where the work will actually be done; it should be
a multithreaded queue, naturally.

To allocate a padata instance with the cpu_possible_mask for both
cpumasks this helper function can be used:

    struct padata_instance *padata_alloc_possible(struct workqueue_struct *wq);

Note: Padata maintains two kinds of cpumasks internally. The user supplied
cpumasks, submitted by padata_alloc/padata_alloc_possible and the 'usable'
cpumasks. The usable cpumasks are always a subset of active CPUs in the
user supplied cpumasks; these are the cpumasks padata actually uses. So
it is legal to supply a cpumask to padata that contains offline CPUs.
Once an offline CPU in the user supplied cpumask comes online, padata
is going to use it.

There are functions for enabling and disabling the instance:

    int padata_start(struct padata_instance *pinst);
    void padata_stop(struct padata_instance *pinst);

These functions are setting or clearing the "PADATA_INIT" flag;
if that flag is not set, other functions will refuse to work.
padata_start returns zero on success (flag set) or -EINVAL if the
padata cpumask contains no active CPU (flag not set).
padata_stop clears the flag and blocks until the padata instance
is unused.

The list of CPUs to be used can be adjusted with these functions:

    int padata_set_cpumasks(struct padata_instance *pinst,
			    cpumask_var_t pcpumask,
			    cpumask_var_t cbcpumask);
    int padata_set_cpumask(struct padata_instance *pinst, int cpumask_type,
			   cpumask_var_t cpumask);
    int padata_add_cpu(struct padata_instance *pinst, int cpu, int mask);
    int padata_remove_cpu(struct padata_instance *pinst, int cpu, int mask);

Changing the CPU masks are expensive operations, though, so it should not be
done with great frequency.

It's possible to change both cpumasks of a padata instance with
padata_set_cpumasks by specifying the cpumasks for parallel execution (pcpumask)
and for the serial callback function (cbcpumask). padata_set_cpumask is used to
change just one of the cpumasks. Here cpumask_type is one of PADATA_CPU_SERIAL,
PADATA_CPU_PARALLEL and cpumask specifies the new cpumask to use.
To simply add or remove one CPU from a certain cpumask the functions
padata_add_cpu/padata_remove_cpu are used. cpu specifies the CPU to add or
remove and mask is one of PADATA_CPU_SERIAL, PADATA_CPU_PARALLEL.

If a user is interested in padata cpumask changes, he can register to
the padata cpumask change notifier:

    int padata_register_cpumask_notifier(struct padata_instance *pinst,
					 struct notifier_block *nblock);

To unregister from that notifier:

    int padata_unregister_cpumask_notifier(struct padata_instance *pinst,
					   struct notifier_block *nblock);

The padata cpumask change notifier notifies about changes of the usable
cpumasks, i.e. the subset of active CPUs in the user supplied cpumask.

Padata calls the notifier chain with:

    blocking_notifier_call_chain(&pinst->cpumask_change_notifier,
				 notification_mask,
				 &pd_new->cpumask);

Here cpumask_change_notifier is registered notifier, notification_mask
is one of PADATA_CPU_SERIAL, PADATA_CPU_PARALLEL and cpumask is a pointer
to a struct padata_cpumask that contains the new cpumask information.

Actually submitting work to the padata instance requires the creation of a
padata_priv structure:

    struct padata_priv {
        /* Other stuff here... */
	void                    (*parallel)(struct padata_priv *padata);
	void                    (*serial)(struct padata_priv *padata);
    };

This structure will almost certainly be embedded within some larger
structure specific to the work to be done.  Most of its fields are private to
padata, but the structure should be zeroed at initialisation time, and the
parallel() and serial() functions should be provided.  Those functions will
be called in the process of getting the work done as we will see
momentarily.

The submission of work is done with:

    int padata_do_parallel(struct padata_instance *pinst,
		           struct padata_priv *padata, int cb_cpu);

The pinst and padata structures must be set up as described above; cb_cpu
specifies which CPU will be used for the final callback when the work is
done; it must be in the current instance's CPU mask.  The return value from
padata_do_parallel() is zero on success, indicating that the work is in
progress. -EBUSY means that somebody, somewhere else is messing with the
instance's CPU mask, while -EINVAL is a complaint about cb_cpu not being
in that CPU mask or about a not running instance.

Each task submitted to padata_do_parallel() will, in turn, be passed to
exactly one call to the above-mentioned parallel() function, on one CPU, so
true parallelism is achieved by submitting multiple tasks.  Despite the
fact that the workqueue is used to make these calls, parallel() is run with
software interrupts disabled and thus cannot sleep.  The parallel()
function gets the padata_priv structure pointer as its lone parameter;
information about the actual work to be done is probably obtained by using
container_of() to find the enclosing structure.

Note that parallel() has no return value; the padata subsystem assumes that
parallel() will take responsibility for the task from this point.  The work
need not be completed during this call, but, if parallel() leaves work
outstanding, it should be prepared to be called again with a new job before
the previous one completes.  When a task does complete, parallel() (or
whatever function actually finishes the job) should inform padata of the
fact with a call to:

    void padata_do_serial(struct padata_priv *padata);

At some point in the future, padata_do_serial() will trigger a call to the
serial() function in the padata_priv structure.  That call will happen on
the CPU requested in the initial call to padata_do_parallel(); it, too, is
done through the workqueue, but with local software interrupts disabled.
Note that this call may be deferred for a while since the padata code takes
pains to ensure that tasks are completed in the order in which they were
submitted.

The one remaining function in the padata API should be called to clean up
when a padata instance is no longer needed:

    void padata_free(struct padata_instance *pinst);

This function will busy-wait while any remaining tasks are completed, so it
might be best not to call it while there is work outstanding.  Shutting
down the workqueue, if necessary, should be done separately.
PARPORT interface documentation
-------------------------------

Time-stamp: <2000-02-24 13:30:20 twaugh>

Described here are the following functions:

Global functions:
  parport_register_driver
  parport_unregister_driver
  parport_enumerate
  parport_register_device
  parport_unregister_device
  parport_claim
  parport_claim_or_block
  parport_release
  parport_yield
  parport_yield_blocking
  parport_wait_peripheral
  parport_poll_peripheral
  parport_wait_event
  parport_negotiate
  parport_read
  parport_write
  parport_open
  parport_close
  parport_device_id
  parport_device_coords
  parport_find_class
  parport_find_device
  parport_set_timeout

Port functions (can be overridden by low-level drivers):
  SPP:
    port->ops->read_data
    port->ops->write_data
    port->ops->read_status
    port->ops->read_control
    port->ops->write_control
    port->ops->frob_control
    port->ops->enable_irq
    port->ops->disable_irq
    port->ops->data_forward
    port->ops->data_reverse

  EPP:
    port->ops->epp_write_data
    port->ops->epp_read_data
    port->ops->epp_write_addr
    port->ops->epp_read_addr

  ECP:
    port->ops->ecp_write_data
    port->ops->ecp_read_data
    port->ops->ecp_write_addr

  Other:
    port->ops->nibble_read_data
    port->ops->byte_read_data
    port->ops->compat_write_data

The parport subsystem comprises 'parport' (the core port-sharing
code), and a variety of low-level drivers that actually do the port
accesses.  Each low-level driver handles a particular style of port
(PC, Amiga, and so on).

The parport interface to the device driver author can be broken down
into global functions and port functions.

The global functions are mostly for communicating between the device
driver and the parport subsystem: acquiring a list of available ports,
claiming a port for exclusive use, and so on.  They also include
'generic' functions for doing standard things that will work on any
IEEE 1284-capable architecture.

The port functions are provided by the low-level drivers, although the
core parport module provides generic 'defaults' for some routines.
The port functions can be split into three groups: SPP, EPP, and ECP.

SPP (Standard Parallel Port) functions modify so-called 'SPP'
registers: data, status, and control.  The hardware may not actually
have registers exactly like that, but the PC does and this interface is
modelled after common PC implementations.  Other low-level drivers may
be able to emulate most of the functionality.

EPP (Enhanced Parallel Port) functions are provided for reading and
writing in IEEE 1284 EPP mode, and ECP (Extended Capabilities Port)
functions are used for IEEE 1284 ECP mode. (What about BECP? Does
anyone care?)

Hardware assistance for EPP and/or ECP transfers may or may not be
available, and if it is available it may or may not be used.  If
hardware is not used, the transfer will be software-driven.  In order
to cope with peripherals that only tenuously support IEEE 1284, a
low-level driver specific function is provided, for altering 'fudge
factors'.

GLOBAL FUNCTIONS
----------------

parport_register_driver - register a device driver with parport
-----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_driver {
	const char *name;
	void (*attach) (struct parport *);
	void (*detach) (struct parport *);
	struct parport_driver *next;
};
int parport_register_driver (struct parport_driver *driver);

DESCRIPTION

In order to be notified about parallel ports when they are detected,
parport_register_driver should be called.  Your driver will
immediately be notified of all ports that have already been detected,
and of each new port as low-level drivers are loaded.

A 'struct parport_driver' contains the textual name of your driver,
a pointer to a function to handle new ports, and a pointer to a
function to handle ports going away due to a low-level driver
unloading.  Ports will only be detached if they are not being used
(i.e. there are no devices registered on them).

The visible parts of the 'struct parport *' argument given to
attach/detach are:

struct parport
{
	struct parport *next; /* next parport in list */
	const char *name;     /* port's name */
	unsigned int modes;   /* bitfield of hardware modes */
	struct parport_device_info probe_info;
			      /* IEEE1284 info */
	int number;           /* parport index */
	struct parport_operations *ops;
	...
};

There are other members of the structure, but they should not be
touched.

The 'modes' member summarises the capabilities of the underlying
hardware.  It consists of flags which may be bitwise-ored together:

  PARPORT_MODE_PCSPP		IBM PC registers are available,
				i.e. functions that act on data,
				control and status registers are
				probably writing directly to the
				hardware.
  PARPORT_MODE_TRISTATE		The data drivers may be turned off.
				This allows the data lines to be used
				for reverse (peripheral to host)
				transfers.
  PARPORT_MODE_COMPAT		The hardware can assist with
				compatibility-mode (printer)
				transfers, i.e. compat_write_block.
  PARPORT_MODE_EPP		The hardware can assist with EPP
				transfers.
  PARPORT_MODE_ECP		The hardware can assist with ECP
				transfers.
  PARPORT_MODE_DMA		The hardware can use DMA, so you might
				want to pass ISA DMA-able memory
				(i.e. memory allocated using the
				GFP_DMA flag with kmalloc) to the
				low-level driver in order to take
				advantage of it.

There may be other flags in 'modes' as well.

The contents of 'modes' is advisory only.  For example, if the
hardware is capable of DMA, and PARPORT_MODE_DMA is in 'modes', it
doesn't necessarily mean that DMA will always be used when possible.
Similarly, hardware that is capable of assisting ECP transfers won't
necessarily be used.

RETURN VALUE

Zero on success, otherwise an error code.

ERRORS

None. (Can it fail? Why return int?)

EXAMPLE

static void lp_attach (struct parport *port)
{
	...
	private = kmalloc (...);
	dev[count++] = parport_register_device (...);
	...
}

static void lp_detach (struct parport *port)
{
	...
}

static struct parport_driver lp_driver = {
	"lp",
	lp_attach,
	lp_detach,
	NULL /* always put NULL here */
};

int lp_init (void)
{
	...
	if (parport_register_driver (&lp_driver)) {
		/* Failed; nothing we can do. */
		return -EIO;
	}
	...
}

SEE ALSO

parport_unregister_driver, parport_register_device, parport_enumerate

parport_unregister_driver - tell parport to forget about this driver
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_driver {
	const char *name;
	void (*attach) (struct parport *);
	void (*detach) (struct parport *);
	struct parport_driver *next;
};
void parport_unregister_driver (struct parport_driver *driver);

DESCRIPTION

This tells parport not to notify the device driver of new ports or of
ports going away.  Registered devices belonging to that driver are NOT
unregistered: parport_unregister_device must be used for each one.

EXAMPLE

void cleanup_module (void)
{
	...
	/* Stop notifications. */
	parport_unregister_driver (&lp_driver);

	/* Unregister devices. */
	for (i = 0; i < NUM_DEVS; i++)
		parport_unregister_device (dev[i]);
	...
}

SEE ALSO

parport_register_driver, parport_enumerate

parport_enumerate - retrieve a list of parallel ports (DEPRECATED)
-----------------

SYNOPSIS

#include <linux/parport.h>

struct parport *parport_enumerate (void);

DESCRIPTION

Retrieve the first of a list of valid parallel ports for this machine.
Successive parallel ports can be found using the 'struct parport
*next' element of the 'struct parport *' that is returned.  If 'next'
is NULL, there are no more parallel ports in the list.  The number of
ports in the list will not exceed PARPORT_MAX.

RETURN VALUE

A 'struct parport *' describing a valid parallel port for the machine,
or NULL if there are none.

ERRORS

This function can return NULL to indicate that there are no parallel
ports to use.

EXAMPLE

int detect_device (void)
{
	struct parport *port;

	for (port = parport_enumerate ();
	     port != NULL;
	     port = port->next) {
		/* Try to detect a device on the port... */
		...
             }
	}

	...
}

NOTES

parport_enumerate is deprecated; parport_register_driver should be
used instead.

SEE ALSO

parport_register_driver, parport_unregister_driver

parport_register_device - register to use a port
-----------------------

SYNOPSIS

#include <linux/parport.h>

typedef int (*preempt_func) (void *handle);
typedef void (*wakeup_func) (void *handle);
typedef int (*irq_func) (int irq, void *handle, struct pt_regs *);

struct pardevice *parport_register_device(struct parport *port,
                                          const char *name,
                                          preempt_func preempt,
                                          wakeup_func wakeup,
                                          irq_func irq,
                                          int flags,
                                          void *handle);

DESCRIPTION

Use this function to register your device driver on a parallel port
('port').  Once you have done that, you will be able to use
parport_claim and parport_release in order to use the port.

The ('name') argument is the name of the device that appears in /proc
filesystem. The string must be valid for the whole lifetime of the
device (until parport_unregister_device is called).

This function will register three callbacks into your driver:
'preempt', 'wakeup' and 'irq'.  Each of these may be NULL in order to
indicate that you do not want a callback.

When the 'preempt' function is called, it is because another driver
wishes to use the parallel port.  The 'preempt' function should return
non-zero if the parallel port cannot be released yet -- if zero is
returned, the port is lost to another driver and the port must be
re-claimed before use.

The 'wakeup' function is called once another driver has released the
port and no other driver has yet claimed it.  You can claim the
parallel port from within the 'wakeup' function (in which case the
claim is guaranteed to succeed), or choose not to if you don't need it
now.

If an interrupt occurs on the parallel port your driver has claimed,
the 'irq' function will be called. (Write something about shared
interrupts here.)

The 'handle' is a pointer to driver-specific data, and is passed to
the callback functions.

'flags' may be a bitwise combination of the following flags:

        Flag            Meaning
  PARPORT_DEV_EXCL	The device cannot share the parallel port at all.
			Use this only when absolutely necessary.

The typedefs are not actually defined -- they are only shown in order
to make the function prototype more readable.

The visible parts of the returned 'struct pardevice' are:

struct pardevice {
	struct parport *port;	/* Associated port */
	void *private;		/* Device driver's 'handle' */
	...
};

RETURN VALUE

A 'struct pardevice *': a handle to the registered parallel port
device that can be used for parport_claim, parport_release, etc.

ERRORS

A return value of NULL indicates that there was a problem registering
a device on that port.

EXAMPLE

static int preempt (void *handle)
{
	if (busy_right_now)
		return 1;

	must_reclaim_port = 1;
	return 0;
}

static void wakeup (void *handle)
{
	struct toaster *private = handle;
	struct pardevice *dev = private->dev;
	if (!dev) return; /* avoid races */

	if (want_port)
		parport_claim (dev);
}

static int toaster_detect (struct toaster *private, struct parport *port)
{
	private->dev = parport_register_device (port, "toaster", preempt,
					        wakeup, NULL, 0,
						private);
	if (!private->dev)
		/* Couldn't register with parport. */
		return -EIO;

	must_reclaim_port = 0;
	busy_right_now = 1;
	parport_claim_or_block (private->dev);
	...
	/* Don't need the port while the toaster warms up. */
	busy_right_now = 0;
	...
	busy_right_now = 1;
	if (must_reclaim_port) {
		parport_claim_or_block (private->dev);
		must_reclaim_port = 0;
	}
	...
}

SEE ALSO

parport_unregister_device, parport_claim

parport_unregister_device - finish using a port
-------------------------

SYNPOPSIS

#include <linux/parport.h>

void parport_unregister_device (struct pardevice *dev);

DESCRIPTION

This function is the opposite of parport_register_device.  After using
parport_unregister_device, 'dev' is no longer a valid device handle.

You should not unregister a device that is currently claimed, although
if you do it will be released automatically.

EXAMPLE

	...
	kfree (dev->private); /* before we lose the pointer */
	parport_unregister_device (dev);
	...

SEE ALSO

parport_unregister_driver

parport_claim, parport_claim_or_block - claim the parallel port for a device
-------------------------------------

SYNOPSIS

#include <linux/parport.h>

int parport_claim (struct pardevice *dev);
int parport_claim_or_block (struct pardevice *dev);

DESCRIPTION

These functions attempt to gain control of the parallel port on which
'dev' is registered.  'parport_claim' does not block, but
'parport_claim_or_block' may do. (Put something here about blocking
interruptibly or non-interruptibly.)

You should not try to claim a port that you have already claimed.

RETURN VALUE

A return value of zero indicates that the port was successfully
claimed, and the caller now has possession of the parallel port.

If 'parport_claim_or_block' blocks before returning successfully, the
return value is positive.

ERRORS

  -EAGAIN  The port is unavailable at the moment, but another attempt
           to claim it may succeed.

SEE ALSO

parport_release

parport_release - release the parallel port
---------------

SYNOPSIS

#include <linux/parport.h>

void parport_release (struct pardevice *dev);

DESCRIPTION

Once a parallel port device has been claimed, it can be released using
'parport_release'.  It cannot fail, but you should not release a
device that you do not have possession of.

EXAMPLE

static size_t write (struct pardevice *dev, const void *buf,
		     size_t len)
{
	...
	written = dev->port->ops->write_ecp_data (dev->port, buf,
						  len);
	parport_release (dev);
	...
}


SEE ALSO

change_mode, parport_claim, parport_claim_or_block, parport_yield

parport_yield, parport_yield_blocking - temporarily release a parallel port
-------------------------------------

SYNOPSIS

#include <linux/parport.h>

int parport_yield (struct pardevice *dev)
int parport_yield_blocking (struct pardevice *dev);

DESCRIPTION

When a driver has control of a parallel port, it may allow another
driver to temporarily 'borrow' it.  'parport_yield' does not block;
'parport_yield_blocking' may do.

RETURN VALUE

A return value of zero indicates that the caller still owns the port
and the call did not block.

A positive return value from 'parport_yield_blocking' indicates that
the caller still owns the port and the call blocked.

A return value of -EAGAIN indicates that the caller no longer owns the
port, and it must be re-claimed before use.

ERRORS

  -EAGAIN  Ownership of the parallel port was given away.

SEE ALSO

parport_release

parport_wait_peripheral - wait for status lines, up to 35ms
-----------------------

SYNOPSIS

#include <linux/parport.h>

int parport_wait_peripheral (struct parport *port,
			     unsigned char mask,
			     unsigned char val);

DESCRIPTION

Wait for the status lines in mask to match the values in val.

RETURN VALUE

 -EINTR  a signal is pending
      0  the status lines in mask have values in val
      1  timed out while waiting (35ms elapsed)

SEE ALSO

parport_poll_peripheral

parport_poll_peripheral - wait for status lines, in usec
-----------------------

SYNOPSIS

#include <linux/parport.h>

int parport_poll_peripheral (struct parport *port,
			     unsigned char mask,
			     unsigned char val,
			     int usec);

DESCRIPTION

Wait for the status lines in mask to match the values in val.

RETURN VALUE

 -EINTR  a signal is pending
      0  the status lines in mask have values in val
      1  timed out while waiting (usec microseconds have elapsed)

SEE ALSO

parport_wait_peripheral

parport_wait_event - wait for an event on a port
------------------

SYNOPSIS

#include <linux/parport.h>

int parport_wait_event (struct parport *port, signed long timeout)

DESCRIPTION

Wait for an event (e.g. interrupt) on a port.  The timeout is in
jiffies.

RETURN VALUE

      0  success
     <0  error (exit as soon as possible)
     >0  timed out

parport_negotiate - perform IEEE 1284 negotiation
-----------------

SYNOPSIS

#include <linux/parport.h>

int parport_negotiate (struct parport *, int mode);

DESCRIPTION

Perform IEEE 1284 negotiation.

RETURN VALUE

     0  handshake OK; IEEE 1284 peripheral and mode available
    -1  handshake failed; peripheral not compliant (or none present)
     1  handshake OK; IEEE 1284 peripheral present but mode not
        available

SEE ALSO

parport_read, parport_write

parport_read - read data from device
------------

SYNOPSIS

#include <linux/parport.h>

ssize_t parport_read (struct parport *, void *buf, size_t len);

DESCRIPTION

Read data from device in current IEEE 1284 transfer mode.  This only
works for modes that support reverse data transfer.

RETURN VALUE

If negative, an error code; otherwise the number of bytes transferred.

SEE ALSO

parport_write, parport_negotiate

parport_write - write data to device
-------------

SYNOPSIS

#include <linux/parport.h>

ssize_t parport_write (struct parport *, const void *buf, size_t len);

DESCRIPTION

Write data to device in current IEEE 1284 transfer mode.  This only
works for modes that support forward data transfer.

RETURN VALUE

If negative, an error code; otherwise the number of bytes transferred.

SEE ALSO

parport_read, parport_negotiate

parport_open - register device for particular device number
------------

SYNOPSIS

#include <linux/parport.h>

struct pardevice *parport_open (int devnum, const char *name,
			        int (*pf) (void *),
				void (*kf) (void *),
				void (*irqf) (int, void *,
					      struct pt_regs *),
				int flags, void *handle);

DESCRIPTION

This is like parport_register_device but takes a device number instead
of a pointer to a struct parport.

RETURN VALUE

See parport_register_device.  If no device is associated with devnum,
NULL is returned.

SEE ALSO

parport_register_device

parport_close - unregister device for particular device number
-------------

SYNOPSIS

#include <linux/parport.h>

void parport_close (struct pardevice *dev);

DESCRIPTION

This is the equivalent of parport_unregister_device for parport_open.

SEE ALSO

parport_unregister_device, parport_open

parport_device_id - obtain IEEE 1284 Device ID
-----------------

SYNOPSIS

#include <linux/parport.h>

ssize_t parport_device_id (int devnum, char *buffer, size_t len);

DESCRIPTION

Obtains the IEEE 1284 Device ID associated with a given device.

RETURN VALUE

If negative, an error code; otherwise, the number of bytes of buffer
that contain the device ID.  The format of the device ID is as
follows:

[length][ID]

The first two bytes indicate the inclusive length of the entire Device
ID, and are in big-endian order.  The ID is a sequence of pairs of the
form:

key:value;

NOTES

Many devices have ill-formed IEEE 1284 Device IDs.

SEE ALSO

parport_find_class, parport_find_device

parport_device_coords - convert device number to device coordinates
------------------

SYNOPSIS

#include <linux/parport.h>

int parport_device_coords (int devnum, int *parport, int *mux,
			   int *daisy);

DESCRIPTION

Convert between device number (zero-based) and device coordinates
(port, multiplexor, daisy chain address).

RETURN VALUE

Zero on success, in which case the coordinates are (*parport, *mux,
*daisy).

SEE ALSO

parport_open, parport_device_id

parport_find_class - find a device by its class
------------------

SYNOPSIS

#include <linux/parport.h>

typedef enum {
	PARPORT_CLASS_LEGACY = 0,       /* Non-IEEE1284 device */
	PARPORT_CLASS_PRINTER,
	PARPORT_CLASS_MODEM,
	PARPORT_CLASS_NET,
	PARPORT_CLASS_HDC,              /* Hard disk controller */
	PARPORT_CLASS_PCMCIA,
	PARPORT_CLASS_MEDIA,            /* Multimedia device */
	PARPORT_CLASS_FDC,              /* Floppy disk controller */
	PARPORT_CLASS_PORTS,
	PARPORT_CLASS_SCANNER,
	PARPORT_CLASS_DIGCAM,
	PARPORT_CLASS_OTHER,            /* Anything else */
	PARPORT_CLASS_UNSPEC,           /* No CLS field in ID */
	PARPORT_CLASS_SCSIADAPTER
} parport_device_class;

int parport_find_class (parport_device_class cls, int from);

DESCRIPTION

Find a device by class.  The search starts from device number from+1.

RETURN VALUE

The device number of the next device in that class, or -1 if no such
device exists.

NOTES

Example usage:

int devnum = -1;
while ((devnum = parport_find_class (PARPORT_CLASS_DIGCAM, devnum)) != -1) {
    struct pardevice *dev = parport_open (devnum, ...);
    ...
}

SEE ALSO

parport_find_device, parport_open, parport_device_id

parport_find_device - find a device by its class
------------------

SYNOPSIS

#include <linux/parport.h>

int parport_find_device (const char *mfg, const char *mdl, int from);

DESCRIPTION

Find a device by vendor and model.  The search starts from device
number from+1.

RETURN VALUE

The device number of the next device matching the specifications, or
-1 if no such device exists.

NOTES

Example usage:

int devnum = -1;
while ((devnum = parport_find_device ("IOMEGA", "ZIP+", devnum)) != -1) {
    struct pardevice *dev = parport_open (devnum, ...);
    ...
}

SEE ALSO

parport_find_class, parport_open, parport_device_id

parport_set_timeout - set the inactivity timeout
-------------------

SYNOPSIS

#include <linux/parport.h>

long parport_set_timeout (struct pardevice *dev, long inactivity);

DESCRIPTION

Set the inactivity timeout, in jiffies, for a registered device.  The
previous timeout is returned.

RETURN VALUE

The previous timeout, in jiffies.

NOTES

Some of the port->ops functions for a parport may take time, owing to
delays at the peripheral.  After the peripheral has not responded for
'inactivity' jiffies, a timeout will occur and the blocking function
will return.

A timeout of 0 jiffies is a special case: the function must do as much
as it can without blocking or leaving the hardware in an unknown
state.  If port operations are performed from within an interrupt
handler, for instance, a timeout of 0 jiffies should be used.

Once set for a registered device, the timeout will remain at the set
value until set again.

SEE ALSO

port->ops->xxx_read/write_yyy

PORT FUNCTIONS
--------------

The functions in the port->ops structure (struct parport_operations)
are provided by the low-level driver responsible for that port.

port->ops->read_data - read the data register
--------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	unsigned char (*read_data) (struct parport *port);
	...
};

DESCRIPTION

If port->modes contains the PARPORT_MODE_TRISTATE flag and the
PARPORT_CONTROL_DIRECTION bit in the control register is set, this
returns the value on the data pins.  If port->modes contains the
PARPORT_MODE_TRISTATE flag and the PARPORT_CONTROL_DIRECTION bit is
not set, the return value _may_ be the last value written to the data
register.  Otherwise the return value is undefined.

SEE ALSO

write_data, read_status, write_control

port->ops->write_data - write the data register
---------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*write_data) (struct parport *port, unsigned char d);
	...
};

DESCRIPTION

Writes to the data register.  May have side-effects (a STROBE pulse,
for instance).

SEE ALSO

read_data, read_status, write_control

port->ops->read_status - read the status register
----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	unsigned char (*read_status) (struct parport *port);
	...
};

DESCRIPTION

Reads from the status register.  This is a bitmask:

- PARPORT_STATUS_ERROR (printer fault, "nFault")
- PARPORT_STATUS_SELECT (on-line, "Select")
- PARPORT_STATUS_PAPEROUT (no paper, "PError")
- PARPORT_STATUS_ACK (handshake, "nAck")
- PARPORT_STATUS_BUSY (busy, "Busy")

There may be other bits set.

SEE ALSO

read_data, write_data, write_control

port->ops->read_control - read the control register
-----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	unsigned char (*read_control) (struct parport *port);
	...
};

DESCRIPTION

Returns the last value written to the control register (either from
write_control or frob_control).  No port access is performed.

SEE ALSO

read_data, write_data, read_status, write_control

port->ops->write_control - write the control register
------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*write_control) (struct parport *port, unsigned char s);
	...
};

DESCRIPTION

Writes to the control register. This is a bitmask:
                          _______
- PARPORT_CONTROL_STROBE (nStrobe)
                          _______
- PARPORT_CONTROL_AUTOFD (nAutoFd)
                        _____
- PARPORT_CONTROL_INIT (nInit)
                          _________
- PARPORT_CONTROL_SELECT (nSelectIn)

SEE ALSO

read_data, write_data, read_status, frob_control

port->ops->frob_control - write control register bits
-----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	unsigned char (*frob_control) (struct parport *port,
				       unsigned char mask,
				       unsigned char val);
	...
};

DESCRIPTION

This is equivalent to reading from the control register, masking out
the bits in mask, exclusive-or'ing with the bits in val, and writing
the result to the control register.

As some ports don't allow reads from the control port, a software copy
of its contents is maintained, so frob_control is in fact only one
port access.

SEE ALSO

read_data, write_data, read_status, write_control

port->ops->enable_irq - enable interrupt generation
---------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*enable_irq) (struct parport *port);
	...
};

DESCRIPTION

The parallel port hardware is instructed to generate interrupts at
appropriate moments, although those moments are
architecture-specific.  For the PC architecture, interrupts are
commonly generated on the rising edge of nAck.

SEE ALSO

disable_irq

port->ops->disable_irq - disable interrupt generation
----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*disable_irq) (struct parport *port);
	...
};

DESCRIPTION

The parallel port hardware is instructed not to generate interrupts.
The interrupt itself is not masked.

SEE ALSO

enable_irq

port->ops->data_forward - enable data drivers
-----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*data_forward) (struct parport *port);
	...
};

DESCRIPTION

Enables the data line drivers, for 8-bit host-to-peripheral
communications.

SEE ALSO

data_reverse

port->ops->data_reverse - tristate the buffer
-----------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	void (*data_reverse) (struct parport *port);
	...
};

DESCRIPTION

Places the data bus in a high impedance state, if port->modes has the
PARPORT_MODE_TRISTATE bit set.

SEE ALSO

data_forward

port->ops->epp_write_data - write EPP data
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*epp_write_data) (struct parport *port, const void *buf,
				  size_t len, int flags);
	...
};

DESCRIPTION

Writes data in EPP mode, and returns the number of bytes written.

The 'flags' parameter may be one or more of the following,
bitwise-or'ed together:

PARPORT_EPP_FAST	Use fast transfers. Some chips provide 16-bit and
			32-bit registers.  However, if a transfer
			times out, the return value may be unreliable.

SEE ALSO

epp_read_data, epp_write_addr, epp_read_addr

port->ops->epp_read_data - read EPP data
------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*epp_read_data) (struct parport *port, void *buf,
				 size_t len, int flags);
	...
};

DESCRIPTION

Reads data in EPP mode, and returns the number of bytes read.

The 'flags' parameter may be one or more of the following,
bitwise-or'ed together:

PARPORT_EPP_FAST	Use fast transfers. Some chips provide 16-bit and
			32-bit registers.  However, if a transfer
			times out, the return value may be unreliable.

SEE ALSO

epp_write_data, epp_write_addr, epp_read_addr

port->ops->epp_write_addr - write EPP address
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*epp_write_addr) (struct parport *port,
				  const void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Writes EPP addresses (8 bits each), and returns the number written.

The 'flags' parameter may be one or more of the following,
bitwise-or'ed together:

PARPORT_EPP_FAST	Use fast transfers. Some chips provide 16-bit and
			32-bit registers.  However, if a transfer
			times out, the return value may be unreliable.

(Does PARPORT_EPP_FAST make sense for this function?)

SEE ALSO

epp_write_data, epp_read_data, epp_read_addr

port->ops->epp_read_addr - read EPP address
------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*epp_read_addr) (struct parport *port, void *buf,
				 size_t len, int flags);
	...
};

DESCRIPTION

Reads EPP addresses (8 bits each), and returns the number read.

The 'flags' parameter may be one or more of the following,
bitwise-or'ed together:

PARPORT_EPP_FAST	Use fast transfers. Some chips provide 16-bit and
			32-bit registers.  However, if a transfer
			times out, the return value may be unreliable.

(Does PARPORT_EPP_FAST make sense for this function?)

SEE ALSO

epp_write_data, epp_read_data, epp_write_addr

port->ops->ecp_write_data - write a block of ECP data
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*ecp_write_data) (struct parport *port,
				  const void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Writes a block of ECP data.  The 'flags' parameter is ignored.

RETURN VALUE

The number of bytes written.

SEE ALSO

ecp_read_data, ecp_write_addr

port->ops->ecp_read_data - read a block of ECP data
------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*ecp_read_data) (struct parport *port,
				 void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Reads a block of ECP data.  The 'flags' parameter is ignored.

RETURN VALUE

The number of bytes read.  NB. There may be more unread data in a
FIFO.  Is there a way of stunning the FIFO to prevent this?

SEE ALSO

ecp_write_block, ecp_write_addr

port->ops->ecp_write_addr - write a block of ECP addresses
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*ecp_write_addr) (struct parport *port,
				  const void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Writes a block of ECP addresses.  The 'flags' parameter is ignored.

RETURN VALUE

The number of bytes written.

NOTES

This may use a FIFO, and if so shall not return until the FIFO is empty.

SEE ALSO

ecp_read_data, ecp_write_data

port->ops->nibble_read_data - read a block of data in nibble mode
---------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*nibble_read_data) (struct parport *port,
				    void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Reads a block of data in nibble mode.  The 'flags' parameter is ignored.

RETURN VALUE

The number of whole bytes read.

SEE ALSO

byte_read_data, compat_write_data

port->ops->byte_read_data - read a block of data in byte mode
-------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*byte_read_data) (struct parport *port,
				  void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Reads a block of data in byte mode.  The 'flags' parameter is ignored.

RETURN VALUE

The number of bytes read.

SEE ALSO

nibble_read_data, compat_write_data

port->ops->compat_write_data - write a block of data in compatibility mode
----------------------------

SYNOPSIS

#include <linux/parport.h>

struct parport_operations {
	...
	size_t (*compat_write_data) (struct parport *port,
				     const void *buf, size_t len, int flags);
	...
};

DESCRIPTION

Writes a block of data in compatibility mode.  The 'flags' parameter
is ignored.

RETURN VALUE

The number of bytes written.

SEE ALSO

nibble_read_data, byte_read_data
The `parport' code provides parallel-port support under Linux.  This
includes the ability to share one port between multiple device
drivers.

You can pass parameters to the parport code to override its automatic
detection of your hardware.  This is particularly useful if you want
to use IRQs, since in general these can't be autoprobed successfully.
By default IRQs are not used even if they _can_ be probed.  This is
because there are a lot of people using the same IRQ for their
parallel port and a sound card or network card.

The parport code is split into two parts: generic (which deals with
port-sharing) and architecture-dependent (which deals with actually
using the port).


Parport as modules
==================

If you load the parport code as a module, say

	# insmod parport

to load the generic parport code.  You then must load the
architecture-dependent code with (for example):

	# insmod parport_pc io=0x3bc,0x378,0x278 irq=none,7,auto

to tell the parport code that you want three PC-style ports, one at
0x3bc with no IRQ, one at 0x378 using IRQ 7, and one at 0x278 with an
auto-detected IRQ.  Currently, PC-style (parport_pc), Sun `bpp',
Amiga, Atari, and MFC3 hardware is supported.

PCI parallel I/O card support comes from parport_pc.  Base I/O
addresses should not be specified for supported PCI cards since they
are automatically detected.


modprobe
--------

If you use modprobe , you will find it useful to add lines as below to a
configuration file in /etc/modprobe.d/ directory:.

	alias parport_lowlevel parport_pc
	options parport_pc io=0x378,0x278 irq=7,auto

modprobe will load parport_pc (with the options "io=0x378,0x278 irq=7,auto")
whenever a parallel port device driver (such as lp) is loaded.

Note that these are example lines only!  You shouldn't in general need
to specify any options to parport_pc in order to be able to use a
parallel port.


Parport probe [optional]
-------------

In 2.2 kernels there was a module called parport_probe, which was used
for collecting IEEE 1284 device ID information.  This has now been
enhanced and now lives with the IEEE 1284 support.  When a parallel
port is detected, the devices that are connected to it are analysed,
and information is logged like this:

	parport0: Printer, BJC-210 (Canon)

The probe information is available from files in /proc/sys/dev/parport/.


Parport linked into the kernel statically
=========================================

If you compile the parport code into the kernel, then you can use
kernel boot parameters to get the same effect.  Add something like the
following to your LILO command line:

	parport=0x3bc parport=0x378,7 parport=0x278,auto,nofifo

You can have many `parport=...' statements, one for each port you want
to add.  Adding `parport=0' to the kernel command-line will disable
parport support entirely.  Adding `parport=auto' to the kernel
command-line will make parport use any IRQ lines or DMA channels that
it auto-detects.


Files in /proc
==============

If you have configured the /proc filesystem into your kernel, you will
see a new directory entry: /proc/sys/dev/parport.  In there will be a
directory entry for each parallel port for which parport is
configured.  In each of those directories are a collection of files
describing that parallel port.

The /proc/sys/dev/parport directory tree looks like:

parport
|-- default
|   |-- spintime
|   `-- timeslice
|-- parport0
|   |-- autoprobe
|   |-- autoprobe0
|   |-- autoprobe1
|   |-- autoprobe2
|   |-- autoprobe3
|   |-- devices
|   |   |-- active
|   |   `-- lp
|   |       `-- timeslice
|   |-- base-addr
|   |-- irq
|   |-- dma
|   |-- modes
|   `-- spintime
`-- parport1
    |-- autoprobe
    |-- autoprobe0
    |-- autoprobe1
    |-- autoprobe2
    |-- autoprobe3
    |-- devices
    |   |-- active
    |   `-- ppa
    |       `-- timeslice
    |-- base-addr
    |-- irq
    |-- dma
    |-- modes
    `-- spintime


File:		Contents:

devices/active	A list of the device drivers using that port.  A "+"
		will appear by the name of the device currently using
		the port (it might not appear against any).  The
		string "none" means that there are no device drivers
		using that port.

base-addr	Parallel port's base address, or addresses if the port
		has more than one in which case they are separated
		with tabs.  These values might not have any sensible
		meaning for some ports.

irq		Parallel port's IRQ, or -1 if none is being used.

dma		Parallel port's DMA channel, or -1 if none is being
		used.

modes		Parallel port's hardware modes, comma-separated,
		meaning:

		PCSPP		PC-style SPP registers are available.
		TRISTATE	Port is bidirectional.
		COMPAT		Hardware acceleration for printers is
				available and will be used.
		EPP		Hardware acceleration for EPP protocol
				is available and will be used.
		ECP		Hardware acceleration for ECP protocol
				is available and will be used.
		DMA		DMA is available and will be used.

		Note that the current implementation will only take
		advantage of COMPAT and ECP modes if it has an IRQ
		line to use.

autoprobe	Any IEEE-1284 device ID information that has been
		acquired from the (non-IEEE 1284.3) device.

autoprobe[0-3]	IEEE 1284 device ID information retrieved from
		daisy-chain devices that conform to IEEE 1284.3.

spintime	The number of microseconds to busy-loop while waiting
		for the peripheral to respond.  You might find that
		adjusting this improves performance, depending on your
		peripherals.  This is a port-wide setting, i.e. it
		applies to all devices on a particular port.

timeslice	The number of milliseconds that a device driver is
		allowed to keep a port claimed for.  This is advisory,
		and driver can ignore it if it must.

default/*	The defaults for spintime and timeslice. When a new
		port is	registered, it picks up the default spintime.
		When a new device is registered, it picks up the
		default timeslice.

Device drivers
==============

Once the parport code is initialised, you can attach device drivers to
specific ports.  Normally this happens automatically; if the lp driver
is loaded it will create one lp device for each port found.  You can
override this, though, by using parameters either when you load the lp
driver:

	# insmod lp parport=0,2

or on the LILO command line:

	lp=parport0 lp=parport2

Both the above examples would inform lp that you want /dev/lp0 to be
the first parallel port, and /dev/lp1 to be the _third_ parallel port,
with no lp device associated with the second port (parport1).  Note
that this is different to the way older kernels worked; there used to
be a static association between the I/O port address and the device
name, so /dev/lp0 was always the port at 0x3bc.  This is no longer the
case - if you only have one port, it will default to being /dev/lp0,
regardless of base address.

Also:

 * If you selected the IEEE 1284 support at compile time, you can say
   `lp=auto' on the kernel command line, and lp will create devices
   only for those ports that seem to have printers attached.

 * If you give PLIP the `timid' parameter, either with `plip=timid' on
   the command line, or with `insmod plip timid=1' when using modules,
   it will avoid any ports that seem to be in use by other devices.

 * IRQ autoprobing works only for a few port types at the moment.

Reporting printer problems with parport
=======================================

If you are having problems printing, please go through these steps to
try to narrow down where the problem area is.

When reporting problems with parport, really you need to give all of
the messages that parport_pc spits out when it initialises.  There are
several code paths:

o polling
o interrupt-driven, protocol in software
o interrupt-driven, protocol in hardware using PIO
o interrupt-driven, protocol in hardware using DMA

The kernel messages that parport_pc logs give an indication of which
code path is being used. (They could be a lot better actually..)

For normal printer protocol, having IEEE 1284 modes enabled or not
should not make a difference.

To turn off the 'protocol in hardware' code paths, disable
CONFIG_PARPORT_PC_FIFO.  Note that when they are enabled they are not
necessarily _used_; it depends on whether the hardware is available,
enabled by the BIOS, and detected by the driver.

So, to start with, disable CONFIG_PARPORT_PC_FIFO, and load parport_pc
with 'irq=none'. See if printing works then.  It really should,
because this is the simplest code path.

If that works fine, try with 'io=0x378 irq=7' (adjust for your
hardware), to make it use interrupt-driven in-software protocol.

If _that_ works fine, then one of the hardware modes isn't working
right.  Enable CONFIG_PARPORT_PC_FIFO (no, it isn't a module option,
and yes, it should be), set the port to ECP mode in the BIOS and note
the DMA channel, and try with:

    io=0x378 irq=7 dma=none (for PIO)
    io=0x378 irq=7 dma=3 (for DMA)
--
philb@gnu.org
tim@cyberelk.net
Percpu rw semaphores
--------------------

Percpu rw semaphores is a new read-write semaphore design that is
optimized for locking for reading.

The problem with traditional read-write semaphores is that when multiple
cores take the lock for reading, the cache line containing the semaphore
is bouncing between L1 caches of the cores, causing performance
degradation.

Locking for reading is very fast, it uses RCU and it avoids any atomic
instruction in the lock and unlock path. On the other hand, locking for
writing is very expensive, it calls synchronize_rcu() that can take
hundreds of milliseconds.

The lock is declared with "struct percpu_rw_semaphore" type.
The lock is initialized percpu_init_rwsem, it returns 0 on success and
-ENOMEM on allocation failure.
The lock must be freed with percpu_free_rwsem to avoid memory leak.

The lock is locked for read with percpu_down_read, percpu_up_read and
for write with percpu_down_write, percpu_up_write.

The idea of using RCU for optimized rw-lock was introduced by
Eric Dumazet <eric.dumazet@gmail.com>.
The code was written by Mikulas Patocka <mpatocka@redhat.com>
			    PHY SUBSYSTEM
		  Kishon Vijay Abraham I <kishon@ti.com>

This document explains the Generic PHY Framework along with the APIs provided,
and how-to-use.

1. Introduction

*PHY* is the abbreviation for physical layer. It is used to connect a device
to the physical medium e.g., the USB controller has a PHY to provide functions
such as serialization, de-serialization, encoding, decoding and is responsible
for obtaining the required data transmission rate. Note that some USB
controllers have PHY functionality embedded into it and others use an external
PHY. Other peripherals that use PHY include Wireless LAN, Ethernet,
SATA etc.

The intention of creating this framework is to bring the PHY drivers spread
all over the Linux kernel to drivers/phy to increase code re-use and for
better code maintainability.

This framework will be of use only to devices that use external PHY (PHY
functionality is not embedded within the controller).

2. Registering/Unregistering the PHY provider

PHY provider refers to an entity that implements one or more PHY instances.
For the simple case where the PHY provider implements only a single instance of
the PHY, the framework provides its own implementation of of_xlate in
of_phy_simple_xlate. If the PHY provider implements multiple instances, it
should provide its own implementation of of_xlate. of_xlate is used only for
dt boot case.

#define of_phy_provider_register(dev, xlate)    \
        __of_phy_provider_register((dev), THIS_MODULE, (xlate))

#define devm_of_phy_provider_register(dev, xlate)       \
        __devm_of_phy_provider_register((dev), THIS_MODULE, (xlate))

of_phy_provider_register and devm_of_phy_provider_register macros can be used to
register the phy_provider and it takes device and of_xlate as
arguments. For the dt boot case, all PHY providers should use one of the above
2 macros to register the PHY provider.

void devm_of_phy_provider_unregister(struct device *dev,
	struct phy_provider *phy_provider);
void of_phy_provider_unregister(struct phy_provider *phy_provider);

devm_of_phy_provider_unregister and of_phy_provider_unregister can be used to
unregister the PHY.

3. Creating the PHY

The PHY driver should create the PHY in order for other peripheral controllers
to make use of it. The PHY framework provides 2 APIs to create the PHY.

struct phy *phy_create(struct device *dev, const struct phy_ops *ops,
        struct phy_init_data *init_data);
struct phy *devm_phy_create(struct device *dev, const struct phy_ops *ops,
	struct phy_init_data *init_data);

The PHY drivers can use one of the above 2 APIs to create the PHY by passing
the device pointer, phy ops and init_data.
phy_ops is a set of function pointers for performing PHY operations such as
init, exit, power_on and power_off. *init_data* is mandatory to get a reference
to the PHY in the case of non-dt boot. See section *Board File Initialization*
on how init_data should be used.

Inorder to dereference the private data (in phy_ops), the phy provider driver
can use phy_set_drvdata() after creating the PHY and use phy_get_drvdata() in
phy_ops to get back the private data.

4. Getting a reference to the PHY

Before the controller can make use of the PHY, it has to get a reference to
it. This framework provides the following APIs to get a reference to the PHY.

struct phy *phy_get(struct device *dev, const char *string);
struct phy *phy_optional_get(struct device *dev, const char *string);
struct phy *devm_phy_get(struct device *dev, const char *string);
struct phy *devm_phy_optional_get(struct device *dev, const char *string);

phy_get, phy_optional_get, devm_phy_get and devm_phy_optional_get can
be used to get the PHY. In the case of dt boot, the string arguments
should contain the phy name as given in the dt data and in the case of
non-dt boot, it should contain the label of the PHY.  The two
devm_phy_get associates the device with the PHY using devres on
successful PHY get. On driver detach, release function is invoked on
the the devres data and devres data is freed. phy_optional_get and
devm_phy_optional_get should be used when the phy is optional. These
two functions will never return -ENODEV, but instead returns NULL when
the phy cannot be found.

It should be noted that NULL is a valid phy reference. All phy
consumer calls on the NULL phy become NOPs. That is the release calls,
the phy_init() and phy_exit() calls, and phy_power_on() and
phy_power_off() calls are all NOP when applied to a NULL phy. The NULL
phy is useful in devices for handling optional phy devices.

5. Releasing a reference to the PHY

When the controller no longer needs the PHY, it has to release the reference
to the PHY it has obtained using the APIs mentioned in the above section. The
PHY framework provides 2 APIs to release a reference to the PHY.

void phy_put(struct phy *phy);
void devm_phy_put(struct device *dev, struct phy *phy);

Both these APIs are used to release a reference to the PHY and devm_phy_put
destroys the devres associated with this PHY.

6. Destroying the PHY

When the driver that created the PHY is unloaded, it should destroy the PHY it
created using one of the following 2 APIs.

void phy_destroy(struct phy *phy);
void devm_phy_destroy(struct device *dev, struct phy *phy);

Both these APIs destroy the PHY and devm_phy_destroy destroys the devres
associated with this PHY.

7. PM Runtime

This subsystem is pm runtime enabled. So while creating the PHY,
pm_runtime_enable of the phy device created by this subsystem is called and
while destroying the PHY, pm_runtime_disable is called. Note that the phy
device created by this subsystem will be a child of the device that calls
phy_create (PHY provider device).

So pm_runtime_get_sync of the phy_device created by this subsystem will invoke
pm_runtime_get_sync of PHY provider device because of parent-child relationship.
It should also be noted that phy_power_on and phy_power_off performs
phy_pm_runtime_get_sync and phy_pm_runtime_put respectively.
There are exported APIs like phy_pm_runtime_get, phy_pm_runtime_get_sync,
phy_pm_runtime_put, phy_pm_runtime_put_sync, phy_pm_runtime_allow and
phy_pm_runtime_forbid for performing PM operations.

8. Board File Initialization

Certain board file initialization is necessary in order to get a reference
to the PHY in the case of non-dt boot.
Say we have a single device that implements 3 PHYs that of USB, SATA and PCIe,
then in the board file the following initialization should be done.

struct phy_consumer consumers[] = {
	PHY_CONSUMER("dwc3.0", "usb"),
	PHY_CONSUMER("pcie.0", "pcie"),
	PHY_CONSUMER("sata.0", "sata"),
};
PHY_CONSUMER takes 2 parameters, first is the device name of the controller
(PHY consumer) and second is the port name.

struct phy_init_data init_data = {
	.consumers = consumers,
	.num_consumers = ARRAY_SIZE(consumers),
};

static const struct platform_device pipe3_phy_dev = {
	.name = "pipe3-phy",
	.id = -1,
	.dev = {
		.platform_data = {
			.init_data = &init_data,
		},
	},
};

then, while doing phy_create, the PHY driver should pass this init_data
	phy_create(dev, ops, pdata->init_data);

and the controller driver (phy consumer) should pass the port name along with
the device to get a reference to the PHY
	phy_get(dev, "pcie");

9. DeviceTree Binding

The documentation for PHY dt binding can be found @
Documentation/devicetree/bindings/phy/phy-bindings.txt
Lightweight PI-futexes
----------------------

We are calling them lightweight for 3 reasons:

 - in the user-space fastpath a PI-enabled futex involves no kernel work
   (or any other PI complexity) at all. No registration, no extra kernel
   calls - just pure fast atomic ops in userspace.

 - even in the slowpath, the system call and scheduling pattern is very
   similar to normal futexes.

 - the in-kernel PI implementation is streamlined around the mutex
   abstraction, with strict rules that keep the implementation
   relatively simple: only a single owner may own a lock (i.e. no
   read-write lock support), only the owner may unlock a lock, no
   recursive locking, etc.

Priority Inheritance - why?
---------------------------

The short reply: user-space PI helps achieving/improving determinism for
user-space applications. In the best-case, it can help achieve
determinism and well-bound latencies. Even in the worst-case, PI will
improve the statistical distribution of locking related application
delays.

The longer reply:
-----------------

Firstly, sharing locks between multiple tasks is a common programming
technique that often cannot be replaced with lockless algorithms. As we
can see it in the kernel [which is a quite complex program in itself],
lockless structures are rather the exception than the norm - the current
ratio of lockless vs. locky code for shared data structures is somewhere
between 1:10 and 1:100. Lockless is hard, and the complexity of lockless
algorithms often endangers to ability to do robust reviews of said code.
I.e. critical RT apps often choose lock structures to protect critical
data structures, instead of lockless algorithms. Furthermore, there are
cases (like shared hardware, or other resource limits) where lockless
access is mathematically impossible.

Media players (such as Jack) are an example of reasonable application
design with multiple tasks (with multiple priority levels) sharing
short-held locks: for example, a highprio audio playback thread is
combined with medium-prio construct-audio-data threads and low-prio
display-colory-stuff threads. Add video and decoding to the mix and
we've got even more priority levels.

So once we accept that synchronization objects (locks) are an
unavoidable fact of life, and once we accept that multi-task userspace
apps have a very fair expectation of being able to use locks, we've got
to think about how to offer the option of a deterministic locking
implementation to user-space.

Most of the technical counter-arguments against doing priority
inheritance only apply to kernel-space locks. But user-space locks are
different, there we cannot disable interrupts or make the task
non-preemptible in a critical section, so the 'use spinlocks' argument
does not apply (user-space spinlocks have the same priority inversion
problems as other user-space locking constructs). Fact is, pretty much
the only technique that currently enables good determinism for userspace
locks (such as futex-based pthread mutexes) is priority inheritance:

Currently (without PI), if a high-prio and a low-prio task shares a lock
[this is a quite common scenario for most non-trivial RT applications],
even if all critical sections are coded carefully to be deterministic
(i.e. all critical sections are short in duration and only execute a
limited number of instructions), the kernel cannot guarantee any
deterministic execution of the high-prio task: any medium-priority task
could preempt the low-prio task while it holds the shared lock and
executes the critical section, and could delay it indefinitely.

Implementation:
---------------

As mentioned before, the userspace fastpath of PI-enabled pthread
mutexes involves no kernel work at all - they behave quite similarly to
normal futex-based locks: a 0 value means unlocked, and a value==TID
means locked. (This is the same method as used by list-based robust
futexes.) Userspace uses atomic ops to lock/unlock these mutexes without
entering the kernel.

To handle the slowpath, we have added two new futex ops:

  FUTEX_LOCK_PI
  FUTEX_UNLOCK_PI

If the lock-acquire fastpath fails, [i.e. an atomic transition from 0 to
TID fails], then FUTEX_LOCK_PI is called. The kernel does all the
remaining work: if there is no futex-queue attached to the futex address
yet then the code looks up the task that owns the futex [it has put its
own TID into the futex value], and attaches a 'PI state' structure to
the futex-queue. The pi_state includes an rt-mutex, which is a PI-aware,
kernel-based synchronization object. The 'other' task is made the owner
of the rt-mutex, and the FUTEX_WAITERS bit is atomically set in the
futex value. Then this task tries to lock the rt-mutex, on which it
blocks. Once it returns, it has the mutex acquired, and it sets the
futex value to its own TID and returns. Userspace has no other work to
perform - it now owns the lock, and futex value contains
FUTEX_WAITERS|TID.

If the unlock side fastpath succeeds, [i.e. userspace manages to do a
TID -> 0 atomic transition of the futex value], then no kernel work is
triggered.

If the unlock fastpath fails (because the FUTEX_WAITERS bit is set),
then FUTEX_UNLOCK_PI is called, and the kernel unlocks the futex on the
behalf of userspace - and it also unlocks the attached
pi_state->rt_mutex and thus wakes up any potential waiters.

Note that under this approach, contrary to previous PI-futex approaches,
there is no prior 'registration' of a PI-futex. [which is not quite
possible anyway, due to existing ABI properties of pthread mutexes.]

Also, under this scheme, 'robustness' and 'PI' are two orthogonal
properties of futexes, and all four combinations are possible: futex,
robust-futex, PI-futex, robust+PI-futex.

More details about priority inheritance can be found in
Documentation/rt-mutex.txt.
PINCTRL (PIN CONTROL) subsystem
This document outlines the pin control subsystem in Linux

This subsystem deals with:

- Enumerating and naming controllable pins

- Multiplexing of pins, pads, fingers (etc) see below for details

- Configuration of pins, pads, fingers (etc), such as software-controlled
  biasing and driving mode specific pins, such as pull-up/down, open drain,
  load capacitance etc.

Top-level interface
===================

Definition of PIN CONTROLLER:

- A pin controller is a piece of hardware, usually a set of registers, that
  can control PINs. It may be able to multiplex, bias, set load capacitance,
  set drive strength, etc. for individual pins or groups of pins.

Definition of PIN:

- PINS are equal to pads, fingers, balls or whatever packaging input or
  output line you want to control and these are denoted by unsigned integers
  in the range 0..maxpin. This numberspace is local to each PIN CONTROLLER, so
  there may be several such number spaces in a system. This pin space may
  be sparse - i.e. there may be gaps in the space with numbers where no
  pin exists.

When a PIN CONTROLLER is instantiated, it will register a descriptor to the
pin control framework, and this descriptor contains an array of pin descriptors
describing the pins handled by this specific pin controller.

Here is an example of a PGA (Pin Grid Array) chip seen from underneath:

        A   B   C   D   E   F   G   H

   8    o   o   o   o   o   o   o   o

   7    o   o   o   o   o   o   o   o

   6    o   o   o   o   o   o   o   o

   5    o   o   o   o   o   o   o   o

   4    o   o   o   o   o   o   o   o

   3    o   o   o   o   o   o   o   o

   2    o   o   o   o   o   o   o   o

   1    o   o   o   o   o   o   o   o

To register a pin controller and name all the pins on this package we can do
this in our driver:

#include <linux/pinctrl/pinctrl.h>

const struct pinctrl_pin_desc foo_pins[] = {
      PINCTRL_PIN(0, "A8"),
      PINCTRL_PIN(1, "B8"),
      PINCTRL_PIN(2, "C8"),
      ...
      PINCTRL_PIN(61, "F1"),
      PINCTRL_PIN(62, "G1"),
      PINCTRL_PIN(63, "H1"),
};

static struct pinctrl_desc foo_desc = {
	.name = "foo",
	.pins = foo_pins,
	.npins = ARRAY_SIZE(foo_pins),
	.maxpin = 63,
	.owner = THIS_MODULE,
};

int __init foo_probe(void)
{
	struct pinctrl_dev *pctl;

	pctl = pinctrl_register(&foo_desc, <PARENT>, NULL);
	if (!pctl)
		pr_err("could not register foo pin driver\n");
}

To enable the pinctrl subsystem and the subgroups for PINMUX and PINCONF and
selected drivers, you need to select them from your machine's Kconfig entry,
since these are so tightly integrated with the machines they are used on.
See for example arch/arm/mach-u300/Kconfig for an example.

Pins usually have fancier names than this. You can find these in the datasheet
for your chip. Notice that the core pinctrl.h file provides a fancy macro
called PINCTRL_PIN() to create the struct entries. As you can see I enumerated
the pins from 0 in the upper left corner to 63 in the lower right corner.
This enumeration was arbitrarily chosen, in practice you need to think
through your numbering system so that it matches the layout of registers
and such things in your driver, or the code may become complicated. You must
also consider matching of offsets to the GPIO ranges that may be handled by
the pin controller.

For a padring with 467 pads, as opposed to actual pins, I used an enumeration
like this, walking around the edge of the chip, which seems to be industry
standard too (all these pads had names, too):


     0 ..... 104
   466        105
     .        .
     .        .
   358        224
    357 .... 225


Pin groups
==========

Many controllers need to deal with groups of pins, so the pin controller
subsystem has a mechanism for enumerating groups of pins and retrieving the
actual enumerated pins that are part of a certain group.

For example, say that we have a group of pins dealing with an SPI interface
on { 0, 8, 16, 24 }, and a group of pins dealing with an I2C interface on pins
on { 24, 25 }.

These two groups are presented to the pin control subsystem by implementing
some generic pinctrl_ops like this:

#include <linux/pinctrl/pinctrl.h>

struct foo_group {
	const char *name;
	const unsigned int *pins;
	const unsigned num_pins;
};

static const unsigned int spi0_pins[] = { 0, 8, 16, 24 };
static const unsigned int i2c0_pins[] = { 24, 25 };

static const struct foo_group foo_groups[] = {
	{
		.name = "spi0_grp",
		.pins = spi0_pins,
		.num_pins = ARRAY_SIZE(spi0_pins),
	},
	{
		.name = "i2c0_grp",
		.pins = i2c0_pins,
		.num_pins = ARRAY_SIZE(i2c0_pins),
	},
};


static int foo_get_groups_count(struct pinctrl_dev *pctldev)
{
	return ARRAY_SIZE(foo_groups);
}

static const char *foo_get_group_name(struct pinctrl_dev *pctldev,
				       unsigned selector)
{
	return foo_groups[selector].name;
}

static int foo_get_group_pins(struct pinctrl_dev *pctldev, unsigned selector,
			       unsigned ** const pins,
			       unsigned * const num_pins)
{
	*pins = (unsigned *) foo_groups[selector].pins;
	*num_pins = foo_groups[selector].num_pins;
	return 0;
}

static struct pinctrl_ops foo_pctrl_ops = {
	.get_groups_count = foo_get_groups_count,
	.get_group_name = foo_get_group_name,
	.get_group_pins = foo_get_group_pins,
};


static struct pinctrl_desc foo_desc = {
       ...
       .pctlops = &foo_pctrl_ops,
};

The pin control subsystem will call the .get_groups_count() function to
determine the total number of legal selectors, then it will call the other functions
to retrieve the name and pins of the group. Maintaining the data structure of
the groups is up to the driver, this is just a simple example - in practice you
may need more entries in your group structure, for example specific register
ranges associated with each group and so on.


Pin configuration
=================

Pins can sometimes be software-configured in various ways, mostly related
to their electronic properties when used as inputs or outputs. For example you
may be able to make an output pin high impedance, or "tristate" meaning it is
effectively disconnected. You may be able to connect an input pin to VDD or GND
using a certain resistor value - pull up and pull down - so that the pin has a
stable value when nothing is driving the rail it is connected to, or when it's
unconnected.

Pin configuration can be programmed by adding configuration entries into the
mapping table; see section "Board/machine configuration" below.

The format and meaning of the configuration parameter, PLATFORM_X_PULL_UP
above, is entirely defined by the pin controller driver.

The pin configuration driver implements callbacks for changing pin
configuration in the pin controller ops like this:

#include <linux/pinctrl/pinctrl.h>
#include <linux/pinctrl/pinconf.h>
#include "platform_x_pindefs.h"

static int foo_pin_config_get(struct pinctrl_dev *pctldev,
		    unsigned offset,
		    unsigned long *config)
{
	struct my_conftype conf;

	... Find setting for pin @ offset ...

	*config = (unsigned long) conf;
}

static int foo_pin_config_set(struct pinctrl_dev *pctldev,
		    unsigned offset,
		    unsigned long config)
{
	struct my_conftype *conf = (struct my_conftype *) config;

	switch (conf) {
		case PLATFORM_X_PULL_UP:
		...
		}
	}
}

static int foo_pin_config_group_get (struct pinctrl_dev *pctldev,
		    unsigned selector,
		    unsigned long *config)
{
	...
}

static int foo_pin_config_group_set (struct pinctrl_dev *pctldev,
		    unsigned selector,
		    unsigned long config)
{
	...
}

static struct pinconf_ops foo_pconf_ops = {
	.pin_config_get = foo_pin_config_get,
	.pin_config_set = foo_pin_config_set,
	.pin_config_group_get = foo_pin_config_group_get,
	.pin_config_group_set = foo_pin_config_group_set,
};

/* Pin config operations are handled by some pin controller */
static struct pinctrl_desc foo_desc = {
	...
	.confops = &foo_pconf_ops,
};

Since some controllers have special logic for handling entire groups of pins
they can exploit the special whole-group pin control function. The
pin_config_group_set() callback is allowed to return the error code -EAGAIN,
for groups it does not want to handle, or if it just wants to do some
group-level handling and then fall through to iterate over all pins, in which
case each individual pin will be treated by separate pin_config_set() calls as
well.


Interaction with the GPIO subsystem
===================================

The GPIO drivers may want to perform operations of various types on the same
physical pins that are also registered as pin controller pins.

First and foremost, the two subsystems can be used as completely orthogonal,
see the section named "pin control requests from drivers" and
"drivers needing both pin control and GPIOs" below for details. But in some
situations a cross-subsystem mapping between pins and GPIOs is needed.

Since the pin controller subsystem have its pinspace local to the pin
controller we need a mapping so that the pin control subsystem can figure out
which pin controller handles control of a certain GPIO pin. Since a single
pin controller may be muxing several GPIO ranges (typically SoCs that have
one set of pins, but internally several GPIO silicon blocks, each modelled as
a struct gpio_chip) any number of GPIO ranges can be added to a pin controller
instance like this:

struct gpio_chip chip_a;
struct gpio_chip chip_b;

static struct pinctrl_gpio_range gpio_range_a = {
	.name = "chip a",
	.id = 0,
	.base = 32,
	.pin_base = 32,
	.npins = 16,
	.gc = &chip_a;
};

static struct pinctrl_gpio_range gpio_range_b = {
	.name = "chip b",
	.id = 0,
	.base = 48,
	.pin_base = 64,
	.npins = 8,
	.gc = &chip_b;
};

{
	struct pinctrl_dev *pctl;
	...
	pinctrl_add_gpio_range(pctl, &gpio_range_a);
	pinctrl_add_gpio_range(pctl, &gpio_range_b);
}

So this complex system has one pin controller handling two different
GPIO chips. "chip a" has 16 pins and "chip b" has 8 pins. The "chip a" and
"chip b" have different .pin_base, which means a start pin number of the
GPIO range.

The GPIO range of "chip a" starts from the GPIO base of 32 and actual
pin range also starts from 32. However "chip b" has different starting
offset for the GPIO range and pin range. The GPIO range of "chip b" starts
from GPIO number 48, while the pin range of "chip b" starts from 64.

We can convert a gpio number to actual pin number using this "pin_base".
They are mapped in the global GPIO pin space at:

chip a:
 - GPIO range : [32 .. 47]
 - pin range  : [32 .. 47]
chip b:
 - GPIO range : [48 .. 55]
 - pin range  : [64 .. 71]

The above examples assume the mapping between the GPIOs and pins is
linear. If the mapping is sparse or haphazard, an array of arbitrary pin
numbers can be encoded in the range like this:

static const unsigned range_pins[] = { 14, 1, 22, 17, 10, 8, 6, 2 };

static struct pinctrl_gpio_range gpio_range = {
	.name = "chip",
	.id = 0,
	.base = 32,
	.pins = &range_pins,
	.npins = ARRAY_SIZE(range_pins),
	.gc = &chip;
};

In this case the pin_base property will be ignored. If the name of a pin
group is known, the pins and npins elements of the above structure can be
initialised using the function pinctrl_get_group_pins(), e.g. for pin
group "foo":

pinctrl_get_group_pins(pctl, "foo", &gpio_range.pins, &gpio_range.npins);

When GPIO-specific functions in the pin control subsystem are called, these
ranges will be used to look up the appropriate pin controller by inspecting
and matching the pin to the pin ranges across all controllers. When a
pin controller handling the matching range is found, GPIO-specific functions
will be called on that specific pin controller.

For all functionalities dealing with pin biasing, pin muxing etc, the pin
controller subsystem will look up the corresponding pin number from the passed
in gpio number, and use the range's internals to retrieve a pin number. After
that, the subsystem passes it on to the pin control driver, so the driver
will get a pin number into its handled number range. Further it is also passed
the range ID value, so that the pin controller knows which range it should
deal with.

Calling pinctrl_add_gpio_range from pinctrl driver is DEPRECATED. Please see
section 2.1 of Documentation/devicetree/bindings/gpio/gpio.txt on how to bind
pinctrl and gpio drivers.


PINMUX interfaces
=================

These calls use the pinmux_* naming prefix.  No other calls should use that
prefix.


What is pinmuxing?
==================

PINMUX, also known as padmux, ballmux, alternate functions or mission modes
is a way for chip vendors producing some kind of electrical packages to use
a certain physical pin (ball, pad, finger, etc) for multiple mutually exclusive
functions, depending on the application. By "application" in this context
we usually mean a way of soldering or wiring the package into an electronic
system, even though the framework makes it possible to also change the function
at runtime.

Here is an example of a PGA (Pin Grid Array) chip seen from underneath:

        A   B   C   D   E   F   G   H
      +---+
   8  | o | o   o   o   o   o   o   o
      |   |
   7  | o | o   o   o   o   o   o   o
      |   |
   6  | o | o   o   o   o   o   o   o
      +---+---+
   5  | o | o | o   o   o   o   o   o
      +---+---+               +---+
   4    o   o   o   o   o   o | o | o
                              |   |
   3    o   o   o   o   o   o | o | o
                              |   |
   2    o   o   o   o   o   o | o | o
      +-------+-------+-------+---+---+
   1  | o   o | o   o | o   o | o | o |
      +-------+-------+-------+---+---+

This is not tetris. The game to think of is chess. Not all PGA/BGA packages
are chessboard-like, big ones have "holes" in some arrangement according to
different design patterns, but we're using this as a simple example. Of the
pins you see some will be taken by things like a few VCC and GND to feed power
to the chip, and quite a few will be taken by large ports like an external
memory interface. The remaining pins will often be subject to pin multiplexing.

The example 8x8 PGA package above will have pin numbers 0 through 63 assigned
to its physical pins. It will name the pins { A1, A2, A3 ... H6, H7, H8 } using
pinctrl_register_pins() and a suitable data set as shown earlier.

In this 8x8 BGA package the pins { A8, A7, A6, A5 } can be used as an SPI port
(these are four pins: CLK, RXD, TXD, FRM). In that case, pin B5 can be used as
some general-purpose GPIO pin. However, in another setting, pins { A5, B5 } can
be used as an I2C port (these are just two pins: SCL, SDA). Needless to say,
we cannot use the SPI port and I2C port at the same time. However in the inside
of the package the silicon performing the SPI logic can alternatively be routed
out on pins { G4, G3, G2, G1 }.

On the bottom row at { A1, B1, C1, D1, E1, F1, G1, H1 } we have something
special - it's an external MMC bus that can be 2, 4 or 8 bits wide, and it will
consume 2, 4 or 8 pins respectively, so either { A1, B1 } are taken or
{ A1, B1, C1, D1 } or all of them. If we use all 8 bits, we cannot use the SPI
port on pins { G4, G3, G2, G1 } of course.

This way the silicon blocks present inside the chip can be multiplexed "muxed"
out on different pin ranges. Often contemporary SoC (systems on chip) will
contain several I2C, SPI, SDIO/MMC, etc silicon blocks that can be routed to
different pins by pinmux settings.

Since general-purpose I/O pins (GPIO) are typically always in shortage, it is
common to be able to use almost any pin as a GPIO pin if it is not currently
in use by some other I/O port.


Pinmux conventions
==================

The purpose of the pinmux functionality in the pin controller subsystem is to
abstract and provide pinmux settings to the devices you choose to instantiate
in your machine configuration. It is inspired by the clk, GPIO and regulator
subsystems, so devices will request their mux setting, but it's also possible
to request a single pin for e.g. GPIO.

Definitions:

- FUNCTIONS can be switched in and out by a driver residing with the pin
  control subsystem in the drivers/pinctrl/* directory of the kernel. The
  pin control driver knows the possible functions. In the example above you can
  identify three pinmux functions, one for spi, one for i2c and one for mmc.

- FUNCTIONS are assumed to be enumerable from zero in a one-dimensional array.
  In this case the array could be something like: { spi0, i2c0, mmc0 }
  for the three available functions.

- FUNCTIONS have PIN GROUPS as defined on the generic level - so a certain
  function is *always* associated with a certain set of pin groups, could
  be just a single one, but could also be many. In the example above the
  function i2c is associated with the pins { A5, B5 }, enumerated as
  { 24, 25 } in the controller pin space.

  The Function spi is associated with pin groups { A8, A7, A6, A5 }
  and { G4, G3, G2, G1 }, which are enumerated as { 0, 8, 16, 24 } and
  { 38, 46, 54, 62 } respectively.

  Group names must be unique per pin controller, no two groups on the same
  controller may have the same name.

- The combination of a FUNCTION and a PIN GROUP determine a certain function
  for a certain set of pins. The knowledge of the functions and pin groups
  and their machine-specific particulars are kept inside the pinmux driver,
  from the outside only the enumerators are known, and the driver core can:

  - Request the name of a function with a certain selector (>= 0)
  - A list of groups associated with a certain function
  - Request that a certain group in that list to be activated for a certain
    function

  As already described above, pin groups are in turn self-descriptive, so
  the core will retrieve the actual pin range in a certain group from the
  driver.

- FUNCTIONS and GROUPS on a certain PIN CONTROLLER are MAPPED to a certain
  device by the board file, device tree or similar machine setup configuration
  mechanism, similar to how regulators are connected to devices, usually by
  name. Defining a pin controller, function and group thus uniquely identify
  the set of pins to be used by a certain device. (If only one possible group
  of pins is available for the function, no group name need to be supplied -
  the core will simply select the first and only group available.)

  In the example case we can define that this particular machine shall
  use device spi0 with pinmux function fspi0 group gspi0 and i2c0 on function
  fi2c0 group gi2c0, on the primary pin controller, we get mappings
  like these:

  {
    {"map-spi0", spi0, pinctrl0, fspi0, gspi0},
    {"map-i2c0", i2c0, pinctrl0, fi2c0, gi2c0}
  }

  Every map must be assigned a state name, pin controller, device and
  function. The group is not compulsory - if it is omitted the first group
  presented by the driver as applicable for the function will be selected,
  which is useful for simple cases.

  It is possible to map several groups to the same combination of device,
  pin controller and function. This is for cases where a certain function on
  a certain pin controller may use different sets of pins in different
  configurations.

- PINS for a certain FUNCTION using a certain PIN GROUP on a certain
  PIN CONTROLLER are provided on a first-come first-serve basis, so if some
  other device mux setting or GPIO pin request has already taken your physical
  pin, you will be denied the use of it. To get (activate) a new setting, the
  old one has to be put (deactivated) first.

Sometimes the documentation and hardware registers will be oriented around
pads (or "fingers") rather than pins - these are the soldering surfaces on the
silicon inside the package, and may or may not match the actual number of
pins/balls underneath the capsule. Pick some enumeration that makes sense to
you. Define enumerators only for the pins you can control if that makes sense.

Assumptions:

We assume that the number of possible function maps to pin groups is limited by
the hardware. I.e. we assume that there is no system where any function can be
mapped to any pin, like in a phone exchange. So the available pin groups for
a certain function will be limited to a few choices (say up to eight or so),
not hundreds or any amount of choices. This is the characteristic we have found
by inspecting available pinmux hardware, and a necessary assumption since we
expect pinmux drivers to present *all* possible function vs pin group mappings
to the subsystem.


Pinmux drivers
==============

The pinmux core takes care of preventing conflicts on pins and calling
the pin controller driver to execute different settings.

It is the responsibility of the pinmux driver to impose further restrictions
(say for example infer electronic limitations due to load, etc.) to determine
whether or not the requested function can actually be allowed, and in case it
is possible to perform the requested mux setting, poke the hardware so that
this happens.

Pinmux drivers are required to supply a few callback functions, some are
optional. Usually the enable() and disable() functions are implemented,
writing values into some certain registers to activate a certain mux setting
for a certain pin.

A simple driver for the above example will work by setting bits 0, 1, 2, 3 or 4
into some register named MUX to select a certain function with a certain
group of pins would work something like this:

#include <linux/pinctrl/pinctrl.h>
#include <linux/pinctrl/pinmux.h>

struct foo_group {
	const char *name;
	const unsigned int *pins;
	const unsigned num_pins;
};

static const unsigned spi0_0_pins[] = { 0, 8, 16, 24 };
static const unsigned spi0_1_pins[] = { 38, 46, 54, 62 };
static const unsigned i2c0_pins[] = { 24, 25 };
static const unsigned mmc0_1_pins[] = { 56, 57 };
static const unsigned mmc0_2_pins[] = { 58, 59 };
static const unsigned mmc0_3_pins[] = { 60, 61, 62, 63 };

static const struct foo_group foo_groups[] = {
	{
		.name = "spi0_0_grp",
		.pins = spi0_0_pins,
		.num_pins = ARRAY_SIZE(spi0_0_pins),
	},
	{
		.name = "spi0_1_grp",
		.pins = spi0_1_pins,
		.num_pins = ARRAY_SIZE(spi0_1_pins),
	},
	{
		.name = "i2c0_grp",
		.pins = i2c0_pins,
		.num_pins = ARRAY_SIZE(i2c0_pins),
	},
	{
		.name = "mmc0_1_grp",
		.pins = mmc0_1_pins,
		.num_pins = ARRAY_SIZE(mmc0_1_pins),
	},
	{
		.name = "mmc0_2_grp",
		.pins = mmc0_2_pins,
		.num_pins = ARRAY_SIZE(mmc0_2_pins),
	},
	{
		.name = "mmc0_3_grp",
		.pins = mmc0_3_pins,
		.num_pins = ARRAY_SIZE(mmc0_3_pins),
	},
};


static int foo_get_groups_count(struct pinctrl_dev *pctldev)
{
	return ARRAY_SIZE(foo_groups);
}

static const char *foo_get_group_name(struct pinctrl_dev *pctldev,
				       unsigned selector)
{
	return foo_groups[selector].name;
}

static int foo_get_group_pins(struct pinctrl_dev *pctldev, unsigned selector,
			       unsigned ** const pins,
			       unsigned * const num_pins)
{
	*pins = (unsigned *) foo_groups[selector].pins;
	*num_pins = foo_groups[selector].num_pins;
	return 0;
}

static struct pinctrl_ops foo_pctrl_ops = {
	.get_groups_count = foo_get_groups_count,
	.get_group_name = foo_get_group_name,
	.get_group_pins = foo_get_group_pins,
};

struct foo_pmx_func {
	const char *name;
	const char * const *groups;
	const unsigned num_groups;
};

static const char * const spi0_groups[] = { "spi0_0_grp", "spi0_1_grp" };
static const char * const i2c0_groups[] = { "i2c0_grp" };
static const char * const mmc0_groups[] = { "mmc0_1_grp", "mmc0_2_grp",
					"mmc0_3_grp" };

static const struct foo_pmx_func foo_functions[] = {
	{
		.name = "spi0",
		.groups = spi0_groups,
		.num_groups = ARRAY_SIZE(spi0_groups),
	},
	{
		.name = "i2c0",
		.groups = i2c0_groups,
		.num_groups = ARRAY_SIZE(i2c0_groups),
	},
	{
		.name = "mmc0",
		.groups = mmc0_groups,
		.num_groups = ARRAY_SIZE(mmc0_groups),
	},
};

int foo_get_functions_count(struct pinctrl_dev *pctldev)
{
	return ARRAY_SIZE(foo_functions);
}

const char *foo_get_fname(struct pinctrl_dev *pctldev, unsigned selector)
{
	return foo_functions[selector].name;
}

static int foo_get_groups(struct pinctrl_dev *pctldev, unsigned selector,
			  const char * const **groups,
			  unsigned * const num_groups)
{
	*groups = foo_functions[selector].groups;
	*num_groups = foo_functions[selector].num_groups;
	return 0;
}

int foo_enable(struct pinctrl_dev *pctldev, unsigned selector,
		unsigned group)
{
	u8 regbit = (1 << selector + group);

	writeb((readb(MUX)|regbit), MUX)
	return 0;
}

void foo_disable(struct pinctrl_dev *pctldev, unsigned selector,
		unsigned group)
{
	u8 regbit = (1 << selector + group);

	writeb((readb(MUX) & ~(regbit)), MUX)
	return 0;
}

struct pinmux_ops foo_pmxops = {
	.get_functions_count = foo_get_functions_count,
	.get_function_name = foo_get_fname,
	.get_function_groups = foo_get_groups,
	.enable = foo_enable,
	.disable = foo_disable,
};

/* Pinmux operations are handled by some pin controller */
static struct pinctrl_desc foo_desc = {
	...
	.pctlops = &foo_pctrl_ops,
	.pmxops = &foo_pmxops,
};

In the example activating muxing 0 and 1 at the same time setting bits
0 and 1, uses one pin in common so they would collide.

The beauty of the pinmux subsystem is that since it keeps track of all
pins and who is using them, it will already have denied an impossible
request like that, so the driver does not need to worry about such
things - when it gets a selector passed in, the pinmux subsystem makes
sure no other device or GPIO assignment is already using the selected
pins. Thus bits 0 and 1 in the control register will never be set at the
same time.

All the above functions are mandatory to implement for a pinmux driver.


Pin control interaction with the GPIO subsystem
===============================================

Note that the following implies that the use case is to use a certain pin
from the Linux kernel using the API in <linux/gpio.h> with gpio_request()
and similar functions. There are cases where you may be using something
that your datasheet calls "GPIO mode", but actually is just an electrical
configuration for a certain device. See the section below named
"GPIO mode pitfalls" for more details on this scenario.

The public pinmux API contains two functions named pinctrl_request_gpio()
and pinctrl_free_gpio(). These two functions shall *ONLY* be called from
gpiolib-based drivers as part of their gpio_request() and
gpio_free() semantics. Likewise the pinctrl_gpio_direction_[input|output]
shall only be called from within respective gpio_direction_[input|output]
gpiolib implementation.

NOTE that platforms and individual drivers shall *NOT* request GPIO pins to be
controlled e.g. muxed in. Instead, implement a proper gpiolib driver and have
that driver request proper muxing and other control for its pins.

The function list could become long, especially if you can convert every
individual pin into a GPIO pin independent of any other pins, and then try
the approach to define every pin as a function.

In this case, the function array would become 64 entries for each GPIO
setting and then the device functions.

For this reason there are two functions a pin control driver can implement
to enable only GPIO on an individual pin: .gpio_request_enable() and
.gpio_disable_free().

This function will pass in the affected GPIO range identified by the pin
controller core, so you know which GPIO pins are being affected by the request
operation.

If your driver needs to have an indication from the framework of whether the
GPIO pin shall be used for input or output you can implement the
.gpio_set_direction() function. As described this shall be called from the
gpiolib driver and the affected GPIO range, pin offset and desired direction
will be passed along to this function.

Alternatively to using these special functions, it is fully allowed to use
named functions for each GPIO pin, the pinctrl_request_gpio() will attempt to
obtain the function "gpioN" where "N" is the global GPIO pin number if no
special GPIO-handler is registered.


GPIO mode pitfalls
==================

Due to the naming conventions used by hardware engineers, where "GPIO"
is taken to mean different things than what the kernel does, the developer
may be confused by a datasheet talking about a pin being possible to set
into "GPIO mode". It appears that what hardware engineers mean with
"GPIO mode" is not necessarily the use case that is implied in the kernel
interface <linux/gpio.h>: a pin that you grab from kernel code and then
either listen for input or drive high/low to assert/deassert some
external line.

Rather hardware engineers think that "GPIO mode" means that you can
software-control a few electrical properties of the pin that you would
not be able to control if the pin was in some other mode, such as muxed in
for a device.

The GPIO portions of a pin and its relation to a certain pin controller
configuration and muxing logic can be constructed in several ways. Here
are two examples:

(A)
                       pin config
                       logic regs
                       |               +- SPI
     Physical pins --- pad --- pinmux -+- I2C
                               |       +- mmc
                               |       +- GPIO
                               pin
                               multiplex
                               logic regs

Here some electrical properties of the pin can be configured no matter
whether the pin is used for GPIO or not. If you multiplex a GPIO onto a
pin, you can also drive it high/low from "GPIO" registers.
Alternatively, the pin can be controlled by a certain peripheral, while
still applying desired pin config properties. GPIO functionality is thus
orthogonal to any other device using the pin.

In this arrangement the registers for the GPIO portions of the pin controller,
or the registers for the GPIO hardware module are likely to reside in a
separate memory range only intended for GPIO driving, and the register
range dealing with pin config and pin multiplexing get placed into a
different memory range and a separate section of the data sheet.

(B)

                       pin config
                       logic regs
                       |               +- SPI
     Physical pins --- pad --- pinmux -+- I2C
                       |       |       +- mmc
                       |       |
                       GPIO    pin
                               multiplex
                               logic regs

In this arrangement, the GPIO functionality can always be enabled, such that
e.g. a GPIO input can be used to "spy" on the SPI/I2C/MMC signal while it is
pulsed out. It is likely possible to disrupt the traffic on the pin by doing
wrong things on the GPIO block, as it is never really disconnected. It is
possible that the GPIO, pin config and pin multiplex registers are placed into
the same memory range and the same section of the data sheet, although that
need not be the case.

From a kernel point of view, however, these are different aspects of the
hardware and shall be put into different subsystems:

- Registers (or fields within registers) that control electrical
  properties of the pin such as biasing and drive strength should be
  exposed through the pinctrl subsystem, as "pin configuration" settings.

- Registers (or fields within registers) that control muxing of signals
  from various other HW blocks (e.g. I2C, MMC, or GPIO) onto pins should
  be exposed through the pinctrl subsystem, as mux functions.

- Registers (or fields within registers) that control GPIO functionality
  such as setting a GPIO's output value, reading a GPIO's input value, or
  setting GPIO pin direction should be exposed through the GPIO subsystem,
  and if they also support interrupt capabilities, through the irqchip
  abstraction.

Depending on the exact HW register design, some functions exposed by the
GPIO subsystem may call into the pinctrl subsystem in order to
co-ordinate register settings across HW modules. In particular, this may
be needed for HW with separate GPIO and pin controller HW modules, where
e.g. GPIO direction is determined by a register in the pin controller HW
module rather than the GPIO HW module.

Electrical properties of the pin such as biasing and drive strength
may be placed at some pin-specific register in all cases or as part
of the GPIO register in case (B) especially. This doesn't mean that such
properties necessarily pertain to what the Linux kernel calls "GPIO".

Example: a pin is usually muxed in to be used as a UART TX line. But during
system sleep, we need to put this pin into "GPIO mode" and ground it.

If you make a 1-to-1 map to the GPIO subsystem for this pin, you may start
to think that you need to come up with something really complex, that the
pin shall be used for UART TX and GPIO at the same time, that you will grab
a pin control handle and set it to a certain state to enable UART TX to be
muxed in, then twist it over to GPIO mode and use gpio_direction_output()
to drive it low during sleep, then mux it over to UART TX again when you
wake up and maybe even gpio_request/gpio_free as part of this cycle. This
all gets very complicated.

The solution is to not think that what the datasheet calls "GPIO mode"
has to be handled by the <linux/gpio.h> interface. Instead view this as
a certain pin config setting. Look in e.g. <linux/pinctrl/pinconf-generic.h>
and you find this in the documentation:

  PIN_CONFIG_OUTPUT: this will configure the pin in output, use argument
     1 to indicate high level, argument 0 to indicate low level.

So it is perfectly possible to push a pin into "GPIO mode" and drive the
line low as part of the usual pin control map. So for example your UART
driver may look like this:

#include <linux/pinctrl/consumer.h>

struct pinctrl          *pinctrl;
struct pinctrl_state    *pins_default;
struct pinctrl_state    *pins_sleep;

pins_default = pinctrl_lookup_state(uap->pinctrl, PINCTRL_STATE_DEFAULT);
pins_sleep = pinctrl_lookup_state(uap->pinctrl, PINCTRL_STATE_SLEEP);

/* Normal mode */
retval = pinctrl_select_state(pinctrl, pins_default);
/* Sleep mode */
retval = pinctrl_select_state(pinctrl, pins_sleep);

And your machine configuration may look like this:
--------------------------------------------------

static unsigned long uart_default_mode[] = {
    PIN_CONF_PACKED(PIN_CONFIG_DRIVE_PUSH_PULL, 0),
};

static unsigned long uart_sleep_mode[] = {
    PIN_CONF_PACKED(PIN_CONFIG_OUTPUT, 0),
};

static struct pinctrl_map pinmap[] __initdata = {
    PIN_MAP_MUX_GROUP("uart", PINCTRL_STATE_DEFAULT, "pinctrl-foo",
                      "u0_group", "u0"),
    PIN_MAP_CONFIGS_PIN("uart", PINCTRL_STATE_DEFAULT, "pinctrl-foo",
                        "UART_TX_PIN", uart_default_mode),
    PIN_MAP_MUX_GROUP("uart", PINCTRL_STATE_SLEEP, "pinctrl-foo",
                      "u0_group", "gpio-mode"),
    PIN_MAP_CONFIGS_PIN("uart", PINCTRL_STATE_SLEEP, "pinctrl-foo",
                        "UART_TX_PIN", uart_sleep_mode),
};

foo_init(void) {
    pinctrl_register_mappings(pinmap, ARRAY_SIZE(pinmap));
}

Here the pins we want to control are in the "u0_group" and there is some
function called "u0" that can be enabled on this group of pins, and then
everything is UART business as usual. But there is also some function
named "gpio-mode" that can be mapped onto the same pins to move them into
GPIO mode.

This will give the desired effect without any bogus interaction with the
GPIO subsystem. It is just an electrical configuration used by that device
when going to sleep, it might imply that the pin is set into something the
datasheet calls "GPIO mode", but that is not the point: it is still used
by that UART device to control the pins that pertain to that very UART
driver, putting them into modes needed by the UART. GPIO in the Linux
kernel sense are just some 1-bit line, and is a different use case.

How the registers are poked to attain the push or pull, and output low
configuration and the muxing of the "u0" or "gpio-mode" group onto these
pins is a question for the driver.

Some datasheets will be more helpful and refer to the "GPIO mode" as
"low power mode" rather than anything to do with GPIO. This often means
the same thing electrically speaking, but in this latter case the
software engineers will usually quickly identify that this is some
specific muxing or configuration rather than anything related to the GPIO
API.


Board/machine configuration
==================================

Boards and machines define how a certain complete running system is put
together, including how GPIOs and devices are muxed, how regulators are
constrained and how the clock tree looks. Of course pinmux settings are also
part of this.

A pin controller configuration for a machine looks pretty much like a simple
regulator configuration, so for the example array above we want to enable i2c
and spi on the second function mapping:

#include <linux/pinctrl/machine.h>

static const struct pinctrl_map mapping[] __initconst = {
	{
		.dev_name = "foo-spi.0",
		.name = PINCTRL_STATE_DEFAULT,
		.type = PIN_MAP_TYPE_MUX_GROUP,
		.ctrl_dev_name = "pinctrl-foo",
		.data.mux.function = "spi0",
	},
	{
		.dev_name = "foo-i2c.0",
		.name = PINCTRL_STATE_DEFAULT,
		.type = PIN_MAP_TYPE_MUX_GROUP,
		.ctrl_dev_name = "pinctrl-foo",
		.data.mux.function = "i2c0",
	},
	{
		.dev_name = "foo-mmc.0",
		.name = PINCTRL_STATE_DEFAULT,
		.type = PIN_MAP_TYPE_MUX_GROUP,
		.ctrl_dev_name = "pinctrl-foo",
		.data.mux.function = "mmc0",
	},
};

The dev_name here matches to the unique device name that can be used to look
up the device struct (just like with clockdev or regulators). The function name
must match a function provided by the pinmux driver handling this pin range.

As you can see we may have several pin controllers on the system and thus
we need to specify which one of them contains the functions we wish to map.

You register this pinmux mapping to the pinmux subsystem by simply:

       ret = pinctrl_register_mappings(mapping, ARRAY_SIZE(mapping));

Since the above construct is pretty common there is a helper macro to make
it even more compact which assumes you want to use pinctrl-foo and position
0 for mapping, for example:

static struct pinctrl_map mapping[] __initdata = {
	PIN_MAP_MUX_GROUP("foo-i2c.o", PINCTRL_STATE_DEFAULT, "pinctrl-foo", NULL, "i2c0"),
};

The mapping table may also contain pin configuration entries. It's common for
each pin/group to have a number of configuration entries that affect it, so
the table entries for configuration reference an array of config parameters
and values. An example using the convenience macros is shown below:

static unsigned long i2c_grp_configs[] = {
	FOO_PIN_DRIVEN,
	FOO_PIN_PULLUP,
};

static unsigned long i2c_pin_configs[] = {
	FOO_OPEN_COLLECTOR,
	FOO_SLEW_RATE_SLOW,
};

static struct pinctrl_map mapping[] __initdata = {
	PIN_MAP_MUX_GROUP("foo-i2c.0", PINCTRL_STATE_DEFAULT, "pinctrl-foo", "i2c0", "i2c0"),
	PIN_MAP_CONFIGS_GROUP("foo-i2c.0", PINCTRL_STATE_DEFAULT, "pinctrl-foo", "i2c0", i2c_grp_configs),
	PIN_MAP_CONFIGS_PIN("foo-i2c.0", PINCTRL_STATE_DEFAULT, "pinctrl-foo", "i2c0scl", i2c_pin_configs),
	PIN_MAP_CONFIGS_PIN("foo-i2c.0", PINCTRL_STATE_DEFAULT, "pinctrl-foo", "i2c0sda", i2c_pin_configs),
};

Finally, some devices expect the mapping table to contain certain specific
named states. When running on hardware that doesn't need any pin controller
configuration, the mapping table must still contain those named states, in
order to explicitly indicate that the states were provided and intended to
be empty. Table entry macro PIN_MAP_DUMMY_STATE serves the purpose of defining
a named state without causing any pin controller to be programmed:

static struct pinctrl_map mapping[] __initdata = {
	PIN_MAP_DUMMY_STATE("foo-i2c.0", PINCTRL_STATE_DEFAULT),
};


Complex mappings
================

As it is possible to map a function to different groups of pins an optional
.group can be specified like this:

...
{
	.dev_name = "foo-spi.0",
	.name = "spi0-pos-A",
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "spi0",
	.group = "spi0_0_grp",
},
{
	.dev_name = "foo-spi.0",
	.name = "spi0-pos-B",
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "spi0",
	.group = "spi0_1_grp",
},
...

This example mapping is used to switch between two positions for spi0 at
runtime, as described further below under the heading "Runtime pinmuxing".

Further it is possible for one named state to affect the muxing of several
groups of pins, say for example in the mmc0 example above, where you can
additively expand the mmc0 bus from 2 to 4 to 8 pins. If we want to use all
three groups for a total of 2+2+4 = 8 pins (for an 8-bit MMC bus as is the
case), we define a mapping like this:

...
{
	.dev_name = "foo-mmc.0",
	.name = "2bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_1_grp",
},
{
	.dev_name = "foo-mmc.0",
	.name = "4bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_1_grp",
},
{
	.dev_name = "foo-mmc.0",
	.name = "4bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_2_grp",
},
{
	.dev_name = "foo-mmc.0",
	.name = "8bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_1_grp",
},
{
	.dev_name = "foo-mmc.0",
	.name = "8bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_2_grp",
},
{
	.dev_name = "foo-mmc.0",
	.name = "8bit"
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "mmc0",
	.group = "mmc0_3_grp",
},
...

The result of grabbing this mapping from the device with something like
this (see next paragraph):

	p = devm_pinctrl_get(dev);
	s = pinctrl_lookup_state(p, "8bit");
	ret = pinctrl_select_state(p, s);

or more simply:

	p = devm_pinctrl_get_select(dev, "8bit");

Will be that you activate all the three bottom records in the mapping at
once. Since they share the same name, pin controller device, function and
device, and since we allow multiple groups to match to a single device, they
all get selected, and they all get enabled and disable simultaneously by the
pinmux core.


Pin control requests from drivers
=================================

When a device driver is about to probe the device core will automatically
attempt to issue pinctrl_get_select_default() on these devices.
This way driver writers do not need to add any of the boilerplate code
of the type found below. However when doing fine-grained state selection
and not using the "default" state, you may have to do some device driver
handling of the pinctrl handles and states.

So if you just want to put the pins for a certain device into the default
state and be done with it, there is nothing you need to do besides
providing the proper mapping table. The device core will take care of
the rest.

Generally it is discouraged to let individual drivers get and enable pin
control. So if possible, handle the pin control in platform code or some other
place where you have access to all the affected struct device * pointers. In
some cases where a driver needs to e.g. switch between different mux mappings
at runtime this is not possible.

A typical case is if a driver needs to switch bias of pins from normal
operation and going to sleep, moving from the PINCTRL_STATE_DEFAULT to
PINCTRL_STATE_SLEEP at runtime, re-biasing or even re-muxing pins to save
current in sleep mode.

A driver may request a certain control state to be activated, usually just the
default state like this:

#include <linux/pinctrl/consumer.h>

struct foo_state {
       struct pinctrl *p;
       struct pinctrl_state *s;
       ...
};

foo_probe()
{
	/* Allocate a state holder named "foo" etc */
	struct foo_state *foo = ...;

	foo->p = devm_pinctrl_get(&device);
	if (IS_ERR(foo->p)) {
		/* FIXME: clean up "foo" here */
		return PTR_ERR(foo->p);
	}

	foo->s = pinctrl_lookup_state(foo->p, PINCTRL_STATE_DEFAULT);
	if (IS_ERR(foo->s)) {
		/* FIXME: clean up "foo" here */
		return PTR_ERR(s);
	}

	ret = pinctrl_select_state(foo->s);
	if (ret < 0) {
		/* FIXME: clean up "foo" here */
		return ret;
	}
}

This get/lookup/select/put sequence can just as well be handled by bus drivers
if you don't want each and every driver to handle it and you know the
arrangement on your bus.

The semantics of the pinctrl APIs are:

- pinctrl_get() is called in process context to obtain a handle to all pinctrl
  information for a given client device. It will allocate a struct from the
  kernel memory to hold the pinmux state. All mapping table parsing or similar
  slow operations take place within this API.

- devm_pinctrl_get() is a variant of pinctrl_get() that causes pinctrl_put()
  to be called automatically on the retrieved pointer when the associated
  device is removed. It is recommended to use this function over plain
  pinctrl_get().

- pinctrl_lookup_state() is called in process context to obtain a handle to a
  specific state for a client device. This operation may be slow, too.

- pinctrl_select_state() programs pin controller hardware according to the
  definition of the state as given by the mapping table. In theory, this is a
  fast-path operation, since it only involved blasting some register settings
  into hardware. However, note that some pin controllers may have their
  registers on a slow/IRQ-based bus, so client devices should not assume they
  can call pinctrl_select_state() from non-blocking contexts.

- pinctrl_put() frees all information associated with a pinctrl handle.

- devm_pinctrl_put() is a variant of pinctrl_put() that may be used to
  explicitly destroy a pinctrl object returned by devm_pinctrl_get().
  However, use of this function will be rare, due to the automatic cleanup
  that will occur even without calling it.

  pinctrl_get() must be paired with a plain pinctrl_put().
  pinctrl_get() may not be paired with devm_pinctrl_put().
  devm_pinctrl_get() can optionally be paired with devm_pinctrl_put().
  devm_pinctrl_get() may not be paired with plain pinctrl_put().

Usually the pin control core handled the get/put pair and call out to the
device drivers bookkeeping operations, like checking available functions and
the associated pins, whereas the enable/disable pass on to the pin controller
driver which takes care of activating and/or deactivating the mux setting by
quickly poking some registers.

The pins are allocated for your device when you issue the devm_pinctrl_get()
call, after this you should be able to see this in the debugfs listing of all
pins.

NOTE: the pinctrl system will return -EPROBE_DEFER if it cannot find the
requested pinctrl handles, for example if the pinctrl driver has not yet
registered. Thus make sure that the error path in your driver gracefully
cleans up and is ready to retry the probing later in the startup process.


Drivers needing both pin control and GPIOs
==========================================

Again, it is discouraged to let drivers lookup and select pin control states
themselves, but again sometimes this is unavoidable.

So say that your driver is fetching its resources like this:

#include <linux/pinctrl/consumer.h>
#include <linux/gpio.h>

struct pinctrl *pinctrl;
int gpio;

pinctrl = devm_pinctrl_get_select_default(&dev);
gpio = devm_gpio_request(&dev, 14, "foo");

Here we first request a certain pin state and then request GPIO 14 to be
used. If you're using the subsystems orthogonally like this, you should
nominally always get your pinctrl handle and select the desired pinctrl
state BEFORE requesting the GPIO. This is a semantic convention to avoid
situations that can be electrically unpleasant, you will certainly want to
mux in and bias pins in a certain way before the GPIO subsystems starts to
deal with them.

The above can be hidden: using the device core, the pinctrl core may be
setting up the config and muxing for the pins right before the device is
probing, nevertheless orthogonal to the GPIO subsystem.

But there are also situations where it makes sense for the GPIO subsystem
to communicate directly with the pinctrl subsystem, using the latter as a
back-end. This is when the GPIO driver may call out to the functions
described in the section "Pin control interaction with the GPIO subsystem"
above. This only involves per-pin multiplexing, and will be completely
hidden behind the gpio_*() function namespace. In this case, the driver
need not interact with the pin control subsystem at all.

If a pin control driver and a GPIO driver is dealing with the same pins
and the use cases involve multiplexing, you MUST implement the pin controller
as a back-end for the GPIO driver like this, unless your hardware design
is such that the GPIO controller can override the pin controller's
multiplexing state through hardware without the need to interact with the
pin control system.


System pin control hogging
==========================

Pin control map entries can be hogged by the core when the pin controller
is registered. This means that the core will attempt to call pinctrl_get(),
lookup_state() and select_state() on it immediately after the pin control
device has been registered.

This occurs for mapping table entries where the client device name is equal
to the pin controller device name, and the state name is PINCTRL_STATE_DEFAULT.

{
	.dev_name = "pinctrl-foo",
	.name = PINCTRL_STATE_DEFAULT,
	.type = PIN_MAP_TYPE_MUX_GROUP,
	.ctrl_dev_name = "pinctrl-foo",
	.function = "power_func",
},

Since it may be common to request the core to hog a few always-applicable
mux settings on the primary pin controller, there is a convenience macro for
this:

PIN_MAP_MUX_GROUP_HOG_DEFAULT("pinctrl-foo", NULL /* group */, "power_func")

This gives the exact same result as the above construction.


Runtime pinmuxing
=================

It is possible to mux a certain function in and out at runtime, say to move
an SPI port from one set of pins to another set of pins. Say for example for
spi0 in the example above, we expose two different groups of pins for the same
function, but with different named in the mapping as described under
"Advanced mapping" above. So that for an SPI device, we have two states named
"pos-A" and "pos-B".

This snippet first muxes the function in the pins defined by group A, enables
it, disables and releases it, and muxes it in on the pins defined by group B:

#include <linux/pinctrl/consumer.h>

struct pinctrl *p;
struct pinctrl_state *s1, *s2;

foo_probe()
{
	/* Setup */
	p = devm_pinctrl_get(&device);
	if (IS_ERR(p))
		...

	s1 = pinctrl_lookup_state(foo->p, "pos-A");
	if (IS_ERR(s1))
		...

	s2 = pinctrl_lookup_state(foo->p, "pos-B");
	if (IS_ERR(s2))
		...
}

foo_switch()
{
	/* Enable on position A */
	ret = pinctrl_select_state(s1);
	if (ret < 0)
	    ...

	...

	/* Enable on position B */
	ret = pinctrl_select_state(s2);
	if (ret < 0)
	    ...

	...
}

The above has to be done from process context. The reservation of the pins
will be done when the state is activated, so in effect one specific pin
can be used by different functions at different times on a running system.
Linux Plug and Play Documentation
by Adam Belay <ambx1@neo.rr.com>
last updated: Oct. 16, 2002
---------------------------------------------------------------------------------------



Overview
--------
	Plug and Play provides a means of detecting and setting resources for legacy or
otherwise unconfigurable devices.  The Linux Plug and Play Layer provides these 
services to compatible drivers.



The User Interface
------------------
	The Linux Plug and Play user interface provides a means to activate PnP devices
for legacy and user level drivers that do not support Linux Plug and Play.  The 
user interface is integrated into sysfs.

In addition to the standard sysfs file the following are created in each
device's directory:
id - displays a list of support EISA IDs
options - displays possible resource configurations
resources - displays currently allocated resources and allows resource changes

-activating a device

#echo "auto" > resources

this will invoke the automatic resource config system to activate the device

-manually activating a device

#echo "manual <depnum> <mode>" > resources
<depnum> - the configuration number
<mode> - static or dynamic
		static = for next boot
		dynamic = now

-disabling a device

#echo "disable" > resources


EXAMPLE:

Suppose you need to activate the floppy disk controller.
1.) change to the proper directory, in my case it is 
/driver/bus/pnp/devices/00:0f
# cd /driver/bus/pnp/devices/00:0f
# cat name
PC standard floppy disk controller

2.) check if the device is already active
# cat resources
DISABLED

- Notice the string "DISABLED".  This means the device is not active.

3.) check the device's possible configurations (optional)
# cat options
Dependent: 01 - Priority acceptable
    port 0x3f0-0x3f0, align 0x7, size 0x6, 16-bit address decoding
    port 0x3f7-0x3f7, align 0x0, size 0x1, 16-bit address decoding
    irq 6
    dma 2 8-bit compatible
Dependent: 02 - Priority acceptable
    port 0x370-0x370, align 0x7, size 0x6, 16-bit address decoding
    port 0x377-0x377, align 0x0, size 0x1, 16-bit address decoding
    irq 6
    dma 2 8-bit compatible

4.) now activate the device
# echo "auto" > resources

5.) finally check if the device is active
# cat resources
io 0x3f0-0x3f5
io 0x3f7-0x3f7
irq 6
dma 2

also there are a series of kernel parameters:
pnp_reserve_irq=irq1[,irq2] ....
pnp_reserve_dma=dma1[,dma2] ....
pnp_reserve_io=io1,size1[,io2,size2] ....
pnp_reserve_mem=mem1,size1[,mem2,size2] ....



The Unified Plug and Play Layer
-------------------------------
	All Plug and Play drivers, protocols, and services meet at a central location 
called the Plug and Play Layer.  This layer is responsible for the exchange of 
information between PnP drivers and PnP protocols.  Thus it automatically 
forwards commands to the proper protocol.  This makes writing PnP drivers 
significantly easier.

The following functions are available from the Plug and Play Layer:

pnp_get_protocol
- increments the number of uses by one

pnp_put_protocol
- deincrements the number of uses by one

pnp_register_protocol
- use this to register a new PnP protocol

pnp_unregister_protocol
- use this function to remove a PnP protocol from the Plug and Play Layer

pnp_register_driver
- adds a PnP driver to the Plug and Play Layer
- this includes driver model integration
- returns zero for success or a negative error number for failure; count
  calls to the .add() method if you need to know how many devices bind to
  the driver

pnp_unregister_driver
- removes a PnP driver from the Plug and Play Layer



Plug and Play Protocols
-----------------------
	This section contains information for PnP protocol developers.

The following Protocols are currently available in the computing world:
- PNPBIOS: used for system devices such as serial and parallel ports.
- ISAPNP: provides PnP support for the ISA bus
- ACPI: among its many uses, ACPI provides information about system level 
devices.
It is meant to replace the PNPBIOS.  It is not currently supported by Linux
Plug and Play but it is planned to be in the near future.


Requirements for a Linux PnP protocol:
1.) the protocol must use EISA IDs
2.) the protocol must inform the PnP Layer of a device's current configuration
- the ability to set resources is optional but preferred.

The following are PnP protocol related functions:

pnp_add_device
- use this function to add a PnP device to the PnP layer
- only call this function when all wanted values are set in the pnp_dev 
structure

pnp_init_device
- call this to initialize the PnP structure

pnp_remove_device
- call this to remove a device from the Plug and Play Layer.
- it will fail if the device is still in use.
- automatically will free mem used by the device and related structures

pnp_add_id
- adds an EISA ID to the list of supported IDs for the specified device

For more information consult the source of a protocol such as
/drivers/pnp/pnpbios/core.c.



Linux Plug and Play Drivers
---------------------------
	This section contains information for Linux PnP driver developers.

The New Way
...........
1.) first make a list of supported EISA IDS
ex:
static const struct pnp_id pnp_dev_table[] = {
	/* Standard LPT Printer Port */
	{.id = "PNP0400", .driver_data = 0},
	/* ECP Printer Port */
	{.id = "PNP0401", .driver_data = 0},
	{.id = ""}
};

Please note that the character 'X' can be used as a wild card in the function
portion (last four characters).
ex:
	/* Unknown PnP modems */
	{	"PNPCXXX",		UNKNOWN_DEV	},

Supported PnP card IDs can optionally be defined.
ex:
static const struct pnp_id pnp_card_table[] = {
	{	"ANYDEVS",		0	},
	{	"",			0	}
};

2.) Optionally define probe and remove functions.  It may make sense not to 
define these functions if the driver already has a reliable method of detecting
the resources, such as the parport_pc driver.
ex:
static int
serial_pnp_probe(struct pnp_dev * dev, const struct pnp_id *card_id, const 
                 struct pnp_id *dev_id)
{
. . .

ex:
static void serial_pnp_remove(struct pnp_dev * dev)
{
. . .

consult /drivers/serial/8250_pnp.c for more information.

3.) create a driver structure
ex:

static struct pnp_driver serial_pnp_driver = {
	.name		= "serial",
	.card_id_table	= pnp_card_table,
	.id_table	= pnp_dev_table,
	.probe		= serial_pnp_probe,
	.remove		= serial_pnp_remove,
};

* name and id_table cannot be NULL.

4.) register the driver
ex:

static int __init serial8250_pnp_init(void)
{
	return pnp_register_driver(&serial_pnp_driver);
}

The Old Way
...........

A series of compatibility functions have been created to make it easy to convert
ISAPNP drivers.  They should serve as a temporary solution only.

They are as follows:

struct pnp_card *pnp_find_card(unsigned short vendor,
				 unsigned short device,
				 struct pnp_card *from)

struct pnp_dev *pnp_find_dev(struct pnp_card *card,
				unsigned short vendor,
				unsigned short function,
				struct pnp_dev *from)

		  Proper Locking Under a Preemptible Kernel:
		       Keeping Kernel Code Preempt-Safe
			 Robert Love <rml@tech9.net>
			  Last Updated: 28 Aug 2002


INTRODUCTION


A preemptible kernel creates new locking issues.  The issues are the same as
those under SMP: concurrency and reentrancy.  Thankfully, the Linux preemptible
kernel model leverages existing SMP locking mechanisms.  Thus, the kernel
requires explicit additional locking for very few additional situations.

This document is for all kernel hackers.  Developing code in the kernel
requires protecting these situations.
 

RULE #1: Per-CPU data structures need explicit protection


Two similar problems arise. An example code snippet:

	struct this_needs_locking tux[NR_CPUS];
	tux[smp_processor_id()] = some_value;
	/* task is preempted here... */
	something = tux[smp_processor_id()];

First, since the data is per-CPU, it may not have explicit SMP locking, but
require it otherwise.  Second, when a preempted task is finally rescheduled,
the previous value of smp_processor_id may not equal the current.  You must
protect these situations by disabling preemption around them.

You can also use put_cpu() and get_cpu(), which will disable preemption.


RULE #2: CPU state must be protected.


Under preemption, the state of the CPU must be protected.  This is arch-
dependent, but includes CPU structures and state not preserved over a context
switch.  For example, on x86, entering and exiting FPU mode is now a critical
section that must occur while preemption is disabled.  Think what would happen
if the kernel is executing a floating-point instruction and is then preempted.
Remember, the kernel does not save FPU state except for user tasks.  Therefore,
upon preemption, the FPU registers will be sold to the lowest bidder.  Thus,
preemption must be disabled around such regions.

Note, some FPU functions are already explicitly preempt safe.  For example,
kernel_fpu_begin and kernel_fpu_end will disable and enable preemption.
However, math_state_restore must be called with preemption disabled.


RULE #3: Lock acquire and release must be performed by same task


A lock acquired in one task must be released by the same task.  This
means you can't do oddball things like acquire a lock and go off to
play while another task releases it.  If you want to do something
like this, acquire and release the task in the same code path and
have the caller wait on an event by the other task.


SOLUTION


Data protection under preemption is achieved by disabling preemption for the
duration of the critical region.

preempt_enable()		decrement the preempt counter
preempt_disable()		increment the preempt counter
preempt_enable_no_resched()	decrement, but do not immediately preempt
preempt_check_resched()		if needed, reschedule
preempt_count()			return the preempt counter

The functions are nestable.  In other words, you can call preempt_disable
n-times in a code path, and preemption will not be reenabled until the n-th
call to preempt_enable.  The preempt statements define to nothing if
preemption is not enabled.

Note that you do not need to explicitly prevent preemption if you are holding
any locks or interrupts are disabled, since preemption is implicitly disabled
in those cases.

But keep in mind that 'irqs disabled' is a fundamentally unsafe way of
disabling preemption - any spin_unlock() decreasing the preemption count
to 0 might trigger a reschedule. A simple printk() might trigger a reschedule.
So use this implicit preemption-disabling property only if you know that the
affected codepath does not do any of this. Best policy is to use this only for
small, atomic code that you wrote and which calls no complex functions.

Example:

	cpucache_t *cc; /* this is per-CPU */
	preempt_disable();
	cc = cc_data(searchp);
	if (cc && cc->avail) {
		__free_block(searchp, cc_entry(cc), cc->avail);
		cc->avail = 0;
	}
	preempt_enable();
	return 0;

Notice how the preemption statements must encompass every reference of the
critical variables.  Another example:

	int buf[NR_CPUS];
	set_cpu_val(buf);
	if (buf[smp_processor_id()] == -1) printf(KERN_INFO "wee!\n");
	spin_lock(&buf_lock);
	/* ... */

This code is not preempt-safe, but see how easily we can fix it by simply
moving the spin_lock up two lines.


PREVENTING PREEMPTION USING INTERRUPT DISABLING


It is possible to prevent a preemption event using local_irq_disable and
local_irq_save.  Note, when doing so, you must be very careful to not cause
an event that would set need_resched and result in a preemption check.  When
in doubt, rely on locking or explicit preemption disabling.

Note in 2.5 interrupt disabling is now only per-CPU (e.g. local).

An additional concern is proper usage of local_irq_disable and local_irq_save.
These may be used to protect from preemption, however, on exit, if preemption
may be enabled, a test to see if preemption is required should be done.  If
these are called from the spin_lock and read/write lock macros, the right thing
is done.  They may also be called within a spin-lock protected region, however,
if they are ever called outside of this context, a test for preemption should
be made. Do note that calls from interrupt context or bottom half/ tasklets
are also protected by preemption locks and so may use the versions which do
not check preemption.
If variable is of Type,		use printk format specifier:
---------------------------------------------------------
		int			%d or %x
		unsigned int		%u or %x
		long			%ld or %lx
		unsigned long		%lu or %lx
		long long		%lld or %llx
		unsigned long long	%llu or %llx
		size_t			%zu or %zx
		ssize_t			%zd or %zx

Raw pointer value SHOULD be printed with %p. The kernel supports
the following extended format specifiers for pointer types:

Symbols/Function Pointers:

	%pF	versatile_init+0x0/0x110
	%pf	versatile_init
	%pS	versatile_init+0x0/0x110
	%pSR	versatile_init+0x9/0x110
		(with __builtin_extract_return_addr() translation)
	%ps	versatile_init
	%pB	prev_fn_of_versatile_init+0x88/0x88

	For printing symbols and function pointers. The 'S' and 's' specifiers
	result in the symbol name with ('S') or without ('s') offsets. Where
	this is used on a kernel without KALLSYMS - the symbol address is
	printed instead.

	The 'B' specifier results in the symbol name with offsets and should be
	used when printing stack backtraces. The specifier takes into
	consideration the effect of compiler optimisations which may occur
	when tail-call's are used and marked with the noreturn GCC attribute.

	On ia64, ppc64 and parisc64 architectures function pointers are
	actually function descriptors which must first be resolved. The 'F' and
	'f' specifiers perform this resolution and then provide the same
	functionality as the 'S' and 's' specifiers.

Kernel Pointers:

	%pK	0x01234567 or 0x0123456789abcdef

	For printing kernel pointers which should be hidden from unprivileged
	users. The behaviour of %pK depends on the kptr_restrict sysctl - see
	Documentation/sysctl/kernel.txt for more details.

Struct Resources:

	%pr	[mem 0x60000000-0x6fffffff flags 0x2200] or
		[mem 0x0000000060000000-0x000000006fffffff flags 0x2200]
	%pR	[mem 0x60000000-0x6fffffff pref] or
		[mem 0x0000000060000000-0x000000006fffffff pref]

	For printing struct resources. The 'R' and 'r' specifiers result in a
	printed resource with ('R') or without ('r') a decoded flags member.

Physical addresses types phys_addr_t:

	%pa[p]	0x01234567 or 0x0123456789abcdef

	For printing a phys_addr_t type (and its derivatives, such as
	resource_size_t) which can vary based on build options, regardless of
	the width of the CPU data path. Passed by reference.

DMA addresses types dma_addr_t:

	%pad	0x01234567 or 0x0123456789abcdef

	For printing a dma_addr_t type which can vary based on build options,
	regardless of the width of the CPU data path. Passed by reference.

Raw buffer as a hex string:
	%*ph	00 01 02  ...  3f
	%*phC	00:01:02: ... :3f
	%*phD	00-01-02- ... -3f
	%*phN	000102 ... 3f

	For printing a small buffers (up to 64 bytes long) as a hex string with
	certain separator. For the larger buffers consider to use
	print_hex_dump().

MAC/FDDI addresses:

	%pM	00:01:02:03:04:05
	%pMR	05:04:03:02:01:00
	%pMF	00-01-02-03-04-05
	%pm	000102030405
	%pmR	050403020100

	For printing 6-byte MAC/FDDI addresses in hex notation. The 'M' and 'm'
	specifiers result in a printed address with ('M') or without ('m') byte
	separators. The default byte separator is the colon (':').

	Where FDDI addresses are concerned the 'F' specifier can be used after
	the 'M' specifier to use dash ('-') separators instead of the default
	separator.

	For Bluetooth addresses the 'R' specifier shall be used after the 'M'
	specifier to use reversed byte order suitable for visual interpretation
	of Bluetooth addresses which are in the little endian order.

IPv4 addresses:

	%pI4	1.2.3.4
	%pi4	001.002.003.004
	%p[Ii]4[hnbl]

	For printing IPv4 dot-separated decimal addresses. The 'I4' and 'i4'
	specifiers result in a printed address with ('i4') or without ('I4')
	leading zeros.

	The additional 'h', 'n', 'b', and 'l' specifiers are used to specify
	host, network, big or little endian order addresses respectively. Where
	no specifier is provided the default network/big endian order is used.

IPv6 addresses:

	%pI6	0001:0002:0003:0004:0005:0006:0007:0008
	%pi6	00010002000300040005000600070008
	%pI6c	1:2:3:4:5:6:7:8

	For printing IPv6 network-order 16-bit hex addresses. The 'I6' and 'i6'
	specifiers result in a printed address with ('I6') or without ('i6')
	colon-separators. Leading zeros are always used.

	The additional 'c' specifier can be used with the 'I' specifier to
	print a compressed IPv6 address as described by
	http://tools.ietf.org/html/rfc5952

IPv4/IPv6 addresses (generic, with port, flowinfo, scope):

	%pIS	1.2.3.4		or 0001:0002:0003:0004:0005:0006:0007:0008
	%piS	001.002.003.004	or 00010002000300040005000600070008
	%pISc	1.2.3.4		or 1:2:3:4:5:6:7:8
	%pISpc	1.2.3.4:12345	or [1:2:3:4:5:6:7:8]:12345
	%p[Ii]S[pfschnbl]

	For printing an IP address without the need to distinguish whether it's
	of type AF_INET or AF_INET6, a pointer to a valid 'struct sockaddr',
	specified through 'IS' or 'iS', can be passed to this format specifier.

	The additional 'p', 'f', and 's' specifiers are used to specify port
	(IPv4, IPv6), flowinfo (IPv6) and scope (IPv6). Ports have a ':' prefix,
	flowinfo a '/' and scope a '%', each followed by the actual value.

	In case of an IPv6 address the compressed IPv6 address as described by
	http://tools.ietf.org/html/rfc5952 is being used if the additional
	specifier 'c' is given. The IPv6 address is surrounded by '[', ']' in
	case of additional specifiers 'p', 'f' or 's' as suggested by
	https://tools.ietf.org/html/draft-ietf-6man-text-addr-representation-07

	In case of IPv4 addresses, the additional 'h', 'n', 'b', and 'l'
	specifiers can be used as well and are ignored in case of an IPv6
	address.

	Further examples:

	%pISfc		1.2.3.4		or [1:2:3:4:5:6:7:8]/123456789
	%pISsc		1.2.3.4		or [1:2:3:4:5:6:7:8]%1234567890
	%pISpfc		1.2.3.4:12345	or [1:2:3:4:5:6:7:8]:12345/123456789

UUID/GUID addresses:

	%pUb	00010203-0405-0607-0809-0a0b0c0d0e0f
	%pUB	00010203-0405-0607-0809-0A0B0C0D0E0F
	%pUl	03020100-0504-0706-0809-0a0b0c0e0e0f
	%pUL	03020100-0504-0706-0809-0A0B0C0E0E0F

	For printing 16-byte UUID/GUIDs addresses. The additional 'l', 'L',
	'b' and 'B' specifiers are used to specify a little endian order in
	lower ('l') or upper case ('L') hex characters - and big endian order
	in lower ('b') or upper case ('B') hex characters.

	Where no additional specifiers are used the default little endian
	order with lower case hex characters will be printed.

dentry names:
	%pd{,2,3,4}
	%pD{,2,3,4}

	For printing dentry name; if we race with d_move(), the name might be
	a mix of old and new ones, but it won't oops.  %pd dentry is a safer
	equivalent of %s dentry->d_name.name we used to use, %pd<n> prints
	n last components.  %pD does the same thing for struct file.

struct va_format:

	%pV

	For printing struct va_format structures. These contain a format string
	and va_list as follows:

	struct va_format {
		const char *fmt;
		va_list *va;
	};

	Do not use this feature without some mechanism to verify the
	correctness of the format string and va_list arguments.

u64 SHOULD be printed with %llu/%llx:

	printk("%llu", u64_var);

s64 SHOULD be printed with %lld/%llx:

	printk("%lld", s64_var);

If <type> is dependent on a config option for its size (e.g., sector_t,
blkcnt_t) or is architecture-dependent for its size (e.g., tcflag_t), use a
format specifier of its largest possible type and explicitly cast to it.
Example:

	printk("test: sector number/total blocks: %llu/%llu\n",
		(unsigned long long)sector, (unsigned long long)blockcount);

Reminder: sizeof() result is of type size_t.

Thank you for your cooperation and attention.


By Randy Dunlap <rdunlap@infradead.org> and
Andrew Murray <amurray@mpc-data.co.uk>
Pulse Width Modulation (PWM) interface

This provides an overview about the Linux PWM interface

PWMs are commonly used for controlling LEDs, fans or vibrators in
cell phones. PWMs with a fixed purpose have no need implementing
the Linux PWM API (although they could). However, PWMs are often
found as discrete devices on SoCs which have no fixed purpose. It's
up to the board designer to connect them to LEDs or fans. To provide
this kind of flexibility the generic PWM API exists.

Identifying PWMs
----------------

Users of the legacy PWM API use unique IDs to refer to PWM devices.

Instead of referring to a PWM device via its unique ID, board setup code
should instead register a static mapping that can be used to match PWM
consumers to providers, as given in the following example:

	static struct pwm_lookup board_pwm_lookup[] = {
		PWM_LOOKUP("tegra-pwm", 0, "pwm-backlight", NULL,
			   50000, PWM_POLARITY_NORMAL),
	};

	static void __init board_init(void)
	{
		...
		pwm_add_table(board_pwm_lookup, ARRAY_SIZE(board_pwm_lookup));
		...
	}

Using PWMs
----------

Legacy users can request a PWM device using pwm_request() and free it
after usage with pwm_free().

New users should use the pwm_get() function and pass to it the consumer
device or a consumer name. pwm_put() is used to free the PWM device. Managed
variants of these functions, devm_pwm_get() and devm_pwm_put(), also exist.

After being requested, a PWM has to be configured using:

int pwm_config(struct pwm_device *pwm, int duty_ns, int period_ns);

To start/stop toggling the PWM output use pwm_enable()/pwm_disable().

Using PWMs with the sysfs interface
-----------------------------------

If CONFIG_SYSFS is enabled in your kernel configuration a simple sysfs
interface is provided to use the PWMs from userspace. It is exposed at
/sys/class/pwm/. Each probed PWM controller/chip will be exported as
pwmchipN, where N is the base of the PWM chip. Inside the directory you
will find:

npwm - The number of PWM channels this chip supports (read-only).

export - Exports a PWM channel for use with sysfs (write-only).

unexport - Unexports a PWM channel from sysfs (write-only).

The PWM channels are numbered using a per-chip index from 0 to npwm-1.

When a PWM channel is exported a pwmX directory will be created in the
pwmchipN directory it is associated with, where X is the number of the
channel that was exported. The following properties will then be available:

period - The total period of the PWM signal (read/write).
	Value is in nanoseconds and is the sum of the active and inactive
	time of the PWM.

duty_cycle - The active time of the PWM signal (read/write).
	Value is in nanoseconds and must be less than the period.

polarity - Changes the polarity of the PWM signal (read/write).
	Writes to this property only work if the PWM chip supports changing
	the polarity. The polarity can only be changed if the PWM is not
	enabled. Value is the string "normal" or "inversed".

enable - Enable/disable the PWM signal (read/write).
	0 - disabled
	1 - enabled

Implementing a PWM driver
-------------------------

Currently there are two ways to implement pwm drivers. Traditionally
there only has been the barebone API meaning that each driver has
to implement the pwm_*() functions itself. This means that it's impossible
to have multiple PWM drivers in the system. For this reason it's mandatory
for new drivers to use the generic PWM framework.

A new PWM controller/chip can be added using pwmchip_add() and removed
again with pwmchip_remove(). pwmchip_add() takes a filled in struct
pwm_chip as argument which provides a description of the PWM chip, the
number of PWM devices provided by the chip and the chip-specific
implementation of the supported PWM operations to the framework.

When implementing polarity support in a PWM driver, make sure to respect the
signal conventions in the PWM framework. By definition, normal polarity
characterizes a signal starts high for the duration of the duty cycle and
goes low for the remainder of the period. Conversely, a signal with inversed
polarity starts low for the duration of the duty cycle and goes high for the
remainder of the period.

Locking
-------

The PWM core list manipulations are protected by a mutex, so pwm_request()
and pwm_free() may not be called from an atomic context. Currently the
PWM core does not enforce any locking to pwm_enable(), pwm_disable() and
pwm_config(), so the calling context is currently driver specific. This
is an issue derived from the former barebone API and should be fixed soon.

Helpers
-------

Currently a PWM can only be configured with period_ns and duty_ns. For several
use cases freq_hz and duty_percent might be better. Instead of calculating
this in your driver please consider adding appropriate helpers to the framework.
Ramoops oops/panic logger
=========================

Sergiu Iordache <sergiu@chromium.org>

Updated: 17 November 2011

0. Introduction

Ramoops is an oops/panic logger that writes its logs to RAM before the system
crashes. It works by logging oopses and panics in a circular buffer. Ramoops
needs a system with persistent RAM so that the content of that area can
survive after a restart.

1. Ramoops concepts

Ramoops uses a predefined memory area to store the dump. The start and size of
the memory area are set using two variables:
  * "mem_address" for the start
  * "mem_size" for the size. The memory size will be rounded down to a
  power of two.

The memory area is divided into "record_size" chunks (also rounded down to
power of two) and each oops/panic writes a "record_size" chunk of
information.

Dumping both oopses and panics can be done by setting 1 in the "dump_oops"
variable while setting 0 in that variable dumps only the panics.

The module uses a counter to record multiple dumps but the counter gets reset
on restart (i.e. new dumps after the restart will overwrite old ones).

Ramoops also supports software ECC protection of persistent memory regions.
This might be useful when a hardware reset was used to bring the machine back
to life (i.e. a watchdog triggered). In such cases, RAM may be somewhat
corrupt, but usually it is restorable.

2. Setting the parameters

Setting the ramoops parameters can be done in 2 different manners:
 1. Use the module parameters (which have the names of the variables described
 as before).
 For quick debugging, you can also reserve parts of memory during boot
 and then use the reserved memory for ramoops. For example, assuming a machine
 with > 128 MB of memory, the following kernel command line will tell the
 kernel to use only the first 128 MB of memory, and place ECC-protected ramoops
 region at 128 MB boundary:
 "mem=128M ramoops.mem_address=0x8000000 ramoops.ecc=1"
 2. Use a platform device and set the platform data. The parameters can then
 be set through that platform data. An example of doing that is:

#include <linux/pstore_ram.h>
[...]

static struct ramoops_platform_data ramoops_data = {
        .mem_size               = <...>,
        .mem_address            = <...>,
        .record_size            = <...>,
        .dump_oops              = <...>,
        .ecc                    = <...>,
};

static struct platform_device ramoops_dev = {
        .name = "ramoops",
        .dev = {
                .platform_data = &ramoops_data,
        },
};

[... inside a function ...]
int ret;

ret = platform_device_register(&ramoops_dev);
if (ret) {
	printk(KERN_ERR "unable to register platform device\n");
	return ret;
}

You can specify either RAM memory or peripheral devices' memory. However, when
specifying RAM, be sure to reserve the memory by issuing memblock_reserve()
very early in the architecture code, e.g.:

#include <linux/memblock.h>

memblock_reserve(ramoops_data.mem_address, ramoops_data.mem_size);

3. Dump format

The data dump begins with a header, currently defined as "====" followed by a
timestamp and a new line. The dump then continues with the actual data.

4. Reading the data

The dump data can be read from the pstore filesystem. The format for these
files is "dmesg-ramoops-N", where N is the record number in memory. To delete
a stored record from RAM, simply unlink the respective pstore file.

5. Persistent function tracing

Persistent function tracing might be useful for debugging software or hardware
related hangs. The functions call chain log is stored in a "ftrace-ramoops"
file. Here is an example of usage:

 # mount -t debugfs debugfs /sys/kernel/debug/
 # echo 1 > /sys/kernel/debug/pstore/record_ftrace
 # reboot -f
 [...]
 # mount -t pstore pstore /mnt/
 # tail /mnt/ftrace-ramoops
 0 ffffffff8101ea64  ffffffff8101bcda  native_apic_mem_read <- disconnect_bsp_APIC+0x6a/0xc0
 0 ffffffff8101ea44  ffffffff8101bcf6  native_apic_mem_write <- disconnect_bsp_APIC+0x86/0xc0
 0 ffffffff81020084  ffffffff8101a4b5  hpet_disable <- native_machine_shutdown+0x75/0x90
 0 ffffffff81005f94  ffffffff8101a4bb  iommu_shutdown_noop <- native_machine_shutdown+0x7b/0x90
 0 ffffffff8101a6a1  ffffffff8101a437  native_machine_emergency_restart <- native_machine_restart+0x37/0x40
 0 ffffffff811f9876  ffffffff8101a73a  acpi_reboot <- native_machine_emergency_restart+0xaa/0x1e0
 0 ffffffff8101a514  ffffffff8101a772  mach_reboot_fixups <- native_machine_emergency_restart+0xe2/0x1e0
 0 ffffffff811d9c54  ffffffff8101a7a0  __const_udelay <- native_machine_emergency_restart+0x110/0x1e0
 0 ffffffff811d9c34  ffffffff811d9c80  __delay <- __const_udelay+0x30/0x40
 0 ffffffff811d9d14  ffffffff811d9c3f  delay_tsc <- __delay+0xf/0x20
Red-black Trees (rbtree) in Linux
January 18, 2007
Rob Landley <rob@landley.net>
=============================

What are red-black trees, and what are they for?
------------------------------------------------

Red-black trees are a type of self-balancing binary search tree, used for
storing sortable key/value data pairs.  This differs from radix trees (which
are used to efficiently store sparse arrays and thus use long integer indexes
to insert/access/delete nodes) and hash tables (which are not kept sorted to
be easily traversed in order, and must be tuned for a specific size and
hash function where rbtrees scale gracefully storing arbitrary keys).

Red-black trees are similar to AVL trees, but provide faster real-time bounded
worst case performance for insertion and deletion (at most two rotations and
three rotations, respectively, to balance the tree), with slightly slower
(but still O(log n)) lookup time.

To quote Linux Weekly News:

    There are a number of red-black trees in use in the kernel.
    The deadline and CFQ I/O schedulers employ rbtrees to
    track requests; the packet CD/DVD driver does the same.
    The high-resolution timer code uses an rbtree to organize outstanding
    timer requests.  The ext3 filesystem tracks directory entries in a
    red-black tree.  Virtual memory areas (VMAs) are tracked with red-black
    trees, as are epoll file descriptors, cryptographic keys, and network
    packets in the "hierarchical token bucket" scheduler.

This document covers use of the Linux rbtree implementation.  For more
information on the nature and implementation of Red Black Trees,  see:

  Linux Weekly News article on red-black trees
    http://lwn.net/Articles/184495/

  Wikipedia entry on red-black trees
    http://en.wikipedia.org/wiki/Red-black_tree

Linux implementation of red-black trees
---------------------------------------

Linux's rbtree implementation lives in the file "lib/rbtree.c".  To use it,
"#include <linux/rbtree.h>".

The Linux rbtree implementation is optimized for speed, and thus has one
less layer of indirection (and better cache locality) than more traditional
tree implementations.  Instead of using pointers to separate rb_node and data
structures, each instance of struct rb_node is embedded in the data structure
it organizes.  And instead of using a comparison callback function pointer,
users are expected to write their own tree search and insert functions
which call the provided rbtree functions.  Locking is also left up to the
user of the rbtree code.

Creating a new rbtree
---------------------

Data nodes in an rbtree tree are structures containing a struct rb_node member:

  struct mytype {
  	struct rb_node node;
  	char *keystring;
  };

When dealing with a pointer to the embedded struct rb_node, the containing data
structure may be accessed with the standard container_of() macro.  In addition,
individual members may be accessed directly via rb_entry(node, type, member).

At the root of each rbtree is an rb_root structure, which is initialized to be
empty via:

  struct rb_root mytree = RB_ROOT;

Searching for a value in an rbtree
----------------------------------

Writing a search function for your tree is fairly straightforward: start at the
root, compare each value, and follow the left or right branch as necessary.

Example:

  struct mytype *my_search(struct rb_root *root, char *string)
  {
  	struct rb_node *node = root->rb_node;

  	while (node) {
  		struct mytype *data = container_of(node, struct mytype, node);
		int result;

		result = strcmp(string, data->keystring);

		if (result < 0)
  			node = node->rb_left;
		else if (result > 0)
  			node = node->rb_right;
		else
  			return data;
	}
	return NULL;
  }

Inserting data into an rbtree
-----------------------------

Inserting data in the tree involves first searching for the place to insert the
new node, then inserting the node and rebalancing ("recoloring") the tree.

The search for insertion differs from the previous search by finding the
location of the pointer on which to graft the new node.  The new node also
needs a link to its parent node for rebalancing purposes.

Example:

  int my_insert(struct rb_root *root, struct mytype *data)
  {
  	struct rb_node **new = &(root->rb_node), *parent = NULL;

  	/* Figure out where to put new node */
  	while (*new) {
  		struct mytype *this = container_of(*new, struct mytype, node);
  		int result = strcmp(data->keystring, this->keystring);

		parent = *new;
  		if (result < 0)
  			new = &((*new)->rb_left);
  		else if (result > 0)
  			new = &((*new)->rb_right);
  		else
  			return FALSE;
  	}

  	/* Add new node and rebalance tree. */
  	rb_link_node(&data->node, parent, new);
  	rb_insert_color(&data->node, root);

	return TRUE;
  }

Removing or replacing existing data in an rbtree
------------------------------------------------

To remove an existing node from a tree, call:

  void rb_erase(struct rb_node *victim, struct rb_root *tree);

Example:

  struct mytype *data = mysearch(&mytree, "walrus");

  if (data) {
  	rb_erase(&data->node, &mytree);
  	myfree(data);
  }

To replace an existing node in a tree with a new one with the same key, call:

  void rb_replace_node(struct rb_node *old, struct rb_node *new,
  			struct rb_root *tree);

Replacing a node this way does not re-sort the tree: If the new node doesn't
have the same key as the old node, the rbtree will probably become corrupted.

Iterating through the elements stored in an rbtree (in sort order)
------------------------------------------------------------------

Four functions are provided for iterating through an rbtree's contents in
sorted order.  These work on arbitrary trees, and should not need to be
modified or wrapped (except for locking purposes):

  struct rb_node *rb_first(struct rb_root *tree);
  struct rb_node *rb_last(struct rb_root *tree);
  struct rb_node *rb_next(struct rb_node *node);
  struct rb_node *rb_prev(struct rb_node *node);

To start iterating, call rb_first() or rb_last() with a pointer to the root
of the tree, which will return a pointer to the node structure contained in
the first or last element in the tree.  To continue, fetch the next or previous
node by calling rb_next() or rb_prev() on the current node.  This will return
NULL when there are no more nodes left.

The iterator functions return a pointer to the embedded struct rb_node, from
which the containing data structure may be accessed with the container_of()
macro, and individual members may be accessed directly via
rb_entry(node, type, member).

Example:

  struct rb_node *node;
  for (node = rb_first(&mytree); node; node = rb_next(node))
	printk("key=%s\n", rb_entry(node, struct mytype, node)->keystring);

Support for Augmented rbtrees
-----------------------------

Augmented rbtree is an rbtree with "some" additional data stored in
each node, where the additional data for node N must be a function of
the contents of all nodes in the subtree rooted at N. This data can
be used to augment some new functionality to rbtree. Augmented rbtree
is an optional feature built on top of basic rbtree infrastructure.
An rbtree user who wants this feature will have to call the augmentation
functions with the user provided augmentation callback when inserting
and erasing nodes.

C files implementing augmented rbtree manipulation must include
<linux/rbtree_augmented.h> instead of <linus/rbtree.h>. Note that
linux/rbtree_augmented.h exposes some rbtree implementations details
you are not expected to rely on; please stick to the documented APIs
there and do not include <linux/rbtree_augmented.h> from header files
either so as to minimize chances of your users accidentally relying on
such implementation details.

On insertion, the user must update the augmented information on the path
leading to the inserted node, then call rb_link_node() as usual and
rb_augment_inserted() instead of the usual rb_insert_color() call.
If rb_augment_inserted() rebalances the rbtree, it will callback into
a user provided function to update the augmented information on the
affected subtrees.

When erasing a node, the user must call rb_erase_augmented() instead of
rb_erase(). rb_erase_augmented() calls back into user provided functions
to updated the augmented information on affected subtrees.

In both cases, the callbacks are provided through struct rb_augment_callbacks.
3 callbacks must be defined:

- A propagation callback, which updates the augmented value for a given
  node and its ancestors, up to a given stop point (or NULL to update
  all the way to the root).

- A copy callback, which copies the augmented value for a given subtree
  to a newly assigned subtree root.

- A tree rotation callback, which copies the augmented value for a given
  subtree to a newly assigned subtree root AND recomputes the augmented
  information for the former subtree root.

The compiled code for rb_erase_augmented() may inline the propagation and
copy callbacks, which results in a large function, so each augmented rbtree
user should have a single rb_erase_augmented() call site in order to limit
compiled code size.


Sample usage:

Interval tree is an example of augmented rb tree. Reference -
"Introduction to Algorithms" by Cormen, Leiserson, Rivest and Stein.
More details about interval trees:

Classical rbtree has a single key and it cannot be directly used to store
interval ranges like [lo:hi] and do a quick lookup for any overlap with a new
lo:hi or to find whether there is an exact match for a new lo:hi.

However, rbtree can be augmented to store such interval ranges in a structured
way making it possible to do efficient lookup and exact match.

This "extra information" stored in each node is the maximum hi
(max_hi) value among all the nodes that are its descendants. This
information can be maintained at each node just be looking at the node
and its immediate children. And this will be used in O(log n) lookup
for lowest match (lowest start address among all possible matches)
with something like:

struct interval_tree_node *
interval_tree_first_match(struct rb_root *root,
			  unsigned long start, unsigned long last)
{
	struct interval_tree_node *node;

	if (!root->rb_node)
		return NULL;
	node = rb_entry(root->rb_node, struct interval_tree_node, rb);

	while (true) {
		if (node->rb.rb_left) {
			struct interval_tree_node *left =
				rb_entry(node->rb.rb_left,
					 struct interval_tree_node, rb);
			if (left->__subtree_last >= start) {
				/*
				 * Some nodes in left subtree satisfy Cond2.
				 * Iterate to find the leftmost such node N.
				 * If it also satisfies Cond1, that's the match
				 * we are looking for. Otherwise, there is no
				 * matching interval as nodes to the right of N
				 * can't satisfy Cond1 either.
				 */
				node = left;
				continue;
			}
		}
		if (node->start <= last) {		/* Cond1 */
			if (node->last >= start)	/* Cond2 */
				return node;	/* node is leftmost match */
			if (node->rb.rb_right) {
				node = rb_entry(node->rb.rb_right,
					struct interval_tree_node, rb);
				if (node->__subtree_last >= start)
					continue;
			}
		}
		return NULL;	/* No match */
	}
}

Insertion/removal are defined using the following augmented callbacks:

static inline unsigned long
compute_subtree_last(struct interval_tree_node *node)
{
	unsigned long max = node->last, subtree_last;
	if (node->rb.rb_left) {
		subtree_last = rb_entry(node->rb.rb_left,
			struct interval_tree_node, rb)->__subtree_last;
		if (max < subtree_last)
			max = subtree_last;
	}
	if (node->rb.rb_right) {
		subtree_last = rb_entry(node->rb.rb_right,
			struct interval_tree_node, rb)->__subtree_last;
		if (max < subtree_last)
			max = subtree_last;
	}
	return max;
}

static void augment_propagate(struct rb_node *rb, struct rb_node *stop)
{
	while (rb != stop) {
		struct interval_tree_node *node =
			rb_entry(rb, struct interval_tree_node, rb);
		unsigned long subtree_last = compute_subtree_last(node);
		if (node->__subtree_last == subtree_last)
			break;
		node->__subtree_last = subtree_last;
		rb = rb_parent(&node->rb);
	}
}

static void augment_copy(struct rb_node *rb_old, struct rb_node *rb_new)
{
	struct interval_tree_node *old =
		rb_entry(rb_old, struct interval_tree_node, rb);
	struct interval_tree_node *new =
		rb_entry(rb_new, struct interval_tree_node, rb);

	new->__subtree_last = old->__subtree_last;
}

static void augment_rotate(struct rb_node *rb_old, struct rb_node *rb_new)
{
	struct interval_tree_node *old =
		rb_entry(rb_old, struct interval_tree_node, rb);
	struct interval_tree_node *new =
		rb_entry(rb_new, struct interval_tree_node, rb);

	new->__subtree_last = old->__subtree_last;
	old->__subtree_last = compute_subtree_last(old);
}

static const struct rb_augment_callbacks augment_callbacks = {
	augment_propagate, augment_copy, augment_rotate
};

void interval_tree_insert(struct interval_tree_node *node,
			  struct rb_root *root)
{
	struct rb_node **link = &root->rb_node, *rb_parent = NULL;
	unsigned long start = node->start, last = node->last;
	struct interval_tree_node *parent;

	while (*link) {
		rb_parent = *link;
		parent = rb_entry(rb_parent, struct interval_tree_node, rb);
		if (parent->__subtree_last < last)
			parent->__subtree_last = last;
		if (start < parent->start)
			link = &parent->rb.rb_left;
		else
			link = &parent->rb.rb_right;
	}

	node->__subtree_last = last;
	rb_link_node(&node->rb, rb_parent, link);
	rb_insert_augmented(&node->rb, root, &augment_callbacks);
}

void interval_tree_remove(struct interval_tree_node *node,
			  struct rb_root *root)
{
	rb_erase_augmented(&node->rb, root, &augment_callbacks);
}
Remote Processor Framework

1. Introduction

Modern SoCs typically have heterogeneous remote processor devices in asymmetric
multiprocessing (AMP) configurations, which may be running different instances
of operating system, whether it's Linux or any other flavor of real-time OS.

OMAP4, for example, has dual Cortex-A9, dual Cortex-M3 and a C64x+ DSP.
In a typical configuration, the dual cortex-A9 is running Linux in a SMP
configuration, and each of the other three cores (two M3 cores and a DSP)
is running its own instance of RTOS in an AMP configuration.

The remoteproc framework allows different platforms/architectures to
control (power on, load firmware, power off) those remote processors while
abstracting the hardware differences, so the entire driver doesn't need to be
duplicated. In addition, this framework also adds rpmsg virtio devices
for remote processors that supports this kind of communication. This way,
platform-specific remoteproc drivers only need to provide a few low-level
handlers, and then all rpmsg drivers will then just work
(for more information about the virtio-based rpmsg bus and its drivers,
please read Documentation/rpmsg.txt).
Registration of other types of virtio devices is now also possible. Firmwares
just need to publish what kind of virtio devices do they support, and then
remoteproc will add those devices. This makes it possible to reuse the
existing virtio drivers with remote processor backends at a minimal development
cost.

2. User API

  int rproc_boot(struct rproc *rproc)
    - Boot a remote processor (i.e. load its firmware, power it on, ...).
      If the remote processor is already powered on, this function immediately
      returns (successfully).
      Returns 0 on success, and an appropriate error value otherwise.
      Note: to use this function you should already have a valid rproc
      handle. There are several ways to achieve that cleanly (devres, pdata,
      the way remoteproc_rpmsg.c does this, or, if this becomes prevalent, we
      might also consider using dev_archdata for this).

  void rproc_shutdown(struct rproc *rproc)
    - Power off a remote processor (previously booted with rproc_boot()).
      In case @rproc is still being used by an additional user(s), then
      this function will just decrement the power refcount and exit,
      without really powering off the device.
      Every call to rproc_boot() must (eventually) be accompanied by a call
      to rproc_shutdown(). Calling rproc_shutdown() redundantly is a bug.
      Notes:
      - we're not decrementing the rproc's refcount, only the power refcount.
        which means that the @rproc handle stays valid even after
        rproc_shutdown() returns, and users can still use it with a subsequent
        rproc_boot(), if needed.

3. Typical usage

#include <linux/remoteproc.h>

/* in case we were given a valid 'rproc' handle */
int dummy_rproc_example(struct rproc *my_rproc)
{
	int ret;

	/* let's power on and boot our remote processor */
	ret = rproc_boot(my_rproc);
	if (ret) {
		/*
		 * something went wrong. handle it and leave.
		 */
	}

	/*
	 * our remote processor is now powered on... give it some work
	 */

	/* let's shut it down now */
	rproc_shutdown(my_rproc);
}

4. API for implementors

  struct rproc *rproc_alloc(struct device *dev, const char *name,
				const struct rproc_ops *ops,
				const char *firmware, int len)
    - Allocate a new remote processor handle, but don't register
      it yet. Required parameters are the underlying device, the
      name of this remote processor, platform-specific ops handlers,
      the name of the firmware to boot this rproc with, and the
      length of private data needed by the allocating rproc driver (in bytes).

      This function should be used by rproc implementations during
      initialization of the remote processor.
      After creating an rproc handle using this function, and when ready,
      implementations should then call rproc_add() to complete
      the registration of the remote processor.
      On success, the new rproc is returned, and on failure, NULL.

      Note: _never_ directly deallocate @rproc, even if it was not registered
      yet. Instead, when you need to unroll rproc_alloc(), use rproc_put().

  void rproc_put(struct rproc *rproc)
    - Free an rproc handle that was allocated by rproc_alloc.
      This function essentially unrolls rproc_alloc(), by decrementing the
      rproc's refcount. It doesn't directly free rproc; that would happen
      only if there are no other references to rproc and its refcount now
      dropped to zero.

  int rproc_add(struct rproc *rproc)
    - Register @rproc with the remoteproc framework, after it has been
      allocated with rproc_alloc().
      This is called by the platform-specific rproc implementation, whenever
      a new remote processor device is probed.
      Returns 0 on success and an appropriate error code otherwise.
      Note: this function initiates an asynchronous firmware loading
      context, which will look for virtio devices supported by the rproc's
      firmware.
      If found, those virtio devices will be created and added, so as a result
      of registering this remote processor, additional virtio drivers might get
      probed.

  int rproc_del(struct rproc *rproc)
    - Unroll rproc_add().
      This function should be called when the platform specific rproc
      implementation decides to remove the rproc device. it should
      _only_ be called if a previous invocation of rproc_add()
      has completed successfully.

      After rproc_del() returns, @rproc is still valid, and its
      last refcount should be decremented by calling rproc_put().

      Returns 0 on success and -EINVAL if @rproc isn't valid.

  void rproc_report_crash(struct rproc *rproc, enum rproc_crash_type type)
    - Report a crash in a remoteproc
      This function must be called every time a crash is detected by the
      platform specific rproc implementation. This should not be called from a
      non-remoteproc driver. This function can be called from atomic/interrupt
      context.

5. Implementation callbacks

These callbacks should be provided by platform-specific remoteproc
drivers:

/**
 * struct rproc_ops - platform-specific device handlers
 * @start:	power on the device and boot it
 * @stop:	power off the device
 * @kick:	kick a virtqueue (virtqueue id given as a parameter)
 */
struct rproc_ops {
	int (*start)(struct rproc *rproc);
	int (*stop)(struct rproc *rproc);
	void (*kick)(struct rproc *rproc, int vqid);
};

Every remoteproc implementation should at least provide the ->start and ->stop
handlers. If rpmsg/virtio functionality is also desired, then the ->kick handler
should be provided as well.

The ->start() handler takes an rproc handle and should then power on the
device and boot it (use rproc->priv to access platform-specific private data).
The boot address, in case needed, can be found in rproc->bootaddr (remoteproc
core puts there the ELF entry point).
On success, 0 should be returned, and on failure, an appropriate error code.

The ->stop() handler takes an rproc handle and powers the device down.
On success, 0 is returned, and on failure, an appropriate error code.

The ->kick() handler takes an rproc handle, and an index of a virtqueue
where new message was placed in. Implementations should interrupt the remote
processor and let it know it has pending messages. Notifying remote processors
the exact virtqueue index to look in is optional: it is easy (and not
too expensive) to go through the existing virtqueues and look for new buffers
in the used rings.

6. Binary Firmware Structure

At this point remoteproc only supports ELF32 firmware binaries. However,
it is quite expected that other platforms/devices which we'd want to
support with this framework will be based on different binary formats.

When those use cases show up, we will have to decouple the binary format
from the framework core, so we can support several binary formats without
duplicating common code.

When the firmware is parsed, its various segments are loaded to memory
according to the specified device address (might be a physical address
if the remote processor is accessing memory directly).

In addition to the standard ELF segments, most remote processors would
also include a special section which we call "the resource table".

The resource table contains system resources that the remote processor
requires before it should be powered on, such as allocation of physically
contiguous memory, or iommu mapping of certain on-chip peripherals.
Remotecore will only power up the device after all the resource table's
requirement are met.

In addition to system resources, the resource table may also contain
resource entries that publish the existence of supported features
or configurations by the remote processor, such as trace buffers and
supported virtio devices (and their configurations).

The resource table begins with this header:

/**
 * struct resource_table - firmware resource table header
 * @ver: version number
 * @num: number of resource entries
 * @reserved: reserved (must be zero)
 * @offset: array of offsets pointing at the various resource entries
 *
 * The header of the resource table, as expressed by this structure,
 * contains a version number (should we need to change this format in the
 * future), the number of available resource entries, and their offsets
 * in the table.
 */
struct resource_table {
	u32 ver;
	u32 num;
	u32 reserved[2];
	u32 offset[0];
} __packed;

Immediately following this header are the resource entries themselves,
each of which begins with the following resource entry header:

/**
 * struct fw_rsc_hdr - firmware resource entry header
 * @type: resource type
 * @data: resource data
 *
 * Every resource entry begins with a 'struct fw_rsc_hdr' header providing
 * its @type. The content of the entry itself will immediately follow
 * this header, and it should be parsed according to the resource type.
 */
struct fw_rsc_hdr {
	u32 type;
	u8 data[0];
} __packed;

Some resources entries are mere announcements, where the host is informed
of specific remoteproc configuration. Other entries require the host to
do something (e.g. allocate a system resource). Sometimes a negotiation
is expected, where the firmware requests a resource, and once allocated,
the host should provide back its details (e.g. address of an allocated
memory region).

Here are the various resource types that are currently supported:

/**
 * enum fw_resource_type - types of resource entries
 *
 * @RSC_CARVEOUT:   request for allocation of a physically contiguous
 *		    memory region.
 * @RSC_DEVMEM:     request to iommu_map a memory-based peripheral.
 * @RSC_TRACE:	    announces the availability of a trace buffer into which
 *		    the remote processor will be writing logs.
 * @RSC_VDEV:       declare support for a virtio device, and serve as its
 *		    virtio header.
 * @RSC_LAST:       just keep this one at the end
 *
 * Please note that these values are used as indices to the rproc_handle_rsc
 * lookup table, so please keep them sane. Moreover, @RSC_LAST is used to
 * check the validity of an index before the lookup table is accessed, so
 * please update it as needed.
 */
enum fw_resource_type {
	RSC_CARVEOUT	= 0,
	RSC_DEVMEM	= 1,
	RSC_TRACE	= 2,
	RSC_VDEV	= 3,
	RSC_LAST	= 4,
};

For more details regarding a specific resource type, please see its
dedicated structure in include/linux/remoteproc.h.

We also expect that platform-specific resource entries will show up
at some point. When that happens, we could easily add a new RSC_PLATFORM
type, and hand those resources to the platform-specific rproc driver to handle.

7. Virtio and remoteproc

The firmware should provide remoteproc information about virtio devices
that it supports, and their configurations: a RSC_VDEV resource entry
should specify the virtio device id (as in virtio_ids.h), virtio features,
virtio config space, vrings information, etc.

When a new remote processor is registered, the remoteproc framework
will look for its resource table and will register the virtio devices
it supports. A firmware may support any number of virtio devices, and
of any type (a single remote processor can also easily support several
rpmsg virtio devices this way, if desired).

Of course, RSC_VDEV resource entries are only good enough for static
allocation of virtio devices. Dynamic allocations will also be made possible
using the rpmsg bus (similar to how we already do dynamic allocations of
rpmsg channels; read more about it in rpmsg.txt).
rfkill - RF kill switch support
===============================

1. Introduction
2. Implementation details
3. Kernel API
4. Userspace support


1. Introduction

The rfkill subsystem provides a generic interface to disabling any radio
transmitter in the system. When a transmitter is blocked, it shall not
radiate any power.

The subsystem also provides the ability to react on button presses and
disable all transmitters of a certain type (or all). This is intended for
situations where transmitters need to be turned off, for example on
aircraft.

The rfkill subsystem has a concept of "hard" and "soft" block, which
differ little in their meaning (block == transmitters off) but rather in
whether they can be changed or not:
 - hard block: read-only radio block that cannot be overridden by software
 - soft block: writable radio block (need not be readable) that is set by
               the system software.


2. Implementation details

The rfkill subsystem is composed of three main components:
 * the rfkill core,
 * the deprecated rfkill-input module (an input layer handler, being
   replaced by userspace policy code) and
 * the rfkill drivers.

The rfkill core provides API for kernel drivers to register their radio
transmitter with the kernel, methods for turning it on and off and, letting
the system know about hardware-disabled states that may be implemented on
the device.

The rfkill core code also notifies userspace of state changes, and provides
ways for userspace to query the current states. See the "Userspace support"
section below.

When the device is hard-blocked (either by a call to rfkill_set_hw_state()
or from query_hw_block) set_block() will be invoked for additional software
block, but drivers can ignore the method call since they can use the return
value of the function rfkill_set_hw_state() to sync the software state
instead of keeping track of calls to set_block(). In fact, drivers should
use the return value of rfkill_set_hw_state() unless the hardware actually
keeps track of soft and hard block separately.


3. Kernel API


Drivers for radio transmitters normally implement an rfkill driver.

Platform drivers might implement input devices if the rfkill button is just
that, a button. If that button influences the hardware then you need to
implement an rfkill driver instead. This also applies if the platform provides
a way to turn on/off the transmitter(s).

For some platforms, it is possible that the hardware state changes during
suspend/hibernation, in which case it will be necessary to update the rfkill
core with the current state is at resume time.

To create an rfkill driver, driver's Kconfig needs to have

	depends on RFKILL || !RFKILL

to ensure the driver cannot be built-in when rfkill is modular. The !RFKILL
case allows the driver to be built when rfkill is not configured, which
case all rfkill API can still be used but will be provided by static inlines
which compile to almost nothing.

Calling rfkill_set_hw_state() when a state change happens is required from
rfkill drivers that control devices that can be hard-blocked unless they also
assign the poll_hw_block() callback (then the rfkill core will poll the
device). Don't do this unless you cannot get the event in any other way.



5. Userspace support

The recommended userspace interface to use is /dev/rfkill, which is a misc
character device that allows userspace to obtain and set the state of rfkill
devices and sets of devices. It also notifies userspace about device addition
and removal. The API is a simple read/write API that is defined in
linux/rfkill.h, with one ioctl that allows turning off the deprecated input
handler in the kernel for the transition period.

Except for the one ioctl, communication with the kernel is done via read()
and write() of instances of 'struct rfkill_event'. In this structure, the
soft and hard block are properly separated (unlike sysfs, see below) and
userspace is able to get a consistent snapshot of all rfkill devices in the
system. Also, it is possible to switch all rfkill drivers (or all drivers of
a specified type) into a state which also updates the default state for
hotplugged devices.

After an application opens /dev/rfkill, it can read the current state of all
devices. Changes can be either obtained by either polling the descriptor for
hotplug or state change events or by listening for uevents emitted by the
rfkill core framework.

Additionally, each rfkill device is registered in sysfs and emits uevents.

rfkill devices issue uevents (with an action of "change"), with the following
environment variables set:

RFKILL_NAME
RFKILL_STATE
RFKILL_TYPE

The contents of these variables corresponds to the "name", "state" and
"type" sysfs files explained above.


For further details consult Documentation/ABI/stable/sysfs-class-rfkill.
Started by Paul Jackson <pj@sgi.com>

The robust futex ABI
--------------------

Robust_futexes provide a mechanism that is used in addition to normal
futexes, for kernel assist of cleanup of held locks on task exit.

The interesting data as to what futexes a thread is holding is kept on a
linked list in user space, where it can be updated efficiently as locks
are taken and dropped, without kernel intervention.  The only additional
kernel intervention required for robust_futexes above and beyond what is
required for futexes is:

 1) a one time call, per thread, to tell the kernel where its list of
    held robust_futexes begins, and
 2) internal kernel code at exit, to handle any listed locks held
    by the exiting thread.

The existing normal futexes already provide a "Fast Userspace Locking"
mechanism, which handles uncontested locking without needing a system
call, and handles contested locking by maintaining a list of waiting
threads in the kernel.  Options on the sys_futex(2) system call support
waiting on a particular futex, and waking up the next waiter on a
particular futex.

For robust_futexes to work, the user code (typically in a library such
as glibc linked with the application) has to manage and place the
necessary list elements exactly as the kernel expects them.  If it fails
to do so, then improperly listed locks will not be cleaned up on exit,
probably causing deadlock or other such failure of the other threads
waiting on the same locks.

A thread that anticipates possibly using robust_futexes should first
issue the system call:

    asmlinkage long
    sys_set_robust_list(struct robust_list_head __user *head, size_t len);

The pointer 'head' points to a structure in the threads address space
consisting of three words.  Each word is 32 bits on 32 bit arch's, or 64
bits on 64 bit arch's, and local byte order.  Each thread should have
its own thread private 'head'.

If a thread is running in 32 bit compatibility mode on a 64 native arch
kernel, then it can actually have two such structures - one using 32 bit
words for 32 bit compatibility mode, and one using 64 bit words for 64
bit native mode.  The kernel, if it is a 64 bit kernel supporting 32 bit
compatibility mode, will attempt to process both lists on each task
exit, if the corresponding sys_set_robust_list() call has been made to
setup that list.

  The first word in the memory structure at 'head' contains a
  pointer to a single linked list of 'lock entries', one per lock,
  as described below.  If the list is empty, the pointer will point
  to itself, 'head'.  The last 'lock entry' points back to the 'head'.

  The second word, called 'offset', specifies the offset from the
  address of the associated 'lock entry', plus or minus, of what will
  be called the 'lock word', from that 'lock entry'.  The 'lock word'
  is always a 32 bit word, unlike the other words above.  The 'lock
  word' holds 3 flag bits in the upper 3 bits, and the thread id (TID)
  of the thread holding the lock in the bottom 29 bits.  See further
  below for a description of the flag bits.

  The third word, called 'list_op_pending', contains transient copy of
  the address of the 'lock entry', during list insertion and removal,
  and is needed to correctly resolve races should a thread exit while
  in the middle of a locking or unlocking operation.

Each 'lock entry' on the single linked list starting at 'head' consists
of just a single word, pointing to the next 'lock entry', or back to
'head' if there are no more entries.  In addition, nearby to each 'lock
entry', at an offset from the 'lock entry' specified by the 'offset'
word, is one 'lock word'.

The 'lock word' is always 32 bits, and is intended to be the same 32 bit
lock variable used by the futex mechanism, in conjunction with
robust_futexes.  The kernel will only be able to wakeup the next thread
waiting for a lock on a threads exit if that next thread used the futex
mechanism to register the address of that 'lock word' with the kernel.

For each futex lock currently held by a thread, if it wants this
robust_futex support for exit cleanup of that lock, it should have one
'lock entry' on this list, with its associated 'lock word' at the
specified 'offset'.  Should a thread die while holding any such locks,
the kernel will walk this list, mark any such locks with a bit
indicating their holder died, and wakeup the next thread waiting for
that lock using the futex mechanism.

When a thread has invoked the above system call to indicate it
anticipates using robust_futexes, the kernel stores the passed in 'head'
pointer for that task.  The task may retrieve that value later on by
using the system call:

    asmlinkage long
    sys_get_robust_list(int pid, struct robust_list_head __user **head_ptr,
                        size_t __user *len_ptr);

It is anticipated that threads will use robust_futexes embedded in
larger, user level locking structures, one per lock.  The kernel
robust_futex mechanism doesn't care what else is in that structure, so
long as the 'offset' to the 'lock word' is the same for all
robust_futexes used by that thread.  The thread should link those locks
it currently holds using the 'lock entry' pointers.  It may also have
other links between the locks, such as the reverse side of a double
linked list, but that doesn't matter to the kernel.

By keeping its locks linked this way, on a list starting with a 'head'
pointer known to the kernel, the kernel can provide to a thread the
essential service available for robust_futexes, which is to help clean
up locks held at the time of (a perhaps unexpectedly) exit.

Actual locking and unlocking, during normal operations, is handled
entirely by user level code in the contending threads, and by the
existing futex mechanism to wait for, and wakeup, locks.  The kernels
only essential involvement in robust_futexes is to remember where the
list 'head' is, and to walk the list on thread exit, handling locks
still held by the departing thread, as described below.

There may exist thousands of futex lock structures in a threads shared
memory, on various data structures, at a given point in time. Only those
lock structures for locks currently held by that thread should be on
that thread's robust_futex linked lock list a given time.

A given futex lock structure in a user shared memory region may be held
at different times by any of the threads with access to that region. The
thread currently holding such a lock, if any, is marked with the threads
TID in the lower 29 bits of the 'lock word'.

When adding or removing a lock from its list of held locks, in order for
the kernel to correctly handle lock cleanup regardless of when the task
exits (perhaps it gets an unexpected signal 9 in the middle of
manipulating this list), the user code must observe the following
protocol on 'lock entry' insertion and removal:

On insertion:
 1) set the 'list_op_pending' word to the address of the 'lock entry'
    to be inserted,
 2) acquire the futex lock,
 3) add the lock entry, with its thread id (TID) in the bottom 29 bits
    of the 'lock word', to the linked list starting at 'head', and
 4) clear the 'list_op_pending' word.

On removal:
 1) set the 'list_op_pending' word to the address of the 'lock entry'
    to be removed,
 2) remove the lock entry for this lock from the 'head' list,
 3) release the futex lock, and
 4) clear the 'lock_op_pending' word.

On exit, the kernel will consider the address stored in
'list_op_pending' and the address of each 'lock word' found by walking
the list starting at 'head'.  For each such address, if the bottom 29
bits of the 'lock word' at offset 'offset' from that address equals the
exiting threads TID, then the kernel will do two things:

 1) if bit 31 (0x80000000) is set in that word, then attempt a futex
    wakeup on that address, which will waken the next thread that has
    used to the futex mechanism to wait on that address, and
 2) atomically set  bit 30 (0x40000000) in the 'lock word'.

In the above, bit 31 was set by futex waiters on that lock to indicate
they were waiting, and bit 30 is set by the kernel to indicate that the
lock owner died holding the lock.

The kernel exit code will silently stop scanning the list further if at
any point:

 1) the 'head' pointer or an subsequent linked list pointer
    is not a valid address of a user space word
 2) the calculated location of the 'lock word' (address plus
    'offset') is not the valid address of a 32 bit user space
    word
 3) if the list contains more than 1 million (subject to
    future kernel configuration changes) elements.

When the kernel sees a list entry whose 'lock word' doesn't have the
current threads TID in the lower 29 bits, it does nothing with that
entry, and goes on to the next entry.

Bit 29 (0x20000000) of the 'lock word' is reserved for future use.
Started by: Ingo Molnar <mingo@redhat.com>

Background
----------

what are robust futexes? To answer that, we first need to understand
what futexes are: normal futexes are special types of locks that in the
noncontended case can be acquired/released from userspace without having
to enter the kernel.

A futex is in essence a user-space address, e.g. a 32-bit lock variable
field. If userspace notices contention (the lock is already owned and
someone else wants to grab it too) then the lock is marked with a value
that says "there's a waiter pending", and the sys_futex(FUTEX_WAIT)
syscall is used to wait for the other guy to release it. The kernel
creates a 'futex queue' internally, so that it can later on match up the
waiter with the waker - without them having to know about each other.
When the owner thread releases the futex, it notices (via the variable
value) that there were waiter(s) pending, and does the
sys_futex(FUTEX_WAKE) syscall to wake them up.  Once all waiters have
taken and released the lock, the futex is again back to 'uncontended'
state, and there's no in-kernel state associated with it. The kernel
completely forgets that there ever was a futex at that address. This
method makes futexes very lightweight and scalable.

"Robustness" is about dealing with crashes while holding a lock: if a
process exits prematurely while holding a pthread_mutex_t lock that is
also shared with some other process (e.g. yum segfaults while holding a
pthread_mutex_t, or yum is kill -9-ed), then waiters for that lock need
to be notified that the last owner of the lock exited in some irregular
way.

To solve such types of problems, "robust mutex" userspace APIs were
created: pthread_mutex_lock() returns an error value if the owner exits
prematurely - and the new owner can decide whether the data protected by
the lock can be recovered safely.

There is a big conceptual problem with futex based mutexes though: it is
the kernel that destroys the owner task (e.g. due to a SEGFAULT), but
the kernel cannot help with the cleanup: if there is no 'futex queue'
(and in most cases there is none, futexes being fast lightweight locks)
then the kernel has no information to clean up after the held lock!
Userspace has no chance to clean up after the lock either - userspace is
the one that crashes, so it has no opportunity to clean up. Catch-22.

In practice, when e.g. yum is kill -9-ed (or segfaults), a system reboot
is needed to release that futex based lock. This is one of the leading
bugreports against yum.

To solve this problem, the traditional approach was to extend the vma
(virtual memory area descriptor) concept to have a notion of 'pending
robust futexes attached to this area'. This approach requires 3 new
syscall variants to sys_futex(): FUTEX_REGISTER, FUTEX_DEREGISTER and
FUTEX_RECOVER. At do_exit() time, all vmas are searched to see whether
they have a robust_head set. This approach has two fundamental problems
left:

 - it has quite complex locking and race scenarios. The vma-based
   approach had been pending for years, but they are still not completely
   reliable.

 - they have to scan _every_ vma at sys_exit() time, per thread!

The second disadvantage is a real killer: pthread_exit() takes around 1
microsecond on Linux, but with thousands (or tens of thousands) of vmas
every pthread_exit() takes a millisecond or more, also totally
destroying the CPU's L1 and L2 caches!

This is very much noticeable even for normal process sys_exit_group()
calls: the kernel has to do the vma scanning unconditionally! (this is
because the kernel has no knowledge about how many robust futexes there
are to be cleaned up, because a robust futex might have been registered
in another task, and the futex variable might have been simply mmap()-ed
into this process's address space).

This huge overhead forced the creation of CONFIG_FUTEX_ROBUST so that
normal kernels can turn it off, but worse than that: the overhead makes
robust futexes impractical for any type of generic Linux distribution.

So something had to be done.

New approach to robust futexes
------------------------------

At the heart of this new approach there is a per-thread private list of
robust locks that userspace is holding (maintained by glibc) - which
userspace list is registered with the kernel via a new syscall [this
registration happens at most once per thread lifetime]. At do_exit()
time, the kernel checks this user-space list: are there any robust futex
locks to be cleaned up?

In the common case, at do_exit() time, there is no list registered, so
the cost of robust futexes is just a simple current->robust_list != NULL
comparison. If the thread has registered a list, then normally the list
is empty. If the thread/process crashed or terminated in some incorrect
way then the list might be non-empty: in this case the kernel carefully
walks the list [not trusting it], and marks all locks that are owned by
this thread with the FUTEX_OWNER_DIED bit, and wakes up one waiter (if
any).

The list is guaranteed to be private and per-thread at do_exit() time,
so it can be accessed by the kernel in a lockless way.

There is one race possible though: since adding to and removing from the
list is done after the futex is acquired by glibc, there is a few
instructions window for the thread (or process) to die there, leaving
the futex hung. To protect against this possibility, userspace (glibc)
also maintains a simple per-thread 'list_op_pending' field, to allow the
kernel to clean up if the thread dies after acquiring the lock, but just
before it could have added itself to the list. Glibc sets this
list_op_pending field before it tries to acquire the futex, and clears
it after the list-add (or list-remove) has finished.

That's all that is needed - all the rest of robust-futex cleanup is done
in userspace [just like with the previous patches].

Ulrich Drepper has implemented the necessary glibc support for this new
mechanism, which fully enables robust mutexes.

Key differences of this userspace-list based approach, compared to the
vma based method:

 - it's much, much faster: at thread exit time, there's no need to loop
   over every vma (!), which the VM-based method has to do. Only a very
   simple 'is the list empty' op is done.

 - no VM changes are needed - 'struct address_space' is left alone.

 - no registration of individual locks is needed: robust mutexes dont
   need any extra per-lock syscalls. Robust mutexes thus become a very
   lightweight primitive - so they dont force the application designer
   to do a hard choice between performance and robustness - robust
   mutexes are just as fast.

 - no per-lock kernel allocation happens.

 - no resource limits are needed.

 - no kernel-space recovery call (FUTEX_RECOVER) is needed.

 - the implementation and the locking is "obvious", and there are no
   interactions with the VM.

Performance
-----------

I have benchmarked the time needed for the kernel to process a list of 1
million (!) held locks, using the new method [on a 2GHz CPU]:

 - with FUTEX_WAIT set [contended mutex]: 130 msecs
 - without FUTEX_WAIT set [uncontended mutex]: 30 msecs

I have also measured an approach where glibc does the lock notification
[which it currently does for !pshared robust mutexes], and that took 256
msecs - clearly slower, due to the 1 million FUTEX_WAKE syscalls
userspace had to do.

(1 million held locks are unheard of - we expect at most a handful of
locks to be held at a time. Nevertheless it's nice to know that this
approach scales nicely.)

Implementation details
----------------------

The patch adds two new syscalls: one to register the userspace list, and
one to query the registered list pointer:

 asmlinkage long
 sys_set_robust_list(struct robust_list_head __user *head,
                     size_t len);

 asmlinkage long
 sys_get_robust_list(int pid, struct robust_list_head __user **head_ptr,
                     size_t __user *len_ptr);

List registration is very fast: the pointer is simply stored in
current->robust_list. [Note that in the future, if robust futexes become
widespread, we could extend sys_clone() to register a robust-list head
for new threads, without the need of another syscall.]

So there is virtually zero overhead for tasks not using robust futexes,
and even for robust futex users, there is only one extra syscall per
thread lifetime, and the cleanup operation, if it happens, is fast and
straightforward. The kernel doesn't have any internal distinction between
robust and normal futexes.

If a futex is found to be held at exit time, the kernel sets the
following bit of the futex word:

	#define FUTEX_OWNER_DIED        0x40000000

and wakes up the next futex waiter (if any). User-space does the rest of
the cleanup.

Otherwise, robust futexes are acquired by glibc by putting the TID into
the futex field atomically. Waiters set the FUTEX_WAITERS bit:

	#define FUTEX_WAITERS           0x80000000

and the remaining bits are for the TID.

Testing, architecture support
-----------------------------

i've tested the new syscalls on x86 and x86_64, and have made sure the
parsing of the userspace list is robust [ ;-) ] even if the list is
deliberately corrupted.

i386 and x86_64 syscalls are wired up at the moment, and Ulrich has
tested the new glibc code (on x86_64 and i386), and it works for his
robust-mutex testcases.

All other architectures should build just fine too - but they won't have
the new syscalls yet.

Architectures need to implement the new futex_atomic_cmpxchg_inatomic()
inline function before writing up the syscalls (that function returns
-ENOSYS right now).
Remote Processor Messaging (rpmsg) Framework

Note: this document describes the rpmsg bus and how to write rpmsg drivers.
To learn how to add rpmsg support for new platforms, check out remoteproc.txt
(also a resident of Documentation/).

1. Introduction

Modern SoCs typically employ heterogeneous remote processor devices in
asymmetric multiprocessing (AMP) configurations, which may be running
different instances of operating system, whether it's Linux or any other
flavor of real-time OS.

OMAP4, for example, has dual Cortex-A9, dual Cortex-M3 and a C64x+ DSP.
Typically, the dual cortex-A9 is running Linux in a SMP configuration,
and each of the other three cores (two M3 cores and a DSP) is running
its own instance of RTOS in an AMP configuration.

Typically AMP remote processors employ dedicated DSP codecs and multimedia
hardware accelerators, and therefore are often used to offload CPU-intensive
multimedia tasks from the main application processor.

These remote processors could also be used to control latency-sensitive
sensors, drive random hardware blocks, or just perform background tasks
while the main CPU is idling.

Users of those remote processors can either be userland apps (e.g. multimedia
frameworks talking with remote OMX components) or kernel drivers (controlling
hardware accessible only by the remote processor, reserving kernel-controlled
resources on behalf of the remote processor, etc..).

Rpmsg is a virtio-based messaging bus that allows kernel drivers to communicate
with remote processors available on the system. In turn, drivers could then
expose appropriate user space interfaces, if needed.

When writing a driver that exposes rpmsg communication to userland, please
keep in mind that remote processors might have direct access to the
system's physical memory and other sensitive hardware resources (e.g. on
OMAP4, remote cores and hardware accelerators may have direct access to the
physical memory, gpio banks, dma controllers, i2c bus, gptimers, mailbox
devices, hwspinlocks, etc..). Moreover, those remote processors might be
running RTOS where every task can access the entire memory/devices exposed
to the processor. To minimize the risks of rogue (or buggy) userland code
exploiting remote bugs, and by that taking over the system, it is often
desired to limit userland to specific rpmsg channels (see definition below)
it can send messages on, and if possible, minimize how much control
it has over the content of the messages.

Every rpmsg device is a communication channel with a remote processor (thus
rpmsg devices are called channels). Channels are identified by a textual name
and have a local ("source") rpmsg address, and remote ("destination") rpmsg
address.

When a driver starts listening on a channel, its rx callback is bound with
a unique rpmsg local address (a 32-bit integer). This way when inbound messages
arrive, the rpmsg core dispatches them to the appropriate driver according
to their destination address (this is done by invoking the driver's rx handler
with the payload of the inbound message).


2. User API

  int rpmsg_send(struct rpmsg_channel *rpdev, void *data, int len);
   - sends a message across to the remote processor on a given channel.
     The caller should specify the channel, the data it wants to send,
     and its length (in bytes). The message will be sent on the specified
     channel, i.e. its source and destination address fields will be
     set to the channel's src and dst addresses.

     In case there are no TX buffers available, the function will block until
     one becomes available (i.e. until the remote processor consumes
     a tx buffer and puts it back on virtio's used descriptor ring),
     or a timeout of 15 seconds elapses. When the latter happens,
     -ERESTARTSYS is returned.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  int rpmsg_sendto(struct rpmsg_channel *rpdev, void *data, int len, u32 dst);
   - sends a message across to the remote processor on a given channel,
     to a destination address provided by the caller.
     The caller should specify the channel, the data it wants to send,
     its length (in bytes), and an explicit destination address.
     The message will then be sent to the remote processor to which the
     channel belongs, using the channel's src address, and the user-provided
     dst address (thus the channel's dst address will be ignored).

     In case there are no TX buffers available, the function will block until
     one becomes available (i.e. until the remote processor consumes
     a tx buffer and puts it back on virtio's used descriptor ring),
     or a timeout of 15 seconds elapses. When the latter happens,
     -ERESTARTSYS is returned.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  int rpmsg_send_offchannel(struct rpmsg_channel *rpdev, u32 src, u32 dst,
							void *data, int len);
   - sends a message across to the remote processor, using the src and dst
     addresses provided by the user.
     The caller should specify the channel, the data it wants to send,
     its length (in bytes), and explicit source and destination addresses.
     The message will then be sent to the remote processor to which the
     channel belongs, but the channel's src and dst addresses will be
     ignored (and the user-provided addresses will be used instead).

     In case there are no TX buffers available, the function will block until
     one becomes available (i.e. until the remote processor consumes
     a tx buffer and puts it back on virtio's used descriptor ring),
     or a timeout of 15 seconds elapses. When the latter happens,
     -ERESTARTSYS is returned.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  int rpmsg_trysend(struct rpmsg_channel *rpdev, void *data, int len);
   - sends a message across to the remote processor on a given channel.
     The caller should specify the channel, the data it wants to send,
     and its length (in bytes). The message will be sent on the specified
     channel, i.e. its source and destination address fields will be
     set to the channel's src and dst addresses.

     In case there are no TX buffers available, the function will immediately
     return -ENOMEM without waiting until one becomes available.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  int rpmsg_trysendto(struct rpmsg_channel *rpdev, void *data, int len, u32 dst)
   - sends a message across to the remote processor on a given channel,
     to a destination address provided by the user.
     The user should specify the channel, the data it wants to send,
     its length (in bytes), and an explicit destination address.
     The message will then be sent to the remote processor to which the
     channel belongs, using the channel's src address, and the user-provided
     dst address (thus the channel's dst address will be ignored).

     In case there are no TX buffers available, the function will immediately
     return -ENOMEM without waiting until one becomes available.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  int rpmsg_trysend_offchannel(struct rpmsg_channel *rpdev, u32 src, u32 dst,
							void *data, int len);
   - sends a message across to the remote processor, using source and
     destination addresses provided by the user.
     The user should specify the channel, the data it wants to send,
     its length (in bytes), and explicit source and destination addresses.
     The message will then be sent to the remote processor to which the
     channel belongs, but the channel's src and dst addresses will be
     ignored (and the user-provided addresses will be used instead).

     In case there are no TX buffers available, the function will immediately
     return -ENOMEM without waiting until one becomes available.
     The function can only be called from a process context (for now).
     Returns 0 on success and an appropriate error value on failure.

  struct rpmsg_endpoint *rpmsg_create_ept(struct rpmsg_channel *rpdev,
		void (*cb)(struct rpmsg_channel *, void *, int, void *, u32),
		void *priv, u32 addr);
   - every rpmsg address in the system is bound to an rx callback (so when
     inbound messages arrive, they are dispatched by the rpmsg bus using the
     appropriate callback handler) by means of an rpmsg_endpoint struct.

     This function allows drivers to create such an endpoint, and by that,
     bind a callback, and possibly some private data too, to an rpmsg address
     (either one that is known in advance, or one that will be dynamically
     assigned for them).

     Simple rpmsg drivers need not call rpmsg_create_ept, because an endpoint
     is already created for them when they are probed by the rpmsg bus
     (using the rx callback they provide when they registered to the rpmsg bus).

     So things should just work for simple drivers: they already have an
     endpoint, their rx callback is bound to their rpmsg address, and when
     relevant inbound messages arrive (i.e. messages which their dst address
     equals to the src address of their rpmsg channel), the driver's handler
     is invoked to process it.

     That said, more complicated drivers might do need to allocate
     additional rpmsg addresses, and bind them to different rx callbacks.
     To accomplish that, those drivers need to call this function.
     Drivers should provide their channel (so the new endpoint would bind
     to the same remote processor their channel belongs to), an rx callback
     function, an optional private data (which is provided back when the
     rx callback is invoked), and an address they want to bind with the
     callback. If addr is RPMSG_ADDR_ANY, then rpmsg_create_ept will
     dynamically assign them an available rpmsg address (drivers should have
     a very good reason why not to always use RPMSG_ADDR_ANY here).

     Returns a pointer to the endpoint on success, or NULL on error.

  void rpmsg_destroy_ept(struct rpmsg_endpoint *ept);
   - destroys an existing rpmsg endpoint. user should provide a pointer
     to an rpmsg endpoint that was previously created with rpmsg_create_ept().

  int register_rpmsg_driver(struct rpmsg_driver *rpdrv);
   - registers an rpmsg driver with the rpmsg bus. user should provide
     a pointer to an rpmsg_driver struct, which contains the driver's
     ->probe() and ->remove() functions, an rx callback, and an id_table
     specifying the names of the channels this driver is interested to
     be probed with.

  void unregister_rpmsg_driver(struct rpmsg_driver *rpdrv);
   - unregisters an rpmsg driver from the rpmsg bus. user should provide
     a pointer to a previously-registered rpmsg_driver struct.
     Returns 0 on success, and an appropriate error value on failure.


3. Typical usage

The following is a simple rpmsg driver, that sends an "hello!" message
on probe(), and whenever it receives an incoming message, it dumps its
content to the console.

#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/rpmsg.h>

static void rpmsg_sample_cb(struct rpmsg_channel *rpdev, void *data, int len,
						void *priv, u32 src)
{
	print_hex_dump(KERN_INFO, "incoming message:", DUMP_PREFIX_NONE,
						16, 1, data, len, true);
}

static int rpmsg_sample_probe(struct rpmsg_channel *rpdev)
{
	int err;

	dev_info(&rpdev->dev, "chnl: 0x%x -> 0x%x\n", rpdev->src, rpdev->dst);

	/* send a message on our channel */
	err = rpmsg_send(rpdev, "hello!", 6);
	if (err) {
		pr_err("rpmsg_send failed: %d\n", err);
		return err;
	}

	return 0;
}

static void rpmsg_sample_remove(struct rpmsg_channel *rpdev)
{
	dev_info(&rpdev->dev, "rpmsg sample client driver is removed\n");
}

static struct rpmsg_device_id rpmsg_driver_sample_id_table[] = {
	{ .name	= "rpmsg-client-sample" },
	{ },
};
MODULE_DEVICE_TABLE(rpmsg, rpmsg_driver_sample_id_table);

static struct rpmsg_driver rpmsg_sample_client = {
	.drv.name	= KBUILD_MODNAME,
	.drv.owner	= THIS_MODULE,
	.id_table	= rpmsg_driver_sample_id_table,
	.probe		= rpmsg_sample_probe,
	.callback	= rpmsg_sample_cb,
	.remove		= rpmsg_sample_remove,
};

static int __init init(void)
{
	return register_rpmsg_driver(&rpmsg_sample_client);
}
module_init(init);

static void __exit fini(void)
{
	unregister_rpmsg_driver(&rpmsg_sample_client);
}
module_exit(fini);

Note: a similar sample which can be built and loaded can be found
in samples/rpmsg/.

4. Allocations of rpmsg channels:

At this point we only support dynamic allocations of rpmsg channels.

This is possible only with remote processors that have the VIRTIO_RPMSG_F_NS
virtio device feature set. This feature bit means that the remote
processor supports dynamic name service announcement messages.

When this feature is enabled, creation of rpmsg devices (i.e. channels)
is completely dynamic: the remote processor announces the existence of a
remote rpmsg service by sending a name service message (which contains
the name and rpmsg addr of the remote service, see struct rpmsg_ns_msg).

This message is then handled by the rpmsg bus, which in turn dynamically
creates and registers an rpmsg channel (which represents the remote service).
If/when a relevant rpmsg driver is registered, it will be immediately probed
by the bus, and can then start sending messages to the remote service.

The plan is also to add static creation of rpmsg channels via the virtio
config space, but it's not implemented yet.

	Real Time Clock (RTC) Drivers for Linux
	=======================================

When Linux developers talk about a "Real Time Clock", they usually mean
something that tracks wall clock time and is battery backed so that it
works even with system power off.  Such clocks will normally not track
the local time zone or daylight savings time -- unless they dual boot
with MS-Windows -- but will instead be set to Coordinated Universal Time
(UTC, formerly "Greenwich Mean Time").

The newest non-PC hardware tends to just count seconds, like the time(2)
system call reports, but RTCs also very commonly represent time using
the Gregorian calendar and 24 hour time, as reported by gmtime(3).

Linux has two largely-compatible userspace RTC API families you may
need to know about:

    *	/dev/rtc ... is the RTC provided by PC compatible systems,
	so it's not very portable to non-x86 systems.

    *	/dev/rtc0, /dev/rtc1 ... are part of a framework that's
	supported by a wide variety of RTC chips on all systems.

Programmers need to understand that the PC/AT functionality is not
always available, and some systems can do much more.  That is, the
RTCs use the same API to make requests in both RTC frameworks (using
different filenames of course), but the hardware may not offer the
same functionality.  For example, not every RTC is hooked up to an
IRQ, so they can't all issue alarms; and where standard PC RTCs can
only issue an alarm up to 24 hours in the future, other hardware may
be able to schedule one any time in the upcoming century.


	Old PC/AT-Compatible driver:  /dev/rtc
	--------------------------------------

All PCs (even Alpha machines) have a Real Time Clock built into them.
Usually they are built into the chipset of the computer, but some may
actually have a Motorola MC146818 (or clone) on the board. This is the
clock that keeps the date and time while your computer is turned off.

ACPI has standardized that MC146818 functionality, and extended it in
a few ways (enabling longer alarm periods, and wake-from-hibernate).
That functionality is NOT exposed in the old driver.

However it can also be used to generate signals from a slow 2Hz to a
relatively fast 8192Hz, in increments of powers of two. These signals
are reported by interrupt number 8. (Oh! So *that* is what IRQ 8 is
for...) It can also function as a 24hr alarm, raising IRQ 8 when the
alarm goes off. The alarm can also be programmed to only check any
subset of the three programmable values, meaning that it could be set to
ring on the 30th second of the 30th minute of every hour, for example.
The clock can also be set to generate an interrupt upon every clock
update, thus generating a 1Hz signal.

The interrupts are reported via /dev/rtc (major 10, minor 135, read only
character device) in the form of an unsigned long. The low byte contains
the type of interrupt (update-done, alarm-rang, or periodic) that was
raised, and the remaining bytes contain the number of interrupts since
the last read.  Status information is reported through the pseudo-file
/proc/driver/rtc if the /proc filesystem was enabled.  The driver has
built in locking so that only one process is allowed to have the /dev/rtc
interface open at a time.

A user process can monitor these interrupts by doing a read(2) or a
select(2) on /dev/rtc -- either will block/stop the user process until
the next interrupt is received. This is useful for things like
reasonably high frequency data acquisition where one doesn't want to
burn up 100% CPU by polling gettimeofday etc. etc.

At high frequencies, or under high loads, the user process should check
the number of interrupts received since the last read to determine if
there has been any interrupt "pileup" so to speak. Just for reference, a
typical 486-33 running a tight read loop on /dev/rtc will start to suffer
occasional interrupt pileup (i.e. > 1 IRQ event since last read) for
frequencies above 1024Hz. So you really should check the high bytes
of the value you read, especially at frequencies above that of the
normal timer interrupt, which is 100Hz.

Programming and/or enabling interrupt frequencies greater than 64Hz is
only allowed by root. This is perhaps a bit conservative, but we don't want
an evil user generating lots of IRQs on a slow 386sx-16, where it might have
a negative impact on performance. This 64Hz limit can be changed by writing
a different value to /proc/sys/dev/rtc/max-user-freq. Note that the
interrupt handler is only a few lines of code to minimize any possibility
of this effect.

Also, if the kernel time is synchronized with an external source, the 
kernel will write the time back to the CMOS clock every 11 minutes. In 
the process of doing this, the kernel briefly turns off RTC periodic 
interrupts, so be aware of this if you are doing serious work. If you
don't synchronize the kernel time with an external source (via ntp or
whatever) then the kernel will keep its hands off the RTC, allowing you
exclusive access to the device for your applications.

The alarm and/or interrupt frequency are programmed into the RTC via
various ioctl(2) calls as listed in ./include/linux/rtc.h
Rather than write 50 pages describing the ioctl() and so on, it is
perhaps more useful to include a small test program that demonstrates
how to use them, and demonstrates the features of the driver. This is
probably a lot more useful to people interested in writing applications
that will be using this driver.  See the code at the end of this document.

(The original /dev/rtc driver was written by Paul Gortmaker.)


	New portable "RTC Class" drivers:  /dev/rtcN
	--------------------------------------------

Because Linux supports many non-ACPI and non-PC platforms, some of which
have more than one RTC style clock, it needed a more portable solution
than expecting a single battery-backed MC146818 clone on every system.
Accordingly, a new "RTC Class" framework has been defined.  It offers
three different userspace interfaces:

    *	/dev/rtcN ... much the same as the older /dev/rtc interface

    *	/sys/class/rtc/rtcN ... sysfs attributes support readonly
	access to some RTC attributes.

    *	/proc/driver/rtc ... the system clock RTC may expose itself
	using a procfs interface. If there is no RTC for the system clock,
	rtc0 is used by default. More information is (currently) shown
	here than through sysfs.

The RTC Class framework supports a wide variety of RTCs, ranging from those
integrated into embeddable system-on-chip (SOC) processors to discrete chips
using I2C, SPI, or some other bus to communicate with the host CPU.  There's
even support for PC-style RTCs ... including the features exposed on newer PCs
through ACPI.

The new framework also removes the "one RTC per system" restriction.  For
example, maybe the low-power battery-backed RTC is a discrete I2C chip, but
a high functionality RTC is integrated into the SOC.  That system might read
the system clock from the discrete RTC, but use the integrated one for all
other tasks, because of its greater functionality.

SYSFS INTERFACE
---------------

The sysfs interface under /sys/class/rtc/rtcN provides access to various
rtc attributes without requiring the use of ioctls. All dates and times
are in the RTC's timezone, rather than in system time.

date:  	   	 RTC-provided date
hctosys:   	 1 if the RTC provided the system time at boot via the
		 CONFIG_RTC_HCTOSYS kernel option, 0 otherwise
max_user_freq:	 The maximum interrupt rate an unprivileged user may request
		 from this RTC.
name:		 The name of the RTC corresponding to this sysfs directory
since_epoch:	 The number of seconds since the epoch according to the RTC
time:		 RTC-provided time
wakealarm:	 The time at which the clock will generate a system wakeup
		 event. This is a one shot wakeup event, so must be reset
		 after wake if a daily wakeup is required. Format is seconds since
		 the epoch by default, or if there's a leading +, seconds in the
		 future, or if there is a leading +=, seconds ahead of the current
		 alarm.

IOCTL INTERFACE
---------------

The ioctl() calls supported by /dev/rtc are also supported by the RTC class
framework.  However, because the chips and systems are not standardized,
some PC/AT functionality might not be provided.  And in the same way, some
newer features -- including those enabled by ACPI -- are exposed by the
RTC class framework, but can't be supported by the older driver.

    *	RTC_RD_TIME, RTC_SET_TIME ... every RTC supports at least reading
	time, returning the result as a Gregorian calendar date and 24 hour
	wall clock time.  To be most useful, this time may also be updated.

    *	RTC_AIE_ON, RTC_AIE_OFF, RTC_ALM_SET, RTC_ALM_READ ... when the RTC
	is connected to an IRQ line, it can often issue an alarm IRQ up to
	24 hours in the future.  (Use RTC_WKALM_* by preference.)

    *	RTC_WKALM_SET, RTC_WKALM_RD ... RTCs that can issue alarms beyond
	the next 24 hours use a slightly more powerful API, which supports
	setting the longer alarm time and enabling its IRQ using a single
	request (using the same model as EFI firmware).

    *	RTC_UIE_ON, RTC_UIE_OFF ... if the RTC offers IRQs, the RTC framework
	will emulate this mechanism.

    *	RTC_PIE_ON, RTC_PIE_OFF, RTC_IRQP_SET, RTC_IRQP_READ ... these icotls
	are emulated via a kernel hrtimer.

In many cases, the RTC alarm can be a system wake event, used to force
Linux out of a low power sleep state (or hibernation) back to a fully
operational state.  For example, a system could enter a deep power saving
state until it's time to execute some scheduled tasks.

Note that many of these ioctls are handled by the common rtc-dev interface.
Some common examples:

    *	RTC_RD_TIME, RTC_SET_TIME: the read_time/set_time functions will be
	called with appropriate values.

    *	RTC_ALM_SET, RTC_ALM_READ, RTC_WKALM_SET, RTC_WKALM_RD: gets or sets
	the alarm rtc_timer. May call the set_alarm driver function.

    *	RTC_IRQP_SET, RTC_IRQP_READ: These are emulated by the generic code.

    *	RTC_PIE_ON, RTC_PIE_OFF: These are also emulated by the generic code.

If all else fails, check out the rtc-test.c driver!


-------------------- 8< ---------------- 8< -----------------------------

/*
 *      Real Time Clock Driver Test/Example Program
 *
 *      Compile with:
 *		     gcc -s -Wall -Wstrict-prototypes rtctest.c -o rtctest
 *
 *      Copyright (C) 1996, Paul Gortmaker.
 *
 *      Released under the GNU General Public License, version 2,
 *      included herein by reference.
 *
 */

#include <stdio.h>
#include <linux/rtc.h>
#include <sys/ioctl.h>
#include <sys/time.h>
#include <sys/types.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdlib.h>
#include <errno.h>


/*
 * This expects the new RTC class driver framework, working with
 * clocks that will often not be clones of what the PC-AT had.
 * Use the command line to specify another RTC if you need one.
 */
static const char default_rtc[] = "/dev/rtc0";


int main(int argc, char **argv)
{
	int i, fd, retval, irqcount = 0;
	unsigned long tmp, data;
	struct rtc_time rtc_tm;
	const char *rtc = default_rtc;

	switch (argc) {
	case 2:
		rtc = argv[1];
		/* FALLTHROUGH */
	case 1:
		break;
	default:
		fprintf(stderr, "usage:  rtctest [rtcdev]\n");
		return 1;
	}

	fd = open(rtc, O_RDONLY);

	if (fd ==  -1) {
		perror(rtc);
		exit(errno);
	}

	fprintf(stderr, "\n\t\t\tRTC Driver Test Example.\n\n");

	/* Turn on update interrupts (one per second) */
	retval = ioctl(fd, RTC_UIE_ON, 0);
	if (retval == -1) {
		if (errno == ENOTTY) {
			fprintf(stderr,
				"\n...Update IRQs not supported.\n");
			goto test_READ;
		}
		perror("RTC_UIE_ON ioctl");
		exit(errno);
	}

	fprintf(stderr, "Counting 5 update (1/sec) interrupts from reading %s:",
			rtc);
	fflush(stderr);
	for (i=1; i<6; i++) {
		/* This read will block */
		retval = read(fd, &data, sizeof(unsigned long));
		if (retval == -1) {
			perror("read");
			exit(errno);
		}
		fprintf(stderr, " %d",i);
		fflush(stderr);
		irqcount++;
	}

	fprintf(stderr, "\nAgain, from using select(2) on /dev/rtc:");
	fflush(stderr);
	for (i=1; i<6; i++) {
		struct timeval tv = {5, 0};     /* 5 second timeout on select */
		fd_set readfds;

		FD_ZERO(&readfds);
		FD_SET(fd, &readfds);
		/* The select will wait until an RTC interrupt happens. */
		retval = select(fd+1, &readfds, NULL, NULL, &tv);
		if (retval == -1) {
		        perror("select");
		        exit(errno);
		}
		/* This read won't block unlike the select-less case above. */
		retval = read(fd, &data, sizeof(unsigned long));
		if (retval == -1) {
		        perror("read");
		        exit(errno);
		}
		fprintf(stderr, " %d",i);
		fflush(stderr);
		irqcount++;
	}

	/* Turn off update interrupts */
	retval = ioctl(fd, RTC_UIE_OFF, 0);
	if (retval == -1) {
		perror("RTC_UIE_OFF ioctl");
		exit(errno);
	}

test_READ:
	/* Read the RTC time/date */
	retval = ioctl(fd, RTC_RD_TIME, &rtc_tm);
	if (retval == -1) {
		perror("RTC_RD_TIME ioctl");
		exit(errno);
	}

	fprintf(stderr, "\n\nCurrent RTC date/time is %d-%d-%d, %02d:%02d:%02d.\n",
		rtc_tm.tm_mday, rtc_tm.tm_mon + 1, rtc_tm.tm_year + 1900,
		rtc_tm.tm_hour, rtc_tm.tm_min, rtc_tm.tm_sec);

	/* Set the alarm to 5 sec in the future, and check for rollover */
	rtc_tm.tm_sec += 5;
	if (rtc_tm.tm_sec >= 60) {
		rtc_tm.tm_sec %= 60;
		rtc_tm.tm_min++;
	}
	if (rtc_tm.tm_min == 60) {
		rtc_tm.tm_min = 0;
		rtc_tm.tm_hour++;
	}
	if (rtc_tm.tm_hour == 24)
		rtc_tm.tm_hour = 0;

	retval = ioctl(fd, RTC_ALM_SET, &rtc_tm);
	if (retval == -1) {
		if (errno == ENOTTY) {
			fprintf(stderr,
				"\n...Alarm IRQs not supported.\n");
			goto test_PIE;
		}
		perror("RTC_ALM_SET ioctl");
		exit(errno);
	}

	/* Read the current alarm settings */
	retval = ioctl(fd, RTC_ALM_READ, &rtc_tm);
	if (retval == -1) {
		perror("RTC_ALM_READ ioctl");
		exit(errno);
	}

	fprintf(stderr, "Alarm time now set to %02d:%02d:%02d.\n",
		rtc_tm.tm_hour, rtc_tm.tm_min, rtc_tm.tm_sec);

	/* Enable alarm interrupts */
	retval = ioctl(fd, RTC_AIE_ON, 0);
	if (retval == -1) {
		perror("RTC_AIE_ON ioctl");
		exit(errno);
	}

	fprintf(stderr, "Waiting 5 seconds for alarm...");
	fflush(stderr);
	/* This blocks until the alarm ring causes an interrupt */
	retval = read(fd, &data, sizeof(unsigned long));
	if (retval == -1) {
		perror("read");
		exit(errno);
	}
	irqcount++;
	fprintf(stderr, " okay. Alarm rang.\n");

	/* Disable alarm interrupts */
	retval = ioctl(fd, RTC_AIE_OFF, 0);
	if (retval == -1) {
		perror("RTC_AIE_OFF ioctl");
		exit(errno);
	}

test_PIE:
	/* Read periodic IRQ rate */
	retval = ioctl(fd, RTC_IRQP_READ, &tmp);
	if (retval == -1) {
		/* not all RTCs support periodic IRQs */
		if (errno == ENOTTY) {
			fprintf(stderr, "\nNo periodic IRQ support\n");
			goto done;
		}
		perror("RTC_IRQP_READ ioctl");
		exit(errno);
	}
	fprintf(stderr, "\nPeriodic IRQ rate is %ldHz.\n", tmp);

	fprintf(stderr, "Counting 20 interrupts at:");
	fflush(stderr);

	/* The frequencies 128Hz, 256Hz, ... 8192Hz are only allowed for root. */
	for (tmp=2; tmp<=64; tmp*=2) {

		retval = ioctl(fd, RTC_IRQP_SET, tmp);
		if (retval == -1) {
			/* not all RTCs can change their periodic IRQ rate */
			if (errno == ENOTTY) {
				fprintf(stderr,
					"\n...Periodic IRQ rate is fixed\n");
				goto done;
			}
			perror("RTC_IRQP_SET ioctl");
			exit(errno);
		}

		fprintf(stderr, "\n%ldHz:\t", tmp);
		fflush(stderr);

		/* Enable periodic interrupts */
		retval = ioctl(fd, RTC_PIE_ON, 0);
		if (retval == -1) {
			perror("RTC_PIE_ON ioctl");
			exit(errno);
		}

		for (i=1; i<21; i++) {
			/* This blocks */
			retval = read(fd, &data, sizeof(unsigned long));
			if (retval == -1) {
				perror("read");
				exit(errno);
			}
			fprintf(stderr, " %d",i);
			fflush(stderr);
			irqcount++;
		}

		/* Disable periodic interrupts */
		retval = ioctl(fd, RTC_PIE_OFF, 0);
		if (retval == -1) {
			perror("RTC_PIE_OFF ioctl");
			exit(errno);
		}
	}

done:
	fprintf(stderr, "\n\n\t\t\t *** Test complete ***\n");

	close(fd);

	return 0;
}
#
# Copyright (c) 2006 Steven Rostedt
# Licensed under the GNU Free Documentation License, Version 1.2
#

RT-mutex implementation design
------------------------------

This document tries to describe the design of the rtmutex.c implementation.
It doesn't describe the reasons why rtmutex.c exists. For that please see
Documentation/rt-mutex.txt.  Although this document does explain problems
that happen without this code, but that is in the concept to understand
what the code actually is doing.

The goal of this document is to help others understand the priority
inheritance (PI) algorithm that is used, as well as reasons for the
decisions that were made to implement PI in the manner that was done.


Unbounded Priority Inversion
----------------------------

Priority inversion is when a lower priority process executes while a higher
priority process wants to run.  This happens for several reasons, and
most of the time it can't be helped.  Anytime a high priority process wants
to use a resource that a lower priority process has (a mutex for example),
the high priority process must wait until the lower priority process is done
with the resource.  This is a priority inversion.  What we want to prevent
is something called unbounded priority inversion.  That is when the high
priority process is prevented from running by a lower priority process for
an undetermined amount of time.

The classic example of unbounded priority inversion is where you have three
processes, let's call them processes A, B, and C, where A is the highest
priority process, C is the lowest, and B is in between. A tries to grab a lock
that C owns and must wait and lets C run to release the lock. But in the
meantime, B executes, and since B is of a higher priority than C, it preempts C,
but by doing so, it is in fact preempting A which is a higher priority process.
Now there's no way of knowing how long A will be sleeping waiting for C
to release the lock, because for all we know, B is a CPU hog and will
never give C a chance to release the lock.  This is called unbounded priority
inversion.

Here's a little ASCII art to show the problem.

   grab lock L1 (owned by C)
     |
A ---+
        C preempted by B
          |
C    +----+

B         +-------->
                B now keeps A from running.


Priority Inheritance (PI)
-------------------------

There are several ways to solve this issue, but other ways are out of scope
for this document.  Here we only discuss PI.

PI is where a process inherits the priority of another process if the other
process blocks on a lock owned by the current process.  To make this easier
to understand, let's use the previous example, with processes A, B, and C again.

This time, when A blocks on the lock owned by C, C would inherit the priority
of A.  So now if B becomes runnable, it would not preempt C, since C now has
the high priority of A.  As soon as C releases the lock, it loses its
inherited priority, and A then can continue with the resource that C had.

Terminology
-----------

Here I explain some terminology that is used in this document to help describe
the design that is used to implement PI.

PI chain - The PI chain is an ordered series of locks and processes that cause
           processes to inherit priorities from a previous process that is
           blocked on one of its locks.  This is described in more detail
           later in this document.

mutex    - In this document, to differentiate from locks that implement
           PI and spin locks that are used in the PI code, from now on
           the PI locks will be called a mutex.

lock     - In this document from now on, I will use the term lock when
           referring to spin locks that are used to protect parts of the PI
           algorithm.  These locks disable preemption for UP (when
           CONFIG_PREEMPT is enabled) and on SMP prevents multiple CPUs from
           entering critical sections simultaneously.

spin lock - Same as lock above.

waiter   - A waiter is a struct that is stored on the stack of a blocked
           process.  Since the scope of the waiter is within the code for
           a process being blocked on the mutex, it is fine to allocate
           the waiter on the process's stack (local variable).  This
           structure holds a pointer to the task, as well as the mutex that
           the task is blocked on.  It also has the plist node structures to
           place the task in the waiter_list of a mutex as well as the
           pi_list of a mutex owner task (described below).

           waiter is sometimes used in reference to the task that is waiting
           on a mutex. This is the same as waiter->task.

waiters  - A list of processes that are blocked on a mutex.

top waiter - The highest priority process waiting on a specific mutex.

top pi waiter - The highest priority process waiting on one of the mutexes
                that a specific process owns.

Note:  task and process are used interchangeably in this document, mostly to
       differentiate between two processes that are being described together.


PI chain
--------

The PI chain is a list of processes and mutexes that may cause priority
inheritance to take place.  Multiple chains may converge, but a chain
would never diverge, since a process can't be blocked on more than one
mutex at a time.

Example:

   Process:  A, B, C, D, E
   Mutexes:  L1, L2, L3, L4

   A owns: L1
           B blocked on L1
           B owns L2
                  C blocked on L2
                  C owns L3
                         D blocked on L3
                         D owns L4
                                E blocked on L4

The chain would be:

   E->L4->D->L3->C->L2->B->L1->A

To show where two chains merge, we could add another process F and
another mutex L5 where B owns L5 and F is blocked on mutex L5.

The chain for F would be:

   F->L5->B->L1->A

Since a process may own more than one mutex, but never be blocked on more than
one, the chains merge.

Here we show both chains:

   E->L4->D->L3->C->L2-+
                       |
                       +->B->L1->A
                       |
                 F->L5-+

For PI to work, the processes at the right end of these chains (or we may
also call it the Top of the chain) must be equal to or higher in priority
than the processes to the left or below in the chain.

Also since a mutex may have more than one process blocked on it, we can
have multiple chains merge at mutexes.  If we add another process G that is
blocked on mutex L2:

  G->L2->B->L1->A

And once again, to show how this can grow I will show the merging chains
again.

   E->L4->D->L3->C-+
                   +->L2-+
                   |     |
                 G-+     +->B->L1->A
                         |
                   F->L5-+


Plist
-----

Before I go further and talk about how the PI chain is stored through lists
on both mutexes and processes, I'll explain the plist.  This is similar to
the struct list_head functionality that is already in the kernel.
The implementation of plist is out of scope for this document, but it is
very important to understand what it does.

There are a few differences between plist and list, the most important one
being that plist is a priority sorted linked list.  This means that the
priorities of the plist are sorted, such that it takes O(1) to retrieve the
highest priority item in the list.  Obviously this is useful to store processes
based on their priorities.

Another difference, which is important for implementation, is that, unlike
list, the head of the list is a different element than the nodes of a list.
So the head of the list is declared as struct plist_head and nodes that will
be added to the list are declared as struct plist_node.


Mutex Waiter List
-----------------

Every mutex keeps track of all the waiters that are blocked on itself. The mutex
has a plist to store these waiters by priority.  This list is protected by
a spin lock that is located in the struct of the mutex. This lock is called
wait_lock.  Since the modification of the waiter list is never done in
interrupt context, the wait_lock can be taken without disabling interrupts.


Task PI List
------------

To keep track of the PI chains, each process has its own PI list.  This is
a list of all top waiters of the mutexes that are owned by the process.
Note that this list only holds the top waiters and not all waiters that are
blocked on mutexes owned by the process.

The top of the task's PI list is always the highest priority task that
is waiting on a mutex that is owned by the task.  So if the task has
inherited a priority, it will always be the priority of the task that is
at the top of this list.

This list is stored in the task structure of a process as a plist called
pi_list.  This list is protected by a spin lock also in the task structure,
called pi_lock.  This lock may also be taken in interrupt context, so when
locking the pi_lock, interrupts must be disabled.


Depth of the PI Chain
---------------------

The maximum depth of the PI chain is not dynamic, and could actually be
defined.  But is very complex to figure it out, since it depends on all
the nesting of mutexes.  Let's look at the example where we have 3 mutexes,
L1, L2, and L3, and four separate functions func1, func2, func3 and func4.
The following shows a locking order of L1->L2->L3, but may not actually
be directly nested that way.

void func1(void)
{
	mutex_lock(L1);

	/* do anything */

	mutex_unlock(L1);
}

void func2(void)
{
	mutex_lock(L1);
	mutex_lock(L2);

	/* do something */

	mutex_unlock(L2);
	mutex_unlock(L1);
}

void func3(void)
{
	mutex_lock(L2);
	mutex_lock(L3);

	/* do something else */

	mutex_unlock(L3);
	mutex_unlock(L2);
}

void func4(void)
{
	mutex_lock(L3);

	/* do something again */

	mutex_unlock(L3);
}

Now we add 4 processes that run each of these functions separately.
Processes A, B, C, and D which run functions func1, func2, func3 and func4
respectively, and such that D runs first and A last.  With D being preempted
in func4 in the "do something again" area, we have a locking that follows:

D owns L3
       C blocked on L3
       C owns L2
              B blocked on L2
              B owns L1
                     A blocked on L1

And thus we have the chain A->L1->B->L2->C->L3->D.

This gives us a PI depth of 4 (four processes), but looking at any of the
functions individually, it seems as though they only have at most a locking
depth of two.  So, although the locking depth is defined at compile time,
it still is very difficult to find the possibilities of that depth.

Now since mutexes can be defined by user-land applications, we don't want a DOS
type of application that nests large amounts of mutexes to create a large
PI chain, and have the code holding spin locks while looking at a large
amount of data.  So to prevent this, the implementation not only implements
a maximum lock depth, but also only holds at most two different locks at a
time, as it walks the PI chain.  More about this below.


Mutex owner and flags
---------------------

The mutex structure contains a pointer to the owner of the mutex.  If the
mutex is not owned, this owner is set to NULL.  Since all architectures
have the task structure on at least a four byte alignment (and if this is
not true, the rtmutex.c code will be broken!), this allows for the two
least significant bits to be used as flags.  This part is also described
in Documentation/rt-mutex.txt, but will also be briefly described here.

Bit 0 is used as the "Pending Owner" flag.  This is described later.
Bit 1 is used as the "Has Waiters" flags.  This is also described later
  in more detail, but is set whenever there are waiters on a mutex.


cmpxchg Tricks
--------------

Some architectures implement an atomic cmpxchg (Compare and Exchange).  This
is used (when applicable) to keep the fast path of grabbing and releasing
mutexes short.

cmpxchg is basically the following function performed atomically:

unsigned long _cmpxchg(unsigned long *A, unsigned long *B, unsigned long *C)
{
	unsigned long T = *A;
	if (*A == *B) {
		*A = *C;
	}
	return T;
}
#define cmpxchg(a,b,c) _cmpxchg(&a,&b,&c)

This is really nice to have, since it allows you to only update a variable
if the variable is what you expect it to be.  You know if it succeeded if
the return value (the old value of A) is equal to B.

The macro rt_mutex_cmpxchg is used to try to lock and unlock mutexes. If
the architecture does not support CMPXCHG, then this macro is simply set
to fail every time.  But if CMPXCHG is supported, then this will
help out extremely to keep the fast path short.

The use of rt_mutex_cmpxchg with the flags in the owner field help optimize
the system for architectures that support it.  This will also be explained
later in this document.


Priority adjustments
--------------------

The implementation of the PI code in rtmutex.c has several places that a
process must adjust its priority.  With the help of the pi_list of a
process this is rather easy to know what needs to be adjusted.

The functions implementing the task adjustments are rt_mutex_adjust_prio,
__rt_mutex_adjust_prio (same as the former, but expects the task pi_lock
to already be taken), rt_mutex_getprio, and rt_mutex_setprio.

rt_mutex_getprio and rt_mutex_setprio are only used in __rt_mutex_adjust_prio.

rt_mutex_getprio returns the priority that the task should have.  Either the
task's own normal priority, or if a process of a higher priority is waiting on
a mutex owned by the task, then that higher priority should be returned.
Since the pi_list of a task holds an order by priority list of all the top
waiters of all the mutexes that the task owns, rt_mutex_getprio simply needs
to compare the top pi waiter to its own normal priority, and return the higher
priority back.

(Note:  if looking at the code, you will notice that the lower number of
        prio is returned.  This is because the prio field in the task structure
        is an inverse order of the actual priority.  So a "prio" of 5 is
        of higher priority than a "prio" of 10.)

__rt_mutex_adjust_prio examines the result of rt_mutex_getprio, and if the
result does not equal the task's current priority, then rt_mutex_setprio
is called to adjust the priority of the task to the new priority.
Note that rt_mutex_setprio is defined in kernel/sched/core.c to implement the
actual change in priority.

It is interesting to note that __rt_mutex_adjust_prio can either increase
or decrease the priority of the task.  In the case that a higher priority
process has just blocked on a mutex owned by the task, __rt_mutex_adjust_prio
would increase/boost the task's priority.  But if a higher priority task
were for some reason to leave the mutex (timeout or signal), this same function
would decrease/unboost the priority of the task.  That is because the pi_list
always contains the highest priority task that is waiting on a mutex owned
by the task, so we only need to compare the priority of that top pi waiter
to the normal priority of the given task.


High level overview of the PI chain walk
----------------------------------------

The PI chain walk is implemented by the function rt_mutex_adjust_prio_chain.

The implementation has gone through several iterations, and has ended up
with what we believe is the best.  It walks the PI chain by only grabbing
at most two locks at a time, and is very efficient.

The rt_mutex_adjust_prio_chain can be used either to boost or lower process
priorities.

rt_mutex_adjust_prio_chain is called with a task to be checked for PI
(de)boosting (the owner of a mutex that a process is blocking on), a flag to
check for deadlocking, the mutex that the task owns, and a pointer to a waiter
that is the process's waiter struct that is blocked on the mutex (although this
parameter may be NULL for deboosting).

For this explanation, I will not mention deadlock detection. This explanation
will try to stay at a high level.

When this function is called, there are no locks held.  That also means
that the state of the owner and lock can change when entered into this function.

Before this function is called, the task has already had rt_mutex_adjust_prio
performed on it.  This means that the task is set to the priority that it
should be at, but the plist nodes of the task's waiter have not been updated
with the new priorities, and that this task may not be in the proper locations
in the pi_lists and wait_lists that the task is blocked on.  This function
solves all that.

A loop is entered, where task is the owner to be checked for PI changes that
was passed by parameter (for the first iteration).  The pi_lock of this task is
taken to prevent any more changes to the pi_list of the task.  This also
prevents new tasks from completing the blocking on a mutex that is owned by this
task.

If the task is not blocked on a mutex then the loop is exited.  We are at
the top of the PI chain.

A check is now done to see if the original waiter (the process that is blocked
on the current mutex) is the top pi waiter of the task.  That is, is this
waiter on the top of the task's pi_list.  If it is not, it either means that
there is another process higher in priority that is blocked on one of the
mutexes that the task owns, or that the waiter has just woken up via a signal
or timeout and has left the PI chain.  In either case, the loop is exited, since
we don't need to do any more changes to the priority of the current task, or any
task that owns a mutex that this current task is waiting on.  A priority chain
walk is only needed when a new top pi waiter is made to a task.

The next check sees if the task's waiter plist node has the priority equal to
the priority the task is set at.  If they are equal, then we are done with
the loop.  Remember that the function started with the priority of the
task adjusted, but the plist nodes that hold the task in other processes
pi_lists have not been adjusted.

Next, we look at the mutex that the task is blocked on. The mutex's wait_lock
is taken.  This is done by a spin_trylock, because the locking order of the
pi_lock and wait_lock goes in the opposite direction. If we fail to grab the
lock, the pi_lock is released, and we restart the loop.

Now that we have both the pi_lock of the task as well as the wait_lock of
the mutex the task is blocked on, we update the task's waiter's plist node
that is located on the mutex's wait_list.

Now we release the pi_lock of the task.

Next the owner of the mutex has its pi_lock taken, so we can update the
task's entry in the owner's pi_list.  If the task is the highest priority
process on the mutex's wait_list, then we remove the previous top waiter
from the owner's pi_list, and replace it with the task.

Note: It is possible that the task was the current top waiter on the mutex,
      in which case the task is not yet on the pi_list of the waiter.  This
      is OK, since plist_del does nothing if the plist node is not on any
      list.

If the task was not the top waiter of the mutex, but it was before we
did the priority updates, that means we are deboosting/lowering the
task.  In this case, the task is removed from the pi_list of the owner,
and the new top waiter is added.

Lastly, we unlock both the pi_lock of the task, as well as the mutex's
wait_lock, and continue the loop again.  On the next iteration of the
loop, the previous owner of the mutex will be the task that will be
processed.

Note: One might think that the owner of this mutex might have changed
      since we just grab the mutex's wait_lock. And one could be right.
      The important thing to remember is that the owner could not have
      become the task that is being processed in the PI chain, since
      we have taken that task's pi_lock at the beginning of the loop.
      So as long as there is an owner of this mutex that is not the same
      process as the tasked being worked on, we are OK.

      Looking closely at the code, one might be confused.  The check for the
      end of the PI chain is when the task isn't blocked on anything or the
      task's waiter structure "task" element is NULL.  This check is
      protected only by the task's pi_lock.  But the code to unlock the mutex
      sets the task's waiter structure "task" element to NULL with only
      the protection of the mutex's wait_lock, which was not taken yet.
      Isn't this a race condition if the task becomes the new owner?

      The answer is No!  The trick is the spin_trylock of the mutex's
      wait_lock.  If we fail that lock, we release the pi_lock of the
      task and continue the loop, doing the end of PI chain check again.

      In the code to release the lock, the wait_lock of the mutex is held
      the entire time, and it is not let go when we grab the pi_lock of the
      new owner of the mutex.  So if the switch of a new owner were to happen
      after the check for end of the PI chain and the grabbing of the
      wait_lock, the unlocking code would spin on the new owner's pi_lock
      but never give up the wait_lock.  So the PI chain loop is guaranteed to
      fail the spin_trylock on the wait_lock, release the pi_lock, and
      try again.

      If you don't quite understand the above, that's OK. You don't have to,
      unless you really want to make a proof out of it ;)


Pending Owners and Lock stealing
--------------------------------

One of the flags in the owner field of the mutex structure is "Pending Owner".
What this means is that an owner was chosen by the process releasing the
mutex, but that owner has yet to wake up and actually take the mutex.

Why is this important?  Why can't we just give the mutex to another process
and be done with it?

The PI code is to help with real-time processes, and to let the highest
priority process run as long as possible with little latencies and delays.
If a high priority process owns a mutex that a lower priority process is
blocked on, when the mutex is released it would be given to the lower priority
process.  What if the higher priority process wants to take that mutex again.
The high priority process would fail to take that mutex that it just gave up
and it would need to boost the lower priority process to run with full
latency of that critical section (since the low priority process just entered
it).

There's no reason a high priority process that gives up a mutex should be
penalized if it tries to take that mutex again.  If the new owner of the
mutex has not woken up yet, there's no reason that the higher priority process
could not take that mutex away.

To solve this, we introduced Pending Ownership and Lock Stealing.  When a
new process is given a mutex that it was blocked on, it is only given
pending ownership.  This means that it's the new owner, unless a higher
priority process comes in and tries to grab that mutex.  If a higher priority
process does come along and wants that mutex, we let the higher priority
process "steal" the mutex from the pending owner (only if it is still pending)
and continue with the mutex.


Taking of a mutex (The walk through)
------------------------------------

OK, now let's take a look at the detailed walk through of what happens when
taking a mutex.

The first thing that is tried is the fast taking of the mutex.  This is
done when we have CMPXCHG enabled (otherwise the fast taking automatically
fails).  Only when the owner field of the mutex is NULL can the lock be
taken with the CMPXCHG and nothing else needs to be done.

If there is contention on the lock, whether it is owned or pending owner
we go about the slow path (rt_mutex_slowlock).

The slow path function is where the task's waiter structure is created on
the stack.  This is because the waiter structure is only needed for the
scope of this function.  The waiter structure holds the nodes to store
the task on the wait_list of the mutex, and if need be, the pi_list of
the owner.

The wait_lock of the mutex is taken since the slow path of unlocking the
mutex also takes this lock.

We then call try_to_take_rt_mutex.  This is where the architecture that
does not implement CMPXCHG would always grab the lock (if there's no
contention).

try_to_take_rt_mutex is used every time the task tries to grab a mutex in the
slow path.  The first thing that is done here is an atomic setting of
the "Has Waiters" flag of the mutex's owner field.  Yes, this could really
be false, because if the mutex has no owner, there are no waiters and
the current task also won't have any waiters.  But we don't have the lock
yet, so we assume we are going to be a waiter.  The reason for this is to
play nice for those architectures that do have CMPXCHG.  By setting this flag
now, the owner of the mutex can't release the mutex without going into the
slow unlock path, and it would then need to grab the wait_lock, which this
code currently holds.  So setting the "Has Waiters" flag forces the owner
to synchronize with this code.

Now that we know that we can't have any races with the owner releasing the
mutex, we check to see if we can take the ownership.  This is done if the
mutex doesn't have a owner, or if we can steal the mutex from a pending
owner.  Let's look at the situations we have here.

  1) Has owner that is pending
  ----------------------------

  The mutex has a owner, but it hasn't woken up and the mutex flag
  "Pending Owner" is set.  The first check is to see if the owner isn't the
  current task.  This is because this function is also used for the pending
  owner to grab the mutex.  When a pending owner wakes up, it checks to see
  if it can take the mutex, and this is done if the owner is already set to
  itself.  If so, we succeed and leave the function, clearing the "Pending
  Owner" bit.

  If the pending owner is not current, we check to see if the current priority is
  higher than the pending owner.  If not, we fail the function and return.

  There's also something special about a pending owner.  That is a pending owner
  is never blocked on a mutex.  So there is no PI chain to worry about.  It also
  means that if the mutex doesn't have any waiters, there's no accounting needed
  to update the pending owner's pi_list, since we only worry about processes
  blocked on the current mutex.

  If there are waiters on this mutex, and we just stole the ownership, we need
  to take the top waiter, remove it from the pi_list of the pending owner, and
  add it to the current pi_list.  Note that at this moment, the pending owner
  is no longer on the list of waiters.  This is fine, since the pending owner
  would add itself back when it realizes that it had the ownership stolen
  from itself.  When the pending owner tries to grab the mutex, it will fail
  in try_to_take_rt_mutex if the owner field points to another process.

  2) No owner
  -----------

  If there is no owner (or we successfully stole the lock), we set the owner
  of the mutex to current, and set the flag of "Has Waiters" if the current
  mutex actually has waiters, or we clear the flag if it doesn't.  See, it was
  OK that we set that flag early, since now it is cleared.

  3) Failed to grab ownership
  ---------------------------

  The most interesting case is when we fail to take ownership. This means that
  there exists an owner, or there's a pending owner with equal or higher
  priority than the current task.

We'll continue on the failed case.

If the mutex has a timeout, we set up a timer to go off to break us out
of this mutex if we failed to get it after a specified amount of time.

Now we enter a loop that will continue to try to take ownership of the mutex, or
fail from a timeout or signal.

Once again we try to take the mutex.  This will usually fail the first time
in the loop, since it had just failed to get the mutex.  But the second time
in the loop, this would likely succeed, since the task would likely be
the pending owner.

If the mutex is TASK_INTERRUPTIBLE a check for signals and timeout is done
here.

The waiter structure has a "task" field that points to the task that is blocked
on the mutex.  This field can be NULL the first time it goes through the loop
or if the task is a pending owner and had its mutex stolen.  If the "task"
field is NULL then we need to set up the accounting for it.

Task blocks on mutex
--------------------

The accounting of a mutex and process is done with the waiter structure of
the process.  The "task" field is set to the process, and the "lock" field
to the mutex.  The plist nodes are initialized to the processes current
priority.

Since the wait_lock was taken at the entry of the slow lock, we can safely
add the waiter to the wait_list.  If the current process is the highest
priority process currently waiting on this mutex, then we remove the
previous top waiter process (if it exists) from the pi_list of the owner,
and add the current process to that list.  Since the pi_list of the owner
has changed, we call rt_mutex_adjust_prio on the owner to see if the owner
should adjust its priority accordingly.

If the owner is also blocked on a lock, and had its pi_list changed
(or deadlock checking is on), we unlock the wait_lock of the mutex and go ahead
and run rt_mutex_adjust_prio_chain on the owner, as described earlier.

Now all locks are released, and if the current process is still blocked on a
mutex (waiter "task" field is not NULL), then we go to sleep (call schedule).

Waking up in the loop
---------------------

The schedule can then wake up for a few reasons.
  1) we were given pending ownership of the mutex.
  2) we received a signal and was TASK_INTERRUPTIBLE
  3) we had a timeout and was TASK_INTERRUPTIBLE

In any of these cases, we continue the loop and once again try to grab the
ownership of the mutex.  If we succeed, we exit the loop, otherwise we continue
and on signal and timeout, will exit the loop, or if we had the mutex stolen
we just simply add ourselves back on the lists and go back to sleep.

Note: For various reasons, because of timeout and signals, the steal mutex
      algorithm needs to be careful. This is because the current process is
      still on the wait_list. And because of dynamic changing of priorities,
      especially on SCHED_OTHER tasks, the current process can be the
      highest priority task on the wait_list.

Failed to get mutex on Timeout or Signal
----------------------------------------

If a timeout or signal occurred, the waiter's "task" field would not be
NULL and the task needs to be taken off the wait_list of the mutex and perhaps
pi_list of the owner.  If this process was a high priority process, then
the rt_mutex_adjust_prio_chain needs to be executed again on the owner,
but this time it will be lowering the priorities.


Unlocking the Mutex
-------------------

The unlocking of a mutex also has a fast path for those architectures with
CMPXCHG.  Since the taking of a mutex on contention always sets the
"Has Waiters" flag of the mutex's owner, we use this to know if we need to
take the slow path when unlocking the mutex.  If the mutex doesn't have any
waiters, the owner field of the mutex would equal the current process and
the mutex can be unlocked by just replacing the owner field with NULL.

If the owner field has the "Has Waiters" bit set (or CMPXCHG is not available),
the slow unlock path is taken.

The first thing done in the slow unlock path is to take the wait_lock of the
mutex.  This synchronizes the locking and unlocking of the mutex.

A check is made to see if the mutex has waiters or not.  On architectures that
do not have CMPXCHG, this is the location that the owner of the mutex will
determine if a waiter needs to be awoken or not.  On architectures that
do have CMPXCHG, that check is done in the fast path, but it is still needed
in the slow path too.  If a waiter of a mutex woke up because of a signal
or timeout between the time the owner failed the fast path CMPXCHG check and
the grabbing of the wait_lock, the mutex may not have any waiters, thus the
owner still needs to make this check. If there are no waiters then the mutex
owner field is set to NULL, the wait_lock is released and nothing more is
needed.

If there are waiters, then we need to wake one up and give that waiter
pending ownership.

On the wake up code, the pi_lock of the current owner is taken.  The top
waiter of the lock is found and removed from the wait_list of the mutex
as well as the pi_list of the current owner.  The task field of the new
pending owner's waiter structure is set to NULL, and the owner field of the
mutex is set to the new owner with the "Pending Owner" bit set, as well
as the "Has Waiters" bit if there still are other processes blocked on the
mutex.

The pi_lock of the previous owner is released, and the new pending owner's
pi_lock is taken.  Remember that this is the trick to prevent the race
condition in rt_mutex_adjust_prio_chain from adding itself as a waiter
on the mutex.

We now clear the "pi_blocked_on" field of the new pending owner, and if
the mutex still has waiters pending, we add the new top waiter to the pi_list
of the pending owner.

Finally we unlock the pi_lock of the pending owner and wake it up.


Contact
-------

For updates on this document, please email Steven Rostedt <rostedt@goodmis.org>


Credits
-------

Author:  Steven Rostedt <rostedt@goodmis.org>

Reviewers:  Ingo Molnar, Thomas Gleixner, Thomas Duetsch, and Randy Dunlap

Updates
-------

This document was originally written for 2.6.17-rc3-mm1
RT-mutex subsystem with PI support
----------------------------------

RT-mutexes with priority inheritance are used to support PI-futexes,
which enable pthread_mutex_t priority inheritance attributes
(PTHREAD_PRIO_INHERIT). [See Documentation/pi-futex.txt for more details
about PI-futexes.]

This technology was developed in the -rt tree and streamlined for
pthread_mutex support.

Basic principles:
-----------------

RT-mutexes extend the semantics of simple mutexes by the priority
inheritance protocol.

A low priority owner of a rt-mutex inherits the priority of a higher
priority waiter until the rt-mutex is released. If the temporarily
boosted owner blocks on a rt-mutex itself it propagates the priority
boosting to the owner of the other rt_mutex it gets blocked on. The
priority boosting is immediately removed once the rt_mutex has been
unlocked.

This approach allows us to shorten the block of high-prio tasks on
mutexes which protect shared resources. Priority inheritance is not a
magic bullet for poorly designed applications, but it allows
well-designed applications to use userspace locks in critical parts of
an high priority thread, without losing determinism.

The enqueueing of the waiters into the rtmutex waiter list is done in
priority order. For same priorities FIFO order is chosen. For each
rtmutex, only the top priority waiter is enqueued into the owner's
priority waiters list. This list too queues in priority order. Whenever
the top priority waiter of a task changes (for example it timed out or
got a signal), the priority of the owner task is readjusted. [The
priority enqueueing is handled by "plists", see include/linux/plist.h
for more details.]

RT-mutexes are optimized for fastpath operations and have no internal
locking overhead when locking an uncontended mutex or unlocking a mutex
without waiters. The optimized fastpath operations require cmpxchg
support. [If that is not available then the rt-mutex internal spinlock
is used]

The state of the rt-mutex is tracked via the owner field of the rt-mutex
structure:

rt_mutex->owner holds the task_struct pointer of the owner. Bit 0 and 1
are used to keep track of the "owner is pending" and "rtmutex has
waiters" state.

 owner		bit1	bit0
 NULL		0	0	mutex is free (fast acquire possible)
 NULL		0	1	invalid state
 NULL		1	0	Transitional state*
 NULL		1	1	invalid state
 taskpointer	0	0	mutex is held (fast release possible)
 taskpointer	0	1	task is pending owner
 taskpointer	1	0	mutex is held and has waiters
 taskpointer	1	1	task is pending owner and mutex has waiters

Pending-ownership handling is a performance optimization:
pending-ownership is assigned to the first (highest priority) waiter of
the mutex, when the mutex is released. The thread is woken up and once
it starts executing it can acquire the mutex. Until the mutex is taken
by it (bit 0 is cleared) a competing higher priority thread can "steal"
the mutex which puts the woken up thread back on the waiters list.

The pending-ownership optimization is especially important for the
uninterrupted workflow of high-prio tasks which repeatedly
takes/releases locks that have lower-prio waiters. Without this
optimization the higher-prio thread would ping-pong to the lower-prio
task [because at unlock time we always assign a new owner].

(*) The "mutex has waiters" bit gets set to take the lock. If the lock
doesn't already have an owner, this bit is quickly cleared if there are
no waiters.  So this is a transitional state to synchronize with looking
at the owner field of the mutex and the mutex owner releasing the lock.
Linux 2.4.2 Secure Attention Key (SAK) handling
18 March 2001, Andrew Morton

An operating system's Secure Attention Key is a security tool which is
provided as protection against trojan password capturing programs.  It
is an undefeatable way of killing all programs which could be
masquerading as login applications.  Users need to be taught to enter
this key sequence before they log in to the system.

From the PC keyboard, Linux has two similar but different ways of
providing SAK.  One is the ALT-SYSRQ-K sequence.  You shouldn't use
this sequence.  It is only available if the kernel was compiled with
sysrq support.

The proper way of generating a SAK is to define the key sequence using
`loadkeys'.  This will work whether or not sysrq support is compiled
into the kernel.

SAK works correctly when the keyboard is in raw mode.  This means that
once defined, SAK will kill a running X server.  If the system is in
run level 5, the X server will restart.  This is what you want to
happen.

What key sequence should you use? Well, CTRL-ALT-DEL is used to reboot
the machine.  CTRL-ALT-BACKSPACE is magical to the X server.  We'll
choose CTRL-ALT-PAUSE.

In your rc.sysinit (or rc.local) file, add the command

	echo "control alt keycode 101 = SAK" | /bin/loadkeys

And that's it!  Only the superuser may reprogram the SAK key.


NOTES
=====

1: Linux SAK is said to be not a "true SAK" as is required by
   systems which implement C2 level security.  This author does not
   know why.


2: On the PC keyboard, SAK kills all applications which have
   /dev/console opened.

   Unfortunately this includes a number of things which you don't
   actually want killed.  This is because these applications are
   incorrectly holding /dev/console open.  Be sure to complain to your
   Linux distributor about this!

   You can identify processes which will be killed by SAK with the
   command

	# ls -l /proc/[0-9]*/fd/* | grep console
	l-wx------    1 root     root           64 Mar 18 00:46 /proc/579/fd/0 -> /dev/console

   Then:

	# ps aux|grep 579
	root       579  0.0  0.1  1088  436 ?        S    00:43   0:00 gpm -t ps/2

   So `gpm' will be killed by SAK.  This is a bug in gpm.  It should
   be closing standard input.  You can work around this by finding the
   initscript which launches gpm and changing it thusly:

   Old:

	daemon gpm

   New:

	daemon gpm < /dev/null

   Vixie cron also seems to have this problem, and needs the same treatment.

   Also, one prominent Linux distribution has the following three
   lines in its rc.sysinit and rc scripts:

	exec 3<&0
	exec 4>&1
	exec 5>&2

   These commands cause *all* daemons which are launched by the
   initscripts to have file descriptors 3, 4 and 5 attached to
   /dev/console.  So SAK kills them all.  A workaround is to simply
   delete these lines, but this may cause system management
   applications to malfunction - test everything well.

                       Linux Serial Console

To use a serial port as console you need to compile the support into your
kernel - by default it is not compiled in. For PC style serial ports
it's the config option next to "Standard/generic (dumb) serial support".
You must compile serial support into the kernel and not as a module.

It is possible to specify multiple devices for console output. You can
define a new kernel command line option to select which device(s) to
use for console output.

The format of this option is:

	console=device,options

	device:		tty0 for the foreground virtual console
			ttyX for any other virtual console
			ttySx for a serial port
			lp0 for the first parallel port
			ttyUSB0 for the first USB serial device

	options:	depend on the driver. For the serial port this
			defines the baudrate/parity/bits/flow control of
			the port, in the format BBBBPNF, where BBBB is the
			speed, P is parity (n/o/e), N is number of bits,
			and F is flow control ('r' for RTS). Default is
			9600n8. The maximum baudrate is 115200.

You can specify multiple console= options on the kernel command line.
Output will appear on all of them. The last device will be used when
you open /dev/console. So, for example:

	console=ttyS1,9600 console=tty0

defines that opening /dev/console will get you the current foreground
virtual console, and kernel messages will appear on both the VGA
console and the 2nd serial port (ttyS1 or COM2) at 9600 baud.

Note that you can only define one console per device type (serial, video).

If no console device is specified, the first device found capable of
acting as a system console will be used. At this time, the system
first looks for a VGA card and then for a serial port. So if you don't
have a VGA card in your system the first serial port will automatically
become the console.

You will need to create a new device to use /dev/console. The official
/dev/console is now character device 5,1.

(You can also use a network device as a console.  See
Documentation/networking/netconsole.txt for information on that.)

Here's an example that will use /dev/ttyS1 (COM2) as the console.
Replace the sample values as needed.

1. Create /dev/console (real console) and /dev/tty0 (master virtual
   console):

   cd /dev
   rm -f console tty0
   mknod -m 622 console c 5 1
   mknod -m 622 tty0 c 4 0

2. LILO can also take input from a serial device. This is a very
   useful option. To tell LILO to use the serial port:
   In lilo.conf (global section): 

   serial  = 1,9600n8 (ttyS1, 9600 bd, no parity, 8 bits)

3. Adjust to kernel flags for the new kernel,
   again in lilo.conf (kernel section)

   append = "console=ttyS1,9600" 

4. Make sure a getty runs on the serial port so that you can login to
   it once the system is done booting. This is done by adding a line
   like this to /etc/inittab (exact syntax depends on your getty):

   S1:23:respawn:/sbin/getty -L ttyS1 9600 vt100

5. Init and /etc/ioctl.save

   Sysvinit remembers its stty settings in a file in /etc, called
   `/etc/ioctl.save'. REMOVE THIS FILE before using the serial
   console for the first time, because otherwise init will probably
   set the baudrate to 38400 (baudrate of the virtual console).

6. /dev/console and X
   Programs that want to do something with the virtual console usually
   open /dev/console. If you have created the new /dev/console device,
   and your console is NOT the virtual console some programs will fail.
   Those are programs that want to access the VT interface, and use
   /dev/console instead of /dev/tty0. Some of those programs are:

   Xfree86, svgalib, gpm, SVGATextMode

   It should be fixed in modern versions of these programs though.

   Note that if you boot without a console= option (or with
   console=/dev/tty0), /dev/console is the same as /dev/tty0. In that
   case everything will still work.

7. Thanks

   Thanks to Geert Uytterhoeven <geert@linux-m68k.org>
   for porting the patches from 2.1.4x to 2.1.6x for taking care of
   the integration of these patches into m68k, ppc and alpha.

Miquel van Smoorenburg <miquels@cistron.nl>, 11-Jun-2000
The SGI IOC4 PCI device is a bit of a strange beast, so some notes on
it are in order.

First, even though the IOC4 performs multiple functions, such as an
IDE controller, a serial controller, a PS/2 keyboard/mouse controller,
and an external interrupt mechanism, it's not implemented as a
multifunction device.  The consequence of this from a software
standpoint is that all these functions share a single IRQ, and
they can't all register to own the same PCI device ID.  To make
matters a bit worse, some of the register blocks (and even registers
themselves) present in IOC4 are mixed-purpose between these several
functions, meaning that there's no clear "owning" device driver.

The solution is to organize the IOC4 driver into several independent
drivers, "ioc4", "sgiioc4", and "ioc4_serial".  Note that there is no
PS/2 controller driver as this functionality has never been wired up
on a shipping IO card.

ioc4
====
This is the core (or shim) driver for IOC4.  It is responsible for
initializing the basic functionality of the chip, and allocating
the PCI resources that are shared between the IOC4 functions.

This driver also provides registration functions that the other
IOC4 drivers can call to make their presence known.  Each driver
needs to provide a probe and remove function, which are invoked
by the core driver at appropriate times.  The interface of these
IOC4 function probe and remove operations isn't precisely the same
as PCI device probe and remove operations, but is logically the
same operation.

sgiioc4
=======
This is the IDE driver for IOC4.  Its name isn't very descriptive
simply for historical reasons (it used to be the only IOC4 driver
component).  There's not much to say about it other than it hooks
up to the ioc4 driver via the appropriate registration, probe, and
remove functions.

ioc4_serial
===========
This is the serial driver for IOC4.  There's not much to say about it
other than it hooks up to the ioc4 driver via the appropriate registration,
probe, and remove functions.
			SM501 Driver
			============

Copyright 2006, 2007 Simtec Electronics

The Silicon Motion SM501 multimedia companion chip is a multifunction device
which may provide numerous interfaces including USB host controller USB gadget,
asynchronous serial ports, audio functions, and a dual display video interface.
The device may be connected by PCI or local bus with varying functions enabled.

Core
----

The core driver in drivers/mfd provides common services for the
drivers which manage the specific hardware blocks. These services
include locking for common registers, clock control and resource
management.

The core registers drivers for both PCI and generic bus based
chips via the platform device and driver system.

On detection of a device, the core initialises the chip (which may
be specified by the platform data) and then exports the selected
peripheral set as platform devices for the specific drivers.

The core re-uses the platform device system as the platform device
system provides enough features to support the drivers without the
need to create a new bus-type and the associated code to go with it.


Resources
---------

Each peripheral has a view of the device which is implicitly narrowed to
the specific set of resources that peripheral requires in order to
function correctly.

The centralised memory allocation allows the driver to ensure that the
maximum possible resource allocation can be made to the video subsystem
as this is by-far the most resource-sensitive of the on-chip functions.

The primary issue with memory allocation is that of moving the video
buffers once a display mode is chosen. Indeed when a video mode change
occurs the memory footprint of the video subsystem changes.

Since video memory is difficult to move without changing the display
(unless sufficient contiguous memory can be provided for the old and new
modes simultaneously) the video driver fully utilises the memory area
given to it by aligning fb0 to the start of the area and fb1 to the end
of it. Any memory left over in the middle is used for the acceleration
functions, which are transient and thus their location is less critical
as it can be moved.


Configuration
-------------

The platform device driver uses a set of platform data to pass
configurations through to the core and the subsidiary drivers
so that there can be support for more than one system carrying
an SM501 built into a single kernel image.

The PCI driver assumes that the PCI card behaves as per the Silicon
Motion reference design.

There is an errata (AB-5) affecting the selection of the
of the M1XCLK and M1CLK frequencies. These two clocks
must be sourced from the same PLL, although they can then
be divided down individually. If this is not set, then SM501 may
lock and hang the whole system. The driver will refuse to
attach if the PLL selection is different.
What is smsc-ece1099?
----------------------

The ECE1099 is a 40-Pin 3.3V Keyboard Scan Expansion
or GPIO Expansion device. The device supports a keyboard
scan matrix of 23x8. The device is connected to a Master
via the SMSC BC-Link interface or via the SMBus.
Keypad scan Input(KSI) and Keypad Scan Output(KSO) signals
are multiplexed with GPIOs.

Interrupt generation
--------------------

Interrupts can be generated by an edge detection on a GPIO
pin or an edge detection on one of the bus interface pins.
Interrupts can also be detected on the keyboard scan interface.
The bus interrupt pin (BC_INT# or SMBUS_INT#) is asserted if
any bit in one of the Interrupt Status registers is 1 and
the corresponding Interrupt Mask bit is also 1.

In order for software to determine which device is the source
of an interrupt, it should first read the Group Interrupt Status Register
to determine which Status register group is a source for the interrupt.
Software should read both the Status register and the associated Mask register,
then AND the two values together. Bits that are 1 in the result of the AND
are active interrupts. Software clears an interrupt by writing a 1 to the
corresponding bit in the Status register.

Communication Protocol
----------------------

- SMbus slave Interface
	The host processor communicates with the ECE1099 device
	through a series of read/write registers via the SMBus
	interface. SMBus is a serial communication protocol between
	a computer host and its peripheral devices. The SMBus data
	rate is 10KHz minimum to 400 KHz maximum

- Slave Bus Interface
	The ECE1099 device SMBus implementation is a subset of the
	SMBus interface to the host. The device is a slave-only SMBus device.
	The implementation in the device is a subset of SMBus since it
	only supports four protocols.

	The Write Byte, Read Byte, Send Byte, and Receive Byte protocols are the
	only valid SMBus protocols for the device.

- BC-LinkTM Interface
	The BC-Link is a proprietary bus that allows communication
	between a Master device and a Companion device. The Master
	device uses this serial bus to read and write registers
	located on the Companion device. The bus comprises three signals,
	BC_CLK, BC_DAT and BC_INT#. The Master device always provides the
	clock, BC_CLK, and the Companion device is the source for an
	independent asynchronous interrupt signal, BC_INT#. The ECE1099
	supports BC-Link speeds up to 24MHz.
Copyright 2004 Linus Torvalds
Copyright 2004 Pavel Machek <pavel@ucw.cz>
Copyright 2006 Bob Copeland <me@bobcopeland.com>

Using sparse for typechecking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

"__bitwise" is a type attribute, so you have to do something like this:

        typedef int __bitwise pm_request_t;

        enum pm_request {
                PM_SUSPEND = (__force pm_request_t) 1,
                PM_RESUME = (__force pm_request_t) 2
        };

which makes PM_SUSPEND and PM_RESUME "bitwise" integers (the "__force" is
there because sparse will complain about casting to/from a bitwise type,
but in this case we really _do_ want to force the conversion). And because
the enum values are all the same type, now "enum pm_request" will be that
type too.

And with gcc, all the __bitwise/__force stuff goes away, and it all ends
up looking just like integers to gcc.

Quite frankly, you don't need the enum there. The above all really just
boils down to one special "int __bitwise" type.

So the simpler way is to just do

        typedef int __bitwise pm_request_t;

        #define PM_SUSPEND ((__force pm_request_t) 1)
        #define PM_RESUME ((__force pm_request_t) 2)

and you now have all the infrastructure needed for strict typechecking.

One small note: the constant integer "0" is special. You can use a
constant zero as a bitwise integer type without sparse ever complaining.
This is because "bitwise" (as the name implies) was designed for making
sure that bitwise types don't get mixed up (little-endian vs big-endian
vs cpu-endian vs whatever), and there the constant "0" really _is_
special.

__bitwise__ - to be used for relatively compact stuff (gfp_t, etc.) that
is mostly warning-free and is supposed to stay that way.  Warnings will
be generated without __CHECK_ENDIAN__.

__bitwise - noisy stuff; in particular, __le*/__be* are that.  We really
don't want to drown in noise unless we'd explicitly asked for it.

Using sparse for lock checking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following macros are undefined for gcc and defined during a sparse
run to use the "context" tracking feature of sparse, applied to
locking.  These annotations tell sparse when a lock is held, with
regard to the annotated function's entry and exit.

__must_hold - The specified lock is held on function entry and exit.

__acquires - The specified lock is held on function exit, but not entry.

__releases - The specified lock is held on function entry, but not exit.

If the function enters and exits without the lock held, acquiring and
releasing the lock inside the function in a balanced way, no
annotation is needed.  The tree annotations above are for cases where
sparse would otherwise report a context imbalance.

Getting sparse
~~~~~~~~~~~~~~

You can get latest released versions from the Sparse homepage at
https://sparse.wiki.kernel.org/index.php/Main_Page

Alternatively, you can get snapshots of the latest development version
of sparse using git to clone..

        git://git.kernel.org/pub/scm/devel/sparse/sparse.git

DaveJ has hourly generated tarballs of the git tree available at..

        http://www.codemonkey.org.uk/projects/git-snapshots/sparse/


Once you have it, just do

        make
        make install

as a regular user, and it will install sparse in your ~/bin directory.

Using sparse
~~~~~~~~~~~~

Do a kernel make with "make C=1" to run sparse on all the C files that get
recompiled, or use "make C=2" to run sparse on the files whether they need to
be recompiled or not.  The latter is a fast way to check the whole tree if you
have already built it.

The optional make variable CF can be used to pass arguments to sparse.  The
build system passes -Wbitwise to sparse automatically.  To perform endianness
checks, you may define __CHECK_ENDIAN__:

        make C=2 CF="-D__CHECK_ENDIAN__"

These checks are disabled by default as they generate a host of warnings.
Lesson 1: Spin locks

The most basic primitive for locking is spinlock.

static DEFINE_SPINLOCK(xxx_lock);

	unsigned long flags;

	spin_lock_irqsave(&xxx_lock, flags);
	... critical section here ..
	spin_unlock_irqrestore(&xxx_lock, flags);

The above is always safe. It will disable interrupts _locally_, but the
spinlock itself will guarantee the global lock, so it will guarantee that
there is only one thread-of-control within the region(s) protected by that
lock. This works well even under UP also, so the code does _not_ need to
worry about UP vs SMP issues: the spinlocks work correctly under both.

   NOTE! Implications of spin_locks for memory are further described in:

     Documentation/memory-barriers.txt
       (5) LOCK operations.
       (6) UNLOCK operations.

The above is usually pretty simple (you usually need and want only one
spinlock for most things - using more than one spinlock can make things a
lot more complex and even slower and is usually worth it only for
sequences that you _know_ need to be split up: avoid it at all cost if you
aren't sure).

This is really the only really hard part about spinlocks: once you start
using spinlocks they tend to expand to areas you might not have noticed
before, because you have to make sure the spinlocks correctly protect the
shared data structures _everywhere_ they are used. The spinlocks are most
easily added to places that are completely independent of other code (for
example, internal driver data structures that nobody else ever touches).

   NOTE! The spin-lock is safe only when you _also_ use the lock itself
   to do locking across CPU's, which implies that EVERYTHING that
   touches a shared variable has to agree about the spinlock they want
   to use.

----

Lesson 2: reader-writer spinlocks.

If your data accesses have a very natural pattern where you usually tend
to mostly read from the shared variables, the reader-writer locks
(rw_lock) versions of the spinlocks are sometimes useful. They allow multiple
readers to be in the same critical region at once, but if somebody wants
to change the variables it has to get an exclusive write lock.

   NOTE! reader-writer locks require more atomic memory operations than
   simple spinlocks.  Unless the reader critical section is long, you
   are better off just using spinlocks.

The routines look the same as above:

   rwlock_t xxx_lock = __RW_LOCK_UNLOCKED(xxx_lock);

	unsigned long flags;

	read_lock_irqsave(&xxx_lock, flags);
	.. critical section that only reads the info ...
	read_unlock_irqrestore(&xxx_lock, flags);

	write_lock_irqsave(&xxx_lock, flags);
	.. read and write exclusive access to the info ...
	write_unlock_irqrestore(&xxx_lock, flags);

The above kind of lock may be useful for complex data structures like
linked lists, especially searching for entries without changing the list
itself.  The read lock allows many concurrent readers.  Anything that
_changes_ the list will have to get the write lock.

   NOTE! RCU is better for list traversal, but requires careful
   attention to design detail (see Documentation/RCU/listRCU.txt).

Also, you cannot "upgrade" a read-lock to a write-lock, so if you at _any_
time need to do any changes (even if you don't do it every time), you have
to get the write-lock at the very beginning.

   NOTE! We are working hard to remove reader-writer spinlocks in most
   cases, so please don't add a new one without consensus.  (Instead, see
   Documentation/RCU/rcu.txt for complete information.)

----

Lesson 3: spinlocks revisited.

The single spin-lock primitives above are by no means the only ones. They
are the most safe ones, and the ones that work under all circumstances,
but partly _because_ they are safe they are also fairly slow. They are slower
than they'd need to be, because they do have to disable interrupts
(which is just a single instruction on a x86, but it's an expensive one -
and on other architectures it can be worse).

If you have a case where you have to protect a data structure across
several CPU's and you want to use spinlocks you can potentially use
cheaper versions of the spinlocks. IFF you know that the spinlocks are
never used in interrupt handlers, you can use the non-irq versions:

	spin_lock(&lock);
	...
	spin_unlock(&lock);

(and the equivalent read-write versions too, of course). The spinlock will
guarantee the same kind of exclusive access, and it will be much faster. 
This is useful if you know that the data in question is only ever
manipulated from a "process context", ie no interrupts involved. 

The reasons you mustn't use these versions if you have interrupts that
play with the spinlock is that you can get deadlocks:

	spin_lock(&lock);
	...
		<- interrupt comes in:
			spin_lock(&lock);

where an interrupt tries to lock an already locked variable. This is ok if
the other interrupt happens on another CPU, but it is _not_ ok if the
interrupt happens on the same CPU that already holds the lock, because the
lock will obviously never be released (because the interrupt is waiting
for the lock, and the lock-holder is interrupted by the interrupt and will
not continue until the interrupt has been processed). 

(This is also the reason why the irq-versions of the spinlocks only need
to disable the _local_ interrupts - it's ok to use spinlocks in interrupts
on other CPU's, because an interrupt on another CPU doesn't interrupt the
CPU that holds the lock, so the lock-holder can continue and eventually
releases the lock). 

Note that you can be clever with read-write locks and interrupts. For
example, if you know that the interrupt only ever gets a read-lock, then
you can use a non-irq version of read locks everywhere - because they
don't block on each other (and thus there is no dead-lock wrt interrupts. 
But when you do the write-lock, you have to use the irq-safe version. 

For an example of being clever with rw-locks, see the "waitqueue_lock" 
handling in kernel/sched/core.c - nothing ever _changes_ a wait-queue from
within an interrupt, they only read the queue in order to know whom to
wake up. So read-locks are safe (which is good: they are very common
indeed), while write-locks need to protect themselves against interrupts.

		Linus

----

Reference information:

For dynamic initialization, use spin_lock_init() or rwlock_init() as
appropriate:

   spinlock_t xxx_lock;
   rwlock_t xxx_rw_lock;

   static int __init xxx_init(void)
   {
	spin_lock_init(&xxx_lock);
	rwlock_init(&xxx_rw_lock);
	...
   }

   module_init(xxx_init);

For static initialization, use DEFINE_SPINLOCK() / DEFINE_RWLOCK() or
__SPIN_LOCK_UNLOCKED() / __RW_LOCK_UNLOCKED() as appropriate.
The Linux Kernel Driver Interface
(all of your questions answered and then some)

Greg Kroah-Hartman <greg@kroah.com>

This is being written to try to explain why Linux does not have a binary
kernel interface, nor does it have a stable kernel interface.  Please
realize that this article describes the _in kernel_ interfaces, not the
kernel to userspace interfaces.  The kernel to userspace interface is
the one that application programs use, the syscall interface.  That
interface is _very_ stable over time, and will not break.  I have old
programs that were built on a pre 0.9something kernel that still work
just fine on the latest 2.6 kernel release.  That interface is the one
that users and application programmers can count on being stable.


Executive Summary
-----------------
You think you want a stable kernel interface, but you really do not, and
you don't even know it.  What you want is a stable running driver, and
you get that only if your driver is in the main kernel tree.  You also
get lots of other good benefits if your driver is in the main kernel
tree, all of which has made Linux into such a strong, stable, and mature
operating system which is the reason you are using it in the first
place.


Intro
-----

It's only the odd person who wants to write a kernel driver that needs
to worry about the in-kernel interfaces changing.  For the majority of
the world, they neither see this interface, nor do they care about it at
all.

First off, I'm not going to address _any_ legal issues about closed
source, hidden source, binary blobs, source wrappers, or any other term
that describes kernel drivers that do not have their source code
released under the GPL.  Please consult a lawyer if you have any legal
questions, I'm a programmer and hence, I'm just going to be describing
the technical issues here (not to make light of the legal issues, they
are real, and you do need to be aware of them at all times.)

So, there are two main topics here, binary kernel interfaces and stable
kernel source interfaces.  They both depend on each other, but we will
discuss the binary stuff first to get it out of the way.


Binary Kernel Interface
-----------------------
Assuming that we had a stable kernel source interface for the kernel, a
binary interface would naturally happen too, right?  Wrong.  Please
consider the following facts about the Linux kernel:
  - Depending on the version of the C compiler you use, different kernel
    data structures will contain different alignment of structures, and
    possibly include different functions in different ways (putting
    functions inline or not.)  The individual function organization
    isn't that important, but the different data structure padding is
    very important.
  - Depending on what kernel build options you select, a wide range of
    different things can be assumed by the kernel:
      - different structures can contain different fields
      - Some functions may not be implemented at all, (i.e. some locks
	compile away to nothing for non-SMP builds.)
      - Memory within the kernel can be aligned in different ways,
	depending on the build options.
  - Linux runs on a wide range of different processor architectures.
    There is no way that binary drivers from one architecture will run
    on another architecture properly.

Now a number of these issues can be addressed by simply compiling your
module for the exact specific kernel configuration, using the same exact
C compiler that the kernel was built with.  This is sufficient if you
want to provide a module for a specific release version of a specific
Linux distribution.  But multiply that single build by the number of
different Linux distributions and the number of different supported
releases of the Linux distribution and you quickly have a nightmare of
different build options on different releases.  Also realize that each
Linux distribution release contains a number of different kernels, all
tuned to different hardware types (different processor types and
different options), so for even a single release you will need to create
multiple versions of your module.

Trust me, you will go insane over time if you try to support this kind
of release, I learned this the hard way a long time ago...


Stable Kernel Source Interfaces
-------------------------------

This is a much more "volatile" topic if you talk to people who try to
keep a Linux kernel driver that is not in the main kernel tree up to
date over time.

Linux kernel development is continuous and at a rapid pace, never
stopping to slow down.  As such, the kernel developers find bugs in
current interfaces, or figure out a better way to do things.  If they do
that, they then fix the current interfaces to work better.  When they do
so, function names may change, structures may grow or shrink, and
function parameters may be reworked.  If this happens, all of the
instances of where this interface is used within the kernel are fixed up
at the same time, ensuring that everything continues to work properly.

As a specific examples of this, the in-kernel USB interfaces have
undergone at least three different reworks over the lifetime of this
subsystem.  These reworks were done to address a number of different
issues:
  - A change from a synchronous model of data streams to an asynchronous
    one.  This reduced the complexity of a number of drivers and
    increased the throughput of all USB drivers such that we are now
    running almost all USB devices at their maximum speed possible.
  - A change was made in the way data packets were allocated from the
    USB core by USB drivers so that all drivers now needed to provide
    more information to the USB core to fix a number of documented
    deadlocks.

This is in stark contrast to a number of closed source operating systems
which have had to maintain their older USB interfaces over time.  This
provides the ability for new developers to accidentally use the old
interfaces and do things in improper ways, causing the stability of the
operating system to suffer.

In both of these instances, all developers agreed that these were
important changes that needed to be made, and they were made, with
relatively little pain.  If Linux had to ensure that it will preserve a
stable source interface, a new interface would have been created, and
the older, broken one would have had to be maintained over time, leading
to extra work for the USB developers.  Since all Linux USB developers do
their work on their own time, asking programmers to do extra work for no
gain, for free, is not a possibility.

Security issues are also very important for Linux.  When a
security issue is found, it is fixed in a very short amount of time.  A
number of times this has caused internal kernel interfaces to be
reworked to prevent the security problem from occurring.  When this
happens, all drivers that use the interfaces were also fixed at the
same time, ensuring that the security problem was fixed and could not
come back at some future time accidentally.  If the internal interfaces
were not allowed to change, fixing this kind of security problem and
insuring that it could not happen again would not be possible.

Kernel interfaces are cleaned up over time.  If there is no one using a
current interface, it is deleted.  This ensures that the kernel remains
as small as possible, and that all potential interfaces are tested as
well as they can be (unused interfaces are pretty much impossible to
test for validity.)


What to do
----------

So, if you have a Linux kernel driver that is not in the main kernel
tree, what are you, a developer, supposed to do?  Releasing a binary
driver for every different kernel version for every distribution is a
nightmare, and trying to keep up with an ever changing kernel interface
is also a rough job.

Simple, get your kernel driver into the main kernel tree (remember we
are talking about GPL released drivers here, if your code doesn't fall
under this category, good luck, you are on your own here, you leech
<insert link to leech comment from Andrew and Linus here>.)  If your
driver is in the tree, and a kernel interface changes, it will be fixed
up by the person who did the kernel change in the first place.  This
ensures that your driver is always buildable, and works over time, with
very little effort on your part.

The very good side effects of having your driver in the main kernel tree
are:
  - The quality of the driver will rise as the maintenance costs (to the
    original developer) will decrease.
  - Other developers will add features to your driver.
  - Other people will find and fix bugs in your driver.
  - Other people will find tuning opportunities in your driver.
  - Other people will update the driver for you when external interface
    changes require it.
  - The driver automatically gets shipped in all Linux distributions
    without having to ask the distros to add it.
    
As Linux supports a larger number of different devices "out of the box"
than any other operating system, and it supports these devices on more
different processor architectures than any other operating system, this
proven type of development model must be doing something right :)



------

Thanks to Randy Dunlap, Andrew Morton, David Brownell, Hanna Linder,
Robert Love, and Nishanth Aravamudan for their review and comments on
early drafts of this paper.
Everything you ever wanted to know about Linux -stable releases.

Rules on what kind of patches are accepted, and which ones are not, into the
"-stable" tree:

 - It must be obviously correct and tested.
 - It cannot be bigger than 100 lines, with context.
 - It must fix only one thing.
 - It must fix a real bug that bothers people (not a, "This could be a
   problem..." type thing).
 - It must fix a problem that causes a build error (but not for things
   marked CONFIG_BROKEN), an oops, a hang, data corruption, a real
   security issue, or some "oh, that's not good" issue.  In short, something
   critical.
 - Serious issues as reported by a user of a distribution kernel may also
   be considered if they fix a notable performance or interactivity issue.
   As these fixes are not as obvious and have a higher risk of a subtle
   regression they should only be submitted by a distribution kernel
   maintainer and include an addendum linking to a bugzilla entry if it
   exists and additional information on the user-visible impact.
 - New device IDs and quirks are also accepted.
 - No "theoretical race condition" issues, unless an explanation of how the
   race can be exploited is also provided.
 - It cannot contain any "trivial" fixes in it (spelling changes,
   whitespace cleanups, etc).
 - It must follow the Documentation/SubmittingPatches rules.
 - It or an equivalent fix must already exist in Linus' tree (upstream).


Procedure for submitting patches to the -stable tree:

 - Send the patch, after verifying that it follows the above rules, to
   stable@vger.kernel.org.  You must note the upstream commit ID in the
   changelog of your submission, as well as the kernel version you wish
   it to be applied to.
 - To have the patch automatically included in the stable tree, add the tag
     Cc: stable@vger.kernel.org
   in the sign-off area. Once the patch is merged it will be applied to
   the stable tree without anything else needing to be done by the author
   or subsystem maintainer.
 - If the patch requires other patches as prerequisites which can be
   cherry-picked, then this can be specified in the following format in
   the sign-off area:

     Cc: <stable@vger.kernel.org> # 3.3.x: a1f84a3: sched: Check for idle
     Cc: <stable@vger.kernel.org> # 3.3.x: 1b9508f: sched: Rate-limit newidle
     Cc: <stable@vger.kernel.org> # 3.3.x: fd21073: sched: Fix affinity logic
     Cc: <stable@vger.kernel.org> # 3.3.x
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

   The tag sequence has the meaning of:
     git cherry-pick a1f84a3
     git cherry-pick 1b9508f
     git cherry-pick fd21073
     git cherry-pick <this commit>

 - The sender will receive an ACK when the patch has been accepted into the
   queue, or a NAK if the patch is rejected.  This response might take a few
   days, according to the developer's schedules.
 - If accepted, the patch will be added to the -stable queue, for review by
   other developers and by the relevant subsystem maintainer.
 - Security patches should not be sent to this alias, but instead to the
   documented security@kernel.org address.


Review cycle:

 - When the -stable maintainers decide for a review cycle, the patches will be
   sent to the review committee, and the maintainer of the affected area of
   the patch (unless the submitter is the maintainer of the area) and CC: to
   the linux-kernel mailing list.
 - The review committee has 48 hours in which to ACK or NAK the patch.
 - If the patch is rejected by a member of the committee, or linux-kernel
   members object to the patch, bringing up issues that the maintainers and
   members did not realize, the patch will be dropped from the queue.
 - At the end of the review cycle, the ACKed patches will be added to the
   latest -stable release, and a new -stable release will happen.
 - Security patches will be accepted into the -stable tree directly from the
   security kernel team, and not go through the normal review cycle.
   Contact the kernel security team for more details on this procedure.

Trees:

 - The queues of patches, for both completed versions and in progress
   versions can be found at:
	http://git.kernel.org/?p=linux/kernel/git/stable/stable-queue.git
 - The finalized and tagged releases of all stable kernels can be found
   in separate branches per version at:
	http://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git


Review committee:

 - This is made up of a number of kernel developers who have volunteered for
   this task, and a few that haven't.
			Static Keys
			-----------

By: Jason Baron <jbaron@redhat.com>

0) Abstract

Static keys allows the inclusion of seldom used features in
performance-sensitive fast-path kernel code, via a GCC feature and a code
patching technique. A quick example:

	struct static_key key = STATIC_KEY_INIT_FALSE;

	...

        if (static_key_false(&key))
                do unlikely code
        else
                do likely code

	...
	static_key_slow_inc();
	...
	static_key_slow_inc();
	...

The static_key_false() branch will be generated into the code with as little
impact to the likely code path as possible.


1) Motivation


Currently, tracepoints are implemented using a conditional branch. The
conditional check requires checking a global variable for each tracepoint.
Although the overhead of this check is small, it increases when the memory
cache comes under pressure (memory cache lines for these global variables may
be shared with other memory accesses). As we increase the number of tracepoints
in the kernel this overhead may become more of an issue. In addition,
tracepoints are often dormant (disabled) and provide no direct kernel
functionality. Thus, it is highly desirable to reduce their impact as much as
possible. Although tracepoints are the original motivation for this work, other
kernel code paths should be able to make use of the static keys facility.


2) Solution


gcc (v4.5) adds a new 'asm goto' statement that allows branching to a label:

http://gcc.gnu.org/ml/gcc-patches/2009-07/msg01556.html

Using the 'asm goto', we can create branches that are either taken or not taken
by default, without the need to check memory. Then, at run-time, we can patch
the branch site to change the branch direction.

For example, if we have a simple branch that is disabled by default:

	if (static_key_false(&key))
		printk("I am the true branch\n");

Thus, by default the 'printk' will not be emitted. And the code generated will
consist of a single atomic 'no-op' instruction (5 bytes on x86), in the
straight-line code path. When the branch is 'flipped', we will patch the
'no-op' in the straight-line codepath with a 'jump' instruction to the
out-of-line true branch. Thus, changing branch direction is expensive but
branch selection is basically 'free'. That is the basic tradeoff of this
optimization.

This lowlevel patching mechanism is called 'jump label patching', and it gives
the basis for the static keys facility.

3) Static key label API, usage and examples:


In order to make use of this optimization you must first define a key:

	struct static_key key;

Which is initialized as:

	struct static_key key = STATIC_KEY_INIT_TRUE;

or:

	struct static_key key = STATIC_KEY_INIT_FALSE;

If the key is not initialized, it is default false. The 'struct static_key',
must be a 'global'. That is, it can't be allocated on the stack or dynamically
allocated at run-time.

The key is then used in code as:

        if (static_key_false(&key))
                do unlikely code
        else
                do likely code

Or:

        if (static_key_true(&key))
                do likely code
        else
                do unlikely code

A key that is initialized via 'STATIC_KEY_INIT_FALSE', must be used in a
'static_key_false()' construct. Likewise, a key initialized via
'STATIC_KEY_INIT_TRUE' must be used in a 'static_key_true()' construct. A
single key can be used in many branches, but all the branches must match the
way that the key has been initialized.

The branch(es) can then be switched via:

	static_key_slow_inc(&key);
	...
	static_key_slow_dec(&key);

Thus, 'static_key_slow_inc()' means 'make the branch true', and
'static_key_slow_dec()' means 'make the branch false' with appropriate
reference counting. For example, if the key is initialized true, a
static_key_slow_dec(), will switch the branch to false. And a subsequent
static_key_slow_inc(), will change the branch back to true. Likewise, if the
key is initialized false, a 'static_key_slow_inc()', will change the branch to
true. And then a 'static_key_slow_dec()', will again make the branch false.

An example usage in the kernel is the implementation of tracepoints:

        static inline void trace_##name(proto)                          \
        {                                                               \
                if (static_key_false(&__tracepoint_##name.key))		\
                        __DO_TRACE(&__tracepoint_##name,                \
                                TP_PROTO(data_proto),                   \
                                TP_ARGS(data_args),                     \
                                TP_CONDITION(cond));                    \
        }

Tracepoints are disabled by default, and can be placed in performance critical
pieces of the kernel. Thus, by using a static key, the tracepoints can have
absolutely minimal impact when not in use.


4) Architecture level code patching interface, 'jump labels'


There are a few functions and macros that architectures must implement in order
to take advantage of this optimization. If there is no architecture support, we
simply fall back to a traditional, load, test, and jump sequence.

* select HAVE_ARCH_JUMP_LABEL, see: arch/x86/Kconfig

* #define JUMP_LABEL_NOP_SIZE, see: arch/x86/include/asm/jump_label.h

* __always_inline bool arch_static_branch(struct static_key *key), see:
					arch/x86/include/asm/jump_label.h

* void arch_jump_label_transform(struct jump_entry *entry, enum jump_label_type type),
					see: arch/x86/kernel/jump_label.c

* __init_or_module void arch_jump_label_transform_static(struct jump_entry *entry, enum jump_label_type type),
					see: arch/x86/kernel/jump_label.c


* struct jump_entry, see: arch/x86/include/asm/jump_label.h


5) Static keys / jump label analysis, results (x86_64):


As an example, let's add the following branch to 'getppid()', such that the
system call now looks like:

SYSCALL_DEFINE0(getppid)
{
        int pid;

+       if (static_key_false(&key))
+               printk("I am the true branch\n");

        rcu_read_lock();
        pid = task_tgid_vnr(rcu_dereference(current->real_parent));
        rcu_read_unlock();

        return pid;
}

The resulting instructions with jump labels generated by GCC is:

ffffffff81044290 <sys_getppid>:
ffffffff81044290:       55                      push   %rbp
ffffffff81044291:       48 89 e5                mov    %rsp,%rbp
ffffffff81044294:       e9 00 00 00 00          jmpq   ffffffff81044299 <sys_getppid+0x9>
ffffffff81044299:       65 48 8b 04 25 c0 b6    mov    %gs:0xb6c0,%rax
ffffffff810442a0:       00 00
ffffffff810442a2:       48 8b 80 80 02 00 00    mov    0x280(%rax),%rax
ffffffff810442a9:       48 8b 80 b0 02 00 00    mov    0x2b0(%rax),%rax
ffffffff810442b0:       48 8b b8 e8 02 00 00    mov    0x2e8(%rax),%rdi
ffffffff810442b7:       e8 f4 d9 00 00          callq  ffffffff81051cb0 <pid_vnr>
ffffffff810442bc:       5d                      pop    %rbp
ffffffff810442bd:       48 98                   cltq
ffffffff810442bf:       c3                      retq
ffffffff810442c0:       48 c7 c7 e3 54 98 81    mov    $0xffffffff819854e3,%rdi
ffffffff810442c7:       31 c0                   xor    %eax,%eax
ffffffff810442c9:       e8 71 13 6d 00          callq  ffffffff8171563f <printk>
ffffffff810442ce:       eb c9                   jmp    ffffffff81044299 <sys_getppid+0x9>

Without the jump label optimization it looks like:

ffffffff810441f0 <sys_getppid>:
ffffffff810441f0:       8b 05 8a 52 d8 00       mov    0xd8528a(%rip),%eax        # ffffffff81dc9480 <key>
ffffffff810441f6:       55                      push   %rbp
ffffffff810441f7:       48 89 e5                mov    %rsp,%rbp
ffffffff810441fa:       85 c0                   test   %eax,%eax
ffffffff810441fc:       75 27                   jne    ffffffff81044225 <sys_getppid+0x35>
ffffffff810441fe:       65 48 8b 04 25 c0 b6    mov    %gs:0xb6c0,%rax
ffffffff81044205:       00 00
ffffffff81044207:       48 8b 80 80 02 00 00    mov    0x280(%rax),%rax
ffffffff8104420e:       48 8b 80 b0 02 00 00    mov    0x2b0(%rax),%rax
ffffffff81044215:       48 8b b8 e8 02 00 00    mov    0x2e8(%rax),%rdi
ffffffff8104421c:       e8 2f da 00 00          callq  ffffffff81051c50 <pid_vnr>
ffffffff81044221:       5d                      pop    %rbp
ffffffff81044222:       48 98                   cltq
ffffffff81044224:       c3                      retq
ffffffff81044225:       48 c7 c7 13 53 98 81    mov    $0xffffffff81985313,%rdi
ffffffff8104422c:       31 c0                   xor    %eax,%eax
ffffffff8104422e:       e8 60 0f 6d 00          callq  ffffffff81715193 <printk>
ffffffff81044233:       eb c9                   jmp    ffffffff810441fe <sys_getppid+0xe>
ffffffff81044235:       66 66 2e 0f 1f 84 00    data32 nopw %cs:0x0(%rax,%rax,1)
ffffffff8104423c:       00 00 00 00

Thus, the disable jump label case adds a 'mov', 'test' and 'jne' instruction
vs. the jump label case just has a 'no-op' or 'jmp 0'. (The jmp 0, is patched
to a 5 byte atomic no-op instruction at boot-time.) Thus, the disabled jump
label case adds:

6 (mov) + 2 (test) + 2 (jne) = 10 - 5 (5 byte jump 0) = 5 addition bytes.

If we then include the padding bytes, the jump label code saves, 16 total bytes
of instruction memory for this small function. In this case the non-jump label
function is 80 bytes long. Thus, we have saved 20% of the instruction
footprint. We can in fact improve this even further, since the 5-byte no-op
really can be a 2-byte no-op since we can reach the branch with a 2-byte jmp.
However, we have not yet implemented optimal no-op sizes (they are currently
hard-coded).

Since there are a number of static key API uses in the scheduler paths,
'pipe-test' (also known as 'perf bench sched pipe') can be used to show the
performance improvement. Testing done on 3.3.0-rc2:

jump label disabled:

 Performance counter stats for 'bash -c /tmp/pipe-test' (50 runs):

        855.700314 task-clock                #    0.534 CPUs utilized            ( +-  0.11% )
           200,003 context-switches          #    0.234 M/sec                    ( +-  0.00% )
                 0 CPU-migrations            #    0.000 M/sec                    ( +- 39.58% )
               487 page-faults               #    0.001 M/sec                    ( +-  0.02% )
     1,474,374,262 cycles                    #    1.723 GHz                      ( +-  0.17% )
   <not supported> stalled-cycles-frontend
   <not supported> stalled-cycles-backend
     1,178,049,567 instructions              #    0.80  insns per cycle          ( +-  0.06% )
       208,368,926 branches                  #  243.507 M/sec                    ( +-  0.06% )
         5,569,188 branch-misses             #    2.67% of all branches          ( +-  0.54% )

       1.601607384 seconds time elapsed                                          ( +-  0.07% )

jump label enabled:

 Performance counter stats for 'bash -c /tmp/pipe-test' (50 runs):

        841.043185 task-clock                #    0.533 CPUs utilized            ( +-  0.12% )
           200,004 context-switches          #    0.238 M/sec                    ( +-  0.00% )
                 0 CPU-migrations            #    0.000 M/sec                    ( +- 40.87% )
               487 page-faults               #    0.001 M/sec                    ( +-  0.05% )
     1,432,559,428 cycles                    #    1.703 GHz                      ( +-  0.18% )
   <not supported> stalled-cycles-frontend
   <not supported> stalled-cycles-backend
     1,175,363,994 instructions              #    0.82  insns per cycle          ( +-  0.04% )
       206,859,359 branches                  #  245.956 M/sec                    ( +-  0.04% )
         4,884,119 branch-misses             #    2.36% of all branches          ( +-  0.85% )

       1.579384366 seconds time elapsed

The percentage of saved branches is .7%, and we've saved 12% on
'branch-misses'. This is where we would expect to get the most savings, since
this optimization is about reducing the number of branches. In addition, we've
saved .2% on instructions, and 2.8% on cycles and 1.4% on elapsed time.
		       Video Mode Selection Support 2.13
		    (c) 1995--1999 Martin Mares, <mj@ucw.cz>
--------------------------------------------------------------------------------

1. Intro
~~~~~~~~
   This small document describes the "Video Mode Selection" feature which
allows the use of various special video modes supported by the video BIOS. Due
to usage of the BIOS, the selection is limited to boot time (before the
kernel decompression starts) and works only on 80X86 machines.

   **  Short intro for the impatient: Just use vga=ask for the first time,
   **  enter `scan' on the video mode prompt, pick the mode you want to use,
   **  remember its mode ID (the four-digit hexadecimal number) and then
   **  set the vga parameter to this number (converted to decimal first).

   The video mode to be used is selected by a kernel parameter which can be
specified in the kernel Makefile (the SVGA_MODE=... line) or by the "vga=..."
option of LILO (or some other boot loader you use) or by the "vidmode" utility
(present in standard Linux utility packages). You can use the following values
of this parameter:

   NORMAL_VGA - Standard 80x25 mode available on all display adapters.

   EXTENDED_VGA	- Standard 8-pixel font mode: 80x43 on EGA, 80x50 on VGA.

   ASK_VGA - Display a video mode menu upon startup (see below).

   0..35 - Menu item number (when you have used the menu to view the list of
      modes available on your adapter, you can specify the menu item you want
      to use). 0..9 correspond to "0".."9", 10..35 to "a".."z". Warning: the
      mode list displayed may vary as the kernel version changes, because the
      modes are listed in a "first detected -- first displayed" manner. It's
      better to use absolute mode numbers instead.

   0x.... - Hexadecimal video mode ID (also displayed on the menu, see below
      for exact meaning of the ID). Warning: rdev and LILO don't support
      hexadecimal numbers -- you have to convert it to decimal manually.

2. Menu
~~~~~~~
   The ASK_VGA mode causes the kernel to offer a video mode menu upon
bootup. It displays a "Press <RETURN> to see video modes available, <SPACE>
to continue or wait 30 secs" message. If you press <RETURN>, you enter the
menu, if you press <SPACE> or wait 30 seconds, the kernel will boot up in
the standard 80x25 mode.

   The menu looks like:

Video adapter: <name-of-detected-video-adapter>
Mode:    COLSxROWS:
0  0F00  80x25
1  0F01  80x50
2  0F02  80x43
3  0F03  80x26
....
Enter mode number or `scan': <flashing-cursor-here>

   <name-of-detected-video-adapter> tells what video adapter did Linux detect
-- it's either a generic adapter name (MDA, CGA, HGC, EGA, VGA, VESA VGA [a VGA
with VESA-compliant BIOS]) or a chipset name (e.g., Trident). Direct detection
of chipsets is turned off by default (see CONFIG_VIDEO_SVGA in chapter 4 to see
how to enable it if you really want) as it's inherently unreliable due to
absolutely insane PC design.

   "0  0F00  80x25" means that the first menu item (the menu items are numbered
from "0" to "9" and from "a" to "z") is a 80x25 mode with ID=0x0f00 (see the
next section for a description of mode IDs).

   <flashing-cursor-here> encourages you to enter the item number or mode ID
you wish to set and press <RETURN>. If the computer complains something about
"Unknown mode ID", it is trying to tell you that it isn't possible to set such
a mode. It's also possible to press only <RETURN> which leaves the current mode.

   The mode list usually contains a few basic modes and some VESA modes.  In
case your chipset has been detected, some chipset-specific modes are shown as
well (some of these might be missing or unusable on your machine as different
BIOSes are often shipped with the same card and the mode numbers depend purely
on the VGA BIOS).

   The modes displayed on the menu are partially sorted: The list starts with
the standard modes (80x25 and 80x50) followed by "special" modes (80x28 and
80x43), local modes (if the local modes feature is enabled), VESA modes and
finally SVGA modes for the auto-detected adapter.

   If you are not happy with the mode list offered (e.g., if you think your card
is able to do more), you can enter "scan" instead of item number / mode ID.  The
program will try to ask the BIOS for all possible video mode numbers and test
what happens then. The screen will be probably flashing wildly for some time and
strange noises will be heard from inside the monitor and so on and then, really
all consistent video modes supported by your BIOS will appear (plus maybe some
`ghost modes'). If you are afraid this could damage your monitor, don't use this
function.

   After scanning, the mode ordering is a bit different: the auto-detected SVGA
modes are not listed at all and the modes revealed by `scan' are shown before
all VESA modes.

3. Mode IDs
~~~~~~~~~~~
   Because of the complexity of all the video stuff, the video mode IDs
used here are also a bit complex. A video mode ID is a 16-bit number usually
expressed in a hexadecimal notation (starting with "0x"). You can set a mode
by entering its mode directly if you know it even if it isn't shown on the menu.

The ID numbers can be divided to three regions:

   0x0000 to 0x00ff - menu item references. 0x0000 is the first item. Don't use
	outside the menu as this can change from boot to boot (especially if you
	have used the `scan' feature).

   0x0100 to 0x017f - standard BIOS modes. The ID is a BIOS video mode number
	(as presented to INT 10, function 00) increased by 0x0100.

   0x0200 to 0x08ff - VESA BIOS modes. The ID is a VESA mode ID increased by
	0x0100. All VESA modes should be autodetected and shown on the menu.

   0x0900 to 0x09ff - Video7 special modes. Set by calling INT 0x10, AX=0x6f05.
	(Usually 940=80x43, 941=132x25, 942=132x44, 943=80x60, 944=100x60,
	945=132x28 for the standard Video7 BIOS)

   0x0f00 to 0x0fff - special modes (they are set by various tricks -- usually
	by modifying one of the standard modes). Currently available:
	0x0f00	standard 80x25, don't reset mode if already set (=FFFF)
	0x0f01	standard with 8-point font: 80x43 on EGA, 80x50 on VGA
	0x0f02	VGA 80x43 (VGA switched to 350 scanlines with a 8-point font)
	0x0f03	VGA 80x28 (standard VGA scans, but 14-point font)
	0x0f04	leave current video mode
	0x0f05	VGA 80x30 (480 scans, 16-point font)
	0x0f06	VGA 80x34 (480 scans, 14-point font)
	0x0f07	VGA 80x60 (480 scans, 8-point font)
	0x0f08	Graphics hack (see the CONFIG_VIDEO_HACK paragraph below)

   0x1000 to 0x7fff - modes specified by resolution. The code has a "0xRRCC"
	form where RR is a number of rows and CC is a number of columns.
	E.g., 0x1950 corresponds to a 80x25 mode, 0x2b84 to 132x43 etc.
	This is the only fully portable way to refer to a non-standard mode,
	but it relies on the mode being found and displayed on the menu
	(remember that mode scanning is not done automatically).

   0xff00 to 0xffff - aliases for backward compatibility:
	0xffff	equivalent to 0x0f00 (standard 80x25)
	0xfffe	equivalent to 0x0f01 (EGA 80x43 or VGA 80x50)

   If you add 0x8000 to the mode ID, the program will try to recalculate
vertical display timing according to mode parameters, which can be used to
eliminate some annoying bugs of certain VGA BIOSes (usually those used for
cards with S3 chipsets and old Cirrus Logic BIOSes) -- mainly extra lines at the
end of the display.

4. Options
~~~~~~~~~~
   Some options can be set in the source text (in arch/i386/boot/video.S).
All of them are simple #define's -- change them to #undef's when you want to
switch them off. Currently supported:

   CONFIG_VIDEO_SVGA - enables autodetection of SVGA cards. This is switched
off by default as it's a bit unreliable due to terribly bad PC design. If you
really want to have the adapter autodetected (maybe in case the `scan' feature
doesn't work on your machine), switch this on and don't cry if the results
are not completely sane. In case you really need this feature, please drop me
a mail as I think of removing it some day.

   CONFIG_VIDEO_VESA - enables autodetection of VESA modes. If it doesn't work
on your machine (or displays a "Error: Scanning of VESA modes failed" message),
you can switch it off and report as a bug.

   CONFIG_VIDEO_COMPACT - enables compacting of the video mode list. If there
are more modes with the same screen size, only the first one is kept (see above
for more info on mode ordering). However, in very strange cases it's possible
that the first "version" of the mode doesn't work although some of the others
do -- in this case turn this switch off to see the rest.

   CONFIG_VIDEO_RETAIN - enables retaining of screen contents when switching
video modes. Works only with some boot loaders which leave enough room for the
buffer. (If you have old LILO, you can adjust heap_end_ptr and loadflags
in setup.S, but it's better to upgrade the boot loader...)

   CONFIG_VIDEO_LOCAL - enables inclusion of "local modes" in the list. The
local modes are added automatically to the beginning of the list not depending
on hardware configuration. The local modes are listed in the source text after
the "local_mode_table:" line. The comment before this line describes the format
of the table (which also includes a video card name to be displayed on the
top of the menu).

   CONFIG_VIDEO_400_HACK - force setting of 400 scan lines for standard VGA
modes. This option is intended to be used on certain buggy BIOSes which draw
some useless logo using font download and then fail to reset the correct mode.
Don't use unless needed as it forces resetting the video card.

   CONFIG_VIDEO_GFX_HACK - includes special hack for setting of graphics modes
to be used later by special drivers (e.g., 800x600 on IBM ThinkPad -- see
ftp://ftp.phys.keio.ac.jp/pub/XFree86/800x600/XF86Configs/XF86Config.IBM_TP560).
Allows to set _any_ BIOS mode including graphic ones and forcing specific
text screen resolution instead of peeking it from BIOS variables. Don't use
unless you think you know what you're doing. To activate this setup, use
mode number 0x0f08 (see section 3).

5. Still doesn't work?
~~~~~~~~~~~~~~~~~~~~~~
   When the mode detection doesn't work (e.g., the mode list is incorrect or
the machine hangs instead of displaying the menu), try to switch off some of
the configuration options listed in section 4. If it fails, you can still use
your kernel with the video mode set directly via the kernel parameter.

   In either case, please send me a bug report containing what _exactly_
happens and how do the configuration switches affect the behaviour of the bug.

   If you start Linux from M$-DOS, you might also use some DOS tools for
video mode setting. In this case, you must specify the 0x0f04 mode ("leave
current settings") to Linux, because if you don't and you use any non-standard
mode, Linux will switch to 80x25 automatically.

   If you set some extended mode and there's one or more extra lines on the
bottom of the display containing already scrolled-out text, your VGA BIOS
contains the most common video BIOS bug called "incorrect vertical display
end setting". Adding 0x8000 to the mode ID might fix the problem. Unfortunately,
this must be done manually -- no autodetection mechanisms are available.

   If you have a VGA card and your display still looks as on EGA, your BIOS
is probably broken and you need to set the CONFIG_VIDEO_400_HACK switch to
force setting of the correct mode.

6. History
~~~~~~~~~~
1.0 (??-Nov-95)	First version supporting all adapters supported by the old
		setup.S + Cirrus Logic 54XX. Present in some 1.3.4? kernels
		and then removed due to instability on some machines.
2.0 (28-Jan-96)	Rewritten from scratch. Cirrus Logic 64XX support added, almost
		everything is configurable, the VESA support should be much more
		stable, explicit mode numbering allowed, "scan" implemented etc.
2.1 (30-Jan-96) VESA modes moved to 0x200-0x3ff. Mode selection by resolution
		supported. Few bugs fixed. VESA modes are listed prior to
		modes supplied by SVGA autodetection as they are more reliable.
		CLGD autodetect works better. Doesn't depend on 80x25 being
		active when started. Scanning fixed. 80x43 (any VGA) added.
		Code cleaned up.
2.2 (01-Feb-96)	EGA 80x43 fixed. VESA extended to 0x200-0x4ff (non-standard 02XX
		VESA modes work now). Display end bug workaround supported.
		Special modes renumbered to allow adding of the "recalculate"
		flag, 0xffff and 0xfffe became aliases instead of real IDs.
		Screen contents retained during mode changes.
2.3 (15-Mar-96)	Changed to work with 1.3.74 kernel.
2.4 (18-Mar-96)	Added patches by Hans Lermen fixing a memory overwrite problem
		with some boot loaders. Memory management rewritten to reflect
		these changes. Unfortunately, screen contents retaining works
		only with some loaders now.
		Added a Tseng 132x60 mode.
2.5 (19-Mar-96)	Fixed a VESA mode scanning bug introduced in 2.4.
2.6 (25-Mar-96)	Some VESA BIOS errors not reported -- it fixes error reports on
		several cards with broken VESA code (e.g., ATI VGA).
2.7 (09-Apr-96)	- Accepted all VESA modes in range 0x100 to 0x7ff, because some
		  cards use very strange mode numbers.
		- Added Realtek VGA modes (thanks to Gonzalo Tornaria).
		- Hardware testing order slightly changed, tests based on ROM
		  contents done as first.
		- Added support for special Video7 mode switching functions
		  (thanks to Tom Vander Aa).
		- Added 480-scanline modes (especially useful for notebooks,
		  original version written by hhanemaa@cs.ruu.nl, patched by
		  Jeff Chua, rewritten by me).
		- Screen store/restore fixed.
2.8 (14-Apr-96)	- Previous release was not compilable without CONFIG_VIDEO_SVGA.
		- Better recognition of text modes during mode scan.
2.9 (12-May-96)	- Ignored VESA modes 0x80 - 0xff (more VESA BIOS bugs!)
2.10 (11-Nov-96)- The whole thing made optional.
		- Added the CONFIG_VIDEO_400_HACK switch.
		- Added the CONFIG_VIDEO_GFX_HACK switch.
		- Code cleanup.
2.11 (03-May-97)- Yet another cleanup, now including also the documentation.
		- Direct testing of SVGA adapters turned off by default, `scan'
		  offered explicitly on the prompt line.
		- Removed the doc section describing adding of new probing
		  functions as I try to get rid of _all_ hardware probing here.
2.12 (25-May-98)- Added support for VESA frame buffer graphics.
2.13 (14-May-99)- Minor documentation fixes.
Rules on how to access information in the Linux kernel sysfs

The kernel-exported sysfs exports internal kernel implementation details
and depends on internal kernel structures and layout. It is agreed upon
by the kernel developers that the Linux kernel does not provide a stable
internal API. Therefore, there are aspects of the sysfs interface that
may not be stable across kernel releases.

To minimize the risk of breaking users of sysfs, which are in most cases
low-level userspace applications, with a new kernel release, the users
of sysfs must follow some rules to use an as-abstract-as-possible way to
access this filesystem. The current udev and HAL programs already
implement this and users are encouraged to plug, if possible, into the
abstractions these programs provide instead of accessing sysfs directly.

But if you really do want or need to access sysfs directly, please follow
the following rules and then your programs should work with future
versions of the sysfs interface.

- Do not use libsysfs
  It makes assumptions about sysfs which are not true. Its API does not
  offer any abstraction, it exposes all the kernel driver-core
  implementation details in its own API. Therefore it is not better than
  reading directories and opening the files yourself.
  Also, it is not actively maintained, in the sense of reflecting the
  current kernel development. The goal of providing a stable interface
  to sysfs has failed; it causes more problems than it solves. It
  violates many of the rules in this document.

- sysfs is always at /sys
  Parsing /proc/mounts is a waste of time. Other mount points are a
  system configuration bug you should not try to solve. For test cases,
  possibly support a SYSFS_PATH environment variable to overwrite the
  application's behavior, but never try to search for sysfs. Never try
  to mount it, if you are not an early boot script.

- devices are only "devices"
  There is no such thing like class-, bus-, physical devices,
  interfaces, and such that you can rely on in userspace. Everything is
  just simply a "device". Class-, bus-, physical, ... types are just
  kernel implementation details which should not be expected by
  applications that look for devices in sysfs.

  The properties of a device are:
    o devpath (/devices/pci0000:00/0000:00:1d.1/usb2/2-2/2-2:1.0)
      - identical to the DEVPATH value in the event sent from the kernel
        at device creation and removal
      - the unique key to the device at that point in time
      - the kernel's path to the device directory without the leading
        /sys, and always starting with a slash
      - all elements of a devpath must be real directories. Symlinks
        pointing to /sys/devices must always be resolved to their real
        target and the target path must be used to access the device.
        That way the devpath to the device matches the devpath of the
        kernel used at event time.
      - using or exposing symlink values as elements in a devpath string
        is a bug in the application

    o kernel name (sda, tty, 0000:00:1f.2, ...)
      - a directory name, identical to the last element of the devpath
      - applications need to handle spaces and characters like '!' in
        the name

    o subsystem (block, tty, pci, ...)
      - simple string, never a path or a link
      - retrieved by reading the "subsystem"-link and using only the
        last element of the target path

    o driver (tg3, ata_piix, uhci_hcd)
      - a simple string, which may contain spaces, never a path or a
        link
      - it is retrieved by reading the "driver"-link and using only the
        last element of the target path
      - devices which do not have "driver"-link just do not have a
        driver; copying the driver value in a child device context is a
        bug in the application

    o attributes
      - the files in the device directory or files below subdirectories
        of the same device directory
      - accessing attributes reached by a symlink pointing to another device,
        like the "device"-link, is a bug in the application

  Everything else is just a kernel driver-core implementation detail
  that should not be assumed to be stable across kernel releases.

- Properties of parent devices never belong into a child device.
  Always look at the parent devices themselves for determining device
  context properties. If the device 'eth0' or 'sda' does not have a
  "driver"-link, then this device does not have a driver. Its value is empty.
  Never copy any property of the parent-device into a child-device. Parent
  device properties may change dynamically without any notice to the
  child device.

- Hierarchy in a single device tree
  There is only one valid place in sysfs where hierarchy can be examined
  and this is below: /sys/devices.
  It is planned that all device directories will end up in the tree
  below this directory.

- Classification by subsystem
  There are currently three places for classification of devices:
  /sys/block, /sys/class and /sys/bus. It is planned that these will
  not contain any device directories themselves, but only flat lists of
  symlinks pointing to the unified /sys/devices tree.
  All three places have completely different rules on how to access
  device information. It is planned to merge all three
  classification directories into one place at /sys/subsystem,
  following the layout of the bus directories. All buses and
  classes, including the converted block subsystem, will show up
  there.
  The devices belonging to a subsystem will create a symlink in the
  "devices" directory at /sys/subsystem/<name>/devices.

  If /sys/subsystem exists, /sys/bus, /sys/class and /sys/block can be
  ignored. If it does not exist, you always have to scan all three
  places, as the kernel is free to move a subsystem from one place to
  the other, as long as the devices are still reachable by the same
  subsystem name.

  Assuming /sys/class/<subsystem> and /sys/bus/<subsystem>, or
  /sys/block and /sys/class/block are not interchangeable is a bug in
  the application.

- Block
  The converted block subsystem at /sys/class/block or
  /sys/subsystem/block will contain the links for disks and partitions
  at the same level, never in a hierarchy. Assuming the block subsystem to
  contain only disks and not partition devices in the same flat list is
  a bug in the application.

- "device"-link and <subsystem>:<kernel name>-links
  Never depend on the "device"-link. The "device"-link is a workaround
  for the old layout, where class devices are not created in
  /sys/devices/ like the bus devices. If the link-resolving of a
  device directory does not end in /sys/devices/, you can use the
  "device"-link to find the parent devices in /sys/devices/. That is the
  single valid use of the "device"-link; it must never appear in any
  path as an element. Assuming the existence of the "device"-link for
  a device in /sys/devices/ is a bug in the application.
  Accessing /sys/class/net/eth0/device is a bug in the application.

  Never depend on the class-specific links back to the /sys/class
  directory.  These links are also a workaround for the design mistake
  that class devices are not created in /sys/devices. If a device
  directory does not contain directories for child devices, these links
  may be used to find the child devices in /sys/class. That is the single
  valid use of these links; they must never appear in any path as an
  element. Assuming the existence of these links for devices which are
  real child device directories in the /sys/devices tree is a bug in
  the application.

  It is planned to remove all these links when all class device
  directories live in /sys/devices.

- Position of devices along device chain can change.
  Never depend on a specific parent device position in the devpath,
  or the chain of parent devices. The kernel is free to insert devices into
  the chain. You must always request the parent device you are looking for
  by its subsystem value. You need to walk up the chain until you find
  the device that matches the expected subsystem. Depending on a specific
  position of a parent device or exposing relative paths using "../" to
  access the chain of parents is a bug in the application.
Linux Magic System Request Key Hacks
Documentation for sysrq.c

*  What is the magic SysRq key?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
It is a 'magical' key combo you can hit which the kernel will respond to
regardless of whatever else it is doing, unless it is completely locked up.

*  How do I enable the magic SysRq key?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
You need to say "yes" to 'Magic SysRq key (CONFIG_MAGIC_SYSRQ)' when
configuring the kernel. When running a kernel with SysRq compiled in,
/proc/sys/kernel/sysrq controls the functions allowed to be invoked via
the SysRq key. The default value in this file is set by the
CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE config symbol, which itself defaults
to 1. Here is the list of possible values in /proc/sys/kernel/sysrq:
   0 - disable sysrq completely
   1 - enable all functions of sysrq
  >1 - bitmask of allowed sysrq functions (see below for detailed function
       description):
          2 =   0x2 - enable control of console logging level
          4 =   0x4 - enable control of keyboard (SAK, unraw)
          8 =   0x8 - enable debugging dumps of processes etc.
         16 =  0x10 - enable sync command
         32 =  0x20 - enable remount read-only
         64 =  0x40 - enable signalling of processes (term, kill, oom-kill)
        128 =  0x80 - allow reboot/poweroff
        256 = 0x100 - allow nicing of all RT tasks

You can set the value in the file by the following command:
    echo "number" >/proc/sys/kernel/sysrq

The number may be written here either as decimal or as hexadecimal
with the 0x prefix. CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE must always be
written in hexadecimal.

Note that the value of /proc/sys/kernel/sysrq influences only the invocation
via a keyboard. Invocation of any operation via /proc/sysrq-trigger is always
allowed (by a user with admin privileges).

*  How do I use the magic SysRq key?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
On x86   - You press the key combo 'ALT-SysRq-<command key>'. Note - Some
           keyboards may not have a key labeled 'SysRq'. The 'SysRq' key is
           also known as the 'Print Screen' key. Also some keyboards cannot
	   handle so many keys being pressed at the same time, so you might
	   have better luck with "press Alt", "press SysRq", "release SysRq",
	   "press <command key>", release everything.

On SPARC - You press 'ALT-STOP-<command key>', I believe.

On the serial console (PC style standard serial ports only) -
           You send a BREAK, then within 5 seconds a command key. Sending
           BREAK twice is interpreted as a normal BREAK.

On PowerPC - Press 'ALT - Print Screen (or F13) - <command key>,  
             Print Screen (or F13) - <command key> may suffice.

On other - If you know of the key combos for other architectures, please
           let me know so I can add them to this section.

On all -  write a character to /proc/sysrq-trigger.  e.g.:

		echo t > /proc/sysrq-trigger

*  What are the 'command' keys?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
'b'     - Will immediately reboot the system without syncing or unmounting
          your disks.

'c'	- Will perform a system crash by a NULL pointer dereference.
          A crashdump will be taken if configured.

'd'	- Shows all locks that are held.

'e'     - Send a SIGTERM to all processes, except for init.

'f'	- Will call oom_kill to kill a memory hog process.

'g'	- Used by kgdb (kernel debugger)

'h'     - Will display help (actually any other key than those listed
          here will display help. but 'h' is easy to remember :-)

'i'     - Send a SIGKILL to all processes, except for init.

'j'     - Forcibly "Just thaw it" - filesystems frozen by the FIFREEZE ioctl.

'k'     - Secure Access Key (SAK) Kills all programs on the current virtual
          console. NOTE: See important comments below in SAK section.

'l'     - Shows a stack backtrace for all active CPUs.

'm'     - Will dump current memory info to your console.

'n'	- Used to make RT tasks nice-able

'o'     - Will shut your system off (if configured and supported).

'p'     - Will dump the current registers and flags to your console.

'q'     - Will dump per CPU lists of all armed hrtimers (but NOT regular
          timer_list timers) and detailed information about all
          clockevent devices.

'r'     - Turns off keyboard raw mode and sets it to XLATE.

's'     - Will attempt to sync all mounted filesystems.

't'     - Will dump a list of current tasks and their information to your
          console.

'u'     - Will attempt to remount all mounted filesystems read-only.

'v'	- Forcefully restores framebuffer console
'v'	- Causes ETM buffer dump [ARM-specific]

'w'	- Dumps tasks that are in uninterruptable (blocked) state.

'x'	- Used by xmon interface on ppc/powerpc platforms.
          Show global PMU Registers on sparc64.

'y'	- Show global CPU Registers [SPARC-64 specific]

'z'	- Dump the ftrace buffer

'0'-'9' - Sets the console log level, controlling which kernel messages
          will be printed to your console. ('0', for example would make
          it so that only emergency messages like PANICs or OOPSes would
          make it to your console.)

*  Okay, so what can I use them for?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Well, unraw(r) is very handy when your X server or a svgalib program crashes.

sak(k) (Secure Access Key) is useful when you want to be sure there is no
trojan program running at console which could grab your password
when you would try to login. It will kill all programs on given console,
thus letting you make sure that the login prompt you see is actually
the one from init, not some trojan program.
IMPORTANT: In its true form it is not a true SAK like the one in a :IMPORTANT
IMPORTANT: c2 compliant system, and it should not be mistaken as   :IMPORTANT
IMPORTANT: such.                                                   :IMPORTANT
       It seems others find it useful as (System Attention Key) which is
useful when you want to exit a program that will not let you switch consoles.
(For example, X or a svgalib program.)

reboot(b) is good when you're unable to shut down. But you should also
sync(s) and umount(u) first.

crash(c) can be used to manually trigger a crashdump when the system is hung.
Note that this just triggers a crash if there is no dump mechanism available.

sync(s) is great when your system is locked up, it allows you to sync your
disks and will certainly lessen the chance of data loss and fscking. Note
that the sync hasn't taken place until you see the "OK" and "Done" appear
on the screen. (If the kernel is really in strife, you may not ever get the
OK or Done message...)

umount(u) is basically useful in the same ways as sync(s). I generally sync(s),
umount(u), then reboot(b) when my system locks. It's saved me many a fsck.
Again, the unmount (remount read-only) hasn't taken place until you see the
"OK" and "Done" message appear on the screen.

The loglevels '0'-'9' are useful when your console is being flooded with
kernel messages you do not want to see. Selecting '0' will prevent all but
the most urgent kernel messages from reaching your console. (They will
still be logged if syslogd/klogd are alive, though.)

term(e) and kill(i) are useful if you have some sort of runaway process you
are unable to kill any other way, especially if it's spawning other
processes.

"just thaw it(j)" is useful if your system becomes unresponsive due to a frozen
(probably root) filesystem via the FIFREEZE ioctl.

*  Sometimes SysRq seems to get 'stuck' after using it, what can I do?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
That happens to me, also. I've found that tapping shift, alt, and control
on both sides of the keyboard, and hitting an invalid sysrq sequence again
will fix the problem. (i.e., something like alt-sysrq-z). Switching to another
virtual console (ALT+Fn) and then back again should also help.

*  I hit SysRq, but nothing seems to happen, what's wrong?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
There are some keyboards that produce a different keycode for SysRq than the
pre-defined value of 99 (see KEY_SYSRQ in include/linux/input.h), or which
don't have a SysRq key at all. In these cases, run 'showkey -s' to find an
appropriate scancode sequence, and use 'setkeycodes <sequence> 99' to map
this sequence to the usual SysRq code (e.g., 'setkeycodes e05b 99'). It's
probably best to put this command in a boot script. Oh, and by the way, you
exit 'showkey' by not typing anything for ten seconds.

*  I want to add SysRQ key events to a module, how does it work?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In order to register a basic function with the table, you must first include
the header 'include/linux/sysrq.h', this will define everything else you need.
Next, you must create a sysrq_key_op struct, and populate it with A) the key
handler function you will use, B) a help_msg string, that will print when SysRQ
prints help, and C) an action_msg string, that will print right before your
handler is called. Your handler must conform to the prototype in 'sysrq.h'.

After the sysrq_key_op is created, you can call the kernel function
register_sysrq_key(int key, struct sysrq_key_op *op_p); this will
register the operation pointed to by 'op_p' at table key 'key',
if that slot in the table is blank. At module unload time, you must call
the function unregister_sysrq_key(int key, struct sysrq_key_op *op_p), which
will remove the key op pointed to by 'op_p' from the key 'key', if and only if
it is currently registered in that slot. This is in case the slot has been
overwritten since you registered it.

The Magic SysRQ system works by registering key operations against a key op
lookup table, which is defined in 'drivers/char/sysrq.c'. This key table has
a number of operations registered into it at compile time, but is mutable,
and 2 functions are exported for interface to it:
	register_sysrq_key and unregister_sysrq_key.
Of course, never ever leave an invalid pointer in the table. I.e., when
your module that called register_sysrq_key() exits, it must call
unregister_sysrq_key() to clean up the sysrq key table entry that it used.
Null pointers in the table are always safe. :)

If for some reason you feel the need to call the handle_sysrq function from
within a function called by handle_sysrq, you must be aware that you are in
a lock (you are also in an interrupt handler, which means don't sleep!), so
you must call __handle_sysrq_nolock instead.

*  When I hit a SysRq key combination only the header appears on the console?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sysrq output is subject to the same console loglevel control as all
other console output.  This means that if the kernel was booted 'quiet'
as is common on distro kernels the output may not appear on the actual
console, even though it will appear in the dmesg buffer, and be accessible
via the dmesg command and to the consumers of /proc/kmsg.  As a specific
exception the header line from the sysrq command is passed to all console
consumers as if the current loglevel was maximum.  If only the header
is emitted it is almost certain that the kernel loglevel is too low.
Should you require the output on the console channel then you will need
to temporarily up the console loglevel using alt-sysrq-8 or:

    echo 8 > /proc/sysrq-trigger

Remember to return the loglevel to normal after triggering the sysrq
command you are interested in.

*  I have more questions, who can I ask?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Just ask them on the linux-kernel mailing list:
	linux-kernel@vger.kernel.org

*  Credits
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Written by Mydraal <vulpyne@vulpyne.net>
Updated by Adam Sulmicki <adam@cfar.umd.edu>
Updated by Jeremy M. Dolan <jmd@turbogeek.org> 2001/01/28 10:15:59
Added to by Crutcher Dunnavant <crutcher+kernel@datastacks.com>
this_cpu operations
-------------------

this_cpu operations are a way of optimizing access to per cpu
variables associated with the *currently* executing processor through
the use of segment registers (or a dedicated register where the cpu
permanently stored the beginning of the per cpu area for a specific
processor).

The this_cpu operations add a per cpu variable offset to the processor
specific percpu base and encode that operation in the instruction
operating on the per cpu variable.

This means there are no atomicity issues between the calculation of
the offset and the operation on the data. Therefore it is not
necessary to disable preempt or interrupts to ensure that the
processor is not changed between the calculation of the address and
the operation on the data.

Read-modify-write operations are of particular interest. Frequently
processors have special lower latency instructions that can operate
without the typical synchronization overhead but still provide some
sort of relaxed atomicity guarantee. The x86 for example can execute
RMV (Read Modify Write) instructions like inc/dec/cmpxchg without the
lock prefix and the associated latency penalty.

Access to the variable without the lock prefix is not synchronized but
synchronization is not necessary since we are dealing with per cpu
data specific to the currently executing processor. Only the current
processor should be accessing that variable and therefore there are no
concurrency issues with other processors in the system.

On x86 the fs: or the gs: segment registers contain the base of the
per cpu area. It is then possible to simply use the segment override
to relocate a per cpu relative address to the proper per cpu area for
the processor. So the relocation to the per cpu base is encoded in the
instruction via a segment register prefix.

For example:

	DEFINE_PER_CPU(int, x);
	int z;

	z = this_cpu_read(x);

results in a single instruction

	mov ax, gs:[x]

instead of a sequence of calculation of the address and then a fetch
from that address which occurs with the percpu operations. Before
this_cpu_ops such sequence also required preempt disable/enable to
prevent the kernel from moving the thread to a different processor
while the calculation is performed.

The main use of the this_cpu operations has been to optimize counter
operations.

	this_cpu_inc(x)

results in the following single instruction (no lock prefix!)

	inc gs:[x]

instead of the following operations required if there is no segment
register.

	int *y;
	int cpu;

	cpu = get_cpu();
	y = per_cpu_ptr(&x, cpu);
	(*y)++;
	put_cpu();

Note that these operations can only be used on percpu data that is
reserved for a specific processor. Without disabling preemption in the
surrounding code this_cpu_inc() will only guarantee that one of the
percpu counters is correctly incremented. However, there is no
guarantee that the OS will not move the process directly before or
after the this_cpu instruction is executed. In general this means that
the value of the individual counters for each processor are
meaningless. The sum of all the per cpu counters is the only value
that is of interest.

Per cpu variables are used for performance reasons. Bouncing cache
lines can be avoided if multiple processors concurrently go through
the same code paths.  Since each processor has its own per cpu
variables no concurrent cacheline updates take place. The price that
has to be paid for this optimization is the need to add up the per cpu
counters when the value of the counter is needed.


Special operations:
-------------------

	y = this_cpu_ptr(&x)

Takes the offset of a per cpu variable (&x !) and returns the address
of the per cpu variable that belongs to the currently executing
processor.  this_cpu_ptr avoids multiple steps that the common
get_cpu/put_cpu sequence requires. No processor number is
available. Instead the offset of the local per cpu area is simply
added to the percpu offset.



Per cpu variables and offsets
-----------------------------

Per cpu variables have *offsets* to the beginning of the percpu
area. They do not have addresses although they look like that in the
code. Offsets cannot be directly dereferenced. The offset must be
added to a base pointer of a percpu area of a processor in order to
form a valid address.

Therefore the use of x or &x outside of the context of per cpu
operations is invalid and will generally be treated like a NULL
pointer dereference.

In the context of per cpu operations

	x is a per cpu variable. Most this_cpu operations take a cpu
	variable.

	&x is the *offset* a per cpu variable. this_cpu_ptr() takes
	the offset of a per cpu variable which makes this look a bit
	strange.



Operations on a field of a per cpu structure
--------------------------------------------

Let's say we have a percpu structure

	struct s {
		int n,m;
	};

	DEFINE_PER_CPU(struct s, p);


Operations on these fields are straightforward

	this_cpu_inc(p.m)

	z = this_cpu_cmpxchg(p.m, 0, 1);


If we have an offset to struct s:

	struct s __percpu *ps = &p;

	z = this_cpu_dec(ps->m);

	z = this_cpu_inc_return(ps->n);


The calculation of the pointer may require the use of this_cpu_ptr()
if we do not make use of this_cpu ops later to manipulate fields:

	struct s *pp;

	pp = this_cpu_ptr(&p);

	pp->m--;

	z = pp->n++;


Variants of this_cpu ops
-------------------------

this_cpu ops are interrupt safe. Some architecture do not support
these per cpu local operations. In that case the operation must be
replaced by code that disables interrupts, then does the operations
that are guaranteed to be atomic and then reenable interrupts. Doing
so is expensive. If there are other reasons why the scheduler cannot
change the processor we are executing on then there is no reason to
disable interrupts. For that purpose the __this_cpu operations are
provided. For example.

	__this_cpu_inc(x);

Will increment x and will not fallback to code that disables
interrupts on platforms that cannot accomplish atomicity through
address relocation and a Read-Modify-Write operation in the same
instruction.



&this_cpu_ptr(pp)->n vs this_cpu_ptr(&pp->n)
--------------------------------------------

The first operation takes the offset and forms an address and then
adds the offset of the n field.

The second one first adds the two offsets and then does the
relocation.  IMHO the second form looks cleaner and has an easier time
with (). The second form also is consistent with the way
this_cpu_read() and friends are used.


Christoph Lameter, April 3rd, 2013
UNALIGNED MEMORY ACCESSES
=========================

Linux runs on a wide variety of architectures which have varying behaviour
when it comes to memory access. This document presents some details about
unaligned accesses, why you need to write code that doesn't cause them,
and how to write such code!


The definition of an unaligned access
=====================================

Unaligned memory accesses occur when you try to read N bytes of data starting
from an address that is not evenly divisible by N (i.e. addr % N != 0).
For example, reading 4 bytes of data from address 0x10004 is fine, but
reading 4 bytes of data from address 0x10005 would be an unaligned memory
access.

The above may seem a little vague, as memory access can happen in different
ways. The context here is at the machine code level: certain instructions read
or write a number of bytes to or from memory (e.g. movb, movw, movl in x86
assembly). As will become clear, it is relatively easy to spot C statements
which will compile to multiple-byte memory access instructions, namely when
dealing with types such as u16, u32 and u64.


Natural alignment
=================

The rule mentioned above forms what we refer to as natural alignment:
When accessing N bytes of memory, the base memory address must be evenly
divisible by N, i.e. addr % N == 0.

When writing code, assume the target architecture has natural alignment
requirements.

In reality, only a few architectures require natural alignment on all sizes
of memory access. However, we must consider ALL supported architectures;
writing code that satisfies natural alignment requirements is the easiest way
to achieve full portability.


Why unaligned access is bad
===========================

The effects of performing an unaligned memory access vary from architecture
to architecture. It would be easy to write a whole document on the differences
here; a summary of the common scenarios is presented below:

 - Some architectures are able to perform unaligned memory accesses
   transparently, but there is usually a significant performance cost.
 - Some architectures raise processor exceptions when unaligned accesses
   happen. The exception handler is able to correct the unaligned access,
   at significant cost to performance.
 - Some architectures raise processor exceptions when unaligned accesses
   happen, but the exceptions do not contain enough information for the
   unaligned access to be corrected.
 - Some architectures are not capable of unaligned memory access, but will
   silently perform a different memory access to the one that was requested,
   resulting in a subtle code bug that is hard to detect!

It should be obvious from the above that if your code causes unaligned
memory accesses to happen, your code will not work correctly on certain
platforms and will cause performance problems on others.


Code that does not cause unaligned access
=========================================

At first, the concepts above may seem a little hard to relate to actual
coding practice. After all, you don't have a great deal of control over
memory addresses of certain variables, etc.

Fortunately things are not too complex, as in most cases, the compiler
ensures that things will work for you. For example, take the following
structure:

	struct foo {
		u16 field1;
		u32 field2;
		u8 field3;
	};

Let us assume that an instance of the above structure resides in memory
starting at address 0x10000. With a basic level of understanding, it would
not be unreasonable to expect that accessing field2 would cause an unaligned
access. You'd be expecting field2 to be located at offset 2 bytes into the
structure, i.e. address 0x10002, but that address is not evenly divisible
by 4 (remember, we're reading a 4 byte value here).

Fortunately, the compiler understands the alignment constraints, so in the
above case it would insert 2 bytes of padding in between field1 and field2.
Therefore, for standard structure types you can always rely on the compiler
to pad structures so that accesses to fields are suitably aligned (assuming
you do not cast the field to a type of different length).

Similarly, you can also rely on the compiler to align variables and function
parameters to a naturally aligned scheme, based on the size of the type of
the variable.

At this point, it should be clear that accessing a single byte (u8 or char)
will never cause an unaligned access, because all memory addresses are evenly
divisible by one.

On a related topic, with the above considerations in mind you may observe
that you could reorder the fields in the structure in order to place fields
where padding would otherwise be inserted, and hence reduce the overall
resident memory size of structure instances. The optimal layout of the
above example is:

	struct foo {
		u32 field2;
		u16 field1;
		u8 field3;
	};

For a natural alignment scheme, the compiler would only have to add a single
byte of padding at the end of the structure. This padding is added in order
to satisfy alignment constraints for arrays of these structures.

Another point worth mentioning is the use of __attribute__((packed)) on a
structure type. This GCC-specific attribute tells the compiler never to
insert any padding within structures, useful when you want to use a C struct
to represent some data that comes in a fixed arrangement 'off the wire'.

You might be inclined to believe that usage of this attribute can easily
lead to unaligned accesses when accessing fields that do not satisfy
architectural alignment requirements. However, again, the compiler is aware
of the alignment constraints and will generate extra instructions to perform
the memory access in a way that does not cause unaligned access. Of course,
the extra instructions obviously cause a loss in performance compared to the
non-packed case, so the packed attribute should only be used when avoiding
structure padding is of importance.


Code that causes unaligned access
=================================

With the above in mind, let's move onto a real life example of a function
that can cause an unaligned memory access. The following function taken
from include/linux/etherdevice.h is an optimized routine to compare two
ethernet MAC addresses for equality.

bool ether_addr_equal(const u8 *addr1, const u8 *addr2)
{
#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
	u32 fold = ((*(const u32 *)addr1) ^ (*(const u32 *)addr2)) |
		   ((*(const u16 *)(addr1 + 4)) ^ (*(const u16 *)(addr2 + 4)));

	return fold == 0;
#else
	const u16 *a = (const u16 *)addr1;
	const u16 *b = (const u16 *)addr2;
	return ((a[0] ^ b[0]) | (a[1] ^ b[1]) | (a[2] ^ b[2])) != 0;
#endif
}

In the above function, when the hardware has efficient unaligned access
capability, there is no issue with this code.  But when the hardware isn't
able to access memory on arbitrary boundaries, the reference to a[0] causes
2 bytes (16 bits) to be read from memory starting at address addr1.

Think about what would happen if addr1 was an odd address such as 0x10003.
(Hint: it'd be an unaligned access.)

Despite the potential unaligned access problems with the above function, it
is included in the kernel anyway but is understood to only work normally on
16-bit-aligned addresses. It is up to the caller to ensure this alignment or
not use this function at all. This alignment-unsafe function is still useful
as it is a decent optimization for the cases when you can ensure alignment,
which is true almost all of the time in ethernet networking context.


Here is another example of some code that could cause unaligned accesses:
	void myfunc(u8 *data, u32 value)
	{
		[...]
		*((u32 *) data) = cpu_to_le32(value);
		[...]
	}

This code will cause unaligned accesses every time the data parameter points
to an address that is not evenly divisible by 4.

In summary, the 2 main scenarios where you may run into unaligned access
problems involve:
 1. Casting variables to types of different lengths
 2. Pointer arithmetic followed by access to at least 2 bytes of data


Avoiding unaligned accesses
===========================

The easiest way to avoid unaligned access is to use the get_unaligned() and
put_unaligned() macros provided by the <asm/unaligned.h> header file.

Going back to an earlier example of code that potentially causes unaligned
access:

	void myfunc(u8 *data, u32 value)
	{
		[...]
		*((u32 *) data) = cpu_to_le32(value);
		[...]
	}

To avoid the unaligned memory access, you would rewrite it as follows:

	void myfunc(u8 *data, u32 value)
	{
		[...]
		value = cpu_to_le32(value);
		put_unaligned(value, (u32 *) data);
		[...]
	}

The get_unaligned() macro works similarly. Assuming 'data' is a pointer to
memory and you wish to avoid unaligned access, its usage is as follows:

	u32 value = get_unaligned((u32 *) data);

These macros work for memory accesses of any length (not just 32 bits as
in the examples above). Be aware that when compared to standard access of
aligned memory, using these macros to access unaligned memory can be costly in
terms of performance.

If use of such macros is not convenient, another option is to use memcpy(),
where the source or destination (or both) are of type u8* or unsigned char*.
Due to the byte-wise nature of this operation, unaligned accesses are avoided.


Alignment vs. Networking
========================

On architectures that require aligned loads, networking requires that the IP
header is aligned on a four-byte boundary to optimise the IP stack. For
regular ethernet hardware, the constant NET_IP_ALIGN is used. On most
architectures this constant has the value 2 because the normal ethernet
header is 14 bytes long, so in order to get proper alignment one needs to
DMA to an address which can be expressed as 4*n + 2. One notable exception
here is powerpc which defines NET_IP_ALIGN to 0 because DMA to unaligned
addresses can be very expensive and dwarf the cost of unaligned loads.

For some ethernet hardware that cannot DMA to unaligned addresses like
4*n+2 or non-ethernet hardware, this can be a problem, and it is then
required to copy the incoming frame into an aligned buffer. Because this is
unnecessary on architectures that can do unaligned accesses, the code can be
made dependent on CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS like so:

#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
	skb = original skb
#else
	skb = copy skb
#endif

--
Authors: Daniel Drake <dsd@gentoo.org>,
         Johannes Berg <johannes@sipsolutions.net>
With help from: Alan Cox, Avuton Olrich, Heikki Orsila, Jan Engelhardt,
Kyle McMartin, Kyle Moffett, Randy Dunlap, Robert Hancock, Uli Kunitz,
Vadim Lobanov

		 Last update: 2005-01-17, version 1.4

This file is maintained by H. Peter Anvin <unicode@lanana.org> as part
of the Linux Assigned Names And Numbers Authority (LANANA) project.
The current version can be found at:

	    http://www.lanana.org/docs/unicode/unicode.txt

		       ------------------------

The Linux kernel code has been rewritten to use Unicode to map
characters to fonts.  By downloading a single Unicode-to-font table,
both the eight-bit character sets and UTF-8 mode are changed to use
the font as indicated.

This changes the semantics of the eight-bit character tables subtly.
The four character tables are now:

Map symbol	Map name			Escape code (G0)

LAT1_MAP	Latin-1 (ISO 8859-1)		ESC ( B
GRAF_MAP	DEC VT100 pseudographics	ESC ( 0
IBMPC_MAP	IBM code page 437		ESC ( U
USER_MAP	User defined			ESC ( K

In particular, ESC ( U is no longer "straight to font", since the font
might be completely different than the IBM character set.  This
permits for example the use of block graphics even with a Latin-1 font
loaded.

Note that although these codes are similar to ISO 2022, neither the
codes nor their uses match ISO 2022; Linux has two 8-bit codes (G0 and
G1), whereas ISO 2022 has four 7-bit codes (G0-G3).

In accordance with the Unicode standard/ISO 10646 the range U+F000 to
U+F8FF has been reserved for OS-wide allocation (the Unicode Standard
refers to this as a "Corporate Zone", since this is inaccurate for
Linux we call it the "Linux Zone").  U+F000 was picked as the starting
point since it lets the direct-mapping area start on a large power of
two (in case 1024- or 2048-character fonts ever become necessary).
This leaves U+E000 to U+EFFF as End User Zone.

[v1.2]: The Unicodes range from U+F000 and up to U+F7FF have been
hard-coded to map directly to the loaded font, bypassing the
translation table.  The user-defined map now defaults to U+F000 to
U+F0FF, emulating the previous behaviour.  In practice, this range
might be shorter; for example, vgacon can only handle 256-character
(U+F000..U+F0FF) or 512-character (U+F000..U+F1FF) fonts.


Actual characters assigned in the Linux Zone
--------------------------------------------

In addition, the following characters not present in Unicode 1.1.4
have been defined; these are used by the DEC VT graphics map.  [v1.2]
THIS USE IS OBSOLETE AND SHOULD NO LONGER BE USED; PLEASE SEE BELOW.

U+F800 DEC VT GRAPHICS HORIZONTAL LINE SCAN 1
U+F801 DEC VT GRAPHICS HORIZONTAL LINE SCAN 3
U+F803 DEC VT GRAPHICS HORIZONTAL LINE SCAN 7
U+F804 DEC VT GRAPHICS HORIZONTAL LINE SCAN 9

The DEC VT220 uses a 6x10 character matrix, and these characters form
a smooth progression in the DEC VT graphics character set.  I have
omitted the scan 5 line, since it is also used as a block-graphics
character, and hence has been coded as U+2500 FORMS LIGHT HORIZONTAL.

[v1.3]: These characters have been officially added to Unicode 3.2.0;
they are added at U+23BA, U+23BB, U+23BC, U+23BD.  Linux now uses the
new values.

[v1.2]: The following characters have been added to represent common
keyboard symbols that are unlikely to ever be added to Unicode proper
since they are horribly vendor-specific.  This, of course, is an
excellent example of horrible design.

U+F810 KEYBOARD SYMBOL FLYING FLAG
U+F811 KEYBOARD SYMBOL PULLDOWN MENU
U+F812 KEYBOARD SYMBOL OPEN APPLE
U+F813 KEYBOARD SYMBOL SOLID APPLE

Klingon language support
------------------------

In 1996, Linux was the first operating system in the world to add
support for the artificial language Klingon, created by Marc Okrand
for the "Star Trek" television series.	This encoding was later
adopted by the ConScript Unicode Registry and proposed (but ultimately
rejected) for inclusion in Unicode Plane 1.  Thus, it remains as a
Linux/CSUR private assignment in the Linux Zone.

This encoding has been endorsed by the Klingon Language Institute.
For more information, contact them at:

	http://www.kli.org/

Since the characters in the beginning of the Linux CZ have been more
of the dingbats/symbols/forms type and this is a language, I have
located it at the end, on a 16-cell boundary in keeping with standard
Unicode practice.

NOTE: This range is now officially managed by the ConScript Unicode
Registry.  The normative reference is at:

	http://www.evertype.com/standards/csur/klingon.html

Klingon has an alphabet of 26 characters, a positional numeric writing
system with 10 digits, and is written left-to-right, top-to-bottom.

Several glyph forms for the Klingon alphabet have been proposed.
However, since the set of symbols appear to be consistent throughout,
with only the actual shapes being different, in keeping with standard
Unicode practice these differences are considered font variants.

U+F8D0	KLINGON LETTER A
U+F8D1	KLINGON LETTER B
U+F8D2	KLINGON LETTER CH
U+F8D3	KLINGON LETTER D
U+F8D4	KLINGON LETTER E
U+F8D5	KLINGON LETTER GH
U+F8D6	KLINGON LETTER H
U+F8D7	KLINGON LETTER I
U+F8D8	KLINGON LETTER J
U+F8D9	KLINGON LETTER L
U+F8DA	KLINGON LETTER M
U+F8DB	KLINGON LETTER N
U+F8DC	KLINGON LETTER NG
U+F8DD	KLINGON LETTER O
U+F8DE	KLINGON LETTER P
U+F8DF	KLINGON LETTER Q
	- Written <q> in standard Okrand Latin transliteration
U+F8E0	KLINGON LETTER QH
	- Written <Q> in standard Okrand Latin transliteration
U+F8E1	KLINGON LETTER R
U+F8E2	KLINGON LETTER S
U+F8E3	KLINGON LETTER T
U+F8E4	KLINGON LETTER TLH
U+F8E5	KLINGON LETTER U
U+F8E6	KLINGON LETTER V
U+F8E7	KLINGON LETTER W
U+F8E8	KLINGON LETTER Y
U+F8E9	KLINGON LETTER GLOTTAL STOP

U+F8F0	KLINGON DIGIT ZERO
U+F8F1	KLINGON DIGIT ONE
U+F8F2	KLINGON DIGIT TWO
U+F8F3	KLINGON DIGIT THREE
U+F8F4	KLINGON DIGIT FOUR
U+F8F5	KLINGON DIGIT FIVE
U+F8F6	KLINGON DIGIT SIX
U+F8F7	KLINGON DIGIT SEVEN
U+F8F8	KLINGON DIGIT EIGHT
U+F8F9	KLINGON DIGIT NINE

U+F8FD	KLINGON COMMA
U+F8FE	KLINGON FULL STOP
U+F8FF	KLINGON SYMBOL FOR EMPIRE

Other Fictional and Artificial Scripts
--------------------------------------

Since the assignment of the Klingon Linux Unicode block, a registry of
fictional and artificial scripts has been established by John Cowan
<jcowan@reutershealth.com> and Michael Everson <everson@evertype.com>.
The ConScript Unicode Registry is accessible at:

	  http://www.evertype.com/standards/csur/

The ranges used fall at the low end of the End User Zone and can hence
not be normatively assigned, but it is recommended that people who
wish to encode fictional scripts use these codes, in the interest of
interoperability.  For Klingon, CSUR has adopted the Linux encoding.
The CSUR people are driving adding Tengwar and Cirth into Unicode
Plane 1; the addition of Klingon to Unicode Plane 1 has been rejected
and so the above encoding remains official.

unshare system call:
--------------------
This document describes the new system call, unshare. The document
provides an overview of the feature, why it is needed, how it can
be used, its interface specification, design, implementation and
how it can be tested.

Change Log:
-----------
version 0.1  Initial document, Janak Desai (janak@us.ibm.com), Jan 11, 2006

Contents:
---------
	1) Overview
	2) Benefits
	3) Cost
	4) Requirements
	5) Functional Specification
	6) High Level Design
	7) Low Level Design
	8) Test Specification
	9) Future Work

1) Overview
-----------
Most legacy operating system kernels support an abstraction of threads
as multiple execution contexts within a process. These kernels provide
special resources and mechanisms to maintain these "threads". The Linux
kernel, in a clever and simple manner, does not make distinction
between processes and "threads". The kernel allows processes to share
resources and thus they can achieve legacy "threads" behavior without
requiring additional data structures and mechanisms in the kernel. The
power of implementing threads in this manner comes not only from
its simplicity but also from allowing application programmers to work
outside the confinement of all-or-nothing shared resources of legacy
threads. On Linux, at the time of thread creation using the clone system
call, applications can selectively choose which resources to share
between threads.

unshare system call adds a primitive to the Linux thread model that
allows threads to selectively 'unshare' any resources that were being
shared at the time of their creation. unshare was conceptualized by
Al Viro in the August of 2000, on the Linux-Kernel mailing list, as part
of the discussion on POSIX threads on Linux.  unshare augments the
usefulness of Linux threads for applications that would like to control
shared resources without creating a new process. unshare is a natural
addition to the set of available primitives on Linux that implement
the concept of process/thread as a virtual machine.

2) Benefits
-----------
unshare would be useful to large application frameworks such as PAM
where creating a new process to control sharing/unsharing of process
resources is not possible. Since namespaces are shared by default
when creating a new process using fork or clone, unshare can benefit
even non-threaded applications if they have a need to disassociate
from default shared namespace. The following lists two use-cases
where unshare can be used.

2.1 Per-security context namespaces
-----------------------------------
unshare can be used to implement polyinstantiated directories using
the kernel's per-process namespace mechanism. Polyinstantiated directories,
such as per-user and/or per-security context instance of /tmp, /var/tmp or
per-security context instance of a user's home directory, isolate user
processes when working with these directories. Using unshare, a PAM
module can easily setup a private namespace for a user at login.
Polyinstantiated directories are required for Common Criteria certification
with Labeled System Protection Profile, however, with the availability
of shared-tree feature in the Linux kernel, even regular Linux systems
can benefit from setting up private namespaces at login and
polyinstantiating /tmp, /var/tmp and other directories deemed
appropriate by system administrators.

2.2 unsharing of virtual memory and/or open files
-------------------------------------------------
Consider a client/server application where the server is processing
client requests by creating processes that share resources such as
virtual memory and open files. Without unshare, the server has to
decide what needs to be shared at the time of creating the process
which services the request. unshare allows the server an ability to
disassociate parts of the context during the servicing of the
request. For large and complex middleware application frameworks, this
ability to unshare after the process was created can be very
useful.

3) Cost
-------
In order to not duplicate code and to handle the fact that unshare
works on an active task (as opposed to clone/fork working on a newly
allocated inactive task) unshare had to make minor reorganizational
changes to copy_* functions utilized by clone/fork system call.
There is a cost associated with altering existing, well tested and
stable code to implement a new feature that may not get exercised
extensively in the beginning. However, with proper design and code
review of the changes and creation of an unshare test for the LTP
the benefits of this new feature can exceed its cost.

4) Requirements
---------------
unshare reverses sharing that was done using clone(2) system call,
so unshare should have a similar interface as clone(2). That is,
since flags in clone(int flags, void *stack) specifies what should
be shared, similar flags in unshare(int flags) should specify
what should be unshared. Unfortunately, this may appear to invert
the meaning of the flags from the way they are used in clone(2).
However, there was no easy solution that was less confusing and that
allowed incremental context unsharing in future without an ABI change.

unshare interface should accommodate possible future addition of
new context flags without requiring a rebuild of old applications.
If and when new context flags are added, unshare design should allow
incremental unsharing of those resources on an as needed basis.

5) Functional Specification
---------------------------
NAME
	unshare - disassociate parts of the process execution context

SYNOPSIS
	#include <sched.h>

	int unshare(int flags);

DESCRIPTION
	unshare allows a process to disassociate parts of its execution
	context that are currently being shared with other processes. Part
	of execution context, such as the namespace, is shared by default
	when a new process is created using fork(2), while other parts,
	such as the virtual memory, open file descriptors, etc, may be
	shared by explicit request to share them when creating a process
	using clone(2).

	The main use of unshare is to allow a process to control its
	shared execution context without creating a new process.

	The flags argument specifies one or bitwise-or'ed of several of
	the following constants.

	CLONE_FS
		If CLONE_FS is set, file system information of the caller
		is disassociated from the shared file system information.

	CLONE_FILES
		If CLONE_FILES is set, the file descriptor table of the
		caller is disassociated from the shared file descriptor
		table.

	CLONE_NEWNS
		If CLONE_NEWNS is set, the namespace of the caller is
		disassociated from the shared namespace.

	CLONE_VM
		If CLONE_VM is set, the virtual memory of the caller is
		disassociated from the shared virtual memory.

RETURN VALUE
	On success, zero returned. On failure, -1 is returned and errno is

ERRORS
	EPERM	CLONE_NEWNS was specified by a non-root process (process
		without CAP_SYS_ADMIN).

	ENOMEM	Cannot allocate sufficient memory to copy parts of caller's
		context that need to be unshared.

	EINVAL	Invalid flag was specified as an argument.

CONFORMING TO
	The unshare() call is Linux-specific and  should  not be used
	in programs intended to be portable.

SEE ALSO
	clone(2), fork(2)

6) High Level Design
--------------------
Depending on the flags argument, the unshare system call allocates
appropriate process context structures, populates it with values from
the current shared version, associates newly duplicated structures
with the current task structure and releases corresponding shared
versions. Helper functions of clone (copy_*) could not be used
directly by unshare because of the following two reasons.
  1) clone operates on a newly allocated not-yet-active task
     structure, where as unshare operates on the current active
     task. Therefore unshare has to take appropriate task_lock()
     before associating newly duplicated context structures
  2) unshare has to allocate and duplicate all context structures
     that are being unshared, before associating them with the
     current task and releasing older shared structures. Failure
     do so will create race conditions and/or oops when trying
     to backout due to an error. Consider the case of unsharing
     both virtual memory and namespace. After successfully unsharing
     vm, if the system call encounters an error while allocating
     new namespace structure, the error return code will have to
     reverse the unsharing of vm. As part of the reversal the
     system call will have to go back to older, shared, vm
     structure, which may not exist anymore.

Therefore code from copy_* functions that allocated and duplicated
current context structure was moved into new dup_* functions. Now,
copy_* functions call dup_* functions to allocate and duplicate
appropriate context structures and then associate them with the
task structure that is being constructed. unshare system call on
the other hand performs the following:
  1) Check flags to force missing, but implied, flags
  2) For each context structure, call the corresponding unshare
     helper function to allocate and duplicate a new context
     structure, if the appropriate bit is set in the flags argument.
  3) If there is no error in allocation and duplication and there
     are new context structures then lock the current task structure,
     associate new context structures with the current task structure,
     and release the lock on the current task structure.
  4) Appropriately release older, shared, context structures.

7) Low Level Design
-------------------
Implementation of unshare can be grouped in the following 4 different
items:
  a) Reorganization of existing copy_* functions
  b) unshare system call service function
  c) unshare helper functions for each different process context
  d) Registration of system call number for different architectures

  7.1) Reorganization of copy_* functions
       Each copy function such as copy_mm, copy_namespace, copy_files,
       etc, had roughly two components. The first component allocated
       and duplicated the appropriate structure and the second component
       linked it to the task structure passed in as an argument to the copy
       function. The first component was split into its own function.
       These dup_* functions allocated and duplicated the appropriate
       context structure. The reorganized copy_* functions invoked
       their corresponding dup_* functions and then linked the newly
       duplicated structures to the task structure with which the
       copy function was called.

  7.2) unshare system call service function
       * Check flags
	 Force implied flags. If CLONE_THREAD is set force CLONE_VM.
	 If CLONE_VM is set, force CLONE_SIGHAND. If CLONE_SIGHAND is
	 set and signals are also being shared, force CLONE_THREAD. If
	 CLONE_NEWNS is set, force CLONE_FS.
       * For each context flag, invoke the corresponding unshare_*
	 helper routine with flags passed into the system call and a
	 reference to pointer pointing the new unshared structure
       * If any new structures are created by unshare_* helper
	 functions, take the task_lock() on the current task,
	 modify appropriate context pointers, and release the
         task lock.
       * For all newly unshared structures, release the corresponding
         older, shared, structures.

  7.3) unshare_* helper functions
       For unshare_* helpers corresponding to CLONE_SYSVSEM, CLONE_SIGHAND,
       and CLONE_THREAD, return -EINVAL since they are not implemented yet.
       For others, check the flag value to see if the unsharing is
       required for that structure. If it is, invoke the corresponding
       dup_* function to allocate and duplicate the structure and return
       a pointer to it.

  7.4) Appropriately modify architecture specific code to register the
       new system call.

8) Test Specification
---------------------
The test for unshare should test the following:
  1) Valid flags: Test to check that clone flags for signal and
	signal handlers, for which unsharing is not implemented
	yet, return -EINVAL.
  2) Missing/implied flags: Test to make sure that if unsharing
	namespace without specifying unsharing of filesystem, correctly
	unshares both namespace and filesystem information.
  3) For each of the four (namespace, filesystem, files and vm)
	supported unsharing, verify that the system call correctly
	unshares the appropriate structure. Verify that unsharing
	them individually as well as in combination with each
	other works as expected.
  4) Concurrent execution: Use shared memory segments and futex on
	an address in the shm segment to synchronize execution of
	about 10 threads. Have a couple of threads execute execve,
	a couple _exit and the rest unshare with different combination
	of flags. Verify that unsharing is performed as expected and
	that there are no oops or hangs.

9) Future Work
--------------
The current implementation of unshare does not allow unsharing of
signals and signal handlers. Signals are complex to begin with and
to unshare signals and/or signal handlers of a currently running
process is even more complex. If in the future there is a specific
need to allow unsharing of signals and/or signal handlers, it can
be incrementally added to unshare without affecting legacy
applications using unshare.

VFIO - "Virtual Function I/O"[1]
-------------------------------------------------------------------------------
Many modern system now provide DMA and interrupt remapping facilities
to help ensure I/O devices behave within the boundaries they've been
allotted.  This includes x86 hardware with AMD-Vi and Intel VT-d,
POWER systems with Partitionable Endpoints (PEs) and embedded PowerPC
systems such as Freescale PAMU.  The VFIO driver is an IOMMU/device
agnostic framework for exposing direct device access to userspace, in
a secure, IOMMU protected environment.  In other words, this allows
safe[2], non-privileged, userspace drivers.

Why do we want that?  Virtual machines often make use of direct device
access ("device assignment") when configured for the highest possible
I/O performance.  From a device and host perspective, this simply
turns the VM into a userspace driver, with the benefits of
significantly reduced latency, higher bandwidth, and direct use of
bare-metal device drivers[3].

Some applications, particularly in the high performance computing
field, also benefit from low-overhead, direct device access from
userspace.  Examples include network adapters (often non-TCP/IP based)
and compute accelerators.  Prior to VFIO, these drivers had to either
go through the full development cycle to become proper upstream
driver, be maintained out of tree, or make use of the UIO framework,
which has no notion of IOMMU protection, limited interrupt support,
and requires root privileges to access things like PCI configuration
space.

The VFIO driver framework intends to unify these, replacing both the
KVM PCI specific device assignment code as well as provide a more
secure, more featureful userspace driver environment than UIO.

Groups, Devices, and IOMMUs
-------------------------------------------------------------------------------

Devices are the main target of any I/O driver.  Devices typically
create a programming interface made up of I/O access, interrupts,
and DMA.  Without going into the details of each of these, DMA is
by far the most critical aspect for maintaining a secure environment
as allowing a device read-write access to system memory imposes the
greatest risk to the overall system integrity.

To help mitigate this risk, many modern IOMMUs now incorporate
isolation properties into what was, in many cases, an interface only
meant for translation (ie. solving the addressing problems of devices
with limited address spaces).  With this, devices can now be isolated
from each other and from arbitrary memory access, thus allowing
things like secure direct assignment of devices into virtual machines.

This isolation is not always at the granularity of a single device
though.  Even when an IOMMU is capable of this, properties of devices,
interconnects, and IOMMU topologies can each reduce this isolation.
For instance, an individual device may be part of a larger multi-
function enclosure.  While the IOMMU may be able to distinguish
between devices within the enclosure, the enclosure may not require
transactions between devices to reach the IOMMU.  Examples of this
could be anything from a multi-function PCI device with backdoors
between functions to a non-PCI-ACS (Access Control Services) capable
bridge allowing redirection without reaching the IOMMU.  Topology
can also play a factor in terms of hiding devices.  A PCIe-to-PCI
bridge masks the devices behind it, making transaction appear as if
from the bridge itself.  Obviously IOMMU design plays a major factor
as well.

Therefore, while for the most part an IOMMU may have device level
granularity, any system is susceptible to reduced granularity.  The
IOMMU API therefore supports a notion of IOMMU groups.  A group is
a set of devices which is isolatable from all other devices in the
system.  Groups are therefore the unit of ownership used by VFIO.

While the group is the minimum granularity that must be used to
ensure secure user access, it's not necessarily the preferred
granularity.  In IOMMUs which make use of page tables, it may be
possible to share a set of page tables between different groups,
reducing the overhead both to the platform (reduced TLB thrashing,
reduced duplicate page tables), and to the user (programming only
a single set of translations).  For this reason, VFIO makes use of
a container class, which may hold one or more groups.  A container
is created by simply opening the /dev/vfio/vfio character device.

On its own, the container provides little functionality, with all
but a couple version and extension query interfaces locked away.
The user needs to add a group into the container for the next level
of functionality.  To do this, the user first needs to identify the
group associated with the desired device.  This can be done using
the sysfs links described in the example below.  By unbinding the
device from the host driver and binding it to a VFIO driver, a new
VFIO group will appear for the group as /dev/vfio/$GROUP, where
$GROUP is the IOMMU group number of which the device is a member.
If the IOMMU group contains multiple devices, each will need to
be bound to a VFIO driver before operations on the VFIO group
are allowed (it's also sufficient to only unbind the device from
host drivers if a VFIO driver is unavailable; this will make the
group available, but not that particular device).  TBD - interface
for disabling driver probing/locking a device.

Once the group is ready, it may be added to the container by opening
the VFIO group character device (/dev/vfio/$GROUP) and using the
VFIO_GROUP_SET_CONTAINER ioctl, passing the file descriptor of the
previously opened container file.  If desired and if the IOMMU driver
supports sharing the IOMMU context between groups, multiple groups may
be set to the same container.  If a group fails to set to a container
with existing groups, a new empty container will need to be used
instead.

With a group (or groups) attached to a container, the remaining
ioctls become available, enabling access to the VFIO IOMMU interfaces.
Additionally, it now becomes possible to get file descriptors for each
device within a group using an ioctl on the VFIO group file descriptor.

The VFIO device API includes ioctls for describing the device, the I/O
regions and their read/write/mmap offsets on the device descriptor, as
well as mechanisms for describing and registering interrupt
notifications.

VFIO Usage Example
-------------------------------------------------------------------------------

Assume user wants to access PCI device 0000:06:0d.0

$ readlink /sys/bus/pci/devices/0000:06:0d.0/iommu_group
../../../../kernel/iommu_groups/26

This device is therefore in IOMMU group 26.  This device is on the
pci bus, therefore the user will make use of vfio-pci to manage the
group:

# modprobe vfio-pci

Binding this device to the vfio-pci driver creates the VFIO group
character devices for this group:

$ lspci -n -s 0000:06:0d.0
06:0d.0 0401: 1102:0002 (rev 08)
# echo 0000:06:0d.0 > /sys/bus/pci/devices/0000:06:0d.0/driver/unbind
# echo 1102 0002 > /sys/bus/pci/drivers/vfio-pci/new_id

Now we need to look at what other devices are in the group to free
it for use by VFIO:

$ ls -l /sys/bus/pci/devices/0000:06:0d.0/iommu_group/devices
total 0
lrwxrwxrwx. 1 root root 0 Apr 23 16:13 0000:00:1e.0 ->
	../../../../devices/pci0000:00/0000:00:1e.0
lrwxrwxrwx. 1 root root 0 Apr 23 16:13 0000:06:0d.0 ->
	../../../../devices/pci0000:00/0000:00:1e.0/0000:06:0d.0
lrwxrwxrwx. 1 root root 0 Apr 23 16:13 0000:06:0d.1 ->
	../../../../devices/pci0000:00/0000:00:1e.0/0000:06:0d.1

This device is behind a PCIe-to-PCI bridge[4], therefore we also
need to add device 0000:06:0d.1 to the group following the same
procedure as above.  Device 0000:00:1e.0 is a bridge that does
not currently have a host driver, therefore it's not required to
bind this device to the vfio-pci driver (vfio-pci does not currently
support PCI bridges).

The final step is to provide the user with access to the group if
unprivileged operation is desired (note that /dev/vfio/vfio provides
no capabilities on its own and is therefore expected to be set to
mode 0666 by the system).

# chown user:user /dev/vfio/26

The user now has full access to all the devices and the iommu for this
group and can access them as follows:

	int container, group, device, i;
	struct vfio_group_status group_status =
					{ .argsz = sizeof(group_status) };
	struct vfio_iommu_type1_info iommu_info = { .argsz = sizeof(iommu_info) };
	struct vfio_iommu_type1_dma_map dma_map = { .argsz = sizeof(dma_map) };
	struct vfio_device_info device_info = { .argsz = sizeof(device_info) };

	/* Create a new container */
	container = open("/dev/vfio/vfio", O_RDWR);

	if (ioctl(container, VFIO_GET_API_VERSION) != VFIO_API_VERSION)
		/* Unknown API version */

	if (!ioctl(container, VFIO_CHECK_EXTENSION, VFIO_TYPE1_IOMMU))
		/* Doesn't support the IOMMU driver we want. */

	/* Open the group */
	group = open("/dev/vfio/26", O_RDWR);

	/* Test the group is viable and available */
	ioctl(group, VFIO_GROUP_GET_STATUS, &group_status);

	if (!(group_status.flags & VFIO_GROUP_FLAGS_VIABLE))
		/* Group is not viable (ie, not all devices bound for vfio) */

	/* Add the group to the container */
	ioctl(group, VFIO_GROUP_SET_CONTAINER, &container);

	/* Enable the IOMMU model we want */
	ioctl(container, VFIO_SET_IOMMU, VFIO_TYPE1_IOMMU);

	/* Get addition IOMMU info */
	ioctl(container, VFIO_IOMMU_GET_INFO, &iommu_info);

	/* Allocate some space and setup a DMA mapping */
	dma_map.vaddr = mmap(0, 1024 * 1024, PROT_READ | PROT_WRITE,
			     MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
	dma_map.size = 1024 * 1024;
	dma_map.iova = 0; /* 1MB starting at 0x0 from device view */
	dma_map.flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE;

	ioctl(container, VFIO_IOMMU_MAP_DMA, &dma_map);

	/* Get a file descriptor for the device */
	device = ioctl(group, VFIO_GROUP_GET_DEVICE_FD, "0000:06:0d.0");

	/* Test and setup the device */
	ioctl(device, VFIO_DEVICE_GET_INFO, &device_info);

	for (i = 0; i < device_info.num_regions; i++) {
		struct vfio_region_info reg = { .argsz = sizeof(reg) };

		reg.index = i;

		ioctl(device, VFIO_DEVICE_GET_REGION_INFO, &reg);

		/* Setup mappings... read/write offsets, mmaps
		 * For PCI devices, config space is a region */
	}

	for (i = 0; i < device_info.num_irqs; i++) {
		struct vfio_irq_info irq = { .argsz = sizeof(irq) };

		irq.index = i;

		ioctl(device, VFIO_DEVICE_GET_IRQ_INFO, &irq);

		/* Setup IRQs... eventfds, VFIO_DEVICE_SET_IRQS */
	}

	/* Gratuitous device reset and go... */
	ioctl(device, VFIO_DEVICE_RESET);

VFIO User API
-------------------------------------------------------------------------------

Please see include/linux/vfio.h for complete API documentation.

VFIO bus driver API
-------------------------------------------------------------------------------

VFIO bus drivers, such as vfio-pci make use of only a few interfaces
into VFIO core.  When devices are bound and unbound to the driver,
the driver should call vfio_add_group_dev() and vfio_del_group_dev()
respectively:

extern int vfio_add_group_dev(struct iommu_group *iommu_group,
                              struct device *dev,
                              const struct vfio_device_ops *ops,
                              void *device_data);

extern void *vfio_del_group_dev(struct device *dev);

vfio_add_group_dev() indicates to the core to begin tracking the
specified iommu_group and register the specified dev as owned by
a VFIO bus driver.  The driver provides an ops structure for callbacks
similar to a file operations structure:

struct vfio_device_ops {
	int	(*open)(void *device_data);
	void	(*release)(void *device_data);
	ssize_t	(*read)(void *device_data, char __user *buf,
			size_t count, loff_t *ppos);
	ssize_t	(*write)(void *device_data, const char __user *buf,
			 size_t size, loff_t *ppos);
	long	(*ioctl)(void *device_data, unsigned int cmd,
			 unsigned long arg);
	int	(*mmap)(void *device_data, struct vm_area_struct *vma);
};

Each function is passed the device_data that was originally registered
in the vfio_add_group_dev() call above.  This allows the bus driver
an easy place to store its opaque, private data.  The open/release
callbacks are issued when a new file descriptor is created for a
device (via VFIO_GROUP_GET_DEVICE_FD).  The ioctl interface provides
a direct pass through for VFIO_DEVICE_* ioctls.  The read/write/mmap
interfaces implement the device region access defined by the device's
own VFIO_DEVICE_GET_REGION_INFO ioctl.


PPC64 sPAPR implementation note
-------------------------------------------------------------------------------

This implementation has some specifics:

1) Only one IOMMU group per container is supported as an IOMMU group
represents the minimal entity which isolation can be guaranteed for and
groups are allocated statically, one per a Partitionable Endpoint (PE)
(PE is often a PCI domain but not always).

2) The hardware supports so called DMA windows - the PCI address range
within which DMA transfer is allowed, any attempt to access address space
out of the window leads to the whole PE isolation.

3) PPC64 guests are paravirtualized but not fully emulated. There is an API
to map/unmap pages for DMA, and it normally maps 1..32 pages per call and
currently there is no way to reduce the number of calls. In order to make things
faster, the map/unmap handling has been implemented in real mode which provides
an excellent performance which has limitations such as inability to do
locked pages accounting in real time.

So 3 additional ioctls have been added:

	VFIO_IOMMU_SPAPR_TCE_GET_INFO - returns the size and the start
		of the DMA window on the PCI bus.

	VFIO_IOMMU_ENABLE - enables the container. The locked pages accounting
		is done at this point. This lets user first to know what
		the DMA window is and adjust rlimit before doing any real job.

	VFIO_IOMMU_DISABLE - disables the container.


The code flow from the example above should be slightly changed:

	.....
	/* Add the group to the container */
	ioctl(group, VFIO_GROUP_SET_CONTAINER, &container);

	/* Enable the IOMMU model we want */
	ioctl(container, VFIO_SET_IOMMU, VFIO_SPAPR_TCE_IOMMU)

	/* Get addition sPAPR IOMMU info */
	vfio_iommu_spapr_tce_info spapr_iommu_info;
	ioctl(container, VFIO_IOMMU_SPAPR_TCE_GET_INFO, &spapr_iommu_info);

	if (ioctl(container, VFIO_IOMMU_ENABLE))
		/* Cannot enable container, may be low rlimit */

	/* Allocate some space and setup a DMA mapping */
	dma_map.vaddr = mmap(0, 1024 * 1024, PROT_READ | PROT_WRITE,
			     MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);

	dma_map.size = 1024 * 1024;
	dma_map.iova = 0; /* 1MB starting at 0x0 from device view */
	dma_map.flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE;

	/* Check here is .iova/.size are within DMA window from spapr_iommu_info */

	ioctl(container, VFIO_IOMMU_MAP_DMA, &dma_map);
	.....

-------------------------------------------------------------------------------

[1] VFIO was originally an acronym for "Virtual Function I/O" in its
initial implementation by Tom Lyon while as Cisco.  We've since
outgrown the acronym, but it's catchy.

[2] "safe" also depends upon a device being "well behaved".  It's
possible for multi-function devices to have backdoors between
functions and even for single function devices to have alternative
access to things like PCI config space through MMIO registers.  To
guard against the former we can include additional precautions in the
IOMMU driver to group multi-function PCI devices together
(iommu=group_mf).  The latter we can't prevent, but the IOMMU should
still provide isolation.  For PCI, SR-IOV Virtual Functions are the
best indicator of "well behaved", as these are designed for
virtualization usage models.

[3] As always there are trade-offs to virtual machine device
assignment that are beyond the scope of VFIO.  It's expected that
future IOMMU technologies will reduce some, but maybe not all, of
these trade-offs.

[4] In this case the device is below a PCI bridge, so transactions
from either function of the device are indistinguishable to the iommu:

-[0000:00]-+-1e.0-[06]--+-0d.0
                        \-0d.1

00:1e.0 PCI bridge: Intel Corporation 82801 PCI Bridge (rev 90)

VGA Arbiter
===========

Graphic devices are accessed through ranges in I/O or memory space. While most
modern devices allow relocation of such ranges, some "Legacy" VGA devices
implemented on PCI will typically have the same "hard-decoded" addresses as
they did on ISA. For more details see "PCI Bus Binding to IEEE Std 1275-1994
Standard for Boot (Initialization Configuration) Firmware Revision 2.1"
Section 7, Legacy Devices.

The Resource Access Control (RAC) module inside the X server [0] existed for
the legacy VGA arbitration task (besides other bus management tasks) when more
than one legacy device co-exists on the same machine. But the problem happens
when these devices are trying to be accessed by different userspace clients
(e.g. two server in parallel). Their address assignments conflict. Moreover,
ideally, being a userspace application, it is not the role of the X server to
control bus resources. Therefore an arbitration scheme outside of the X server
is needed to control the sharing of these resources. This document introduces
the operation of the VGA arbiter implemented for the Linux kernel.

----------------------------------------------------------------------------

I.  Details and Theory of Operation
        I.1 vgaarb
        I.2 libpciaccess
        I.3 xf86VGAArbiter (X server implementation)
II. Credits
III.References


I. Details and Theory of Operation
==================================

I.1 vgaarb
----------

The vgaarb is a module of the Linux Kernel. When it is initially loaded, it
scans all PCI devices and adds the VGA ones inside the arbitration. The
arbiter then enables/disables the decoding on different devices of the VGA
legacy instructions. Devices which do not want/need to use the arbiter may
explicitly tell it by calling vga_set_legacy_decoding().

The kernel exports a char device interface (/dev/vga_arbiter) to the clients,
which has the following semantics:

 open       : open user instance of the arbiter. By default, it's attached to
              the default VGA device of the system.

 close      : close user instance. Release locks made by the user

 read       : return a string indicating the status of the target like:

              "<card_ID>,decodes=<io_state>,owns=<io_state>,locks=<io_state> (ic,mc)"

              An IO state string is of the form {io,mem,io+mem,none}, mc and
              ic are respectively mem and io lock counts (for debugging/
              diagnostic only). "decodes" indicate what the card currently
              decodes, "owns" indicates what is currently enabled on it, and
              "locks" indicates what is locked by this card. If the card is
              unplugged, we get "invalid" then for card_ID and an -ENODEV
              error is returned for any command until a new card is targeted.


 write       : write a command to the arbiter. List of commands:

  target <card_ID>   : switch target to card <card_ID> (see below)
  lock <io_state>    : acquires locks on target ("none" is an invalid io_state)
  trylock <io_state> : non-blocking acquire locks on target (returns EBUSY if
                       unsuccessful)
  unlock <io_state>  : release locks on target
  unlock all         : release all locks on target held by this user (not
                       implemented yet)
  decodes <io_state> : set the legacy decoding attributes for the card

  poll               : event if something changes on any card (not just the
                       target)

  card_ID is of the form "PCI:domain:bus:dev.fn". It can be set to "default"
  to go back to the system default card (TODO: not implemented yet). Currently,
  only PCI is supported as a prefix, but the userland API may support other bus
  types in the future, even if the current kernel implementation doesn't.

Note about locks:

The driver keeps track of which user has which locks on which card. It
supports stacking, like the kernel one. This complexifies the implementation
a bit, but makes the arbiter more tolerant to user space problems and able
to properly cleanup in all cases when a process dies.
Currently, a max of 16 cards can have locks simultaneously issued from
user space for a given user (file descriptor instance) of the arbiter.

In the case of devices hot-{un,}plugged, there is a hook - pci_notify() - to
notify them being added/removed in the system and automatically added/removed
in the arbiter.

There is also an in-kernel API of the arbiter in case DRM, vgacon, or other
drivers want to use it.


I.2 libpciaccess
----------------

To use the vga arbiter char device it was implemented an API inside the
libpciaccess library. One field was added to struct pci_device (each device
on the system):

    /* the type of resource decoded by the device */
    int vgaarb_rsrc;

Besides it, in pci_system were added:

    int vgaarb_fd;
    int vga_count;
    struct pci_device *vga_target;
    struct pci_device *vga_default_dev;


The vga_count is used to track how many cards are being arbitrated, so for
instance, if there is only one card, then it can completely escape arbitration.


These functions below acquire VGA resources for the given card and mark those
resources as locked. If the resources requested are "normal" (and not legacy)
resources, the arbiter will first check whether the card is doing legacy
decoding for that type of resource. If yes, the lock is "converted" into a
legacy resource lock. The arbiter will first look for all VGA cards that
might conflict and disable their IOs and/or Memory access, including VGA
forwarding on P2P bridges if necessary, so that the requested resources can
be used. Then, the card is marked as locking these resources and the IO and/or
Memory access is enabled on the card (including VGA forwarding on parent
P2P bridges if any). In the case of vga_arb_lock(), the function will block
if some conflicting card is already locking one of the required resources (or
any resource on a different bus segment, since P2P bridges don't differentiate
VGA memory and IO afaik). If the card already owns the resources, the function
succeeds.  vga_arb_trylock() will return (-EBUSY) instead of blocking. Nested
calls are supported (a per-resource counter is maintained).


Set the target device of this client.
    int  pci_device_vgaarb_set_target   (struct pci_device *dev);


For instance, in x86 if two devices on the same bus want to lock different
resources, both will succeed (lock). If devices are in different buses and
trying to lock different resources, only the first who tried succeeds.
    int  pci_device_vgaarb_lock         (void);
    int  pci_device_vgaarb_trylock      (void);

Unlock resources of device.
    int  pci_device_vgaarb_unlock       (void);

Indicates to the arbiter if the card decodes legacy VGA IOs, legacy VGA
Memory, both, or none. All cards default to both, the card driver (fbdev for
example) should tell the arbiter if it has disabled legacy decoding, so the
card can be left out of the arbitration process (and can be safe to take
interrupts at any time.
    int  pci_device_vgaarb_decodes      (int new_vgaarb_rsrc);

Connects to the arbiter device, allocates the struct
    int  pci_device_vgaarb_init         (void);

Close the connection
    void pci_device_vgaarb_fini         (void);


I.3 xf86VGAArbiter (X server implementation)
--------------------------------------------

(TODO)

X server basically wraps all the functions that touch VGA registers somehow.


II. Credits
===========

Benjamin Herrenschmidt (IBM?) started this work when he discussed such design
with the Xorg community in 2005 [1, 2]. In the end of 2007, Paulo Zanoni and
Tiago Vignatti (both of C3SL/Federal University of Paraná) proceeded his work
enhancing the kernel code to adapt as a kernel module and also did the
implementation of the user space side [3]. Now (2009) Tiago Vignatti and Dave
Airlie finally put this work in shape and queued to Jesse Barnes' PCI tree.


III. References
==============

[0] http://cgit.freedesktop.org/xorg/xserver/commit/?id=4b42448a2388d40f257774fbffdccaea87bd0347
[1] http://lists.freedesktop.org/archives/xorg/2005-March/006663.html
[2] http://lists.freedesktop.org/archives/xorg/2005-March/006745.html
[3] http://lists.freedesktop.org/archives/xorg/2007-October/029507.html
Software cursor for VGA    by Pavel Machek <pavel@atrey.karlin.mff.cuni.cz>
=======================    and Martin Mares <mj@atrey.karlin.mff.cuni.cz>

   Linux now has some ability to manipulate cursor appearance. Normally, you
can set the size of hardware cursor (and also work around some ugly bugs in
those miserable Trident cards--see #define TRIDENT_GLITCH in drivers/video/
vgacon.c). You can now play a few new tricks:  you can make your cursor look
like a non-blinking red block, make it inverse background of the character it's
over or to highlight that character and still choose whether the original
hardware cursor should remain visible or not.  There may be other things I have
never thought of.

   The cursor appearance is controlled by a "<ESC>[?1;2;3c" escape sequence
where 1, 2 and 3 are parameters described below. If you omit any of them,
they will default to zeroes.

   Parameter 1 specifies cursor size (0=default, 1=invisible, 2=underline, ...,
8=full block) + 16 if you want the software cursor to be applied + 32 if you
want to always change the background color + 64 if you dislike having the
background the same as the foreground.  Highlights are ignored for the last two
flags.

   The second parameter selects character attribute bits you want to change
(by simply XORing them with the value of this parameter). On standard VGA,
the high four bits specify background and the low four the foreground. In both
groups, low three bits set color (as in normal color codes used by the console)
and the most significant one turns on highlight (or sometimes blinking--it
depends on the configuration of your VGA).

   The third parameter consists of character attribute bits you want to set.
Bit setting takes place before bit toggling, so you can simply clear a bit by 
including it in both the set mask and the toggle mask.

Examples:
=========

To get normal blinking underline, use: echo -e '\033[?2c'
To get blinking block, use:            echo -e '\033[?6c'
To get red non-blinking block, use:    echo -e '\033[?17;0;64c'

		Video Output Switcher Control
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		2006 luming.yu@intel.com

The output sysfs class driver provides an abstract video output layer that
can be used to hook platform specific methods to enable/disable video output
device through common sysfs interface. For example, on my IBM ThinkPad T42
laptop, The ACPI video driver registered its output devices and read/write
method for 'state' with output sysfs class. The user interface under sysfs is:

linux:/sys/class/video_output # tree .
.
|-- CRT0
|   |-- device -> ../../../devices/pci0000:00/0000:00:01.0
|   |-- state
|   |-- subsystem -> ../../../class/video_output
|   `-- uevent
|-- DVI0
|   |-- device -> ../../../devices/pci0000:00/0000:00:01.0
|   |-- state
|   |-- subsystem -> ../../../class/video_output
|   `-- uevent
|-- LCD0
|   |-- device -> ../../../devices/pci0000:00/0000:00:01.0
|   |-- state
|   |-- subsystem -> ../../../class/video_output
|   `-- uevent
`-- TV0
   |-- device -> ../../../devices/pci0000:00/0000:00:01.0
   |-- state
   |-- subsystem -> ../../../class/video_output
   `-- uevent

			VME Device Driver API
			=====================

Driver registration
===================

As with other subsystems within the Linux kernel, VME device drivers register
with the VME subsystem, typically called from the devices init routine.  This is
achieved via a call to the following function:

	int vme_register_driver (struct vme_driver *driver);

If driver registration is successful this function returns zero, if an error
occurred a negative error code will be returned.

A pointer to a structure of type 'vme_driver' must be provided to the
registration function. The structure is as follows:

	struct vme_driver {
		struct list_head node;
		const char *name;
		int (*match)(struct vme_dev *);
		int (*probe)(struct vme_dev *);
		int (*remove)(struct vme_dev *);
		void (*shutdown)(void);
		struct device_driver driver;
		struct list_head devices;
		unsigned int ndev;
	};

At the minimum, the '.name', '.match' and '.probe' elements of this structure
should be correctly set. The '.name' element is a pointer to a string holding
the device driver's name.

The '.match' function allows controlling the number of devices that need to
be registered. The match function should return 1 if a device should be
probed and 0 otherwise. This example match function (from vme_user.c) limits
the number of devices probed to one:

	#define USER_BUS_MAX	1
	...
	static int vme_user_match(struct vme_dev *vdev)
	{
		if (vdev->id.num >= USER_BUS_MAX)
			return 0;
		return 1;
	}

The '.probe' element should contain a pointer to the probe routine. The
probe routine is passed a 'struct vme_dev' pointer as an argument. The
'struct vme_dev' structure looks like the following:

	struct vme_dev {
		int num;
		struct vme_bridge *bridge;
		struct device dev;
		struct list_head drv_list;
		struct list_head bridge_list;
	};

Here, the 'num' field refers to the sequential device ID for this specific
driver. The bridge number (or bus number) can be accessed using
dev->bridge->num.

A function is also provided to unregister the driver from the VME core and is
usually called from the device driver's exit routine:

	void vme_unregister_driver (struct vme_driver *driver);


Resource management
===================

Once a driver has registered with the VME core the provided match routine will
be called the number of times specified during the registration. If a match
succeeds, a non-zero value should be returned. A zero return value indicates
failure. For all successful matches, the probe routine of the corresponding
driver is called. The probe routine is passed a pointer to the devices
device structure. This pointer should be saved, it will be required for
requesting VME resources.

The driver can request ownership of one or more master windows, slave windows
and/or dma channels. Rather than allowing the device driver to request a
specific window or DMA channel (which may be used by a different driver) this
driver allows a resource to be assigned based on the required attributes of the
driver in question:

	struct vme_resource * vme_master_request(struct vme_dev *dev,
		u32 aspace, u32 cycle, u32 width);

	struct vme_resource * vme_slave_request(struct vme_dev *dev, u32 aspace,
		u32 cycle);

	struct vme_resource *vme_dma_request(struct vme_dev *dev, u32 route);

For slave windows these attributes are split into the VME address spaces that
need to be accessed in 'aspace' and VME bus cycle types required in 'cycle'.
Master windows add a further set of attributes in 'width' specifying the
required data transfer widths. These attributes are defined as bitmasks and as
such any combination of the attributes can be requested for a single window,
the core will assign a window that meets the requirements, returning a pointer
of type vme_resource that should be used to identify the allocated resource
when it is used. For DMA controllers, the request function requires the
potential direction of any transfers to be provided in the route attributes.
This is typically VME-to-MEM and/or MEM-to-VME, though some hardware can
support VME-to-VME and MEM-to-MEM transfers as well as test pattern generation.
If an unallocated window fitting the requirements can not be found a NULL
pointer will be returned.

Functions are also provided to free window allocations once they are no longer
required. These functions should be passed the pointer to the resource provided
during resource allocation:

	void vme_master_free(struct vme_resource *res);

	void vme_slave_free(struct vme_resource *res);

	void vme_dma_free(struct vme_resource *res);


Master windows
==============

Master windows provide access from the local processor[s] out onto the VME bus.
The number of windows available and the available access modes is dependent on
the underlying chipset. A window must be configured before it can be used.


Master window configuration
---------------------------

Once a master window has been assigned the following functions can be used to
configure it and retrieve the current settings:

	int vme_master_set (struct vme_resource *res, int enabled,
		unsigned long long base, unsigned long long size, u32 aspace,
		u32 cycle, u32 width);

	int vme_master_get (struct vme_resource *res, int *enabled,
		unsigned long long *base, unsigned long long *size, u32 *aspace,
		u32 *cycle, u32 *width);

The address spaces, transfer widths and cycle types are the same as described
under resource management, however some of the options are mutually exclusive.
For example, only one address space may be specified.

These functions return 0 on success or an error code should the call fail.


Master window access
--------------------

The following functions can be used to read from and write to configured master
windows. These functions return the number of bytes copied:

	ssize_t vme_master_read(struct vme_resource *res, void *buf,
		size_t count, loff_t offset);

	ssize_t vme_master_write(struct vme_resource *res, void *buf,
		size_t count, loff_t offset);

In addition to simple reads and writes, a function is provided to do a
read-modify-write transaction. This function returns the original value of the
VME bus location :

	unsigned int vme_master_rmw (struct vme_resource *res,
		unsigned int mask, unsigned int compare, unsigned int swap,
		loff_t offset);

This functions by reading the offset, applying the mask. If the bits selected in
the mask match with the values of the corresponding bits in the compare field,
the value of swap is written the specified offset.


Slave windows
=============

Slave windows provide devices on the VME bus access into mapped portions of the
local memory. The number of windows available and the access modes that can be
used is dependent on the underlying chipset. A window must be configured before
it can be used.


Slave window configuration
--------------------------

Once a slave window has been assigned the following functions can be used to
configure it and retrieve the current settings:

	int vme_slave_set (struct vme_resource *res, int enabled,
		unsigned long long base, unsigned long long size,
		dma_addr_t mem, u32 aspace, u32 cycle);

	int vme_slave_get (struct vme_resource *res, int *enabled,
		unsigned long long *base, unsigned long long *size,
		dma_addr_t *mem, u32 *aspace, u32 *cycle);

The address spaces, transfer widths and cycle types are the same as described
under resource management, however some of the options are mutually exclusive.
For example, only one address space may be specified.

These functions return 0 on success or an error code should the call fail.


Slave window buffer allocation
------------------------------

Functions are provided to allow the user to allocate and free a contiguous
buffers which will be accessible by the VME bridge. These functions do not have
to be used, other methods can be used to allocate a buffer, though care must be
taken to ensure that they are contiguous and accessible by the VME bridge:

	void * vme_alloc_consistent(struct vme_resource *res, size_t size,
		dma_addr_t *mem);

	void vme_free_consistent(struct vme_resource *res, size_t size,
		void *virt,	dma_addr_t mem);


Slave window access
-------------------

Slave windows map local memory onto the VME bus, the standard methods for
accessing memory should be used.


DMA channels
============

The VME DMA transfer provides the ability to run link-list DMA transfers. The
API introduces the concept of DMA lists. Each DMA list is a link-list which can
be passed to a DMA controller. Multiple lists can be created, extended,
executed, reused and destroyed.


List Management
---------------

The following functions are provided to create and destroy DMA lists. Execution
of a list will not automatically destroy the list, thus enabling a list to be
reused for repetitive tasks:

	struct vme_dma_list *vme_new_dma_list(struct vme_resource *res);

	int vme_dma_list_free(struct vme_dma_list *list);


List Population
---------------

An item can be added to a list using the following function ( the source and
destination attributes need to be created before calling this function, this is
covered under "Transfer Attributes"):

	int vme_dma_list_add(struct vme_dma_list *list,
		struct vme_dma_attr *src, struct vme_dma_attr *dest,
		size_t count);

NOTE:	The detailed attributes of the transfers source and destination
	are not checked until an entry is added to a DMA list, the request
	for a DMA channel purely checks the directions in which the
	controller is expected to transfer data. As a result it is
	possible for this call to return an error, for example if the
	source or destination is in an unsupported VME address space.

Transfer Attributes
-------------------

The attributes for the source and destination are handled separately from adding
an item to a list. This is due to the diverse attributes required for each type
of source and destination. There are functions to create attributes for PCI, VME
and pattern sources and destinations (where appropriate):

Pattern source:

	struct vme_dma_attr *vme_dma_pattern_attribute(u32 pattern, u32 type);

PCI source or destination:

	struct vme_dma_attr *vme_dma_pci_attribute(dma_addr_t mem);

VME source or destination:

	struct vme_dma_attr *vme_dma_vme_attribute(unsigned long long base,
		u32 aspace, u32 cycle, u32 width);

The following function should be used to free an attribute:

	void vme_dma_free_attribute(struct vme_dma_attr *attr);


List Execution
--------------

The following function queues a list for execution. The function will return
once the list has been executed:

	int vme_dma_list_exec(struct vme_dma_list *list);


Interrupts
==========

The VME API provides functions to attach and detach callbacks to specific VME
level and status ID combinations and for the generation of VME interrupts with
specific VME level and status IDs.


Attaching Interrupt Handlers
----------------------------

The following functions can be used to attach and free a specific VME level and
status ID combination. Any given combination can only be assigned a single
callback function. A void pointer parameter is provided, the value of which is
passed to the callback function, the use of this pointer is user undefined:

	int vme_irq_request(struct vme_dev *dev, int level, int statid,
		void (*callback)(int, int, void *), void *priv);

	void vme_irq_free(struct vme_dev *dev, int level, int statid);

The callback parameters are as follows. Care must be taken in writing a callback
function, callback functions run in interrupt context:

	void callback(int level, int statid, void *priv);


Interrupt Generation
--------------------

The following function can be used to generate a VME interrupt at a given VME
level and VME status ID:

	int vme_irq_generate(struct vme_dev *dev, int level, int statid);


Location monitors
=================

The VME API provides the following functionality to configure the location
monitor.


Location Monitor Management
---------------------------

The following functions are provided to request the use of a block of location
monitors and to free them after they are no longer required:

	struct vme_resource * vme_lm_request(struct vme_dev *dev);

	void vme_lm_free(struct vme_resource * res);

Each block may provide a number of location monitors, monitoring adjacent
locations. The following function can be used to determine how many locations
are provided:

	int vme_lm_count(struct vme_resource * res);


Location Monitor Configuration
------------------------------

Once a bank of location monitors has been allocated, the following functions
are provided to configure the location and mode of the location monitor:

	int vme_lm_set(struct vme_resource *res, unsigned long long base,
		u32 aspace, u32 cycle);

	int vme_lm_get(struct vme_resource *res, unsigned long long *base,
		u32 *aspace, u32 *cycle);


Location Monitor Use
--------------------

The following functions allow a callback to be attached and detached from each
location monitor location. Each location monitor can monitor a number of
adjacent locations:

	int vme_lm_attach(struct vme_resource *res, int num,
		void (*callback)(int));

	int vme_lm_detach(struct vme_resource *res, int num);

The callback function is declared as follows.

	void callback(int num);


Slot Detection
==============

This function returns the slot ID of the provided bridge.

	int vme_slot_num(struct vme_dev *dev);


Bus Detection
=============

This function returns the bus ID of the provided bridge.

	int vme_bus_num(struct vme_dev *dev);


Why the "volatile" type class should not be used
------------------------------------------------

C programmers have often taken volatile to mean that the variable could be
changed outside of the current thread of execution; as a result, they are
sometimes tempted to use it in kernel code when shared data structures are
being used.  In other words, they have been known to treat volatile types
as a sort of easy atomic variable, which they are not.  The use of volatile in
kernel code is almost never correct; this document describes why.

The key point to understand with regard to volatile is that its purpose is
to suppress optimization, which is almost never what one really wants to
do.  In the kernel, one must protect shared data structures against
unwanted concurrent access, which is very much a different task.  The
process of protecting against unwanted concurrency will also avoid almost
all optimization-related problems in a more efficient way.

Like volatile, the kernel primitives which make concurrent access to data
safe (spinlocks, mutexes, memory barriers, etc.) are designed to prevent
unwanted optimization.  If they are being used properly, there will be no
need to use volatile as well.  If volatile is still necessary, there is
almost certainly a bug in the code somewhere.  In properly-written kernel
code, volatile can only serve to slow things down.

Consider a typical block of kernel code:

    spin_lock(&the_lock);
    do_something_on(&shared_data);
    do_something_else_with(&shared_data);
    spin_unlock(&the_lock);

If all the code follows the locking rules, the value of shared_data cannot
change unexpectedly while the_lock is held.  Any other code which might
want to play with that data will be waiting on the lock.  The spinlock
primitives act as memory barriers - they are explicitly written to do so -
meaning that data accesses will not be optimized across them.  So the
compiler might think it knows what will be in shared_data, but the
spin_lock() call, since it acts as a memory barrier, will force it to
forget anything it knows.  There will be no optimization problems with
accesses to that data.

If shared_data were declared volatile, the locking would still be
necessary.  But the compiler would also be prevented from optimizing access
to shared_data _within_ the critical section, when we know that nobody else
can be working with it.  While the lock is held, shared_data is not
volatile.  When dealing with shared data, proper locking makes volatile
unnecessary - and potentially harmful.

The volatile storage class was originally meant for memory-mapped I/O
registers.  Within the kernel, register accesses, too, should be protected
by locks, but one also does not want the compiler "optimizing" register
accesses within a critical section.  But, within the kernel, I/O memory
accesses are always done through accessor functions; accessing I/O memory
directly through pointers is frowned upon and does not work on all
architectures.  Those accessors are written to prevent unwanted
optimization, so, once again, volatile is unnecessary.

Another situation where one might be tempted to use volatile is
when the processor is busy-waiting on the value of a variable.  The right
way to perform a busy wait is:

    while (my_variable != what_i_want)
        cpu_relax();

The cpu_relax() call can lower CPU power consumption or yield to a
hyperthreaded twin processor; it also happens to serve as a compiler
barrier, so, once again, volatile is unnecessary.  Of course, busy-
waiting is generally an anti-social act to begin with.

There are still a few rare situations where volatile makes sense in the
kernel:

  - The above-mentioned accessor functions might use volatile on
    architectures where direct I/O memory access does work.  Essentially,
    each accessor call becomes a little critical section on its own and
    ensures that the access happens as expected by the programmer.

  - Inline assembly code which changes memory, but which has no other
    visible side effects, risks being deleted by GCC.  Adding the volatile
    keyword to asm statements will prevent this removal.

  - The jiffies variable is special in that it can have a different value
    every time it is referenced, but it can be read without any special
    locking.  So jiffies can be volatile, but the addition of other
    variables of this type is strongly frowned upon.  Jiffies is considered
    to be a "stupid legacy" issue (Linus's words) in this regard; fixing it
    would be more trouble than it is worth.

  - Pointers to data structures in coherent memory which might be modified
    by I/O devices can, sometimes, legitimately be volatile.  A ring buffer
    used by a network adapter, where that adapter changes pointers to
    indicate which descriptors have been processed, is an example of this
    type of situation.

For most code, none of the above justifications for volatile apply.  As a
result, the use of volatile is likely to be seen as a bug and will bring
additional scrutiny to the code.  Developers who are tempted to use
volatile should take a step back and think about what they are truly trying
to accomplish.

Patches to remove volatile variables are generally welcome - as long as
they come with a justification which shows that the concurrency issues have
been properly thought through.


NOTES
-----

[1] http://lwn.net/Articles/233481/
[2] http://lwn.net/Articles/233482/

CREDITS
-------

Original impetus and research by Randy Dunlap
Written by Jonathan Corbet
Improvements via comments from Satyam Sharma, Johannes Stezenbach, Jesper
	Juhl, Heikki Orsila, H. Peter Anvin, Philipp Hahn, and Stefan
	Richter.

Concurrency Managed Workqueue (cmwq)

September, 2010		Tejun Heo <tj@kernel.org>
			Florian Mickler <florian@mickler.org>

CONTENTS

1. Introduction
2. Why cmwq?
3. The Design
4. Application Programming Interface (API)
5. Example Execution Scenarios
6. Guidelines
7. Debugging


1. Introduction

There are many cases where an asynchronous process execution context
is needed and the workqueue (wq) API is the most commonly used
mechanism for such cases.

When such an asynchronous execution context is needed, a work item
describing which function to execute is put on a queue.  An
independent thread serves as the asynchronous execution context.  The
queue is called workqueue and the thread is called worker.

While there are work items on the workqueue the worker executes the
functions associated with the work items one after the other.  When
there is no work item left on the workqueue the worker becomes idle.
When a new work item gets queued, the worker begins executing again.


2. Why cmwq?

In the original wq implementation, a multi threaded (MT) wq had one
worker thread per CPU and a single threaded (ST) wq had one worker
thread system-wide.  A single MT wq needed to keep around the same
number of workers as the number of CPUs.  The kernel grew a lot of MT
wq users over the years and with the number of CPU cores continuously
rising, some systems saturated the default 32k PID space just booting
up.

Although MT wq wasted a lot of resource, the level of concurrency
provided was unsatisfactory.  The limitation was common to both ST and
MT wq albeit less severe on MT.  Each wq maintained its own separate
worker pool.  A MT wq could provide only one execution context per CPU
while a ST wq one for the whole system.  Work items had to compete for
those very limited execution contexts leading to various problems
including proneness to deadlocks around the single execution context.

The tension between the provided level of concurrency and resource
usage also forced its users to make unnecessary tradeoffs like libata
choosing to use ST wq for polling PIOs and accepting an unnecessary
limitation that no two polling PIOs can progress at the same time.  As
MT wq don't provide much better concurrency, users which require
higher level of concurrency, like async or fscache, had to implement
their own thread pool.

Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with
focus on the following goals.

* Maintain compatibility with the original workqueue API.

* Use per-CPU unified worker pools shared by all wq to provide
  flexible level of concurrency on demand without wasting a lot of
  resource.

* Automatically regulate worker pool and level of concurrency so that
  the API users don't need to worry about such details.


3. The Design

In order to ease the asynchronous execution of functions a new
abstraction, the work item, is introduced.

A work item is a simple struct that holds a pointer to the function
that is to be executed asynchronously.  Whenever a driver or subsystem
wants a function to be executed asynchronously it has to set up a work
item pointing to that function and queue that work item on a
workqueue.

Special purpose threads, called worker threads, execute the functions
off of the queue, one after the other.  If no work is queued, the
worker threads become idle.  These worker threads are managed in so
called worker-pools.

The cmwq design differentiates between the user-facing workqueues that
subsystems and drivers queue work items on and the backend mechanism
which manages worker-pools and processes the queued work items.

There are two worker-pools, one for normal work items and the other
for high priority ones, for each possible CPU and some extra
worker-pools to serve work items queued on unbound workqueues - the
number of these backing pools is dynamic.

Subsystems and drivers can create and queue work items through special
workqueue API functions as they see fit. They can influence some
aspects of the way the work items are executed by setting flags on the
workqueue they are putting the work item on. These flags include
things like CPU locality, concurrency limits, priority and more.  To
get a detailed overview refer to the API description of
alloc_workqueue() below.

When a work item is queued to a workqueue, the target worker-pool is
determined according to the queue parameters and workqueue attributes
and appended on the shared worklist of the worker-pool.  For example,
unless specifically overridden, a work item of a bound workqueue will
be queued on the worklist of either normal or highpri worker-pool that
is associated to the CPU the issuer is running on.

For any worker pool implementation, managing the concurrency level
(how many execution contexts are active) is an important issue.  cmwq
tries to keep the concurrency at a minimal but sufficient level.
Minimal to save resources and sufficient in that the system is used at
its full capacity.

Each worker-pool bound to an actual CPU implements concurrency
management by hooking into the scheduler.  The worker-pool is notified
whenever an active worker wakes up or sleeps and keeps track of the
number of the currently runnable workers.  Generally, work items are
not expected to hog a CPU and consume many cycles.  That means
maintaining just enough concurrency to prevent work processing from
stalling should be optimal.  As long as there are one or more runnable
workers on the CPU, the worker-pool doesn't start execution of a new
work, but, when the last running worker goes to sleep, it immediately
schedules a new worker so that the CPU doesn't sit idle while there
are pending work items.  This allows using a minimal number of workers
without losing execution bandwidth.

Keeping idle workers around doesn't cost other than the memory space
for kthreads, so cmwq holds onto idle ones for a while before killing
them.

For unbound workqueues, the number of backing pools is dynamic.
Unbound workqueue can be assigned custom attributes using
apply_workqueue_attrs() and workqueue will automatically create
backing worker pools matching the attributes.  The responsibility of
regulating concurrency level is on the users.  There is also a flag to
mark a bound wq to ignore the concurrency management.  Please refer to
the API section for details.

Forward progress guarantee relies on that workers can be created when
more execution contexts are necessary, which in turn is guaranteed
through the use of rescue workers.  All work items which might be used
on code paths that handle memory reclaim are required to be queued on
wq's that have a rescue-worker reserved for execution under memory
pressure.  Else it is possible that the worker-pool deadlocks waiting
for execution contexts to free up.


4. Application Programming Interface (API)

alloc_workqueue() allocates a wq.  The original create_*workqueue()
functions are deprecated and scheduled for removal.  alloc_workqueue()
takes three arguments - @name, @flags and @max_active.  @name is the
name of the wq and also used as the name of the rescuer thread if
there is one.

A wq no longer manages execution resources but serves as a domain for
forward progress guarantee, flush and work item attributes.  @flags
and @max_active control how work items are assigned execution
resources, scheduled and executed.

@flags:

  WQ_UNBOUND

	Work items queued to an unbound wq are served by the special
	woker-pools which host workers which are not bound to any
	specific CPU.  This makes the wq behave as a simple execution
	context provider without concurrency management.  The unbound
	worker-pools try to start execution of work items as soon as
	possible.  Unbound wq sacrifices locality but is useful for
	the following cases.

	* Wide fluctuation in the concurrency level requirement is
	  expected and using bound wq may end up creating large number
	  of mostly unused workers across different CPUs as the issuer
	  hops through different CPUs.

	* Long running CPU intensive workloads which can be better
	  managed by the system scheduler.

  WQ_FREEZABLE

	A freezable wq participates in the freeze phase of the system
	suspend operations.  Work items on the wq are drained and no
	new work item starts execution until thawed.

  WQ_MEM_RECLAIM

	All wq which might be used in the memory reclaim paths _MUST_
	have this flag set.  The wq is guaranteed to have at least one
	execution context regardless of memory pressure.

  WQ_HIGHPRI

	Work items of a highpri wq are queued to the highpri
	worker-pool of the target cpu.  Highpri worker-pools are
	served by worker threads with elevated nice level.

	Note that normal and highpri worker-pools don't interact with
	each other.  Each maintain its separate pool of workers and
	implements concurrency management among its workers.

  WQ_CPU_INTENSIVE

	Work items of a CPU intensive wq do not contribute to the
	concurrency level.  In other words, runnable CPU intensive
	work items will not prevent other work items in the same
	worker-pool from starting execution.  This is useful for bound
	work items which are expected to hog CPU cycles so that their
	execution is regulated by the system scheduler.

	Although CPU intensive work items don't contribute to the
	concurrency level, start of their executions is still
	regulated by the concurrency management and runnable
	non-CPU-intensive work items can delay execution of CPU
	intensive work items.

	This flag is meaningless for unbound wq.

Note that the flag WQ_NON_REENTRANT no longer exists as all workqueues
are now non-reentrant - any work item is guaranteed to be executed by
at most one worker system-wide at any given time.

@max_active:

@max_active determines the maximum number of execution contexts per
CPU which can be assigned to the work items of a wq.  For example,
with @max_active of 16, at most 16 work items of the wq can be
executing at the same time per CPU.

Currently, for a bound wq, the maximum limit for @max_active is 512
and the default value used when 0 is specified is 256.  For an unbound
wq, the limit is higher of 512 and 4 * num_possible_cpus().  These
values are chosen sufficiently high such that they are not the
limiting factor while providing protection in runaway cases.

The number of active work items of a wq is usually regulated by the
users of the wq, more specifically, by how many work items the users
may queue at the same time.  Unless there is a specific need for
throttling the number of active work items, specifying '0' is
recommended.

Some users depend on the strict execution ordering of ST wq.  The
combination of @max_active of 1 and WQ_UNBOUND is used to achieve this
behavior.  Work items on such wq are always queued to the unbound
worker-pools and only one work item can be active at any given time thus
achieving the same ordering property as ST wq.


5. Example Execution Scenarios

The following example execution scenarios try to illustrate how cmwq
behave under different configurations.

 Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU.
 w0 burns CPU for 5ms then sleeps for 10ms then burns CPU for 5ms
 again before finishing.  w1 and w2 burn CPU for 5ms then sleep for
 10ms.

Ignoring all other tasks, works and processing overhead, and assuming
simple FIFO scheduling, the following is one highly simplified version
of possible sequences of events with the original wq.

 TIME IN MSECS	EVENT
 0		w0 starts and burns CPU
 5		w0 sleeps
 15		w0 wakes up and burns CPU
 20		w0 finishes
 20		w1 starts and burns CPU
 25		w1 sleeps
 35		w1 wakes up and finishes
 35		w2 starts and burns CPU
 40		w2 sleeps
 50		w2 wakes up and finishes

And with cmwq with @max_active >= 3,

 TIME IN MSECS	EVENT
 0		w0 starts and burns CPU
 5		w0 sleeps
 5		w1 starts and burns CPU
 10		w1 sleeps
 10		w2 starts and burns CPU
 15		w2 sleeps
 15		w0 wakes up and burns CPU
 20		w0 finishes
 20		w1 wakes up and finishes
 25		w2 wakes up and finishes

If @max_active == 2,

 TIME IN MSECS	EVENT
 0		w0 starts and burns CPU
 5		w0 sleeps
 5		w1 starts and burns CPU
 10		w1 sleeps
 15		w0 wakes up and burns CPU
 20		w0 finishes
 20		w1 wakes up and finishes
 20		w2 starts and burns CPU
 25		w2 sleeps
 35		w2 wakes up and finishes

Now, let's assume w1 and w2 are queued to a different wq q1 which has
WQ_CPU_INTENSIVE set,

 TIME IN MSECS	EVENT
 0		w0 starts and burns CPU
 5		w0 sleeps
 5		w1 and w2 start and burn CPU
 10		w1 sleeps
 15		w2 sleeps
 15		w0 wakes up and burns CPU
 20		w0 finishes
 20		w1 wakes up and finishes
 25		w2 wakes up and finishes


6. Guidelines

* Do not forget to use WQ_MEM_RECLAIM if a wq may process work items
  which are used during memory reclaim.  Each wq with WQ_MEM_RECLAIM
  set has an execution context reserved for it.  If there is
  dependency among multiple work items used during memory reclaim,
  they should be queued to separate wq each with WQ_MEM_RECLAIM.

* Unless strict ordering is required, there is no need to use ST wq.

* Unless there is a specific need, using 0 for @max_active is
  recommended.  In most use cases, concurrency level usually stays
  well under the default limit.

* A wq serves as a domain for forward progress guarantee
  (WQ_MEM_RECLAIM, flush and work item attributes.  Work items which
  are not involved in memory reclaim and don't need to be flushed as a
  part of a group of work items, and don't require any special
  attribute, can use one of the system wq.  There is no difference in
  execution characteristics between using a dedicated wq and a system
  wq.

* Unless work items are expected to consume a huge amount of CPU
  cycles, using a bound wq is usually beneficial due to the increased
  level of locality in wq operations and work item execution.


7. Debugging

Because the work functions are executed by generic worker threads
there are a few tricks needed to shed some light on misbehaving
workqueue users.

Worker threads show up in the process list as:

root      5671  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/0:1]
root      5672  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/1:2]
root      5673  0.0  0.0      0     0 ?        S    12:12   0:00 [kworker/0:0]
root      5674  0.0  0.0      0     0 ?        S    12:13   0:00 [kworker/1:0]

If kworkers are going crazy (using too much cpu), there are two types
of possible problems:

	1. Something beeing scheduled in rapid succession
	2. A single work item that consumes lots of cpu cycles

The first one can be tracked using tracing:

	$ echo workqueue:workqueue_queue_work > /sys/kernel/debug/tracing/set_event
	$ cat /sys/kernel/debug/tracing/trace_pipe > out.txt
	(wait a few secs)
	^C

If something is busy looping on work queueing, it would be dominating
the output and the offender can be determined with the work item
function.

For the second type of problems it should be possible to just check
the stack trace of the offending worker thread.

	$ cat /proc/THE_OFFENDING_KWORKER/stack

The work item's function should be trivially visible in the stack
trace.
Wait/Wound Deadlock-Proof Mutex Design
======================================

Please read mutex-design.txt first, as it applies to wait/wound mutexes too.

Motivation for WW-Mutexes
-------------------------

GPU's do operations that commonly involve many buffers.  Those buffers
can be shared across contexts/processes, exist in different memory
domains (for example VRAM vs system memory), and so on.  And with
PRIME / dmabuf, they can even be shared across devices.  So there are
a handful of situations where the driver needs to wait for buffers to
become ready.  If you think about this in terms of waiting on a buffer
mutex for it to become available, this presents a problem because
there is no way to guarantee that buffers appear in a execbuf/batch in
the same order in all contexts.  That is directly under control of
userspace, and a result of the sequence of GL calls that an application
makes.	Which results in the potential for deadlock.  The problem gets
more complex when you consider that the kernel may need to migrate the
buffer(s) into VRAM before the GPU operates on the buffer(s), which
may in turn require evicting some other buffers (and you don't want to
evict other buffers which are already queued up to the GPU), but for a
simplified understanding of the problem you can ignore this.

The algorithm that the TTM graphics subsystem came up with for dealing with
this problem is quite simple.  For each group of buffers (execbuf) that need
to be locked, the caller would be assigned a unique reservation id/ticket,
from a global counter.  In case of deadlock while locking all the buffers
associated with a execbuf, the one with the lowest reservation ticket (i.e.
the oldest task) wins, and the one with the higher reservation id (i.e. the
younger task) unlocks all of the buffers that it has already locked, and then
tries again.

In the RDBMS literature this deadlock handling approach is called wait/wound:
The older tasks waits until it can acquire the contended lock. The younger tasks
needs to back off and drop all the locks it is currently holding, i.e. the
younger task is wounded.

Concepts
--------

Compared to normal mutexes two additional concepts/objects show up in the lock
interface for w/w mutexes:

Acquire context: To ensure eventual forward progress it is important the a task
trying to acquire locks doesn't grab a new reservation id, but keeps the one it
acquired when starting the lock acquisition. This ticket is stored in the
acquire context. Furthermore the acquire context keeps track of debugging state
to catch w/w mutex interface abuse.

W/w class: In contrast to normal mutexes the lock class needs to be explicit for
w/w mutexes, since it is required to initialize the acquire context.

Furthermore there are three different class of w/w lock acquire functions:

* Normal lock acquisition with a context, using ww_mutex_lock.

* Slowpath lock acquisition on the contending lock, used by the wounded task
  after having dropped all already acquired locks. These functions have the
  _slow postfix.

  From a simple semantics point-of-view the _slow functions are not strictly
  required, since simply calling the normal ww_mutex_lock functions on the
  contending lock (after having dropped all other already acquired locks) will
  work correctly. After all if no other ww mutex has been acquired yet there's
  no deadlock potential and hence the ww_mutex_lock call will block and not
  prematurely return -EDEADLK. The advantage of the _slow functions is in
  interface safety:
  - ww_mutex_lock has a __must_check int return type, whereas ww_mutex_lock_slow
    has a void return type. Note that since ww mutex code needs loops/retries
    anyway the __must_check doesn't result in spurious warnings, even though the
    very first lock operation can never fail.
  - When full debugging is enabled ww_mutex_lock_slow checks that all acquired
    ww mutex have been released (preventing deadlocks) and makes sure that we
    block on the contending lock (preventing spinning through the -EDEADLK
    slowpath until the contended lock can be acquired).

* Functions to only acquire a single w/w mutex, which results in the exact same
  semantics as a normal mutex. This is done by calling ww_mutex_lock with a NULL
  context.

  Again this is not strictly required. But often you only want to acquire a
  single lock in which case it's pointless to set up an acquire context (and so
  better to avoid grabbing a deadlock avoidance ticket).

Of course, all the usual variants for handling wake-ups due to signals are also
provided.

Usage
-----

Three different ways to acquire locks within the same w/w class. Common
definitions for methods #1 and #2:

static DEFINE_WW_CLASS(ww_class);

struct obj {
	struct ww_mutex lock;
	/* obj data */
};

struct obj_entry {
	struct list_head head;
	struct obj *obj;
};

Method 1, using a list in execbuf->buffers that's not allowed to be reordered.
This is useful if a list of required objects is already tracked somewhere.
Furthermore the lock helper can use propagate the -EALREADY return code back to
the caller as a signal that an object is twice on the list. This is useful if
the list is constructed from userspace input and the ABI requires userspace to
not have duplicate entries (e.g. for a gpu commandbuffer submission ioctl).

int lock_objs(struct list_head *list, struct ww_acquire_ctx *ctx)
{
	struct obj *res_obj = NULL;
	struct obj_entry *contended_entry = NULL;
	struct obj_entry *entry;

	ww_acquire_init(ctx, &ww_class);

retry:
	list_for_each_entry (entry, list, head) {
		if (entry->obj == res_obj) {
			res_obj = NULL;
			continue;
		}
		ret = ww_mutex_lock(&entry->obj->lock, ctx);
		if (ret < 0) {
			contended_entry = entry;
			goto err;
		}
	}

	ww_acquire_done(ctx);
	return 0;

err:
	list_for_each_entry_continue_reverse (entry, list, head)
		ww_mutex_unlock(&entry->obj->lock);

	if (res_obj)
		ww_mutex_unlock(&res_obj->lock);

	if (ret == -EDEADLK) {
		/* we lost out in a seqno race, lock and retry.. */
		ww_mutex_lock_slow(&contended_entry->obj->lock, ctx);
		res_obj = contended_entry->obj;
		goto retry;
	}
	ww_acquire_fini(ctx);

	return ret;
}

Method 2, using a list in execbuf->buffers that can be reordered. Same semantics
of duplicate entry detection using -EALREADY as method 1 above. But the
list-reordering allows for a bit more idiomatic code.

int lock_objs(struct list_head *list, struct ww_acquire_ctx *ctx)
{
	struct obj_entry *entry, *entry2;

	ww_acquire_init(ctx, &ww_class);

	list_for_each_entry (entry, list, head) {
		ret = ww_mutex_lock(&entry->obj->lock, ctx);
		if (ret < 0) {
			entry2 = entry;

			list_for_each_entry_continue_reverse (entry2, list, head)
				ww_mutex_unlock(&entry2->obj->lock);

			if (ret != -EDEADLK) {
				ww_acquire_fini(ctx);
				return ret;
			}

			/* we lost out in a seqno race, lock and retry.. */
			ww_mutex_lock_slow(&entry->obj->lock, ctx);

			/*
			 * Move buf to head of the list, this will point
			 * buf->next to the first unlocked entry,
			 * restarting the for loop.
			 */
			list_del(&entry->head);
			list_add(&entry->head, list);
		}
	}

	ww_acquire_done(ctx);
	return 0;
}

Unlocking works the same way for both methods #1 and #2:

void unlock_objs(struct list_head *list, struct ww_acquire_ctx *ctx)
{
	struct obj_entry *entry;

	list_for_each_entry (entry, list, head)
		ww_mutex_unlock(&entry->obj->lock);

	ww_acquire_fini(ctx);
}

Method 3 is useful if the list of objects is constructed ad-hoc and not upfront,
e.g. when adjusting edges in a graph where each node has its own ww_mutex lock,
and edges can only be changed when holding the locks of all involved nodes. w/w
mutexes are a natural fit for such a case for two reasons:
- They can handle lock-acquisition in any order which allows us to start walking
  a graph from a starting point and then iteratively discovering new edges and
  locking down the nodes those edges connect to.
- Due to the -EALREADY return code signalling that a given objects is already
  held there's no need for additional book-keeping to break cycles in the graph
  or keep track off which looks are already held (when using more than one node
  as a starting point).

Note that this approach differs in two important ways from the above methods:
- Since the list of objects is dynamically constructed (and might very well be
  different when retrying due to hitting the -EDEADLK wound condition) there's
  no need to keep any object on a persistent list when it's not locked. We can
  therefore move the list_head into the object itself.
- On the other hand the dynamic object list construction also means that the -EALREADY return
  code can't be propagated.

Note also that methods #1 and #2 and method #3 can be combined, e.g. to first lock a
list of starting nodes (passed in from userspace) using one of the above
methods. And then lock any additional objects affected by the operations using
method #3 below. The backoff/retry procedure will be a bit more involved, since
when the dynamic locking step hits -EDEADLK we also need to unlock all the
objects acquired with the fixed list. But the w/w mutex debug checks will catch
any interface misuse for these cases.

Also, method 3 can't fail the lock acquisition step since it doesn't return
-EALREADY. Of course this would be different when using the _interruptible
variants, but that's outside of the scope of these examples here.

struct obj {
	struct ww_mutex ww_mutex;
	struct list_head locked_list;
};

static DEFINE_WW_CLASS(ww_class);

void __unlock_objs(struct list_head *list)
{
	struct obj *entry, *temp;

	list_for_each_entry_safe (entry, temp, list, locked_list) {
		/* need to do that before unlocking, since only the current lock holder is
		allowed to use object */
		list_del(&entry->locked_list);
		ww_mutex_unlock(entry->ww_mutex)
	}
}

void lock_objs(struct list_head *list, struct ww_acquire_ctx *ctx)
{
	struct obj *obj;

	ww_acquire_init(ctx, &ww_class);

retry:
	/* re-init loop start state */
	loop {
		/* magic code which walks over a graph and decides which objects
		 * to lock */

		ret = ww_mutex_lock(obj->ww_mutex, ctx);
		if (ret == -EALREADY) {
			/* we have that one already, get to the next object */
			continue;
		}
		if (ret == -EDEADLK) {
			__unlock_objs(list);

			ww_mutex_lock_slow(obj, ctx);
			list_add(&entry->locked_list, list);
			goto retry;
		}

		/* locked a new object, add it to the list */
		list_add_tail(&entry->locked_list, list);
	}

	ww_acquire_done(ctx);
	return 0;
}

void unlock_objs(struct list_head *list, struct ww_acquire_ctx *ctx)
{
	__unlock_objs(list);
	ww_acquire_fini(ctx);
}

Method 4: Only lock one single objects. In that case deadlock detection and
prevention is obviously overkill, since with grabbing just one lock you can't
produce a deadlock within just one class. To simplify this case the w/w mutex
api can be used with a NULL context.

Implementation Details
----------------------

Design:
  ww_mutex currently encapsulates a struct mutex, this means no extra overhead for
  normal mutex locks, which are far more common. As such there is only a small
  increase in code size if wait/wound mutexes are not used.

  In general, not much contention is expected. The locks are typically used to
  serialize access to resources for devices. The only way to make wakeups
  smarter would be at the cost of adding a field to struct mutex_waiter. This
  would add overhead to all cases where normal mutexes are used, and
  ww_mutexes are generally less performance sensitive.

Lockdep:
  Special care has been taken to warn for as many cases of api abuse
  as possible. Some common api abuses will be caught with
  CONFIG_DEBUG_MUTEXES, but CONFIG_PROVE_LOCKING is recommended.

  Some of the errors which will be warned about:
   - Forgetting to call ww_acquire_fini or ww_acquire_init.
   - Attempting to lock more mutexes after ww_acquire_done.
   - Attempting to lock the wrong mutex after -EDEADLK and
     unlocking all mutexes.
   - Attempting to lock the right mutex after -EDEADLK,
     before unlocking all mutexes.

   - Calling ww_mutex_lock_slow before -EDEADLK was returned.

   - Unlocking mutexes with the wrong unlock function.
   - Calling one of the ww_acquire_* twice on the same context.
   - Using a different ww_class for the mutex than for the ww_acquire_ctx.
   - Normal lockdep errors that can result in deadlocks.

  Some of the lockdep errors that can result in deadlocks:
   - Calling ww_acquire_init to initialize a second ww_acquire_ctx before
     having called ww_acquire_fini on the first.
   - 'normal' deadlocks that can occur.

FIXME: Update this section once we have the TASK_DEADLOCK task state flag magic
implemented.

XZ data compression in Linux
============================

Introduction

    XZ is a general purpose data compression format with high compression
    ratio and relatively fast decompression. The primary compression
    algorithm (filter) is LZMA2. Additional filters can be used to improve
    compression ratio even further. E.g. Branch/Call/Jump (BCJ) filters
    improve compression ratio of executable data.

    The XZ decompressor in Linux is called XZ Embedded. It supports
    the LZMA2 filter and optionally also BCJ filters. CRC32 is supported
    for integrity checking. The home page of XZ Embedded is at
    <http://tukaani.org/xz/embedded.html>, where you can find the
    latest version and also information about using the code outside
    the Linux kernel.

    For userspace, XZ Utils provide a zlib-like compression library
    and a gzip-like command line tool. XZ Utils can be downloaded from
    <http://tukaani.org/xz/>.

XZ related components in the kernel

    The xz_dec module provides XZ decompressor with single-call (buffer
    to buffer) and multi-call (stateful) APIs. The usage of the xz_dec
    module is documented in include/linux/xz.h.

    The xz_dec_test module is for testing xz_dec. xz_dec_test is not
    useful unless you are hacking the XZ decompressor. xz_dec_test
    allocates a char device major dynamically to which one can write
    .xz files from userspace. The decompressed output is thrown away.
    Keep an eye on dmesg to see diagnostics printed by xz_dec_test.
    See the xz_dec_test source code for the details.

    For decompressing the kernel image, initramfs, and initrd, there
    is a wrapper function in lib/decompress_unxz.c. Its API is the
    same as in other decompress_*.c files, which is defined in
    include/linux/decompress/generic.h.

    scripts/xz_wrap.sh is a wrapper for the xz command line tool found
    from XZ Utils. The wrapper sets compression options to values suitable
    for compressing the kernel image.

    For kernel makefiles, two commands are provided for use with
    $(call if_needed). The kernel image should be compressed with
    $(call if_needed,xzkern) which will use a BCJ filter and a big LZMA2
    dictionary. It will also append a four-byte trailer containing the
    uncompressed size of the file, which is needed by the boot code.
    Other things should be compressed with $(call if_needed,xzmisc)
    which will use no BCJ filter and 1 MiB LZMA2 dictionary.

Notes on compression options

    Since the XZ Embedded supports only streams with no integrity check or
    CRC32, make sure that you don't use some other integrity check type
    when encoding files that are supposed to be decoded by the kernel. With
    liblzma, you need to use either LZMA_CHECK_NONE or LZMA_CHECK_CRC32
    when encoding. With the xz command line tool, use --check=none or
    --check=crc32.

    Using CRC32 is strongly recommended unless there is some other layer
    which will verify the integrity of the uncompressed data anyway.
    Double checking the integrity would probably be waste of CPU cycles.
    Note that the headers will always have a CRC32 which will be validated
    by the decoder; you can only change the integrity check type (or
    disable it) for the actual uncompressed data.

    In userspace, LZMA2 is typically used with dictionary sizes of several
    megabytes. The decoder needs to have the dictionary in RAM, thus big
    dictionaries cannot be used for files that are intended to be decoded
    by the kernel. 1 MiB is probably the maximum reasonable dictionary
    size for in-kernel use (maybe more is OK for initramfs). The presets
    in XZ Utils may not be optimal when creating files for the kernel,
    so don't hesitate to use custom settings. Example:

        xz --check=crc32 --lzma2=dict=512KiB inputfile

    An exception to above dictionary size limitation is when the decoder
    is used in single-call mode. Decompressing the kernel itself is an
    example of this situation. In single-call mode, the memory usage
    doesn't depend on the dictionary size, and it is perfectly fine to
    use a big dictionary: for maximum compression, the dictionary should
    be at least as big as the uncompressed data itself.

Future plans

    Creating a limited XZ encoder may be considered if people think it is
    useful. LZMA2 is slower to compress than e.g. Deflate or LZO even at
    the fastest settings, so it isn't clear if LZMA2 encoder is wanted
    into the kernel.

    Support for limited random-access reading is planned for the
    decompression code. I don't know if it could have any use in the
    kernel, but I know that it would be useful in some embedded projects
    outside the Linux kernel.

Conformance to the .xz file format specification

    There are a couple of corner cases where things have been simplified
    at expense of detecting errors as early as possible. These should not
    matter in practice all, since they don't cause security issues. But
    it is good to know this if testing the code e.g. with the test files
    from XZ Utils.

Reporting bugs

    Before reporting a bug, please check that it's not fixed already
    at upstream. See <http://tukaani.org/xz/embedded.html> to get the
    latest code.

    Report bugs to <lasse.collin@tukaani.org> or visit #tukaani on
    Freenode and talk to Larhzu. I don't actively read LKML or other
    kernel-related mailing lists, so if there's something I should know,
    you should email to me personally or use IRC.

    Don't bother Igor Pavlov with questions about the XZ implementation
    in the kernel or about XZ Utils. While these two implementations
    include essential code that is directly based on Igor Pavlov's code,
    these implementations aren't maintained nor supported by him.
		Writing Device Drivers for Zorro Devices
		----------------------------------------

Written by Geert Uytterhoeven <geert@linux-m68k.org>
Last revised: September 5, 2003


1. Introduction
---------------

The Zorro bus is the bus used in the Amiga family of computers. Thanks to
AutoConfig(tm), it's 100% Plug-and-Play.

There are two types of Zorro busses, Zorro II and Zorro III:

  - The Zorro II address space is 24-bit and lies within the first 16 MB of the
    Amiga's address map.

  - Zorro III is a 32-bit extension of Zorro II, which is backwards compatible
    with Zorro II. The Zorro III address space lies outside the first 16 MB.


2. Probing for Zorro Devices
----------------------------

Zorro devices are found by calling `zorro_find_device()', which returns a
pointer to the `next' Zorro device with the specified Zorro ID. A probe loop
for the board with Zorro ID `ZORRO_PROD_xxx' looks like:

    struct zorro_dev *z = NULL;

    while ((z = zorro_find_device(ZORRO_PROD_xxx, z))) {
	if (!zorro_request_region(z->resource.start+MY_START, MY_SIZE,
				  "My explanation"))
	...
    }

`ZORRO_WILDCARD' acts as a wildcard and finds any Zorro device. If your driver
supports different types of boards, you can use a construct like:

    struct zorro_dev *z = NULL;

    while ((z = zorro_find_device(ZORRO_WILDCARD, z))) {
	if (z->id != ZORRO_PROD_xxx1 && z->id != ZORRO_PROD_xxx2 && ...)
	    continue;
	if (!zorro_request_region(z->resource.start+MY_START, MY_SIZE,
				  "My explanation"))
	...
    }


3. Zorro Resources
------------------

Before you can access a Zorro device's registers, you have to make sure it's
not yet in use. This is done using the I/O memory space resource management
functions:

    request_mem_region()
    release_mem_region()

Shortcuts to claim the whole device's address space are provided as well:

    zorro_request_device
    zorro_release_device


4. Accessing the Zorro Address Space
------------------------------------

The address regions in the Zorro device resources are Zorro bus address
regions. Due to the identity bus-physical address mapping on the Zorro bus,
they are CPU physical addresses as well.

The treatment of these regions depends on the type of Zorro space:

  - Zorro II address space is always mapped and does not have to be mapped
    explicitly using z_ioremap().
    
    Conversion from bus/physical Zorro II addresses to kernel virtual addresses
    and vice versa is done using:

	virt_addr = ZTWO_VADDR(bus_addr);
	bus_addr = ZTWO_PADDR(virt_addr);

  - Zorro III address space must be mapped explicitly using z_ioremap() first
    before it can be accessed:
 
	virt_addr = z_ioremap(bus_addr, size);
	...
	z_iounmap(virt_addr);


5. References
-------------

linux/include/linux/zorro.h
linux/include/uapi/linux/zorro.h
linux/include/uapi/linux/zorro_ids.h
linux/arch/m68k/include/asm/zorro.h
linux/drivers/zorro
/proc/bus/zorro

