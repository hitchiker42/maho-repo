* class 1
* WFF = well formed formula, syntax and semantics
** if A and B are wff's and have the same truth table then A \equiv B
** if A \equiv T; then A is a tautology

* (P \land P \rightarrow Q) \rightarrow Q (if P and P implies Q then Q)
** To show this:
*** Basic: write out truth table
*** Intermediate: solve equation to show (P \land P \rightarrow Q) \rightarrow Q \equiv T

* Atoms and Clauses
** Atoms (A,B,\not A,\not B)
** Clauses (A \lor B, A \land \not{}B, etc..)
** DNF(distrubated normal form) / Full DNF (look up the defn)

* Complete set of operators (can write every possible truth function)
** (e.g \lor and \not, or just nor/nand)

* Logic Systems
** Natural Deduction (Rules)
Not all rules are strictly necesary, but attempt to emulate how
actual deduction works
*** M.P: (Modus Ponens) P and P \rightarrow Q then Q
*** M.T: (Modus Tolens) P \rightarrow Q and \not Q then \not P
*** ADD:  A \rightarrow (A \lor B) , A \rightarrow (B \lor A)
*** CONJ: A, B \rightarrow A \land B
*** D.N: (double negation) \not\not{}A \rightarrow A, and reverse
*** CONT: A \land \not{}A \rightarrow F
*** SIMP: A \land B \rightarrow A, A \land B \rightarrow B
*** D.S: (Disjunctive Syllogism) (A \lor B) \land (\not A) \rightarrow B and reverse
*** C.P: (Conditional Proof) if we can derive B from A then we can say A \rightarrow B

*** H.S: (hypothitical syllogism) A\rightarrow{}B, B\rightarrow{}C, then A\rightarrow{}C
*** C.D: (constructive delema) A \lor B, A\rightarrow{}C, B\rightarrow{}D then C \lor D
*** Cases: A \lor B, A \rightarrow C, B \rightarrow C then C
** Incomplete logic system, has something that is a tautology which can't be proved
** Inconsistant logic system, can prove something that isn't a tautology

** (using C operators here)
prove (A | B) & !A & (B -> C) -> (B & C)
1. (A | B) & !A & (B -> C) P for C.P (premise for conditional proof)
2. A | B, 1 by SIMP
3. !A & B->C, 1 by SIMP
4. !A, 3 by SIMP
5. B->C, 1 by SIMP
6. B, 2,4 by D.S
7. C, 5,6 by M.P
8. B & C, 6,7 by CONJ
9 (A | B) & !A & (B->C) -> (B & C), 1,8 C.P
QED
** simplifications
A & B & C ... & N implies A, B, C, ... N
so still using above
1. A | B, P
2. !A, P
3. B->C, P
4. B, 1,2 D.S
5. C, 3,4, MP
6. B & C, 4,5 CONJ
QED, 1,2,3,6 CP

** (A | B -> C) & A  -> C
1. A|B->C, P
2. A, P
3. A|B, 2, ADD
4. C, 1,3 M.P
QED 1,2,3,4 M.P.
** Proof by Contradiction (I.P)
*** From \not A derive F
*** (A|B)&(B|C)&!C -> A
1. A|B, P
2. B->C, P
3. !C, P
4.     !A, P for I.P. //Start subproof
5.     B, 1,4 D.S
6.     C, 2,5 M.P
7.     F, 3,6 CONT
8. A, 4-7 I.P
QED 1,2,3,8 C.P.

*** (A|B -> C&D) -> (B->D)
1. A|B -> C&D, P for CP
2.           B, P for CP //subproof
3.           A|B, 1,2 by ADD
4.           C&D, by 1,3 MP
5.           D, 4 by SIMP
6. B->D, 2-5 CP
QED 1,6 C.P


# class 2

* Natural Deduction (Rules)
Not all rules are strictly necesary, but attempt to emulate how
actual deduction works
*** M.P: (Modus Ponens) P and P \rightarrow Q then Q
*** M.T: (Modus Tolens) P \rightarrow Q and \not Q then \not P
*** ADD:  A \rightarrow (A \lor B) , A \rightarrow (B \lor A)
*** CONJ: A, B \rightarrow A \land B
*** D.N: (double negation) \not\not{}A \rightarrow A, and reverse
*** CONT: A \land \not{}A \rightarrow F
*** SIMP: A \land B \rightarrow A, A \land B \rightarrow B
*** D.S: (Disjunctive Syllogism) (A \lor B) \land (\not A) \rightarrow B and reverse
*** C.P: (Conditional Proof) if we can derive B from A then we can say A \rightarrow B

*** H.S: (hypothitical syllogism) A\rightarrow{}B, B\rightarrow{}C, then A\rightarrow{}C
*** C.D: (constructive delema) A \lor B, A\rightarrow{}C, B\rightarrow{}D then C \lor D
** Cases: A \lor B, A \rightarrow C, B \rightarrow C then C
* class 2
NOTE: Not all in class examples copied.
* Subproofs
** when using proofs by contradiction or a cp proof within
another cp proof we use a subproof.
Subproofs basically exist within a nested lexical scope in the main proof
and `return` their last line.
ex. (A \lor B \rightarrow C \land D) \rightarrow (B \rightarrow D)
1. A \lor B \rightarrow C \land D, P
 subproof starts here
2.   B, P for CP
3.   A \lor B, 2 ADD
4.   C \land D, 1,3 MP
5.   D, 4 SIMP, Only this line is visable in the main proof
6. B \rightarrow D, 2-5 CP
QED 1,6 CP
* Introduce More Natural Deduction Rules
(also added to Natural Deduction above)
** M.T (Modus Tolens): (P\not{}Q and \not{}Q then \not{}P)
** H.S (hypothitical syllogism) A\rightarrow{}B, B\rightarrow{}C, then A\rightarrow{}C
** C.D (constructive delema) A \lor B, A\rightarrow{}C, B\rightarrow{}D then C \lor D
** Cases: A \lor B, A \rightarrow C, B \rightarrow C then C

* defination of \equiv
** if A \equiv B then
A \rightarrow B and B \rightarrow A
thus if
A \rightarrow B and B \rightarrow A then A \equiv B

** show A \land A \equiv A
we don't have and identity, so this is weird
show A \land A \rightarrow A
1. A \land A, P for C.P
2. A,  SIMP
QED 1-2 CP
show A \rightarrow A \land A
1. A, P for CP
2. A \lor A, 1 add
3. A \land (A \lor A), 1,2 conj
4. A, 3 simp
5. A \land A, 1,4 add
QED 5, CP

* Predicates
** No predicates in natural deduction
so for example,
All A can Q
B can't Q
B isn't A
can't be proven with our current logic
** for a predicate p(x) we can create propisitions in 3 ways
** plug in a value for x, i.e p(1);
** \forall{}x p(x), universal quantification
** \exists{}x p(x), existential quantification
** for this to make sense we need a Domain
i.e where do the values of x come from

* Predicates (second class)
** p(x) is a predicate
when x is filled in with a value it becomes a proposition
** Commonly used symbols
 x,y,z,... variables
 a,b,c,... constants
 p,q,<,>,... predicate names (p(),q() are propisitons, and are written p,q)
 f,g,max,min,+,-.. function names
 \exists, \forall Quantifiers
 punctuation (), ','
a term is a variable or constant or
a function applied to terms

atom is a predicate applied to terms

** predicate wffs
if W and V are wffs then:
 (W),\not{}W, W \land V, W \rightarrow V, etc... are wffs
 \exists{}xW, \forall{}xW are wffs
 predicates, functions, etc..
 combinations of the above

** precedence rules
1: \not \exists \forall
2: \land
3: \lor
4: \rightarrow
binary operators associate left to right
unary operators associate right to left
** Variable rules
the scope of the quantifer \exists{}x \forall{}x in \forall{}xP(x) is P(x)

the 'x' in \exists{}x is the variable the quantifier applies to,
we say that the x is bound to the quantifier. if a variable is not
bound it is free.

we can also bind variables by assigning them a constant value

if a wff has no free variables it is a proposition

in order for \exists{}x and \forall{}x to make sense we must have a
domain of discourse(aka universe)
** Interpretation/Domains
An interpretation of a wff consists of a domain D, together with the following:
assignment of symbols of the wff:
1. each predicate letter is assigned a predicate over D
2. each function lettel is assigned a function over D
3. each constant letter and each free variable is assigned a value in D

ex.
\forall{}x(p(a,x) \rightarrow \not{}q(f(x)))
1. D = N (natural numbers)
   a = 0
   p(z,w) = z < w
   q(w) = w<=2
   f(w) = w+1
\forall{}x(0 < x -> \not (x+1 <= 2))

if W is a wff and x is a free variable in W
(we normally write W(x)) and t is a term
then the wff that results from replacing x with t
is denoted W(x/t)

so the truth value of the wff \forall{}x(W(x)) with respect to the
domain D is:
\forall{}xW(x) is true if W(x/d) is true for all d \epsilon D
ditto  for \exists

** Equivalence
two wffs are equivlent if they are true with respect to every interpretation

since we can't actually test every interpretation we need to prove
that they are equivalent via Natural Deduction

** Rules For quantifiers

\not (\forall{}xW) \equiv \exists{}x \not W
\not (\exists{}xW) \equiv \forall{}x \not W
\exists{}x (p(x) \rightarrow q(x)) \equiv \forall{}x p(x) \rightarrow \exists{}x q(x)

for Q in {\exists,\forall}
QxQy W \equiv QyQx W
Qx(p(x) \lor q(x)) \equiv Qx p(x) \lor Qx q(x)

\exists{}x(p(x) \land \not{}q(x)) \equiv \not\forall{}x(p(x) \rightarrow q(x))

** Quantifier Rules
Universal Instantiation(U.I): \forall{}xW(x) then W(c)(or W(y)); if something is true for everything
it is true for a specific thing

Existential Generalization(E.G): W(c) then \exists{}xW(x)

Extensional Instantiation(E.I): \exists{}xW(x) then W(c) (c is a new unbound symbol
(variable or constant))

Universal Generalization(U.G): W(c) then \forall{}xW(x) (c is an arbitrary symbol)
i.e if something is true for an arbitrary/generalized value it is true for all
values
*** Example
\exists{}x(p(x) \land q(x)) \rightarrow \exists{}xp(x) \land \exists{}xq(x)
1. \exists{}x(p(x) \land q(x)), P for CP
2. p(x) \land q(x), 1 E.I
3. p(x), 2 SIMP
4. q(x), 2 SIMP
5. \exists{}xp(x), 3 E.G
6. \exists{}xq(x), 4 E.G
7. \exists{}xp(x) \land \exists{}xq(x), 5,6 CONJ
QED 1,7 CP
*** Example
\exists{}x(w(x) \rightarrow c) \rightarrow (\forall{}x W(x) \rightarrow c)
1. \exists{}x(w(x) \rightarrow c), P for CP
2.  \forall{}xW(x), P for CP
#Note here, the E.I needs to come before the U.I. in order to
#use the same symbol for both
3.  W(y)->c, E.I. 1
4.  W(y), U.I. 2
5.  C, 3,4 MP
6. \forall{}xW(x) \rightarrow c, 1,6 CP
QED
*** weird note on meanings of different letters
const fun ind dim ?  pred ?     vars
abcde fgh ijk lmn o? pqr  stuv? wxyz
* "Truth"
In propositional logic truth is a tautology

In predicate logic "truth" is validity, a wff is valid if it is true in any
interpretation.

To improve the usefulness of predicate logic we add axioms, i.e \forall{}x = x,
\forall{}x\forall{}y x=y -> y=x

* Proof systems
A system of axioms built on top of predicate logic,

Consistent: you can only prove valid wff's
Complete: you can prove all valid wff's

Generally useful proof systems are consistent but not complete.

* Correctness of Programs

Add all axioms of algebra to default propositional logic.

given a precondition P, a program S and a postcondition Q

{P} S {Q} states that running S when P is true results in Q being true
if S is a correct program

e.g
{x=5} y=x; {y=5}

more formally:
if P is true and S executes and terminates then after S runs Q is true

{Q(x/t)} x:=t {Q} A.A(assignment axiom) (assignment is :=, not = )
ex: {x>0} x := 2 + x {x > 2}

P->R, {R} S {Q} then {P} S {Q}; consequence rule

** ex: prove the following program is correct:
{x<5} x:=x-2 {x < 4}
1. {(x-2)<4 } x:=x-2 {x < 4} A.A
2.    x<5, P for CP
3.    x-1<4, T alg
4.    (x-2)<4, T alg
5. x<5 -> (x-2)<4, 2-4 CP proof
(optional)6. {x<5} x:=x-2 {x<4} 1,5 conquence
QED

** S_1;S_2 \equiv {P} S_1 {R}, {R} S_2 {Q}, then {P} S_1;S_2 {Q} Composition


** ex: {(x>2) \land (y>3)} X:=x+1; y:= y + x {y > 6}
1. {y+x > 6} y:=y+x {y>6} A.A
2. {y+x+1 > 6} x:= x+1 {y+x >6}
3. {y+x+1 > 6} x:=x+1; y = y+x {y  > 6}; 1,2 comp
4.   x>2 \land y>3 P for CP
5.   x+y > 2+3
6.   y+x+1 > 6
6. (x>2) \land (y>3) -> y+x+1 > 6, 4-6 CP
QED 3,7 consequence

** {P\land{}C} S {Q}, (P \land \not{}C) -> Q
{P} if C then S end {Q}

** ex {T} if x<0 then x:=-x end {x>=0} # i.e absolute value
use two proofs, one for each branch
a)1. {-x>=0} x:=-x {x>=0} A.A
  2. T \land x<0, p for cp
  3. x<0 2.simp
  4. -x>0 3.T
  5. -x>=0 4.T
  6. T\land(x<0)-> -x >=0, x-5 cp
  7. (T\land(x<0)) x:=-x {x>=0} 1,6 consequenc
b)1. (T\land\not(x<0)
  2. \not(x<=0)
  3. x>=0
  4. T\land\not(x<0) -> x>=0
QED By a and b and if/then
** {P\land{}C} if C then S_1 else S_2 end {Q} if/then/else
P;
if (C) {
 S_1;
} else {
 S_2;
}
Q;
* Sets
-A set is a collection of objects
-The objects in the set are called elements
-we write x \in A to mean x is an element of A
-"      " x \notin A to mean x is not an element of A
-we say a set contains it's elements
-{} = \empty
- N = natural numbers
- Z = integers
- Q = rationals, ...

-set builder notation
{x | P(x)}, set of elements x such that P(x) is true
i.e P(x) = x > 0, positive integers (assuming the domain is integers)

-two sets A and B are equal iff they contain  (exactly) the same elements.
i.e \forall{}x (x \in A \rightarrow x \in B) \land (x \in B \rightarrow x \in A)

-sets can be arbitrarily nested.
e.g {{\empty},{{\empty}},{{{\empty}}},\empty} is a set containing 4 unique elements.
** Relations
Subsets;
if A and B are sets and \forall{}x(x \in A -> x \in B) (every element in A is also in B)
we say A is a subset of B, A \sube B
\forall{}A, \empty \sube A (or A \supe \empty)

proof:
if A \sube B  and B \sube C then A \sube C
formally: \forall{}x (x \in A \rightarrow x \in B) \land \forall{}x(x \in B \rightarrow x \in C) \rightarrow \forall{}x(x \in A \rightarrow x \in C)
1. \forall{}x(x \in A \rightarrow x \in B), P for CP
2. \forall{}x(x \in B \rightarrow x \in C), P for CP
3. x \in A \rightarrow x \in B, 1. UI
3. x \in B \rightarrow x \in C, 2. UI
5.   x \in A, P
6.   x \in B, 3,5 MP
7.   x \in C, 4,6 MP
8. x \in A \rightarrow x \in C, 5-7 CP
9. \forall{}x(x \in A \rightarrow x \in C), 8 UG.
QED 1,2,9 CP

As a mathmatical proof

if A \sube B and B \sub C then A \sube C
proof: suppose A \sube B and B \sube C
suppose x \in A, since A \sube B, x \in B
since B \sube C then x \in C
thus A \sube C QED

** Power sets
given a set S the power set of S
denoted P(S) is the set of all subsets of S

Given a set of n elements the power set of n will have 2^n elements

ex. suppose A \sube B prove P(A) \sube P(B)
proof: suppose A \sube B, and x \in P(A)
so x \sube A, since x \sube A and A \sube B then x \sube B
thus x \in P(B)
so P(A) \sube P(B) QED

If A and B are sets and
A \sube B and B \sube A then we say the sets are equal and A = B

** Set definations
A \cup B = {x | x \in A \land x \in B}
A \land B = {x | x \in A \lor x \in B}
A - B = {x | x \in A \land x \notin B}
given a domain U
A\prime = {x | x \in U \land x \notin A} = {x | x \notin A}
A\prime is basically the inverse of A, i.e every element in U but not in A
* More Sets
** More Set Properties
A \cup \empty = \empty
A \cap \empty = A
A \cap B = B \cap A
A \cup (B \cap C) = (A \cup B) \cap (A \cup C)
(A \cup B)\prime = A\prime \cap B\prime
(A \cap B)\prime = A\prime \cup B\prime
** Sequences of sets
U_{i=1}^n A_i = {x | \exists{}i x \in A_i}
\cap_{i=1}^n A_i = {x | \forall{}i x \in A_i}
this works for n = \infty
** Weird Sets
A = {A, 0, 1} is a technically correct set, but is kinda weird

if P(x) = x !\in x # x isn't in itself

if D = {S | P(S)}
is D \in D ? this is something of a paradox
This is a flaw in this set theory, which is naive set theory
Axiomatic set theory rectifies this
** Ordered Sets (Tuples)
for n > 0 an N-Tuple (a_1, a_2,...a_n) is a ordered set of n objects a_1,a_2,...a_n.
Two N-Tuples iff each corresponding value is equal. This implies that tuples
can have the set of elements but not be equal.

** Cartesian Product
if A and B are sets the Cartesian product of A and B (written A \times B) is
{(a,b) | a \in A \land b \in B} A \times B is not necessarily equal to B \times A.
If A has n elements and B has k elements A \times B has n*b elements
A \times \empty = \empty
** Functions / Relations
a relation from A to B is a subset of A \times B
a relation from A to A we call a relation on A (this is not the identity)
*** Examples
relation on N
{(x,y) | x \leq y} {(0,0),(0,1),(1,3),(4,8),...}

let A = {a,b,c,d} and R = {(a,c),(b,c),(a,a)}
Digraph (directed graph) (a set of vertexes/nodes/points connected by directed
edges)

** Properties of Relations
Suppose R is a relation on A, we say R is reflexive if for every a \in A we have
(a,a) \in R (i.e R is a superset of the identity on A)

we say R is symmetric if for every (a,b) \in R, we have (b,a) \in R

we say R is transitive if for every  (a,b) \in R and (b,c) \in R we also have
(a,c)\in R

we say R is anti-symmetric if for every (a,b) \in R and (b,a) \in R then a = b
or if a \neq b then (a,b) \notin R \lor (b,a) \notin R

anti-symmetric is not the opposite of symmetric, oddly enough

There is no relation between symmetry and anti-symmetry

Reflexive, symmetric, anti-symmetric and transitive are orthogonal

** operations on relations
various operations on sets (union, intersection...) work on relations.

** Compositions
given a relation R from A to B and a relation S from B to C
than S o R is a relation from A to C (composition)

(R o S)(A)  is the relation created by applying R to the result of S on A.

R o R is perfectly legal.

** Relations/sets -> Matrices
\delta{}A = {(a,a) | a \in A}
 (also called the diagonal (i.e it's the diagonal of a matrix))

If R is a relation define R_0 = \delta and R_n = R_{n-1} o R

When looking at the digraph for R_n it will have all paths of length N
that exist on the digraph of R

We can also represent relations via adjency matrices,
given the relation R on {a,b,c,d} = {(a,a),(a,b),(b,c),(c,d)} we get
the matrix
|   | a | b | c | d |
| a | 1 | 1 | 0 | 0 |
| b | 0 | 0 | 1 | 0 |
| c | 0 | 0 | 0 | 1 |
| d | 0 | 0 | 0 | 0 |

** closures of relations
If R is a relation then the reflexive closure of R written r(R) is the smallest
relation that contains R and is reflexive, this is akin to R U \delta.

the symmetric closure of R, written s(R) is the smallest relation that contains
R and is symmetric

Suppose T is a relation, define T^c = {(a,b) | (b,a) \in T} or T^t (transpose of T)

The simplest way to obtain the symetric closure of R, s(R) is R U R^c.

Finally the transitive closure of R, t(R), is the smallest relation that
contains R and is transitive
t(R) is represented as R U_i^\infty R_i, or from i=0...n if R is a finite set
with n elements

** Warshall algorithm to find the transitive closure
Input: The adjcenty Matrix M for a relation R on 0,1,2,3,...m
Output: The adjancey Matrix for t(R)
given an nxn matrix R, where the i,jth element is set if there is a relation
between i and j in R-
#+BEGIN_SRC c
  int* Warshall(int *R,int n){
    int i,j,k;
    for(k = 0;i<n;i++){
      for(i = 0;j<n;j++){
        for(j = 0;k<n;k++){
          /*
            if(R[i][k] & R[k][j]){
            R[i][j] = 1;
          ,*/
          if(*(R+(i*n)+k) & *(R+(k*n)+j)){
            ,*(R+(i*n)+j) = 1
              }
        }
      }
    }
  }
  /*
    if R is weighted
    if(R[i][j] > R[i][k] + R[k][j]){
    R[i][j] = R[i][k] + R[k][j];
    }
    if(*(R+(i*n)+j ) > (*(R+(i*n)+k)+*(R+(k*n)+j))){
    ,*(R+(i*n)+j) = (*(R+(i*n)+k)+*(R+(k*n)+j));
  */
#+END_SRC


* Missed a class
  Sequences I guess?
* Functions
  A function f is a relation between two sets A and B where each element
in A is mapped to one element of B.

f is injective (an injection) if for every a \in A if a \neq a\prime then f(a) \neq f(a\prime)
  i.e there is a one to one relation between elements of A and B

f is surjective (not subjective) (a surjection)
if for every a \in A there exists a b \in B such that f(a) = b
  this is also known as onto.

if f:A\rightarrow{}B is both surjective and injective that it is bijective (a bijection).

The identity relation \delta:A->A is a Bijection.

if R is a relation  from A to B than R^c in a relation from B to A.
This does not necessarily hold for functions, only for functions that are
bijective. I.e f^-1 exists iff f is bijective.

if f:A->B and g:B->C are functions then g o f:A->C is a function.
if f and g are bijective than g o f is a bijection

** related sets
two sets are related if there is a bijection between them.
the relation R between related sets is Reflexive, Symmetric, and Transitive

* Cardinality
if there is a bjiection f:A->B we say A and B have the same cardinally.


Two finite sets are related iff they have the same number of elements,
by definition they also have the same cardinality.

We say a set A is finite with cardinality K if there exists a bijection
f from A->S_k where S_k = { x | x=0...k }
** Countably infinite sets
We say a set A is infinite but countable if there exists a bijection f:N->A
i.e between the natural numbers and A. It has cardinality \aleph_o

For instance the set of integers Z has a bijection between the natural numbers
of f(N) = {M/2 if m is even; -(M+1)/2 if m is odd}. Thus the set of integers
is countably infinite.

ex. integers, rational numbers, etc...
** Un-countably infinite sets
s:N->{0,1}
S={s | s:N->{0,1}}; the sequence of all possible binary sequences.
Prove S in uncountable
1. Assume S is countable
2. Thus there exists a bijection b:N->S, so we can list S as the union
of (b_i(x) for i \in N) for x \in N.
3. define x-bar as f(x) = (x == 1 ? 0 : 1)
4. define d:N->{0,1} by d_i=b_i-bar
5. Since this is a binary sequence and b is subjective  have some k so that
   b(k)=d
6. but d_k = b(k)_k-bar so d \neq b(k)
7. S is not countable by contridiction.
Essentially if we wrote out the sequences b(x) = b_0(x) b_1(x) b_2(x) ...
in a grid and set d equal to the sequence formed by taking b_i(i) of each
row and inverting it. So d is by definition different than any b and thus
b can't be bijective.

If we let the elements of S be the expantions of real numbers in base 2
this shows real numbers are uncountable.

** Uncountability of infinite power sets
Suppose A is a countably infinite set, show P(A) is uncountable.
1. Since A is countable there exists a bijection x:N->A
2. Let S = {s | s:N->{0,1}}; binary sequences
3. Define b:s->P(A) by b(s) = {x_i | s_i = 1} (b finds a subset of A by creating
   a set containing elements where the corsponding digit in the given binary
   sequenece is set)
-show b is injective
4. Suppose s and s' are binary sequences and s \neq s'
   if we can show b(s) \neq b(s') then it shows P(A) is uncountable
5. since s \neq s' then there exists some k such that s_k \neq s'_k
6. suppose s_k = 1, thus s'_k = 0
7. so  x_k \in b(s) and x_k \notin b(s') so b(s) \neq b(s')
-show b is surjective
8. suppose X \sube A
9. define s:N->{0,1} by {s_i = 1 if x_k \in X; s_i = 0 if x_k \notin X}
10. Thus b is a bjection between P(A) and S
11. Suppose that P(A) is countable
12. Thus there exists a bijection f:N->P(A)
13. So b o f is a bijection from N to S, which is a contridiction
14. Thus P(A) is uncountable

** Pigeon hole principal
if A and B are finite sets and |A| > |B| (more elements in A than B)
then there is no injection from A to B.

i.e if then are n pigeons and k holes and n>k then there must be
at least 2 pigeons in one hole.


* (Mathematical) Strings
  Start with an alphabet \Sigma, a finite nonempty set of (simple) symbols.

  If \Sigma is an alphabet then a finite sequence over \Sigma is called a string

  If X is a string over \Sigma the length of X, written |X|, is the length as a
  sequence.

  Quick note, sequences are always indexed starting at 1.
  A sequence of length n, over \Sigma, is a function from S_n -> \Sigma

  The set of all strings over \Sigma is denoted \Sigma*

  If s \in \Sigma* and |s|=0 then we write s = \Lambda (the empty string)

  Suppose u \in \Sigma* and v \in \Sigma* then concatenation of u and v, written uv
  in the string consisting of the symbols of u followed by the symbols of v.
  uv_i = {u_i if 1 \leq i \leq |u|; v_{i-|u|} if |u|\lt i \leq |u|+|v|

  concatenation identities:
  (uv)w = u(vw)
  \Lambda{}w = w\Lambda = w

  Suppose z \in \Sigma* and z = uvm for u,v,w \in \Sigma* we call v a substring of z,
  any of u,v,w can be \Lambda.

  if z = uw for u,w \in \Sigma* then we call u a prefix, and w a suffix.
** Mathematical Induction (M.I)
   In the domain N (natural numbers)
   to show \forall{}xP(x) it suffices to show that P(0) holds and that
   \forall{}x(P(x)`>P(x+1))

   More generally.
   show P(c) and \forall{}x((x\ge{}c)\rightarrow(P(x)\rightarrow{}P(x+1)) then \forall{}x((x\ge{}c)\rightarrow{}P(x))
** ex
   if k\ge1 then \Sigma_{j=1}^k 2j-1 = k^2
   proof by mathematical induction:
   1. wee will induct over k\ge1
   2. Basis: when k = 1 we have \Sigma_1^1 = 2-1 = 1 = 1^2 = k^2
   3. IH(inductive hypothesis): Assume k\ge2 and arbitary but fixed and suppose
      \sum_{j=1}^{k-1} 2j-1 = k^2
   4. Show \sum_{j=1}^k = k^2, \sum_{j=1}^k =  2k-1 + \sum_{j=1}^{k-1} = 2k-1 + (k-1)^2 = 1 + 2(k-1) + (k-1)^2
      (k+1)(k-1) + 1 = k^2 +k -k -1 + 1 = k^2
** Inductive proof about strings
   suppose w \in \Sigma* define w^k for all k\ge0 by:
   w^0 = \Lambda
   w^k = w^{k-1}w for all k\ge0

   Suppose w \in \Sigma*, show for any k,j\ge0 w^{k}w^{j} = w^{k+j}
   Proof omitted because I was busy fiddling with org sub/superscripts during
   class
** Reversal
   suppose w \in \Sigma*, define w\prime, the reversal of w by:
   w\prime = w iff w = \Lambda
   if |w|>0 Then let w = ua where u \in \Sigma* and a \in \Sigma and then w\prime = au\prime
*** ex
   Prove (uv)\prime = v\prime{}u\prime
   Proof:
   1. variable: will induct on k=|v|\ge0
   2. basis: when k=0 then |v|=0 and (uv)\prime = (u\Lambda)\prime= (u)\prime = \Lambda{}u\prime =
      \Lambda\prime{}u\prime = v\prime{}u\prime
   3. I.H: suppose k\ge0 is arbitrary but fixed and for any string v
      where |v|=k-1 we know (uv)\prime = v\prime{}u\prime
   4. If w is any string and |w| = k, show (uw)\prime=w\prime{}u\prime
   5. Suppose |w|=k then we can write w as va where |v| = k-1 and a \in \Sigma
      We know (uv)\prime = v\prime{}u\prime and w\prime = av\prime, so we can write (uw)\prime as
      (uva)\prime = a(uv)\prime = av\prime{}u\prime = w\prime{}u\prime QED

* Ordering of strings
** dictionary order
  Suppose \Sigma is an alphabet with order <_\Sigma we define the simple
  dictionary order on  \Sigma* as follows:
  for any u,v \in \Sigma* u<v if u\ne{}v and
  1. u is a prefix of v, i.e |u|<|v| and u_i = v_i for i=0...|u|
  2. u = zw and v = zx, x \ne \Lambda where u_i = v_i for i= 0 ... |z| and w_1 < x_1
** Enumeration order
   define the order < on \Sigma* by: for u,v \in \Sigma* u<v if
   1. |u|<|v|
   2. |u| == |v| and u <_D v
* Language
  Suppose \Sigma is a language, \Sigma* is the set of all strings over \Sigma
  A Language L is a set of strings.
  L \sube \Sigma*
  P(\Sigma*) the power set of \Sigma* is the set of all languages

  The C programming language in an infinite set of strings
  A C compiler is a finite string which can determine if another
    finite string is part of the C programming language
  Thus the finite C compiler can represent the infinite C language

  Since \Sigma* has is countably infinite but P(\Sigma*) is uncountably infinite
  the majority of languages can not be represented by finite strings
** Operations on languages
   Given two languages L and M
   L \cup M, L \cap M, L-M, L^c  all have the standard meanings
   L^c = \Sigma* - L, just for reference

   Define the reversal of L, L^R as L^R = {w | w\prime \in L}
   
   Define L o M, or just LM as L o M = {uv | u \in L and v \in M}

   So A\empty = \empty (\empty is {} NOT {\Lambda}) (akin to A * 0)
   And A{\Lambda} = A  (akin to A * 1)

   Association (AB)C = A(BC) = ABC
   
   If A \sube B and C \sube D then AC \sube BC
   A(B \cup C) = AB \cup AC

   Define L^n where n\ge0 as:
   L^0 = {\Lambda} for n = 0
   L^n = L^{n-1}L for n>0

   This implies that \empty^0 = {\Lambda} and \empty^n, n > 0 = \empty
** Powers of languages
   Prove A^m o A^n = A^{m+n} where A is a Language and n,m \in N
   1. induct on n\ge0
   2. when n=0 we have A^n o A^0 = A^n o {\Lambda} = A^n = A^{n+0}
   3. let n be arbitrary but fixed and assume A^{n-1} o A^m = A^{n-1+m}
   4. so A^n o A^m = AA^{n-1} o A^m = AA^{n-1+m} = A^{n+m}
      QED
** kleene star
  A is a language
  kleene star closure of A, denoted A^* = {w | w \in A^n for n \ge 0}
  or A^* = U_0^{\infty} L^n
  
  Properties (currently unproven) for languages A and B
  Let A^{**} = (A^*)^*
  1. A \sube AB^* (trivial proof, \Lambda \in B, A{\Lambda} = A, QED)
  2. A \sube B \rightarrow A^* \sube B^*
  3. A^*A^* = A^*
  4. A^* \sube A^{**}  (true from defn of kleene star)
  5. (A^*)^n \sube A^*
  6. A^{**} \sube A^* ((A^*)^n \sube A^{**} by defn and (A^*)^n \sube A^* so A^{**} \sube A^*)
  7. {\Lambda} \cup AA^* = A^*
     
  To prove things about kleene stars change A^* to A^k for arbitrary k \ge 0
  2.
  Show A \sube B \rightarrow A^* \sube B^*
  Suppose w \in A^*
  We have k\ge0 with w \in A^k
  Since A \sube B we know A^k \sube B^k
  so w \in B^k and thus w \in B^* QED
  
  3.
  Show A^*A^* = A^*
    show A^*A^* \sube A^*
    suppose w \in A^*A^*, so w = vu where v \in A^* and u \in A^*
    so we have k,l such that v \in A^k and u \in A^l
    so w = vu \in (A^k A^l = A^{k+l} so w \in A^* QED
  now
    show A^* \sube A^*A^*
    from property #1 this is true.
  so A^*A^* \sube A^* and A^* \sube A^*A^*, thus A^*A^* = A^* QED
    
  5.
  show (A^*)^n \sube A^*
  induct on n
  (A^*)^0 = {\Lambda} \sube A^*
  Assume (A^*)^{n-1} \sube A^*
  Show (A^*)^n \sube A^*; (A^*)^n = (A^*)^{n+1-1} = A^*A^{n-1}
  Since A^{n-1} \sube A^* then A^n = A^*A^{n-1} \sube A^*A^* = A^* so A^n \sube A^*

  7.
  Show {\Lambda} \cup AA^* = A^*
  A^0 = {\Lambda}
  AA^* = AU_0^{\infty} A^n = U_1^{\infty} A^n
  {\Lambda} \cup AA* = A_0 \cup U_i^{\infty} A^n = U_0^{\infty} A^n = A*
* Regex
  Suppose \Sigma is an alphabet
  the superscript ^'s should really go over the symbols, they have ^'s to
  differentate them from normal symbols
  
  Define \Sigma_{reg} = \Sigma \cup {\empty^{^}, \Lambda^{^}, (,),\cdot,*^{^},+^{^}}
  where \Sigma \cap {\empty^{^}, \Lambda^{^},(,),\cdot,*^{^},+^{^}}^{} = \empty
  
  A regular expression will be a string over \Sigma_reg
** Syntax
  For an alphabet \Sigma the set of regular expressions over \Sigma is
  the set of strings over \Sigma_reg defined inductively by:
  1.(Basis) \empty^{^},\Lambda^{^}^{}, and every a \in \Sigma
  2.(Induction) if R and S are R.E then so are:
     1.(R+S), 2.(R\cdot{}S), and 3. R*
  That's it apparently

** Semantics
   Suppose \Sigma is an alphabet, for a regex R over \Sigma The Language
   of R, denoted L(R) is defined inductively by:
   1.(Basis) L(\empty^{^}) = \empty, L(\Lambda^{^}) = {\Lambda}, L(a) = {a} \forall{}a a \in{} \Sigma
   2.(Induction) Suppose R and S are regexs and we know L(R) and L(S) then:
     L((R\cdot{}S)) = L(R)L(S), L(R^*) = (L(R))^*, L((R+S)) = L(R)\cup{}L(S)

   So + is akin to brackets, i.e A+B+C = [A,B,C]
      \cdot is not actually anything
** Examples (re->language)
   for \Sigma={0,1}
   L(((0+1)\cdot1)) = L((0+1))L(1) = (L(0)\cup{}L(1))L(1) = {0,1}{1} = {01,11}
   or [01]1
   
   L((O+1)*1) = [01]*1 
   
   L((0\cdot1)*1) = (01)*1
** Examples (language->re)
   all strings that contain 001
   .*001.* -> (0+1)*001(0+1)*
   
   all strings that don't contain 001
   (0?1)*0* -> (01+1)*0*
** Regular Languages
   A language L is called Regular if L = L(R) for some R.E. R.
   For some string s in a regular language where r = xyz, and |y|\ne0
   xy*z is also a string in the language (pumping lemma)
** Simplifications
   r(s+t) = rs+rt
   r+s = s+r
   \Lambda + rr* = r*
   rr* = r*r
* Finite State Machine
  A machine with a finite number of states.
  represented by a function that given a state and an input returns a state
  i.e. f:(state x inputs) -> states
  f is the transition function

  Can also be reprenented by a directed graph (a transition diagram)

  Has initial and final states

** Determininstic finite automation
   Represented by a 5-Tuple (Q, \Sigma, \delta, q_{0}, F)
   1. Q is a finite set (the set of states)
   2. \Sigma is an alphabet
   3. \delta:Qx\Sigma -> Q (transition function);
   4. q_o \in Q (start state)
   5. F \in Q (set of final states)

   ex. M = {Q,\Sigma,\delta,q_{o},F}
   Q={q_0,q_1,q_2}
   \Sigma={a,b}
   \delta = (q_n , b)->q_1, (q_1, a)->q_2,(q_0, a)->q_0, (q_2,a)->q_1
   q_o = q_o
   F={q_1}
   (I can't really draw the diagram)
Finite state algorithm:
input w \in \Sigma*
current-state = q_0;
foreach i from 0 upto |W| //loop over the string
current-state = \delta(current-state,w_i)
end
if current-state \in F
return "ACCEPT"
else
return "REJECT"
end
end

Suppose M=(Q,\Sigma,\delta,q_0,F) is a DFA
and w \in \Sigma* where |w|= n
we say M accpets w if there exists a sequence of states r_0,r_1,r_2,...r_n so that:
r_0 = q_0
r_n \in F
\delta(r_i,w_{i})=r_{i+1} for 0\le{}i<n

suppose M is a DFA the language of M, denoted L(M) is the set 
{w | M accepts w}. 

if A is a language we say M accepts A if A=L(M); (this is a different
defination of accepts then for strings)
** A DFA language is a language accepted by some DFA
** Nondeterministic finite state automaton 
*** Formal defination
      a 5-tuple (Q,\Sigma,\delta,q_o,F)
      same as a DFA except \delta is \delta:Qx(\Sigma{}U{\Lambda}) -> P(Q) (power set of Q)
*** NFA stuff
      
   No longer require an arrow for every symbol   

   states can emit multiple arrows for a single symbol

   allow states to emit arrows labled with \Lambda, this \Lambda is the same
   as the \sigma in shift/reduce parsers

   Don't reject a correct input

   Basically just a theorical device, but useful since any NFA can
   be transformed into a DFA

   Suppose N is an NFA and w \in \Sigma* we say that N accepts w. if we can
   write w=y_{0}y_{1}y_{2}y_{3}...y_{m} where each y_{i}\in{}\Sigma{}U{\Lambda} and a sequence of
   states r_{0}r_{1}...r_{m}_{} so that:
   1. r_0 = q_0
   2. r_m \in F
   3. r_{i+1} \in \delta(r_i,y_{i}) for i = 0,1,2,...m-1
      
** NFA vs DFA
   All DFAs are by defination NFAs, but obviously the reverse isn't true.
   However any NFA can be converted into a DFA
   by defination DFA-languages \sube NFA-languages.
   we need to show NFA-languages \sube DFA-languages
** NFA-to-DFA
   Subset Construction.
   Given an NFA M create a DFA N (ignoring lambdas):
   
   The states of M are composed of subsets of the states in N,
   formally N.Q = P(M.Q), the states of N are composed of the elments
   in the power set of the states of M.
   
   The end states of M are the set of states in M which contain
   an end state of N

   \delta\prime(S,a) = U_{x=S}\delta(x,a) 
   essentially for a state s in M where s = {q_i,q_{i+1},...,q_{j-1},q_j}
   the transition function goes to the state consisting of the union
   of all states in N reachable from some q_k \in s by the input a.
   ex. if s = q_0 and \Sigma = {0,1} and \delta(q_0,0) = q_0, q_1 (i.e a 0 in state 0 can go
   to state 1 or state 0) then \delta\prime(q_0,0) =  {q_0,q_1}.
   if \delta(q_1,0) = q_0 then \delta\prime({q_0,q_1},0) = {q_0,q_1} and \delta\prime(q_1,0) = q_0.
   Its complicated, and as an algorithm runs in exponential time.

   Now with lambdas

   Suppose N = (Q,\Sigma,\delta,q_{0},F) is an NFA and there is a sequence of states
   in Q, q_0,q_1,...,q_k with each q_{i+1} \in(q_i,\Lambda), we say q_k is \Lambda
   reachable from q_0. If S \sube Q then the \Lambda-closure of S denoted \Lambda(S)
   is the set of states \Lambda-reachable from any state in S.

   Using lambdas the transition function of M is:
   \delta\prime(S,a) = U_{x\in{}S}\Lambda(\delta(x,a))
   
   The start state of M is then \Lambda(N.q_0) (i.e the set of all states reachable
   from the start state of N by \Lambda).

   In computing the DFA we never consider \Lambda as an input (since it can't be).
   If the set of states reachable from some state q in M = {q_i,...,q_j}
   to take lambdas into account we re-compute q by doing: 
   q' = q; do{q=q'; foreach(s \in q){q' = q' U \delta(s,\Lambda)}}while(q!=q');
   which is really inefficent, but so is the whole algorithm

   Since we can create a DFA from any NFA we can just call the
   set of NFA/DFA languages FA Languages.
** Closed operations on FA languages
   Given two NFA's N_1 and N_2 which accept languages L and M.
*** union
    Is L U M a FA language:
    Create a new NFA N\prime with a start state q_0 that has two lambda transitions
    one to N_1 and one to N_2. Then N\prime is a NFA which accepts L U M
*** concatenation
    Is L o M a FA language:
    Add lambda transitions to the end states of N_1 which go to the
    start states of N_2, this new NFA accepts L o M
*** Kleene star
    Concatenable the NFA for {\Lambda} to N, then connect the end states of 
    N to the start states of N with lambdas.
* Equality of regular expressions and NFAs
  Proof using incudtion on regular language L
  Basis: if L=L(\Lambda), L=L(\empty), L=L(a) a\in\Sigma. L is regular and can be represented
  by an NFA
  
  Induction: Assume L_1 and L_2 are regular languages and can be represented
  by NFAs
  
  Goal: Show L_1 U L_2, L_{1}L_{2}, and L_1* have NFAs.
  Since FA languages are closed under union, concatenation and *
  L_1 U L_2, L_{1}L_{2} and L_1* are FA languages and so have NFAs
** NFA -> regex
   Define special form for NFAs (generalized NFAs or GNFAs):
   -Only one final state (if there are multiple final states create a new
     state and create \Lambda transitions from the final states to the new
     state, now let the new state be the final state)
   -The initial state is not the final state
   -The transition lables are regular expressions.
   -Strings (matching the regular expression transitions) are consumed
     during translation
   -There is exactly one arrow out of every state to every other state
   (including itself) except there are no arrows out of the final state
   -There is exactly one arrow into every state from every other state
   except for the start state
*** Algorithm (NFA to GNFA)
    Given an NFA M create a GNFA G:
    1. Create a new end state and connect all old start states to it via
       lambda transitons
    2. Create a new start state and connect it to the old start state
       via a lambda transation
    3. combine multiple arrows from one state to a second state by
       creating a regex composed an alternation of all symbols on the
       orignial arrows
    4. Create transitions labeled with the empty state in order to
       satisify the conditon that each state must have arrows to every
       state and from every state.
*** Algorithm (GNFA to regular expression) 
    RIP Algorithm:
    explaination of the line  \delta\prime(q_i,q_j) = \delta(q_i,q_r)\delta(q_r,q_r)*\delta(q_r,q_j)|\delta(q_i,q_j)
    Given three states, i,r,j where r is not a start state or end state,
    i is not an end state and j is not a start state, with transitons
    i->j, i->r, r->r and r->j. Remove j by changing the transiton i->k 
    into the regular expression (i->j | i->r r->r* r->j), so \delta(i,j) changes
    from i->j into (i->j | i->r r->r* r->j)
    Main algorithm:
    Given a GNFA G = (Q, \Sigma, \delta, q_0, q_f)
    while(\exists{}q_r \in (Q - {q_0, q_f}))
      Q\prime = Q - {q_r}
      for each q_i \in Q\prime-{q_f}, q_j \in Q\prime-{q_0}
        \delta\prime(q_i,q_j) = \delta(q_i,q_r)\delta(q_r,q_r)*\delta(q_r,q_j)|\delta(q_i,q_j)
      end
    end
    return (Q\prime,\Sigma, \delta\prime, q_0, q_f)
    end

    Convert alg(gnfa to regex)
    convert(G)
    if(size(G.Q) == 2)
      return \delta(d_0,d_f)
    else
      return convert(rip(G))

    
    
* Missed a class (not sure on what)
** Pumping lemma 
   For a regular language you can take any string uvw where
   u,v,w \in \Sigma* and duplicate v any number of times and 

* Context free languages
  I know this stuff quite well
** Lexical analysis
   Get tokens using regular expressions
** Context Free Gramar(CFG)
   Push down automaton 
*** Example (Palindromes)
    Define L_{pal} = {w | w = w'}, and let \Sigma = {a,b}
    The following strings are in L_{pal}:
    1. \Lambda,a,b
    2. if w \in L_{pal} then so are awa and bwb
    
    For L_{pal} the grammar contains:
    Terminals: a,b
    Nonterminal: P
    P = a | b | \Lambda | aPa | bPb

** CFG Formal definition
   A CFS is a 4-tuple:
   G = (V,\Sigma,R,S)
   V is a finite non-empty set of the non-terminals of the grammar
   \Sigma is a finite non-empty set disjoint from V, i.e non-terminals
   R \sube V x (\Sigma U V)*, the production rules
   S is the start variable

   A string in \Sigma* is called a sentence 
   A string in (\Sigma U V)* is called a form
*** More definitions
    For the following assume G = (V,\Sigma,R,S) is a CFG

    1. If u,v,w are forms and A->W is a production then we say
       uAv yields uwv and write uAv \Rightarrow uwv

    2. If u and v are forms then we can say u derives v (u \Rightarrow v)
       if there exists a sequence of forms u_1,u_2,...u_n so that
       u \Rightarrow{} u_{1 }\Rightarrow{} u_{2 }\Rightarrow{} u_{n }\Rightarrow{} v    
    3. If v is a sentence and S \Rightarrow V then we call
       S \Rightarrow u_1 \Rightarrow u_2 \Rightarrow ... \Rightarrow u_n \Rightarrow v a derivation of V and we say
       G generates v
    4. The language of G, denoted L(G) is {w | w \in \Sigma* and S \Rightarrow w}
    5. A Language L is called context free if L = L(G) for some
       CFG G
*** Some examples

    a. S = OS1 | \Lambda, the set of strings 0{n}1{n}
    b. S = < S > | SS | \Lambda, the set of nested <>
    c. S = U1U1U1U, the set of all binary strings with at least 3 1s
       U = 0 | 1 | \Lambda
    d. {w | |w| = 2n+1, n \in N, w[n] = 1 (assuming w is indexed from 0)}
       S = 1S0 | 0S1 | 0S0 | 1S1 | 1
     



     
* Closure of CFL's
  Proofs ommited
** Union
   I don't totally get what we did here
   L = {O^{m}1^{n} | m \neq n} = L_1 U L_2
   S = S_1 | S_2
   S_1 = 0 | 0S_1 | 0S_{1}1
   S_2 = 1 | S_2 | 0S_{2}1
   
** Concatenation
   (O+1)*.(O+1)* == [01]*\.[01]
   S = S_{1}S_{2}S_{1}
   S_1 = 0S_1 | 1S_3 | \Lambda
   S_2 = '.'
** Kleene star
   L = a{m}b{m}
   S_1 = aS_{1}b | \Lambda
   S = SS_1 | S_1 | \Lambda
* NFA/DFA to CFG
  given an NFA (Q,\Sigma,\delta,q_0,F) construct a CFG to generate the language
  Make a variable V_i for each state q_i \in Q
  add a production V_i -> aV_j for every transition \delta(q_i,a) = q_j
  if q_f is a finite state add the production V_f -> \Lambda

  This implies all regular languages are context free languages as well as
  the converse, that any CFG where all rules have the form 
  V = aV_n | bV_m ... describes a regular language
* leftmost/rightmost derivations
  let L be the language of properly nested parenthesis
  S = (S) | SS | \Lambda
  The parse tree for '('')' is (S (S '(' (S \Lambda) ')) (S \Lambda))
  
  There are multiple ways of deriving this tree

  A yield uAv -> uwv is called a leftmost derivation if u \in \Sigma*
  Put more simply in a leftmost derivation the next nonterminal to resolve is
  always the leftmost nonterminal
  The oppisite is known as rightmost derivation 

** Ambiguous grammar
  We call grammars with more than one possible parse tree (via leftmost
  derivation) ambigous grammars
  
  for example:
  given the language defined by E = a | b | E - E
  a - b - a can parse to (E (E a) - (E (E b) - (E a))) 
  or (E (E (E a) - (E b)) - (E a))

  we can modify this to make an unambigous grammar,
  E = E - T | T
  T = a | b
  then the only parse tree is (a - b) - a (simplified parse tree)
  We call T a term

  Now we'll add to E to make a slightly more complex grammar
  initial ambigous form E = E + E | E * E | [a-z]
  Now add terms
  E = E + T | E * T | T
  T = [a-z]
  Now add precidence (via factors)
  E = E + T  | T
  T = T * F | F
  F = [a-z]
  repeat this process for more levels of precidence

* Chomsky normal form (CNF)
  Every production rule has the form:
  A -> BC or
  A -> t
  no \epsilon transitions, the start variable is only on the left hand side

** Conversion to CNF
   any context free language can be converted into CNF form, by eliminating
   the following
   1. Start variable (i.e start symbol on RHS)
   2. \epsilon productions
   3. unit productions (A -> B)
   4. non-binary productions (A -> BCD...N)

   Method:
   1. Add new start symbol (to elimnate start variable)
   2. Elimnate all \epsilon transitions, for each rule A -> \epsilon
      replace all instances of A on the RHS of rules with all possible
      combinations of A/\epsilon aka AbA  becomes Ab | AbA | bA | b
   3. Eliminate unit productions:
        1. For any production A -> ... | A | ..., remove A 
           (i.e remove any productions from something to itself)
        2. For any production A -> ... | B | ..., replace B
           with the left hand side of the rule for B.
   4. Elimnate non binary productions:
        1. Translate A -> BCD to A->BE, E->CD, or simlar for
           A -> BCDE...
        2. Translate A -> aB into V_a -> a, A->V_{a}B

** CNF parse trees
   CNF lets us build a parse tree which is a full binary tree (i.e every node
   has two children or no children), with the exception of the terminal
   production rules.

   if G is in CNF, w \in ?(G) and |w| = m, m > 1
   there are 2m-1 steps in the derivation of w
   we need m unit productions, and m-1 binary productions to get
   the length m, thus 2m-1

   For a full binary tree with depth d the number of leaves n obeys the
   following, d+1 \leq n \leq 2^d

   For the CNF parse tree, t with depth d, we have an extra layer of terminals, so the
   length of a sentence p, that is accepted by t is:
   d \leq |p| \leq 2^{d-1} (i.e the number of leaves in a full binary tree with depth
   d-1), and ceil(lg(p))+1 \leq d 
* Pumping context free languages
  Suppose A is a context free grammar, with G = (V,\Sigma,R,S) in CNF and
  suppose |V| = m.
  Suppose w \in A |w| = p = 2^m
  Thus the height of the parse tree must be at least m+1
  Then let d be a derivation with length at least m+1

  There must a repitition in d, i.e the production rule R appears twice in
  d. let w,v,x,y,z be the sentence split by the leaves of the two R's (this is
     really hard to represent without drawing)
          S
         / \
        / R \
       / / \ \
      / / R \ \
     / / / \ \ \
    /u/v/ x \y\z\
   
   Formally, if L is context free there exists a p>0 such that if s \in L
   with |s|\ge p s can be decomposed into s=u,v,x,y,z where
   1. or every i\ge0 wv^{i}xy^{i}z \in L
   2. |xy| >0
   3. |Vxy| \leq p

ex. let L = {a^{m}b^{m}c^{m} | m \ge 0}, show L is not context free
  1. let p\ge0 be the pumping length
  2. concider w = a^{p}b^{p}c^{p}
  3. |w| \ge p
  4. w \in L
  5. let uvxyz = w be a pumping decomposition of w, so |xy|>0 and |vxy| \le p
  6. Claim s = uv^{2}xy^{2}z is not in L
     case 1. both v and y contain the same type of symbol, than
     s will have too many of that symbol
     case 2. v and y contain more than one type of symbol then
     s will have the wrong order

Note: a language being pumpable does not mean it is context free, i.e
context-free -> pumpable, but not(pumpable -> context-free), the same
applies to the pumping lemma for regular languages
* Push down automaton
  A 6 tuple (Q,\Sigma,\Gamma,\delta,q_0,F)
  Q is a finite set of states
  \Sigma is an alphabet (the alphabet of the input), \Sigma_{\Lambda} = \Sigma U {\Lambda}
  \Gamma is another alphabet (the things we can put on the stack)
    Formally \Gamma = \Sigma U T, where T is a possibly empty alphabet 
    of auxiliary symbols needed for computation
  \delta(Qx\Sigma_{\Lambda}x\Gamma_{\Lambda}) ->P(Qx\Gamma_{\Lambda})
  \delta(state, input, top-of-stack) -> Power-Set(state, top-of-stack)
  q_0 \in Q Start state
  F \sube Q End states

  A push down automaton has a tuple for state which is (state, stack)
  Examples of \delta
  \delta(q,a,\Lambda) -> (p,x) (push(x))(aka. \Lambda->x)
  \delta(q,a,x) -> (p,\Lambda) (pop(x))(aka. x->\Lambda)
  \delta(q,a,\Lambda) -> (p,\Lambda) (noop)(aka. \Lambda->\Lambda)
  \delta(q,a,x) -> (p,y) (replace(x,y))(aka. x->y)
     
  

* Local Variables
# Local Variables:
# eval: (auto-fill-mode)
# eval: (flyspell-mode)
# eval: (org-cdlatex-mode)
# org-pretty-entities: t
# org-enable-table-editor: nil
# End:
